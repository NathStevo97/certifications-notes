{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"DevOps Learning and Research","text":"<p>Welcome to my personal DevOps learning documentation! This site serves as a knowledge repository for various DevOps certifications, tools, and best practices I've encountered throughout my career journey.</p> <ul> <li>About</li> <li>Certifications</li> <li>Tooling</li> <li>Tags</li> </ul>"},{"location":"about.html","title":"About","text":""},{"location":"certifications.html","title":"Certifications","text":""},{"location":"certifications.html#aws","title":"AWS","text":"<ul> <li>AWS Cloud Practitioner</li> </ul>"},{"location":"certifications.html#hashicorp-terraform","title":"HashiCorp Terraform","text":"<ul> <li>HashiCorp Certified: Terraform Associate</li> </ul>"},{"location":"certifications.html#hashicorp-vault","title":"HashiCorp Vault","text":"<ul> <li>HashiCorp Certified: Vault Associate</li> </ul>"},{"location":"certifications.html#kubernetes","title":"Kubernetes","text":"<ul> <li>Certified Kubernetes Application Developer</li> <li>Certified Kubernetes Administrator</li> <li>Certified Kubernetes Security Specialist</li> </ul>"},{"location":"certifications.html#open-source-best-practice","title":"Open Source Best Practice","text":"<ul> <li>Green Software for Practitioners</li> </ul>"},{"location":"certifications.html#vmware","title":"VMware","text":"<ul> <li>VMware Application Modernization Professional</li> </ul>"},{"location":"tags.html","title":"Tags","text":"<p>Following is a list of relevant tags:</p>"},{"location":"tags.html#tag:aws","title":"AWS","text":"<ul> <li>            AWS Certified Cloud Practitioner          </li> </ul>"},{"location":"tags.html#tag:ansible","title":"Ansible","text":"<ul> <li>            Ansible for Beginners          </li> <li>            Automating EC2 Deployments          </li> </ul>"},{"location":"tags.html#tag:argocd","title":"ArgoCD","text":"<ul> <li>            ArgoCD          </li> </ul>"},{"location":"tags.html#tag:certifications","title":"Certifications","text":"<ul> <li>            AWS Certified Cloud Practitioner          </li> <li>            Certified Kubernetes Administrator (CKA)          </li> <li>            Certified Kubernetes Application Developer (CKAD)          </li> <li>            Certified Kubernetes Security Specialist (CKS)          </li> <li>            Green Software for Practitioners          </li> <li>            HashiCorp Certified - Terraform Associate          </li> <li>            HashiCorp Certified - Vault Associate          </li> <li>            VMware Application Modernization Professional          </li> </ul>"},{"location":"tags.html#tag:grafana-loki","title":"Grafana-Loki","text":"<ul> <li>            Grafana Loki          </li> </ul>"},{"location":"tags.html#tag:hashicorp-vault","title":"HashiCorp Vault","text":"<ul> <li>            HashiCorp Certified - Vault Associate          </li> </ul>"},{"location":"tags.html#tag:helm","title":"Helm","text":"<ul> <li>            Helm          </li> </ul>"},{"location":"tags.html#tag:kubernetes","title":"Kubernetes","text":"<ul> <li>            Certified Kubernetes Administrator (CKA)          </li> <li>            Certified Kubernetes Application Developer (CKAD)          </li> <li>            Certified Kubernetes Security Specialist (CKS)          </li> </ul>"},{"location":"tags.html#tag:packer","title":"Packer","text":"<ul> <li>            Packer          </li> </ul>"},{"location":"tags.html#tag:terraform","title":"Terraform","text":"<ul> <li>            HashiCorp Certified - Terraform Associate          </li> </ul>"},{"location":"tags.html#tag:tooling","title":"Tooling","text":"<ul> <li>            Ansible for Beginners          </li> <li>            ArgoCD          </li> <li>            Automating EC2 Deployments          </li> <li>            Grafana Loki          </li> <li>            Helm          </li> <li>            Packer          </li> </ul>"},{"location":"tags.html#tag:vmware","title":"VMware","text":"<ul> <li>            VMware Application Modernization Professional          </li> </ul>"},{"location":"tooling.html","title":"Tooling","text":""},{"location":"tooling.html#ansible","title":"Ansible","text":"<ul> <li>Kodekloud: Ansible for Beginners</li> <li>Kodekloud: Ansible Advanced</li> <li>Pluralsight: Automating EC2 Deployment</li> </ul>"},{"location":"tooling.html#argocd","title":"ArgoCD","text":"<ul> <li>Pluralsight: Getting Started with ArgoCD</li> </ul>"},{"location":"tooling.html#grafana-loki","title":"Grafana Loki","text":"<ul> <li>DevOps Toolkit: Using Centralized Logging</li> </ul>"},{"location":"tooling.html#helm","title":"Helm","text":"<ul> <li>Pluralsight: Kubernetes Package Administration with Helm</li> </ul>"},{"location":"tooling.html#hashicorp-packer","title":"HashiCorp Packer","text":"<ul> <li>Pluralsight: Getting Started with HashiCorp Packer</li> </ul>"},{"location":"certifications/CKA.html","title":"Certified Kubernetes Administrator (CKA)","text":"<p>Repo to host notes, documentation, exercise solutions associated with the Certified Kubernetes Security Specialist (CKA) certification.</p>","tags":["Certifications","Kubernetes"]},{"location":"certifications/CKA.html#course-notes","title":"Course Notes","text":"<ul> <li>1.0 - Introduction</li> <li>2.0 - Core Concepts</li> <li>3.0 - Scheduling</li> <li>4.0 - Logging and Monitoring</li> <li>5.0 - Application Lifecycle Management</li> <li>6.0 - Cluster Maintenance</li> <li>7.0 - Security</li> <li>8.0 - Storage</li> <li>9.0 - Networking</li> <li>10.0 - Design a Kubernetes Cluster</li> <li>11.0 - Install Kubernetes the kubeadm Way</li> <li>13.0 - Troubleshooting</li> <li>14.0 - JSONPath Introduction</li> </ul>","tags":["Certifications","Kubernetes"]},{"location":"certifications/CKA.html#mocks","title":"Mocks","text":"<ul> <li>Mock Exam 2</li> <li>Mock Exam 3</li> </ul>","tags":["Certifications","Kubernetes"]},{"location":"certifications/CKAD.html","title":"Certified Kubernetes Application Developer (CKAD)","text":"<p>Repo to host notes, documentation, exercise solutions associated with the Certified Kubernetes Security Specialist (CKS) certification.</p>","tags":["Certifications","Kubernetes"]},{"location":"certifications/CKAD.html#course-notes","title":"Course Notes","text":"<ul> <li>1.0 - Introduction</li> <li>2.0 - Core Concepts</li> <li>3.0 - Configuration</li> <li>4.0 - Multi-Container Pods</li> <li>5.0 - Observability</li> <li>6.0 - Pod Design</li> <li>7.0 - Services and Networking</li> <li>8.0 - Persistent Volumes</li> <li>9.0 - 2021 Updates</li> </ul>","tags":["Certifications","Kubernetes"]},{"location":"certifications/CKAD.html#mock-exams","title":"Mock Exams","text":"<ul> <li>Mock Exam 1</li> <li>Mock Exam 2</li> <li>Mock Exam 3</li> </ul>","tags":["Certifications","Kubernetes"]},{"location":"certifications/CKS.html","title":"Certified Kubernetes Security Specialist (CKS)","text":"<p>Repo to host notes, documentation, exercise solutions associated with the Certified Kubernetes Security Specialist (CKS) certification.</p>","tags":["Certifications","Kubernetes"]},{"location":"certifications/aws-cloud-practitioner.html","title":"AWS Cloud Practitioner","text":"","tags":["Certifications","AWS"]},{"location":"certifications/green-software-for-practitioners.html","title":"Green Software for Practitioners","text":"","tags":["Certifications"]},{"location":"certifications/terraform-associate.html","title":"HashiCorp Certified: Terraform Associate","text":"<p>Notes for HashiCorp Certified: Terraform Associate</p>","tags":["Certifications","Terraform"]},{"location":"certifications/terraform-associate.html#additional-resources","title":"Additional Resources","text":"<ul> <li>Exam Information Page</li> <li>HashiCorp Exam Study Guide</li> <li>HashiCorp Exam Review Guide</li> <li>Sample Questions</li> </ul>","tags":["Certifications","Terraform"]},{"location":"certifications/vault-associate.html","title":"Hashicorp Vault Associate","text":"","tags":["Certifications","HashiCorp Vault"]},{"location":"certifications/vault-associate.html#additional-resources","title":"Additional Resources","text":"<ul> <li>Exam Information Page</li> <li>HashiCorp Exam Study Guide</li> <li>HashiCorp Exam Review Guide</li> <li>Sample Questions</li> </ul>","tags":["Certifications","HashiCorp Vault"]},{"location":"certifications/vmware-application-modernization.html","title":"VMware Tanzu Kubernetes Grid: Install, Configure, Manage [v1.3]","text":"<p>Supporting notes for version 1.3 of the VMware on-demand training for VMware Tanzu Kubernetes Grid: Install, Configure, Manage - course link</p>","tags":["Certifications","VMware"]},{"location":"certifications/CKA/01_Course-Introduction.html","title":"1.0 - Introduction","text":""},{"location":"certifications/CKA/01_Course-Introduction.html#course-introduction","title":"Course Introduction","text":""},{"location":"certifications/CKA/01_Course-Introduction.html#structure","title":"Structure","text":"<ul> <li>Lectures</li> <li>Demos</li> <li>Quizzes</li> <li>Practice Questions</li> </ul>"},{"location":"certifications/CKA/01_Course-Introduction.html#prerequisites","title":"Prerequisites","text":"<ul> <li>Basic knowledge of Docker and Kubernetes</li> <li>Good knowledge of YAML File format</li> </ul> <p>Note: This course isn't just for the certification, a number of related topics will be discussed to ensure you can install, configure and troubleshoot your own Kubernetes cluster.</p>"},{"location":"certifications/CKA/01_Course-Introduction.html#practice-tests","title":"Practice Tests","text":"<p>As with the CKAD certificate, CKA is entirely practical, therefore it's ESSENTIAL that you take part in practice labs to gain an understanding of the relevant skills to apply in production scenarios.</p>"},{"location":"certifications/CKA/01_Course-Introduction.html#certification-details","title":"Certification Details","text":"<p>Kubernetes usage is growing exponentially in the market, therefore the need for certified engineers is also growing.</p> <p>Obtaining the certification makes you fully certified to design, build and administer highly available Kubernetes clusters.</p> <p>The exam length is 2 HOURS. As with CKAD, you are allowed access to the official Kubernetes documentation throughout.</p>"},{"location":"certifications/CKA/02_Core-Concepts.html","title":"2.0 - Core Concepts","text":""},{"location":"certifications/CKA/02_Core-Concepts.html#21-cluster-architecture","title":"2.1 - Cluster Architecture","text":"<p>Kubernetes exists to allow the hosting of containerized applications in an automated fashion, allowing communication between the different services associated, and facilitating the creation of however many instances you like.</p> <p>Master Node - The node that manages, plans, schedules workloads and monitors worker nodes.</p> <p>Worker Nodes - Nodes that host the containerised applications.</p> <p>Master node is comprised of multiple tools/clusters, making up the control plane:</p> <ul> <li>ETCD Cluster - Stores information about worker nodes, such as containers running within.</li> <li>Schedulers - Identifies the appropriate nodes that a container should be allocated to depending on metrics such as resource requests, node affinity and selectors etc.</li> <li>Controllers - Tools responsible for monitoring and responding to node changes, such as optimizing the number of containers running, responding to faulty nodes etc.</li> <li>Kube-API Server - Orchestrates all operations within the cluster, exposes the Kubernetes API, which users use to perform management operations. To run containers on the master and worker nodes, a standardized runtime environment is required, such as Docker.</li> </ul> <p>On the worker nodes, tools included are:</p> <ul> <li>Kubelet - An engine on each node that carries out operations based on requests from the master node, occasionally sending statistic reports to the kube-apiserver as part of the monitoring</li> <li>Kube-proxy - Service that ensures ingress/egress rules are in place to allow inter-pod and node-node communications</li> </ul>"},{"location":"certifications/CKA/02_Core-Concepts.html#22-etcd-for-beginners","title":"2.2 - ETCD For Beginners","text":"<ul> <li>ETCD Is a distributed reliable key-value store that is simple and fast to operate.</li> <li>Key-value store stores information ina key-value format, each value is associated with a unique key and stored in a database.</li> <li> <p>To install and run ETCD:</p> </li> <li> <p>Download and extract the binaries from https://github.com/etcd-io/etcd</p> </li> <li> <p>Run the associated executable <code>./etcd</code></p> </li> <li> <p>This starts a service running on port 2379 by default.</p> </li> <li>Clients can then be attached to the etcd service to store and retrieve information.</li> <li> <p>A default client included is the etcd control client, a CLI client for etcd; used to store and retrieve key-value-pairs.</p> </li> <li> <p>To store a key-value-pair: <code>./etcdctl set &lt;key&gt; &lt;value&gt;</code></p> </li> <li>To retrieve a value: <code>./etcdctl get &lt;key&gt;</code></li> <li>For additional information: <code>./etcdctl</code></li> </ul>"},{"location":"certifications/CKA/02_Core-Concepts.html#23-etcd-in-kubernetes","title":"2.3 - ETCD In Kubernetes","text":"<ul> <li>The etcd data store stores information relating to the cluster, such as:</li> <li>Nodes</li> <li>Pods</li> <li>Configs</li> <li>Secrets</li> <li>Roles</li> <li>Every piece of information obtained from a kubectl get command is obtained from the etcd server.</li> <li>Additionally, any changes made to the cluster, such as adding nodes, deploying pods or updating resources, the same changes are updated in the etcd server.</li> <li>Changes cannot be confirmed until the etcd server has been updated.</li> <li>The manner in which etcd is deployed is heavily dependent on your cluster setup. For the purposes of this course, 2 cases will be considered:</li> <li>A cluster built from scratch</li> <li>A cluster deployed using kubeadm</li> <li>Setting the cluster up from scratch requires manual downloading and installing the binaries; then configuring ETCD as a service in the master node.</li> <li>There are many options that can be added to this service, most of them relate to certificates, the rest describe configuring etcd as a cluster.</li> <li>One of the primary options to consider is the flag: <code>--advertise-client-urls https://${INTERNAL_IP}:2379</code></li> <li>This defines the address which the ETCD service listens on, by default it's port 2379 on the IP of the server</li> <li>It is in fact this URL that should be configured on the kube-api server when it attempts communication with the etcd server.</li> <li>If creating a cluster using kubeadm, the ETCD server is deployed as a pod in the kube-system namespace.</li> <li>The database can then be explored using the etcdctl command within the pod, such as the following; which lists all the keys stored by kubernetes.</li> </ul> <p><code>shell   kubectl exec etcd-mater -n kube-system etcdctl get / --prefix-keys-only</code></p> <ul> <li>The data stored on the etcd server adheres to a particular structure. The root is a registry containing various kubernetes resources e.g. pods, replicasets.</li> <li>In high-availability (HA) environments, multiple master nodes will be present, each containing their own ETCD instance.</li> <li>In this sort of scenario, each instance must be made aware of another, which can be configured by adding the following flag in the etcd service file:</li> </ul> <p><code>shell   --initial-cluster controller-0=https://${CONTROLLER0_IP}:2380, controller-1=https://${CONTROLLER1_IP}:2380, ...,   controller-N=https://${CONTROLLERN_IP}:2380</code></p>"},{"location":"certifications/CKA/02_Core-Concepts.html#24-etcd-common-commands","title":"2.4 - Etcd Common Commands","text":"<ul> <li>ETCDCTL is the CLI tool used to interact with ETCD.</li> <li>ETCDCTL can interact with ETCD Server using 2 API versions - Version 2 and Version 3.</li> <li>By default, it's set to use Version 2. Each version has different sets of commands.</li> <li>For example, ETCDCTL version 2 supports the following commands:</li> </ul> <pre><code>etcdctl backup\netcdctl cluster-health\netcdctl mk\netcdctl mkdir\netcdctl set\n</code></pre> <ul> <li>Whereas the commands are different in version 3</li> </ul> <pre><code>etcdctl snapshot save\netcdctl endpoint health\netcdctl get\netcdctl put\n</code></pre> <ul> <li> <p>To set the right version of API set the environment variable <code>ETCDCTL_API</code> command <code>export ETCDCTL_API=3</code></p> </li> <li> <p>When API version is not set, it is assumed to be set to version 2, therefore version 3 commands listed above don't work. And vice versa for when set to version 3.</p> </li> <li>Apart from that, you must also specify path to certificate files so that ETCDCTL can authenticate to the ETCD API Server.</li> <li>The certificate files are available in the etcd-master at the following path. We discuss more about certificates in the security section of this course. So don't worry if this looks complex:</li> </ul> <pre><code>--cacert /etc/kubernetes/pki/etcd/ca.crt\n--cert /etc/kubernetes/pki/etcd/server.crt\n--key /etc/kubernetes/pki/etcd/server.key\n</code></pre> <ul> <li>So for the commands I showed in the previous video to work you must specify the ETCDCTL API version and path to certificate files. Below is the final form:</li> </ul> <pre><code>kubectl exec etcd-master -n kube-system -- sh -c \"ETCDCTL_API=3 etcdctl get /\n--prefix --keys-only --limit=10 --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt --key /etc/kubernetes/pki/etcd/server.key\"\n</code></pre>"},{"location":"certifications/CKA/02_Core-Concepts.html#25-kube-api-server","title":"2.5 - Kube API Server","text":"<ul> <li>The primary management component in Kubernetes</li> <li>When a kubectl command is ran, it is the Kube-API server that is contacted by the kubectl utility for the desired action</li> <li>The kube-api-server authenticates and validates the request, then retrieves and displays the requested information from the etcd server.</li> <li>Note: The kubectl command isn't always necessary to set the API server running.</li> <li>Can instead send a post request requesting a resource creation such as: \u25a0 <code>curl -X POST /api/v1/namespaces/default/pods ... [other]</code></li> <li>In this scenario, the following steps occur: \u25a0 The request is authenticated \u25a0 The request is validated \u25a0 The API server creates a POD without assigning it to a node \u25a0 Updates the information in the ETCD server and the user to inform of the pod creation.</li> <li>The updated information of the nodeless pod is acknowledged by the scheduler; which monitors the API server continuously \u25a0 The scheduler then identifies the appropriate node to place the pod onto; communicating this back to the API server</li> <li>The API server updates the ETCD server with this information and passes this information to the kubelet in the chosen worker node</li> <li>Kubelet agent creates the pod on the node and instructs the container runtime engine to deploy the chosen application image.</li> <li>Finally, the kubelet updates the API server with the change(s) in status of the resources, which in turn updates the ETCD cluster's information.</li> <li>This pattern is loosely followed every time a change is requested within the cluster, with the kube-api server at the centre of it all.</li> <li>In short, the Kube-api server is:</li> <li>Responsible for validating requests</li> <li>Retrieving and updating the ETCD data store \u25a0 The API-Server is the only component that interacts directly with the etcd data store</li> <li>Other components such as the scheduler and kubelet only use the API server to perform updates in the cluster to their respective areas</li> <li>Note: The next point doesn't need to be considered if you bootstrapped your cluster using kubeadm</li> <li>If setting Kubernetes up \"The Hard Way\", the kube-apiserver is available asa binary in the kubernetes release pages</li> <li>Once downloaded and installed, you can configure it to run as a service on your master node</li> <li>Kubernetes architecture consists of a lot of different components working with each other and interacting with each other to know where they are and what they're doing</li> <li>Many different modes of authentication, authorization, encryption and security, leading to many options and parameters being associated with the API server.</li> <li>The options within the kube-apiserver's service file will be covered in detail later in the notes, for now, the important ones are mainly certificates, such as:</li> <li><code>--etcd-certifile=/var/lib/kubernetes/kubernetes.pem</code></li> <li><code>--kubelet-certificate-authority=/var/lib/kubernetes/ca.pem</code></li> <li>Each of the components to be considered in this section will have their own associated certificates.</li> <li>Note: To specify the location of the etcd servers, add the optional argument:</li> <li><code>--etcd-servers=https://127.0.0.1:2379</code> \u25a0 Change IP address where appropriate or port, it's via this address the kube-api-server communicates with the etcd server</li> <li>Viewing the options of the kube-api server in an existing cluster depends on how the cluster was set up:</li> <li>Kubeadm: \u25a0 The kube-api server is deployed as a pod in the kube-system namespace \u25a0 Options can be viewed within the pod definition file at: <code>/etc/kubernetes/manifests/kube-apiserver.yaml</code></li> <li>Non-kubeadm setup: \u25a0 Options displayed in kube-apiserver.service file at /etc/systemd/system/kube-apiserver.service \u25a0 Alternatively, use <code>ps -aux | grep kube-apiserver</code> to view the process and its associated options</li> </ul>"},{"location":"certifications/CKA/02_Core-Concepts.html#26-kube-controller-manager","title":"2.6 - Kube-Controller Manager","text":"<ul> <li>The kubernetes component that manages each of the controllers within Kubernetes.</li> <li>Each controller has their own set of responsibilities, such as monitoring and responding to changes in kubernetes resources or containers.</li> <li>Continuous monitoring determined by \"Watch Status\"</li> <li>Responsive actions carried out to \"Remediate the situation\"</li> <li>In terms of Kubernetes:</li> <li>Controllers are processes that continuously monitor the state of various components within the system</li> <li>If any changes occur that negatively affect the system, the controllers work towards bringing the system back to \"normal\"</li> <li>A common example of a controller us the node controller</li> <li>Monitors the status of nodes in the cluster and takes responsive actions to keep it running</li> <li>Any actions a controller takes are done via the kube-api server.</li> <li>The monitoring period for controllers can be configured and varies, for example the node controller checks the status of the nodes every 5 seconds.</li> <li>Allows frequent and accurate monitoring</li> <li>If the controller cannot communicate with a node after 40 seconds, it's marked as \"Unreachable\"</li> <li>If after 5 minutes the node is still unreachable, the controller takes any pods originally on the node and places them on a healthy available one.</li> <li>Another example is the replication controller</li> <li>Responsible for monitoring the status of replicasets and ensuring that the desired number of pods are available at all times within the set.</li> <li>If a pod dies, it creates another.</li> <li>Many more controllers are found within Kubernetes, such as:</li> <li>Deployments</li> <li>CronJobs</li> <li>Namespace</li> <li>All controllers are packaged as part of the Kube-Controller Manager; a single process</li> <li>To install and view the Kubernetes Controller Manager. You can download and extract the binary from the Kubernetes release page via wget etc, where you can then run it as a service.</li> <li>When running the Kubernetes Controller Manager as a service, you can see that there are a list of customisable options available.</li> <li>Some options that are customizable include node monitor grace period, monitoring period etc.</li> <li>You can also use the <code>--controllers</code> flag to configure and view what controllers you're using.</li> <li>As with the Kube-API Server, the way you view the options on the Kube-Controller Manager depends on your cluster's setup:</li> <li>Kubeadm: \u25a0 The kube-api server is deployed as a pod in the kube-system namespace \u25a0 Options can be viewed within the pod definition file at: <code>/etc/kubernetes/manifests/kube-controller-manager.yaml</code></li> <li>Non-kubeadm setup: \u25a0 Options displayed in <code>kube-controller-manager.service</code>file at <code>/etc/systemd/system/kube-apiserver.service</code> \u25a0 Alternatively, use <code>ps -aux | grep kube-controller-manager</code> to view the process and its associated options.</li> </ul>"},{"location":"certifications/CKA/02_Core-Concepts.html#27-kube-scheduler","title":"2.7 - Kube-Scheduler","text":"<ul> <li>Responsible for scheduling pods on nodes i.e. identifying the best node for objects such as pods and deployments to be placed on.</li> <li>It's a common misconception that the scheduler is responsible for actually placing the resources onto the nodes, this is actually Kubelet's responsibility.</li> <li>A scheduler is needed to ensure that containers and resources end up on the nodes that can successfully accommodate them based on certain criteria:</li> <li>Resource requirements for pod</li> <li>Resource capacity/quota for nodes</li> <li>The scheduler follows the 2-step process to make its decision:</li> <li>Filters nodes that don't fit the resource requirements for the pod/object</li> <li>Uses a priority function to determine which of the remaining nodes is the best fit for the object based on the node's resource capacity, scoring from 0-10. \u25a0 For example, if a pod requiring 10 cpu units could be placed on a node with 12 total units or 16, it's preferable to place it on the 16-unit one as this leaves more space for additional objects to be deployed to the pod.</li> <li>The scheduler also utilises other tools such as taints and tolerations, and node selectors/affinity.</li> <li>To install the kube-scheduler, extract and run the binary from the release page as a service; under the file kube-scheduler.service, where you can configure the options as per usual.</li> <li>As with the Kube-API Server, the way you view the options on the Kube-Scheduler depends on your cluster's setup:</li> <li>Kubeadm: \u25a0 The kube-api server is deployed as a pod in the kube-system namespace \u25a0 Options can be viewed within the pod definition file at: <code>/etc/kubernetes/manifests/kube-scheduler.yaml</code></li> <li>Non-kubeadm setup: \u25a0 Options displayed in <code>kube-scheduler.service</code> file at <code>/etc/systemd/system/kube-scheduler.service</code> \u25a0 Alternatively, use <code>ps -aux | grep kube-scheduler</code> to view the process and its associated options.</li> </ul>"},{"location":"certifications/CKA/02_Core-Concepts.html#28-kubelet","title":"2.8 - Kubelet","text":"<ul> <li>Kubelet registers nodes and other objects within Kubernetes to their required places on a cluster.</li> <li>When it receives instructions from the kube-scheduler via the kube-api server to load a container, pod etc on the node, it requests the container runtime (usually Docker), to pull the required image.</li> <li>Once the request is made, it continues to periodically monitor the state of the pod and the containers within, reporting its findings to the kube-apiserver.</li> <li>When installing the kubelet, it must be noted that if setting up a cluster via kubeadm, the kubelt isn't automatically deployed.</li> <li>This is a KEY difference.</li> <li>You must always manually install the kubelet on your worker nodes.</li> <li>To install, download the binary from the release page, from which you can extract and run it as a service under <code>kubelet.service</code></li> <li>The associated options can be viewed by either:</li> <li><code>/etc/systemd/system/kubelet.service</code> \u25a0 Options can be configured within the file</li> <li><code>ps -aux | grep kubelet</code></li> </ul>"},{"location":"certifications/CKA/02_Core-Concepts.html#29-kube-proxy","title":"2.9 - Kube-Proxy","text":"<ul> <li>In a cluster, every pod can interact with one another as long as a Pod Networking solution is deployed to the cluster</li> <li>Pod Network - An internal virtual network across all nodes within the cluster, allowing any pod within the cluster to communicate with one another</li> <li>There are multiple solutions for deploying a pod network.</li> <li>In one scenario, suppose you have a web application and a database running on two separate nodes.</li> <li>The two instances can communicate with each other via the IP of the respective pods.</li> <li>In the example above, the problem arises when the IP of the pods aren't static, to work around this, you can expose the pods across the cluster via a service.</li> <li>The service will have its own static IP address, so whenever a pod is to be accessed or communicated with, communications are routed through the service's IP address to the pod it's exposing.</li> <li>Note: The service cannot join the pod network as it's not an actual component, more of an abstraction or virtual component.</li> <li>It's not got any interfaces or an actively listening process.</li> <li>Despite the note, the service needs to be accessible across the cluster from any node. This is achieved via the Kube-Proxy.</li> <li>Kube-Proxy: A process that runs on each node in the kubernetes cluster.</li> <li>The process looks for new services continuously, creating the appropriate rules on each node to forward traffic directed to the service to the associated pods.</li> <li>To allow the rule creation, the Kube-Proxy uses IPTables rules.</li> <li>Kube-Proxy creates an IP Tables rule on each node within the cluster to forward traffic heading to the specific service to the designated pod; almost like a key-value-pair.</li> <li>To install, download the binary from the release page, from which you can extract and run it as a service under kube-proxy.service</li> <li>The kubeadm tool deploys kube-proxy as a PODs on each node</li> <li>Kube-Proxy deployed as a Daemon Set, a single POD is always deployed on each node in the cluster.</li> </ul>"},{"location":"certifications/CKA/02_Core-Concepts.html#210-pods-recap","title":"2.10 - Pods Recap","text":"<ul> <li>Kubernetes doesn't deploy containers directly to nodes, they're encapsulated into Pods; a Kubernetes object.</li> <li>Pod: A single instance of an application</li> <li> <p>A pod is the smallest possible object in Kubernetes</p> </li> <li> <p>Suppose a containersied app is running on a single pod in a single node. If the user demand increases, how is the load balanced?</p> </li> <li>One cannot have multiple containers to a pod</li> <li>Instead, a new pod will be required with a new instance of the application</li> <li> <p>If the user demand increases further, but no pods are available on the node; a new node has to be created.</p> </li> <li> <p>In general, pods and containers have a 1-to-1 relationship.</p> </li> </ul>"},{"location":"certifications/CKA/02_Core-Concepts.html#multi-container-pods","title":"Multi-Container Pods","text":"<ul> <li>A single pod can contain more than 1 container, however it cannot be running the same application.</li> <li>In some cases, one may have a \"helper\" container running alongside the primary application</li> <li>The helper container usually runs support processes such as:<ul> <li>Process user-entered data</li> <li>Carry out initial configuration</li> <li>Process uploaded files</li> </ul> </li> <li>When new pod is created, an additional helper-container will automatically be created alongside it.</li> <li>App and helper container communicate and share resources across a shared network</li> <li>The two containers have a 1-to-1 relationship.</li> </ul>"},{"location":"certifications/CKA/02_Core-Concepts.html#example-kubectl-commands","title":"Example Kubectl Commands","text":"<ul> <li><code>kubectl run &lt;container name&gt;</code></li> <li>Runs docker container by creating a pod</li> <li>To specify image, append <code>--image &lt;image name&gt;:&lt;image tag&gt;</code></li> <li> <p>Image will then be pulled from DockerHub</p> </li> <li> <p><code>kubectl get pods</code></p> </li> <li>Return a list detailing the pods in the default namespace</li> <li>Append <code>--namespace &lt;namespace&gt;</code> to specify a namespace.</li> </ul>"},{"location":"certifications/CKA/02_Core-Concepts.html#211-pods-with-yaml-recap","title":"2.11 - Pods with YAML Recap","text":"<ul> <li>Kubernetes uses YAML files as inputs for object creation e.g. pods, deployments, services.</li> <li>These YAML files always contain 4 key fields:</li> <li>apiVersion:<ul> <li>Version of Kubernetes API used to create the object</li> <li>Correct api version required for varying objects e.g. <code>v1</code> for Pods and Services, <code>apps/v1</code> for Deployments</li> </ul> </li> <li>kind:<ul> <li>type of object being created</li> </ul> </li> <li>metadata:<ul> <li>data referring to specifics of the object</li> <li>Expressed as a dictionary</li> <li>Labels: Children of metadata</li> <li>Indents denote what metadata is related toa  child of a property</li> <li>Used to differentiate pods</li> <li>Any key-value pairs allowed in labels</li> </ul> </li> <li> <p>spec:</p> <ul> <li>specification containing additional information around the object</li> <li>Written in a dictionary</li> <li><code>-</code> denotes first item in a dictionary</li> </ul> </li> <li> <p>To create a resource from YAML: <code>kubectl create -f &lt;definition&gt;.yaml</code></p> </li> <li> <p>To view pods: <code>kubectl get pods</code></p> </li> <li> <p>To view detailed info of a particular pod: <code>kubectl describe pod &lt;pod name&gt;</code></p> </li> </ul>"},{"location":"certifications/CKA/02_Core-Concepts.html#212-replicasets-recap","title":"2.12 - Replicasets Recap","text":"<ul> <li>Controllers monitor Kubernetes objects and respond accordingly</li> <li>A key one used is the replication controller</li> <li>Consider a single pod running an application:</li> <li>If this pod crashes, the app becomes inaccessible</li> <li>To prevent this, it'd be better to have multiple instances of the same app running simultaneously</li> <li>Replication controller allows the running of multiple instances of the same pod in the cluster; leading to higher availability</li> <li>Note: Even if there is a single pod, the replication controller will automatically replace it in the event of failure - this leans into the idea of the \"desired state\"; Kubernetes will ensure that the desired amount of replicas are available.</li> </ul>"},{"location":"certifications/CKA/02_Core-Concepts.html#load-balancing-and-scaling","title":"Load Balancing and Scaling","text":"<ul> <li>The replication controller is needed to create replicas of the same pod and share the load across it.</li> <li>Consider a single pod serving a single user:</li> <li>If a new user wants to acces the service, the controller automatically deploys an additional pod(s) to balance the load</li> <li> <p>If demand exceeds node space, the controller will create additional pods on other available node(s) in the cluster automatically</p> </li> <li> <p>One can therefore see the replication controller spans multiple nodes</p> </li> <li> <p>It helps to balance the load across multiple pods on different nodes and supports scalability.</p> </li> <li> <p>In terms of the replication controller, 2 terms are considered:</p> </li> <li>Replication Controller</li> <li> <p>Replica Set</p> </li> <li> <p>ReplicaSet is the newer technology for the role of Replication Controller</p> </li> <li> <p>Replication controllers are defined in YAML format similar to the following:</p> </li> </ul> <pre><code>apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: myapp-rc\n  labels:\n    app: myapp\n    type: frontend\nspec:\n  replicas: &lt;replica number&gt;\n  template:\n    metadata:\n      &lt;pod metadata&gt;\n    spec:\n      &lt;pod spec&gt;\n</code></pre> <ul> <li>There are 2 definition files posted, the replication controller's definition being a \"parent\" of the pod's definition file.</li> <li>The replication controller is created in standard practice via <code>kubectl create -f &lt;filename&gt;.yaml</code></li> <li>To view the RC: <code>kubectl get replicationcontroller</code></li> <li>Displays the number of desired, currently available, and ready pods for associated replication controllers.</li> <li> <p>Pods are still viewable via <code>kubectl get pods</code></p> </li> <li> <p>ReplicaSet example definition:</p> </li> </ul> <pre><code>apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: myapp-replicaset\n  labels:\n    app: myapp\n    type: frontend\nspec:\n  template:\n    metadata:\n      &lt;pod metadata&gt;\n    spec:\n      &lt;pod spec&gt;\n  selector:\n    matchLabels:\n      type: frontend\n</code></pre> <ul> <li>Selectors ghelp replicasets determine what pods to focus on</li> <li>This is required as replicasets can also manage pods not created or associated with the replicaset - so long as the labels match the selectr.</li> <li>Selectors are the main difference between ReplicaSets and ReplicationControllers</li> <li>If not defined, it will assume the same lable provided in the pod definition file</li> <li>Creation done via <code>kubectl create -f &lt;filename&gt;.yaml</code> as per usual</li> <li>Pods can be checked via <code>kubectl get pods</code></li> </ul>"},{"location":"certifications/CKA/02_Core-Concepts.html#labels-and-selectors","title":"Labels and Selectors","text":"<ul> <li>Consider a deployment of an application with 3 pods:</li> <li>To create a replication controller or replicaset, one can ensure that at any given point, 3 pods will be running.</li> <li>If the pods weren't created, the ReplicaSet will automatically create them.</li> <li> <p>ReplicaSet monitors the pods and deploys the replacements in the event of failure.</p> </li> <li> <p>The ReplicaSet knows what pods to monitor via labels</p> </li> <li> <p>In the <code>matchLabels</code> parameter, the label entered denotes the pods the replicaset should manage.</p> </li> <li> <p>The template section is required such that the pod can be redeployed based on the template defined.</p> </li> </ul>"},{"location":"certifications/CKA/02_Core-Concepts.html#scaling","title":"Scaling","text":"<ul> <li>To scale a Kubernetes deployment, one can update the <code>replicas</code> number in the <code>.yaml</code> file associated, and run <code>kubectl replace -f &lt;definition&gt;.yaml</code></li> <li>Alternatively: <code>kubectl scale --replicas &lt;new amount&gt; &lt;defintion&gt;.yaml</code></li> <li>Alternatively: <code>kubectl scale --replicas=&lt;number&gt; --replicaset &lt;replicaset name&gt;</code></li> <li>This method does not update the YAML file.</li> </ul>"},{"location":"certifications/CKA/02_Core-Concepts.html#command-summary","title":"Command Summary","text":"<ul> <li>Create a ReplicaSet or object in KuberneteS: <code>kubectl create -f &lt;definition&gt;.yaml</code></li> <li>List ReplicaSets: <code>kubectl get replicasets</code></li> <li>Delete a replicaset and its underlying pods: <code>kubectl delete replicaset &lt;replicaset name&gt;</code></li> <li>Replace or update the replicaset: <code>kubectl replace -f &lt;replicaset definition&gt;.yaml</code></li> <li>Scale a replicaset: <code>kubectl scale --replicas=&lt;number&gt; -f &lt;defintiion&gt;.yaml</code></li> </ul>"},{"location":"certifications/CKA/02_Core-Concepts.html#213-deployments","title":"2.13 - Deployments","text":"<ul> <li>When deploying an application in a production environment, like a web server:</li> <li>Many instances of the web server could be needed</li> <li>Need to be able to upgrade the instances seamlessly one-after-another (rolling updates)</li> <li> <p>Need to avoid simultaneous updates as this could impact user accessibility</p> </li> <li> <p>In the event of update failure, one should be able to rollback upgrades to a previously working iteration</p> </li> <li> <p>If wanting to make multiple changes to the environment, can pause each environment to make the changes, and resume when updates are in effect.</p> </li> <li> <p>These capabilities are provided via Kubernetes Deployments.</p> </li> <li>These are objects higher in the hierarchy than a ReplicaSet</li> <li> <p>Provides capabilities to:</p> <ul> <li>Upgrade underlying instances seamlessly</li> <li>Utilise rolling updates</li> <li>Rollback changes during failure</li> <li>Pause and resume environments to allow changes to take place.</li> </ul> </li> <li> <p>As usual, Deployments can be defined by YAML definitions:</p> </li> </ul> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp-deployment\n  labels:\n    app: myapp\n    type: frontend\nspec:\n  template:\n    metadata:\n      name: myapp-prod\n      labels:\n        app: myapp\n        type: frontend\n    spec:\n      containers:\n      - name: nginx-controller\n        image: nginx\n  replicas: 3\n  selector:\n    matchLabels:\n      type: frontend\n</code></pre> <ul> <li>To create deployment: <code>kubectl create -f &lt;deployment&gt;.yaml</code></li> <li> <p>View deployments: <code>kubectl get deployments</code></p> </li> <li> <p>Other commands: <code>kubectl get all</code> -&gt; Display all Kubernetes objects</p> </li> </ul>"},{"location":"certifications/CKA/02_Core-Concepts.html#214-general-tips","title":"2.14 - General Tips","text":"<ul> <li>When using the CLI, it can become difficult to create and edit the YAML files associated with objects in Kubernetes</li> <li>A quick alternative is to copy and paste a template file for the designated object and edit it as required, in Linux Distributions this can be done via:</li> <li><code>CTRL+Insert = Copy</code></li> <li><code>SHIFT+Insert= = Paste</code></li> <li>Alternatively, the kubectl run command can be used to generate a YAML template which can be easily modified, though in some cases you can get away with using kubectl run without creating a new YAML file, such as the following examples.</li> </ul> <pre><code># Creating an NGINX Pod\n\nkubectl run nginx --image=nginx\n\n# Creating an NGINX Deployment\n\nkubectl create deployment --image=nginx nginx\n</code></pre> <ul> <li>In cases where a YAML file is needed, one can add the <code>--dry-run</code> flag to the kubectl run command and direct its output to a YAML file</li> <li>The <code>--dry-run=client</code> flag signals to Kubernetes to not physically create the object described, only generate a YAML template that describes the specified object</li> </ul> <pre><code># Create an NGINX Pod YAML without Deploying the Pod\n\nkubectl run nginx --image=nginx --dry-run=client -o yaml &gt; nginx-pod.yaml\n\n# Create a deployment YAML\n\nkubectl create deployment --image=nginx nginx --dry-run=client -o yaml &gt; nginx-deployment.yaml\n\n# Create a deployment YAML with specific replica numbers:\n\nkubectl create deployment --image=nginx nginx --replicas=4 --dry-run=client -o yaml &gt; nginx-deployment.yaml\n</code></pre>"},{"location":"certifications/CKA/02_Core-Concepts.html#215-namespaces","title":"2.15 - Namespaces","text":"<ul> <li>A namespace is automatically created when a cluster is created</li> <li> <p>They serve to isolate the cluster resources such that they aren't accidentally maniuplated.</p> </li> <li> <p>Example:</p> </li> <li> <p>When developing an application, one can create a <code>Dev</code> and <code>Prod</code> namespace to keep resources isolated</p> </li> <li> <p>Each namespace can then have their own policies, detailing user access and controlm etc.</p> </li> <li>Resource limits may also be namespace-scoped.</li> </ul>"},{"location":"certifications/CKA/02_Core-Concepts.html#domain-name-service-dns","title":"Domain Name Service - DNS","text":"<ul> <li>For objects communicating in their namespace, they simply refer to the other object by their name.</li> <li>Example, for a web application pod connecting to a database service titled <code>db-service</code>, you would specify: <code>mysql.connect(\"db-service\")</code>.</li> <li>For objects communicating outside of their namespace, need to append the name of the namespace to access and communicate.</li> <li>Example: <code>mysql.connect(\"db-service.dev.svc.cluster.local\")</code></li> <li>In general format followed: <code>&lt;service name&gt;.&lt;namespace&gt;.svc.cluster.local</code></li> <li>This can be done as when a service is created, a DNS entry is added automatically in this format.</li> <li><code>cluster.local</code> is the default cluster's domain name.</li> <li><code>svc</code> = subdomain for service.</li> <li>List all pods in default namespace: <code>kubectl get pods</code></li> <li>List all pods in specific namespace: <code>kubectl get pods --namespace &lt;namespace&gt;</code></li> <li>When creating a pod via a definition file, it will automatically be added to the default namespace if no namespace is specified.</li> <li> <p>To add to a particular namespace: <code>kubectl create -f &lt;definition&gt;.yaml --namespace=&lt;namespace name&gt;</code></p> </li> <li> <p>To set default namespace of a pod, add <code>namespace: &lt;namespace name&gt;</code> to metadata in the definition file.</p> </li> <li> <p>Namespaces can be created via YAML definitions:</p> </li> </ul> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: namespace-name\n</code></pre> <ul> <li>To create: <code>kubectl create -f &lt;namespace&gt;.yaml</code></li> <li>Alternatively: <code>kubectl create namespace &lt;namespace name&gt;</code></li> <li>To switch context: <code>kubectl config set-context $(kubectl config current-context --namespace=&lt;namespace&gt;)</code></li> <li>To view all pods in each namespace add <code>--all-namespaces</code> to the <code>get pods</code> commands.</li> </ul>"},{"location":"certifications/CKA/02_Core-Concepts.html#resource-quota","title":"Resource Quota","text":"<ul> <li>Creates limitations on resources for namespaces</li> <li>Created via definition file</li> <li>Kind: ResourceQuota</li> <li>Spec must specify variables such as:</li> <li>Pod numbers</li> <li>Memory limits</li> <li>CPU limits</li> <li> <p>Minimum requested/required CPU and Memory</p> </li> <li> <p>Example:</p> </li> </ul> <pre><code>apiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: compute-quota\n  namespace: dev\nspec:\n  hard:\n    pods: \"10\"\n    requests.cpu: \"4\"\n    requests.memory: 5Gi\n    limits.cpu: \"10\"\n</code></pre>"},{"location":"certifications/CKA/02_Core-Concepts.html#216-services","title":"2.16 - Services","text":"<ul> <li>Kubernetes services allows inter-component communication both within and outside Kubernetes.</li> <li>Additionally, services allow connections of applications with users or other applications.</li> </ul>"},{"location":"certifications/CKA/02_Core-Concepts.html#example","title":"Example","text":"<ul> <li>Suppose an application has a group of pods running various aspects of the app</li> <li>One for serving frontend users</li> <li>Another for backend etc</li> <li> <p>Another for connecting to an external data source</p> </li> <li> <p>All these groups of pods are connected by use of services</p> </li> <li>Additionally, services allow frontend accessibility to users</li> <li> <p>Allows front-to-back pod communication and external data source communication</p> </li> <li> <p>One of the key services used in Kubernetes is the facilitation of external communication:</p> </li> <li>Suppose a pod has been deployed and is running a web app:</li> <li>The Kubernetes node's IP address is in the same network as the local machine</li> <li>The pod's network is separate</li> <li>To access the container's contents, could either use a <code>curl</code> request to the IP or access via local browser</li> <li>In practice, wouldn't want to have to ssh into the node to access the container's content, you'd want to access it as an \"external\" user.</li> <li>Kubernetes service(s) cna be introduced to map the request from a local machine -&gt; node -&gt; pod</li> <li>Kubernetes services are treated as objects in Kubernetes like Pods, ReplicaSets, etc.</li> <li> <p>To facilitate external communications, one can use NodePort:</p> <ul> <li>Listens to a port on the node</li> <li>Forwards requests to the required pods</li> </ul> </li> <li> <p>Primary service types include:</p> </li> <li>NodePort: Makes an internal pod accessible via a port on a node</li> <li>ClusterIP:<ul> <li>Creates a virtual IP inside the cluster</li> <li>Enables communication between services e.g. frontend &lt;--&gt; backend</li> </ul> </li> <li>Load Balancer:<ul> <li>Provisions a load balancer for application support</li> <li>Typically cloud-provider only.</li> </ul> </li> </ul>"},{"location":"certifications/CKA/02_Core-Concepts.html#nodeport","title":"NodePort","text":"<ul> <li>Maps a port on the clsuter node to a port on the pod to allow accessibility</li> <li>On closer inspection, this service type can b e broken down into 3 parts:</li> <li>The port of the application on the pod it's running from -&gt; <code>targetPort</code></li> <li>The port on the service itself -&gt; <code>port</code></li> <li> <p>Port on the node used to access the application externally -&gt; <code>NodePort</code></p> </li> <li> <p>Note: NodePort range only available for <code>30000 - 32767</code></p> </li> <li> <p>To create the service, create a definition file similar to the following:</p> </li> </ul> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: myapp-service\nspec:\n  type: NodePort\n  ports:\n  - targetPort: 80\n    port: 80\n    nodePort: 30080\n</code></pre> <ul> <li>Note: The prior definition file is acceptable for using only one pod on the cluster.</li> <li>If there are multiple pods with a <code>targetPort</code> of say 80 in this case, this will cause issues, the service needs to only focus on certain pods</li> <li> <p>This can be worked around via the use of selectors</p> </li> <li> <p>Under <code>selector</code>, add any labels associated with the pod definition file e.g.:</p> </li> </ul> <pre><code>selector:\n  app: myapp\n  type: frontend\n</code></pre> <ul> <li> <p>The service can then be created using <code>kubectl create -f &lt;filename&gt;.yaml</code> as per usual.</p> </li> <li> <p>To view services: <code>kubectl get services</code></p> </li> <li> <p>To access the web service: <code>curl &lt;node IP&gt;:&lt;node Port&gt;</code></p> </li> <li> <p>Suppose you're in a production environment:</p> </li> <li>Multiple pods or instances running in the same application</li> <li> <p>Allows high availability and load balancing</p> </li> <li> <p>If all pods considered share the same labels, the selector will automatically assign the pods as the service endpoints -&gt; no additional configuration is required.</p> </li> <li> <p>If the pods are distributed across multiple nodes:</p> </li> <li> <p>Without any additional configuration Kubernetes automatically creates the service to span the entire set of nodes in the cluster.</p> <ul> <li>Maps the target port to the same node port for each node.</li> <li>The application can be accessed through the IP of any of the nodes in the cluster, but via the same port.</li> </ul> </li> <li> <p>Regardless of the number of pods or nodes involved, the service creation method is exactly the same, no additional steps are required.</p> </li> <li> <p>Note: When pods are removed or added, the service will automatically be updated =&gt; offering higher flexibility and adaptability.</p> </li> </ul>"},{"location":"certifications/CKA/02_Core-Concepts.html#217-clusterip-services","title":"2.17 - ClusterIP Services","text":"<ul> <li>In general, a fill stack will comprise of groups of pods, hosting different parts of an application, such as:</li> <li>Frontend</li> <li>Backend</li> <li> <p>Key-value store</p> </li> <li> <p>Each of these groups of pods need to be able to interact with one another for the application to fully function.</p> </li> <li> <p>Each pod will automatically be assigned its own IP address</p> </li> <li>Not static</li> <li>Pods could be removed or added at any given point</li> <li> <p>One cannot therefore rely on these IP addresses for inter-service communication</p> </li> <li> <p>Kubernetes ClusterIP services can be used to group the pods together by functionality and provide a single interface to access them.</p> </li> <li> <p>Any requests to that group is assigned randomly to one of the pods within.</p> </li> <li> <p>This provides an easy and effective deployment of a microservice-based application on a Kubernetes cluster.</p> </li> <li> <p>Each layer or group gets assigned its own IP address and name within the cluster</p> </li> <li>To be used by other pods to access the service.</li> <li> <p>Each layer can scale up or down without impacting service-service communications</p> </li> <li> <p>To create, write a definition file:</p> </li> </ul> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: backend\nspec:\n  type: ClusterIP\n  ports:\n  - targetPort: 80\n    port: 80\n  selector:\n    app: myapp\n    type: backend\n</code></pre> <ul> <li> <p>To create the service: <code>kubectl create -f &lt;service file&gt;.yaml</code> or <code>kubectl expose &lt;deployment or pod name&gt; --port=&lt;port&gt; --target-port=&lt;port&gt; --type=clusterIP</code></p> </li> <li> <p>View services via <code>kubectl get services</code></p> </li> <li> <p>From here, the services can be accessed via the ClusterIP or service name.</p> </li> </ul>"},{"location":"certifications/CKA/02_Core-Concepts.html#218-loadbalancer-services","title":"2.18 - LoadBalancer Services","text":"<ul> <li>Kubernetes service type that helps balance traffic routing to underlying services and nodes within the cluster.</li> <li>Only supported on separate cloud platforms such as GCP, Azure etc</li> <li>Unsupported in environments such as Virtualbox, if still used, it basically has the same effect as a service of type NodePort.</li> </ul>"},{"location":"certifications/CKA/02_Core-Concepts.html#219-imperative-vs-declarative-commands","title":"2.19 - Imperative vs Declarative Commands","text":"<ul> <li>Imperative: The use of statements to change a programs state, give a program step-by-step instructions on how to perform a task, specify the \"how\" to get to the \"what\"</li> <li>Declarative: Writing a program describing an operation by specifying only the end goal, specify the \"what\" only</li> <li>In Kubernetes, this split in programming language can be broken down as:</li> <li>Imperative - Using kubectl commands to perform CRUD operations like scaling and updating images, as well as operations with .yaml definition files. \u25a0 These commands specify the exact commands and how they should be performed.</li> <li>Declarative: \u25a0 Using kubectl apply commands with definition files, Kubernetes will consider the information provided and determine what changes need to be made</li> <li>Imperative commands in kubernetes include:</li> <li>Creation \u25a0 Run \u25a0 Create \u25a0 Expose</li> <li>Update Objects \u25a0 Edit \u25a0 Scale \u25a0 Set (image)</li> <li>It should be noted that Imperative commands are often \"one-time-use\" and are limited in functionality, for advanced operations it's better to work with definition files, and that's where using the <code>-f &lt;filename&gt;</code> commands are better-used.</li> <li>Imperative commands can become taxing as they require prior knowledge of pre-existing configurations, which can result in extensive troubleshooting if unfamiliar.</li> <li>For the declarative approach, it's more recommended to use this when making extensive changes or updates without having to worry about manual troubleshooting or management.</li> </ul>"},{"location":"certifications/CKA/02_Core-Concepts.html#220-kubectl-apply","title":"2.20 - Kubectl Apply","text":""},{"location":"certifications/CKA/03_Scheduling.html","title":"3.0 - Scheduling","text":""},{"location":"certifications/CKA/03_Scheduling.html#31-manual-scheduling","title":"3.1 - Manual Scheduling","text":"<ul> <li>When a pod is made available for scheduling, the Scheduler looks at the PODs definition file for the value associated with the field NodeName</li> <li>By default, NodeName's value isn't set and is added automatically when scheduling</li> <li>The scheduler looks at all pods currently on the system and checks if a value has been added to the NodeName field, any which do not are a candidate for scheduling.</li> <li>The scheduler identifies the best candidate for scheduling using its algorithm and schedules the pod onto that Node by adding the node's name to the NodeName field.</li> <li>This setting of the NodeName field value binds the Pod to the Node.</li> <li>If there is no scheduler to monitor and schedule the nodes, the pods will remain in a pending state.</li> <li>Pods can be manually assigned to nodes if a scheduler isn't present.</li> <li>This can be done by manually setting the pod's value for NodeName in the definition file.</li> <li>This can only be done before the pod is created for the first time, it cannot be done post-creation for a pre-existing pod</li> <li>To configure, as a child of the pod's Spec, add the field: <code>nodeName: &lt;nodename&gt;</code></li> <li>Alternatively, you can assign a node by creating a binding object definition file to send a post request to the pod binding API; mimicking the scheduler's actions.</li> </ul> <pre><code>apiVersion: v1\nkind: Binding\nmetadata:\n name: nginx\ntarget:\n apiVersion: v1\n kind: Node\n name: node02\n</code></pre> <ul> <li>Once the binding definition file is written, a post request can be sent to the pod's binding API; with the data set to the binding object in a JSON format in a similar vein to:</li> </ul> <pre><code>curl --header \"Content-Type:application/json\" --request POST --data \u2018{\"apiVersion\":\"v1\", \"kind\": \"Binding\" ...}\n\nhttp://$SERVER/api/v1/namespaces/default/pods/$PODNAME/binding\n</code></pre>"},{"location":"certifications/CKA/03_Scheduling.html#32-labels-and-selectors","title":"3.2 - Labels and Selectors","text":"<ul> <li>Built-In Kubernetes features used to help distinguish objects of similar nature from one another by grouping them</li> <li>Labels are added under the metadata section, where an infinite number of labels can be added to the Kubernetes object in a key value format</li> <li>To filter objects with labels, use the kubectl get command and add the flag --selector followed by the key-value pair in the format <code>key=value</code></li> </ul> <pre><code>kubectl get &lt;object&gt; --selector key=value\n</code></pre> <ul> <li>Selectors are used to link objects to one another, for example, when writing a replica set definition file, use the selector feature in the spec to specify the labels the object should look for in pods to manage.</li> <li>The same can be applied to services to help identify what pods or deployments it is exposing.</li> <li>Annotations - Used to record data associated with the object for integration purposes e.g. version number, build name etc</li> </ul>"},{"location":"certifications/CKA/03_Scheduling.html#33-taints-and-tolerations","title":"3.3 - Taints and Tolerations","text":"<ul> <li>Used to set restrictions regarding what pods can be scheduled on a node.</li> <li>Consider a cluster of 3 nodes with 4 pods preparing for launch:</li> <li> <p>The scheduler will place the pods across all nodes equally if no restriction applies</p> </li> <li> <p>Suppose now only 1 node has resources available to run a particular application:</p> </li> <li>A taint can be applied to the node in question; preventing any unwanted pods from being scheduled on it.</li> <li> <p>Tolerations then need to be applied to the pod(s) to specifically run on node 1</p> </li> <li> <p>Pods can only run on a node if their tolerations match the taint applied to the node.</p> </li> <li> <p>Taints and tolerations allow the scheduler to allocate pods to required nodes, such that all resources are used and allocated accordingly.</p> </li> <li> <p>Note: By default, no tolerations are applied to pods.</p> </li> </ul>"},{"location":"certifications/CKA/03_Scheduling.html#taints-node","title":"Taints - Node","text":"<ul> <li>To apply a taint: <code>kubectl taint nodes &lt;nodename&gt; key=value:&lt;taint-effect&gt;</code></li> <li>The key-value pair defined could match labels defined for resources e.g <code>app=frontend</code></li> <li>The taint effect determines what happens to pods that are intolerant to the taint, 1 of 3 possibilities can be specified:</li> <li><code>NoSchedule</code> - Pods won't be scheduled.</li> <li><code>PreferNoSchedule</code> - Try to avoid scheduling if possible.</li> <li><code>NoExecute</code> - New pods won't be scheduled, and any pre-existing pods intolerant to the taint are stopped and evicted.</li> </ul>"},{"location":"certifications/CKA/03_Scheduling.html#tolerations-pod","title":"Tolerations - Pod","text":"<ul> <li>To apply a toleration to a pod, one can look at the definition file</li> <li>In the spec section, add similar to the following:</li> </ul> <pre><code>tolerations:\n- key: app\n  operator: \"Equal\"\n  value: \"blue\"\n  effect: \"NoSchedule\"\n</code></pre> <ul> <li>Be sure to apply the same values used when applying the taint to the node.</li> <li>All values added need to be enclosed in \" \".</li> </ul>"},{"location":"certifications/CKA/03_Scheduling.html#taint-noexecute","title":"Taint - NoExecute","text":"<ul> <li>Suppose Node1 is to be used for a particular application:</li> <li>Apply a taint to node 1 with the app name and add a toleration to the pod running the app.</li> <li> <p>Setting the taint effect to <code>NoExecute</code> causes existing pods on the node that are intolerant to be stopped and evicted.</p> </li> <li> <p>Taints and tolerations are only used to restrict pod access to nodes.</p> </li> <li>As there are no restrictions / taints applied to the other pods, there's a chance the app could still be placed on a different node(s).</li> <li> <p>If wanting the pod to go to a particular node, one can utilise node affinity.</p> </li> <li> <p>Note: A taint is automatically applied to the master node, such that no pods can be scheduled to it.</p> </li> <li>View it via <code>kubectl describe node kubemaster | grep Taint</code></li> </ul>"},{"location":"certifications/CKA/03_Scheduling.html#34-node-selectors","title":"3.4 - Node Selectors","text":"<ul> <li>Consider a 3-node cluster, with 1 node having a larger resource configuration:</li> <li>In this scenario, one would like the task/process requiring more resources to go to the larger node.</li> <li>To solve, can place limitations on pods</li> <li>This can be done via the <code>nodeSelector</code> property in the definition file:</li> </ul> <pre><code>nodeSelector:\n  size: node-label\n</code></pre> <ul> <li> <p>NodeSelectors require the node to be labelled: <code>kubectl label nodes &lt;node name&gt; &lt;label key&gt;=&lt;key value&gt;</code></p> </li> <li> <p>When pod is created, it should be assigned to the labelled node so long as the resources allow it.</p> </li> </ul>"},{"location":"certifications/CKA/03_Scheduling.html#limitations-of-nodeselectors","title":"Limitations of NodeSelectors","text":"<ul> <li>NodeSelectors are beneficial for simple allocation tasks, but if more complex allocation is needed, Node Affinity is recommended, e.g. \"go to either 1 of 2 nodes\".</li> </ul>"},{"location":"certifications/CKA/03_Scheduling.html#35-node-affinity","title":"3.5 - Node Affinity","text":"<ul> <li>Node affinity looks to ensure that pods are hosted on the desired nodes</li> <li> <p>Can ensure high-resource consumption jobs are allocated to high-resource nodes</p> </li> <li> <p>Node affinity allows more complex capabilities regarding pod-node limitation.</p> </li> <li> <p>To specify in the spec section of a pod definition filem add in a new field:</p> </li> </ul> <pre><code>affinity:\n  nodeAffinity:\n    requiredDuringSchedulingIgnoredDuringExecution:\n      matchExpressions:\n      - key: size\n        operator: In\n        values:\n        - Large\n</code></pre> <ul> <li>Note: For the example above, the <code>NotIn</code> operator could also be used to avoid particular nodes.</li> <li> <p>Note: If just needing a pod to go to any node with a particular label, regardless of value, use the <code>Exists</code> operator -&gt; no values are required in this case.</p> </li> <li> <p>Additional operators are available, with further details provided in the documentation.</p> </li> <li>In the event that a node cannot be allocated due to a lable fault, the resulting action is dependent upon the NodeAffinityType set.</li> </ul>"},{"location":"certifications/CKA/03_Scheduling.html#node-affinity-types","title":"Node Affinity Types","text":"<ul> <li> <p>Defines the scheduler's behaviour regarding Node Affinity and pod lifecycle stages</p> </li> <li> <p>2 main types available:</p> </li> <li><code>RequireDuringSchedulingIgnoredDuringExecution</code></li> <li> <p><code>PreferredDuringSchedulingIgnoredDuringExecution</code></p> </li> <li> <p>Other types are to be released such as <code>requiredDuringSchedulingRequiredDuringExecution</code></p> </li> <li> <p>Considering the 2 available types, can break it down into the 2 stages of a pod lifecycle:</p> </li> <li>DuringScheduling -&gt; The pod has been created for the first time and not deployed</li> <li> <p>DuringExecution</p> </li> <li> <p>If the node isn't available according to the NodeAffinity, the resultant action is dependent upon the NodeAffinity type:</p> </li> <li> <p>Required:</p> </li> <li>Pod must be placed on a node that satisfies the node affinity criteria</li> <li>If no node satisfies the criteria, the pod won't be scheduled</li> <li> <p>Generally used when the node placement is crucial</p> </li> <li> <p>Preferred:</p> </li> <li>Used if the pod placement is less important than the need for running the task</li> <li>If a matching node not found, the scheduler ignores the NodeAffinity</li> <li> <p>Pod placed on any available node</p> </li> <li> <p>Suppose a pod has been running and a change is made to the Node Affinity:</p> </li> <li>The response is determined by the prefix of <code>DuringExecution</code>:<ul> <li>Ignored:</li> <li>Pods continue to run</li> <li>Any changes in Node Affinity will have no effect once scheduled.</li> <li>Required:</li> <li>When applied, if any current pods that don't meet the NodeAffinity requirements are evicted.</li> </ul> </li> </ul>"},{"location":"certifications/CKA/03_Scheduling.html#36-taints-and-tolerations-vs-node-affinity","title":"3.6 - Taints and Tolerations vs Node Affinity","text":"<ul> <li>Consider a 5-cluster setup:</li> <li>Blue Node: Runs the blue pod</li> <li>Red Node: Runs the red pod</li> <li>Green Node: runs the green pod</li> <li>Node 1: To run the grey pod</li> <li> <p>Node 2: \" \"</p> </li> <li> <p>Applying a taint ot each of the coloured nodes to accept their respective pod</p> </li> <li> <p>Tolerances are then are applied to the pods</p> </li> <li> <p>Need to apply a taint to node 1 and node 2 as the coloured pods can still be allocated to nodes where they're not wanted.</p> </li> <li> <p>To overcome, use Node Affinity:</p> </li> <li>Label nodes with respective colours</li> <li> <p>Pods end up in the correct nodes via use of Node Selector.</p> </li> <li> <p>There's a chance that the unwanted pods could still be allocated.</p> </li> <li> <p>A combination of taints and tolerations, and node affinity must be used.</p> </li> <li>Apply taints and tolerations to present unwanted pod placement on nodes</li> <li>Use node affinity to prevent the correct pods from being placed on incorrect nodes.</li> </ul>"},{"location":"certifications/CKA/03_Scheduling.html#37-resource-limits","title":"3.7 - Resource Limits","text":"<ul> <li>Consider a 3-node setup, each has a set amount of resources available i.e.:</li> <li>CPU</li> <li>Memory</li> <li> <p>Disk Space</p> </li> <li> <p>The Kubernetes scheduler is responsible for allocating pods to nodes</p> </li> <li>To do so, it takes into account the node's current resource allocation and the resources requested by the pod.</li> <li>If no resources are available, the scheduler will hold the pod back for release</li> <li>Kubernetes automatically assumes a pod or container within a pod will require at least:</li> <li>0.5 CPU Units</li> <li> <p>256Mi Memory</p> </li> <li> <p>If the pod or container requires more resources than allocated above, one can configure the pod definition file's spec, in particular, add the following under the <code>containers</code> list:</p> </li> </ul> <pre><code>resources:\n  requests:\n    memory: \"1Gi\"\n    cpu: 1\n</code></pre>"},{"location":"certifications/CKA/03_Scheduling.html#resource-cpu","title":"Resource - CPU","text":"<ul> <li>Can be set from <code>1m</code> (1 micro) to as high as required / supported by the host system.</li> <li>1 CPU = 1 AWS vCPU = 1 GCP Core = 1 Azure Core = 1 Hyperthread</li> </ul>"},{"location":"certifications/CKA/03_Scheduling.html#resources-memory","title":"Resources - Memory","text":"<ul> <li>Allocate within any of the following suffix for the givne purpose and the system's capabilities:</li> </ul> Memory Metric Shorthand Notation Equivalency Gigagbyte G 1000M Megabyte M 1000K Kilobyte K 1000 Bytes Gigibyte Gi 1024Mi Mebibyte Mi 1024Ki Kilibyte Ki 1024 Bytes <ul> <li>Docker containers have no limit to the resources they can consume</li> <li>When only running on a node, it can only use a maximum of 1vCPU unit - if the limits need changing, update the pod definition file:</li> </ul> <pre><code>resources:\n  requests:\n    ....\n  limits:\n    memory: &lt; value and unit&gt;\n    cpu: &lt;number&gt;\n</code></pre> <ul> <li>The limits and requests are set for each pod and container</li> <li>If CPU overload occurs, CPU usage is \"throttled\" ont he node</li> <li>If repeated memory use is exceeded, the pod is terminated.</li> </ul>"},{"location":"certifications/CKA/03_Scheduling.html#default-resource-requirements-and-limits","title":"Default Resource Requirements and Limits","text":"<ul> <li>Naturally Kubernetes assumes containers request 0.5 units of CPU and 256Mi of memory</li> <li>These defaults can be configured to suit for each namespace within the Kubernetes cluster by setting a limitrange, which can be produced via a yaml definition file similar to:</li> </ul> <pre><code>apiVersion: v1\nkind: LimitRange\nmetadata:\n  name: mem-limit-range # keep memory and CPU limit ranges separate\nspec:\n  limits:\n  - default:\n      memory: 512Mi\n      cpu: 1\n    defaultRequest:\n      memory: 256Mi\n      cpu: 0.5\n    type: Container\n</code></pre>"},{"location":"certifications/CKA/03_Scheduling.html#pod-deployment-editing","title":"Pod / Deployment Editing","text":"<ul> <li>When editing an existing pod, only the following aspects can be edited in the spec:</li> <li>Image (for containers and initcontainers)</li> <li>activeDeadlineSeconds</li> <li>Tolerations</li> <li>Aspects such as environment variables, service accounts and resource limits cannot be edited easily, but there are ways to do it:</li> <li>Editing the specification: \u25a0 Run <code>kubectl edit pod &lt;podname&gt;</code> and edit the appropriate features \u25a0 When saving to log the changes, if the feature cannot be edited for an existing pod, the changes will be denied \u25a0 A copy of the definition file with the changes will saved to a temporary location, which can be used to recreate the pod with the changes once the current version is deleted</li> <li>Extracting and editing the yaml definition file: \u25a0 Run <code>kubectl get pod &lt;podname&gt; -o yaml &gt; &lt;filename.yaml&gt;</code> \u25a0 Make the desired changes to the yaml file and delete the current version of the pod \u25a0 Create the pod again with the file</li> <li>When editing the deployment, any aspect of its underlying pods can be edited as the pod template is a child of the deployment spec</li> <li>When changes are made, the deployment will automatically delete and create new pods to apply the updates as appropriate</li> </ul>"},{"location":"certifications/CKA/03_Scheduling.html#38-daemonsets","title":"3.8 - DaemonSets","text":"<ul> <li>Daemonsets are similar in nature to replicasets, they provide assistance in the deployment of multiple instances of a pod</li> <li>Daemonsets run only one instance of the pod per node</li> <li>Whenever a new node is added, the pod is automatically added to the node and vice versa for when the node is removed</li> <li>Use cases of Daemonsets include monitoring and logging agents</li> <li>Removes the need for manually deploying one of these pods to any new nodes within the cluster</li> <li>Kubernetes components such as Kube-Proxy could be deployed as a Daemonset as one pod is required per cluster \u25a0 Similar network solutions could also be deployed as a Daemonset</li> <li>Daemonsets can be deployed via a definition file, it's similar in structure to that of a Replicaset, with the only difference being the Kind</li> </ul> <pre><code>apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: monitoring-daemon\nspec:\n  selector:\n    matchLabels:\n      app: monitoring-agent\n    template:\n      metadata:\n        labels:\n          app: monitoring-agent\n      spec:\n        containers:\n        - name: monitoring-agent\n          image: monitoring-agent\n</code></pre> <ul> <li>To view daemonsets, use the kubectl get daemonsets command</li> <li>You can view the details of the daemonset with the kubectl describe command i.e. <code>kubectl describe daemonset &lt;daemonset name&gt;</code></li> <li>Prior to Kubernetes v1.12, a Daemonset would work by manually setting the nodename for each pod to be allocated, thus bypassing the scheduler</li> <li>Post Kubernetes v1.12, the Daemonset uses the default scheduler and Node Affinity rules discussed previously to allocate the single pod to each node</li> </ul>"},{"location":"certifications/CKA/03_Scheduling.html#39-static-pods","title":"3.9 - Static Pods","text":"<ul> <li>Kubelet relies on the kube-apiserver for instructions on what pods to load on its respective node</li> <li>The instructions are determined by the kube scheduler which was stored in the etcd server</li> <li>Considerations must be made if any of the kube-api server, scheduler and etcd server are not present, as a worst case scenario, suppose none of them are available</li> <li>Kubelet is capable of managing a node independently to an extent</li> <li>The only thing kubelet knows to do is to create pods, however in this scenario there's no api server to feed it the instructions based on yaml definition files</li> <li>To work around this, you can configure the kubelet to read the pod definition files from a directory on the server designated to store information about pods</li> <li>Once configured, the Kubelet will periodically check the directory for any new files, where it reads the information and creates pods based on the information provided</li> <li>In addition to creating the pods, the kubelet would take actions to ensure the pod remains running, i.e.:</li> <li>if a pod crashes, kubelet will attempt to restart it</li> <li>if any changes are made to any files within the directory, the kubelet will recreate the pod to cause the changes to occur</li> <li>Pods created in this manner, without the intervention of the API server or any other aspects of a kubernetes cluster, are Static Pods</li> <li>Note: Only pods could be created in this manner, objects such as Deployments and Replicasets cannot be created in this manner</li> <li>To configure the \"Desginated Folder\" for the kubelet to look in for pod definition files, add the following option to the kubelet service file <code>kubelet.service</code>; note the directory could be any directory on the system: <code>--pod-manifest-path=/etc/kubernetes/manifests</code></li> <li>Alternatively, you could create a yaml file to specify the path the kubelet should look at, i.e. <code>staticPodPath: /etc/Kubernetes/manifests</code>, which can be referenced by adding the <code>--config=/path/to/yaml/file</code> to the service file</li> <li>Note this is the kubeadm approach</li> <li>Once static pods are created, they can be viewed by docker ps (can't use kubectl due to no api server)</li> <li>It should be noted that even if the api server is present, both static pods and traditional pods can be created</li> <li>The api server is made aware of the static pods because when the kubelet is part of a cluster and creates static pods, it creates a mirror object in the kube api server</li> <li>You can read details of the pod but cannot make changes via the kubectl edit command, only via the actual manifest</li> <li>Note: the name of the pod is automatically appended with the name of the node it's assigned to</li> <li>Because static pods are independent of the Kubernetes control plane, they can be used to deploy the control plane components themselves as pods on a node</li> <li>Install kubelet on all the master nodes</li> <li>Create pod definition files that use docker images of the various control plane components (api server, controller, etcd server etc)</li> <li>Place the definition files in the chosen manifests folder</li> <li>The pods will be deployed via the kubelet functionality</li> <li>Note: By doing this, you don't have to download the associated binaries, configure services or worry about services crashing</li> <li>In the event that any of these pods crash, they will automatically be restarted by Kubelet with them being a static pod</li> <li>Note: To delete a static pod, you have to delete the yaml file associated with it from the path configured</li> </ul>"},{"location":"certifications/CKA/03_Scheduling.html#static-pods-vs-daemonsets","title":"Static Pods vs Daemonsets","text":"Static Pods Daemonsets Created via Kubelet Created via Kube-API Server (Daemonset controller) Used to deploy control plane components as static pods Used to deploy monitoring and logging agents on nodes Ignored by Kube-Scheduler Ignored by Kube-Scheduler"},{"location":"certifications/CKA/03_Scheduling.html#310-multiple-schedulers","title":"3.10 - Multiple Schedulers","text":"<ul> <li>The default scheduler has its own algorithm that takes into accounts variables such as taints and tolerations and node affinity to distribute pods across nodes</li> <li>In the event that advanced conditions must be met for scheduling, such as placing particular components on specific nodes after performing a task, the default scheduler falls down</li> <li>To get around this, Kubernetes allows you to write your own scheduling algorithm to be deployed as the new default scheduler or an additional scheduler</li> <li>Via this, the default scheduler still runs for all usual purposes, but for the particular task, the new scheduler takes over</li> <li>You can have as many schedulers as you like for a cluster</li> <li>When creating a pod or deployment, you can specify Kubernetes to use a particular scheduler</li> <li>When downloading the binary for kube-scheduler, there is an option in he kube-scheduler.service file that can be configured; --scheduler-name</li> <li>Scheduler name is set to default-scheduler if not specified</li> <li>To deploy an additional scheduler, you can use the same kube-scheduler binary or use one built by yourself</li> <li>In either case, the two schedulers will run as their own services</li> <li>It goes without saying that the two schedulers should have separate names for differentiation purposes</li> <li>If a cluster has been created via the Kubeadm manner, schedulers are deployed via yaml definition files, which you can then use to create additional schedulers by copying the file</li> <li>Note: customise the scheduler name with the --scheduler-name flag</li> <li>Note: The leader-elect option should be used when you have multiple copies of the scheduler running on different master nodes</li> <li>Usually observed in a high-availability setup where there are multiple master nodes running the kube-scheduler process</li> <li>If multiple copies of the same scheduler are running on different nodes, only one can be active at a time</li> <li>The leader-elect option helps in choosing a leading scheduler for activities, to get multiple schedulers working, do the following: \u25a0 If you don't have multiple master nodes running, set it to false \u25a0 If you do have multiple masters, set an additional parameter to set a lock object name</li> <li>This differentiates the new custom scheduler from the default during the leader election process</li> <li>The custom scheduler can then be created using the definition file and deployed to the kube-system namespace</li> <li>From here, pods can be created and configured to be scheduled by a particular scheduler by adding the field schedulerName to its definition file</li> <li>Note: Any pods created in this manner to be scheduled by the custom scheduler will remain in a pending state if the scheduler wasn't configured correctly</li> <li>To confirm the correct scheduler picked the pod up, use <code>kubectl get events</code></li> <li>To view the logs associated with the scheduler, run: <code>kubectl logs &lt;scheduler name&gt; --namespace=kube-system</code></li> </ul>"},{"location":"certifications/CKA/03_Scheduling.html#311-configuring-scheduler-profiles","title":"3.11 - Configuring Scheduler Profiles","text":"<ul> <li>Schedulers can be configured manually or set up via kubeadm</li> <li>Additional schedulers can be created via yaml files, which can then be configured with naming and identifying the \"leader\" of the schedulers for high-availability setups</li> <li>Advanced options are available, but are outside the scope of the course</li> </ul>"},{"location":"certifications/CKA/04_Logging-and-Monitoring.html","title":"4.0 - Logging and Monitoring","text":""},{"location":"certifications/CKA/04_Logging-and-Monitoring.html#41-monitor-cluster-components","title":"4.1 - Monitor Cluster Components","text":"<ul> <li>Suppose we want to monitor performance metrics relating to resource consumption across a cluster or at a pod level, where we can analyse it</li> <li>There isn't any built-in solution for Kubernetes that satisfied this, but plenty of third-party options like Prometheus and Dynatrace are available</li> <li>Heapster was one of the first solutions for this, but is now deprecated</li> <li>Slimmed down version available via Metrics Server</li> <li>One metrics server allowed per cluster</li> <li>It's an in-memory solution, data isn't persisted to system storage</li> <li>To generate the metrics, a sub-component of the kublet, known as the container advisor (c-advisor) collects the metrics from the pods and exposes them via the Kubelet API</li> <li>If using minikube, the metrics-server can be deployed via: <code>minikube addons enable metrics-server</code></li> <li>For all other kubernetes setups, the metrics server is enabled via downloading and applying the yaml files from the associated Github repository</li> <li>To collect metrics about a particular object, run: <code>kubectl top &lt;object&gt;</code></li> <li>Objects that can be monitored include pods, nodes and more</li> </ul>"},{"location":"certifications/CKA/04_Logging-and-Monitoring.html#42-managing-application-logs","title":"4.2 - Managing Application Logs","text":"<ul> <li>When running a docker container in the background, one can view the associated logs of a container by running <code>docker logs -f &lt;container ID&gt;</code></li> <li>For kubernetes, run <code>kubectl logs -f &lt;pod name&gt;</code> for a standalone container</li> <li>For a pod with multiple containers, you must specify the container name you want to view, append this to the end of the above command</li> </ul>"},{"location":"certifications/CKA/05_Application-Lifecycle-Management.html","title":"5.0 - Application Lifecycle Management","text":""},{"location":"certifications/CKA/05_Application-Lifecycle-Management.html#51-rolling-updates-and-rollbacks","title":"5.1 - Rolling Updates and Rollbacks","text":""},{"location":"certifications/CKA/05_Application-Lifecycle-Management.html#rollout-and-versioning","title":"Rollout and Versioning","text":"<ul> <li>When first creating a deployment, a rollout is triggered</li> <li>Rollout is equivalent to a deployment revision in definition</li> <li>When future updates occur, a new rollout will occur creating a new deployment revision</li> <li>This functionality allows tracking of deployment changes</li> <li>Rollback functionality is therefore available in the event of application failure</li> </ul>"},{"location":"certifications/CKA/05_Application-Lifecycle-Management.html#rollout-commands","title":"Rollout Commands","text":"<ul> <li>To view rollout status:</li> <li><code>kubectl rollout status &lt;deployment name&gt;</code></li> <li>To view rollout history and versioning:</li> <li><code>kubectl rollout history &lt;deployment-name&gt;</code></li> </ul>"},{"location":"certifications/CKA/05_Application-Lifecycle-Management.html#deployment-strategy","title":"Deployment Strategy","text":"<ul> <li>There are multiple deployment strategies available, the two main versions are:</li> <li>Recreate:<ul> <li>When a new version of an application is ready, tear down all instances currently deployed at once</li> <li>Deploy new versions once \"current\" version is unavailable</li> <li>Results in significant user downtime</li> </ul> </li> <li>Rolling Updates:<ul> <li>Destroys current instance and upgrades with a new version one after another (take down one old version, upload a new version, repeat)</li> <li>Leads to higher availability of the application</li> <li>Upgrade appears \"seamless\"</li> </ul> </li> <li>To update a deployment, simply make the changes to the definition file and run <code>kubectl apply -f &lt;deployment-definiton-file&gt;.yaml</code></li> <li>This triggers a new rollout</li> <li>It should be noted, you could also update the deployment via the CLI only, for example, updating a deployment's image:</li> <li><code>kubectl set image deployment &lt;deployment-name&gt; &lt;image&gt;=&lt;image&gt;:&lt;tag&gt;</code><ul> <li>This method doesn't update the YAML file associated with the deployment</li> </ul> </li> <li>You can view deployment strategies in detail via the <code>kubectl describe deployment &lt;deployment-name&gt;</code></li> <li>For the recreate strategy, it can be seen that during the upgrade process the deployment is scaled from maximum size, to zero, then back again</li> <li>For rolling updates, the pods are scaled individually, one old pod removed, one new pod added, and so on.</li> <li>When a new deployment is created, a new replicaset is automatically created, hosting the desired number of replica pods</li> <li>During an upgrade a new replica set is created</li> <li>New pods with new application added sequentially</li> <li>At the same time, the new old pods are sequentially taken down</li> <li>Once upgraded, if a rollback is required, run: <code>kubectl rollout undo &lt;deployment&gt;</code></li> <li>The command <code>kubectl run &lt;deployment&gt; --image=&lt;image&gt;</code> will create a deployment</li> <li>Serves as an alternative to using a definition file</li> <li>Required replicasets and pods automatically created in the backend</li> <li>It's still highly recommended to use a definition file for editing and versioning deployments</li> <li>Command Summary:</li> <li>To create a deployment from a yaml file:<ul> <li><code>kubectl create -f &lt;deployment.yaml&gt;</code></li> </ul> </li> <li>To get a list of all deployments<ul> <li>kubectl get deployments</li> </ul> </li> <li>To update a deployment, run one of the following two:<ul> <li><code>kubectl apply -f &lt;deployment.yaml&gt;</code></li> <li><code>kubectl set image &lt;deployment&gt; &lt;image ID&gt;=&lt;image&gt;:&lt;tag&gt;</code></li> </ul> </li> <li>To get the status of a deployment rollout:<ul> <li><code>kubectl rollout status &lt;deployment&gt;</code></li> </ul> </li> <li>To view the rollout history:<ul> <li><code>kubectl rollout history &lt;deployment&gt;</code></li> </ul> </li> <li>To rollback:<ul> <li><code>kubectl rollout undo &lt;deployment&gt;</code></li> </ul> </li> </ul>"},{"location":"certifications/CKA/05_Application-Lifecycle-Management.html#52-commands-and-arguments-docker","title":"5.2 - Commands and Arguments: Docker","text":"<ul> <li>Note: This isn't a requirement for the CKAD/CKA curriculum</li> <li>Consider a simple scenario:</li> <li>Run an ubuntu image with Docker: <code>docker run ubuntu</code></li> <li>This runs an instance of the Ubuntu image and exits immediately</li> <li>If <code>docker ps -a</code> is ran to list the containers, it won't appear</li> <li>Due to the fact that containers aren't designed to host an OS</li> <li>They're instead designed to run a specific task/process e.g. host a web server</li> <li>The container will exist as long as the hosted process is active, if the service is stopped or crashes, the container exits</li> <li>The Dockerfile's CMD section was set as <code>\"bash\"</code></li> <li>This isn't a command, but a CLI instead</li> <li>When the container ran, Docker created a container based on the Ubuntu image and launched bash</li> <li>Note: By default, Docker doesn't attach a terminal to a container when it's ran</li> <li>Bash cannot find a terminal</li> <li>Container exits as the process is finished</li> <li>To solve a situation like this, you can add container commands to the docker run command, e.g.</li> <li><code>docker run ubuntu sleep 5</code></li> <li>These changes can be made permanent via editing the Docker file either in a:</li> <li>Shell format: <code>CMD command param1</code></li> <li>JSON format: <code>CMD [\"command\", \"param1\"]</code></li> <li>To build the new image, run: <code>docker build -t image_name .</code></li> <li>Run the new image via <code>docker run &lt;image_name&gt;</code></li> <li>To use a command but with a different value of parameter to change each time, change the <code>CMD</code> to <code>\"ENTRYPOINT\"</code> i.e. <code>ENTRYPOINT [\"command\"]</code></li> <li>Any parameters specified on the CLI will automatically be appended to the entrypoint command</li> <li>If using entrypoint and a command parameter isn't specified, an error is likely</li> <li>A default value should be specified</li> <li>To overcome the problem, use a combination of entrypoints and command in a JSON format i.e.:</li> <li><code>ENTRYPOINT [\"command\"]</code></li> <li><code>CMD [\"parameter\"]</code></li> <li>From this configuration, if no additional parameter is provided, the CMD parameter will be applied</li> <li>Any parameter on the CLI will override the CMD parameter</li> <li>To override the entrypoint command, run:</li> <li><code>docker run --entrypoint &lt;new command&gt; &lt;image name&gt;</code></li> </ul>"},{"location":"certifications/CKA/05_Application-Lifecycle-Management.html#53-commands-and-arguments-kubernetes","title":"5.3 - Commands and Arguments: Kubernetes","text":"<ul> <li>Using the previously defined image, one can create a yaml definition file:</li> </ul> <pre><code>apiVersion: v1\nKind: Pod\nmetadata:\n  name: pod-name\nspec:\n  containers:\n    name: container-name\n    image: container-image\n    command: [\"command\"]\n    args: [\"10\"]\n</code></pre> <ul> <li>To add anything to be appended to the docker run command for the container, add args to the container spec</li> <li>Pod may then be created using the <code>kubectl create -f</code> command as per</li> <li>To override entrypoint commands, add \"command\" field to the pod spec</li> <li>To summarise, in Kubernetes Pod Specs:</li> <li>command overrides Dockerfile entrypoint commands</li> <li>args override Dockerfile CMD commands</li> <li>Note: You cannot edit specifications of a preexisting pod aside from:</li> <li>Containers</li> <li>Initcontainers</li> <li>activeDeadlineSeconds</li> <li>Tolerations</li> </ul>"},{"location":"certifications/CKA/05_Application-Lifecycle-Management.html#54-configure-environment-variables-in-applications","title":"5.4 - Configure Environment Variables in Applications","text":"<ul> <li>For a given definition file, one can set environment variable via the env field under containers in a pod's spec</li> <li>Each environment variable is an array, so each one has its own name, value and is denoted by a - prior to the name field</li> </ul> <pre><code>env:\n- name: APP_COLOR\n  value: blue\n</code></pre> <ul> <li>Environment variables can also be referenced via two other methods:</li> <li>Configmaps: rather than env, replace with <code>valueFrom</code>, and add <code>configMapKeyRef</code> underneath</li> <li>Secrets: rather than <code>env</code>, replace with <code>valueFrom</code>, and add <code>secretKeyRef</code> underneath</li> </ul>"},{"location":"certifications/CKA/05_Application-Lifecycle-Management.html#55-configure-configmaps-in-applications","title":"5.5 - Configure ConfigMaps in Applications","text":"<ul> <li>When there are multiple pod definition files, it becomes difficult to manage environmental data</li> <li>This can info can be removed from the definition files and managed centrally via ConfigMaps</li> <li>ConfigMaps used to pass configuration data as key-value pairs in Kubernetes</li> <li>When a pod is created, the configmap's data can be injected into the pod, making the kvps available as environmental variables for the application within the container</li> <li>Configuring the ConfigMap involves 2 phases:</li> <li>Create the ConfigMap</li> <li>Inject it to the Pod</li> <li>To create, can run either:</li> <li><code>kubectl create configmap &lt;configmap name&gt;</code></li> <li><code>kubectl create -f &lt;configmap-definition&gt;.yaml</code></li> <li>By using the first command specified above, you can immediately create key-value pairs:</li> </ul> <pre><code>kubectl create configmap &lt;configmap-name&gt; --from-literal=&lt;key&gt;=&lt;value&gt; ... --from-literal=&lt;key&gt;=&lt;value&gt;\n</code></pre>"},{"location":"certifications/CKA/05_Application-Lifecycle-Management.html#56-configure-secrets-in-applications","title":"5.6 - Configure Secrets in Applications","text":"<ul> <li>Considering a simple python server:</li> <li>The hostname username and passwords are hardcoded in bad practice =&gt; high security risk.</li> <li>It would be better to store this data as a ConfigMap based on previous discussion - the problem though is that ConfigMap data is stored in a plaintext format.</li> <li> <p>Not applicable for sensitive info like passwords</p> </li> <li> <p>Variables like username and passwords are better stored as <code>secrets</code> in Kubernetes.</p> </li> <li> <p>These are similar to ConfigMaps, but the values are stored in encrypted format.</p> </li> <li> <p>Analagous to ConfigMaps, there are 2 steps:</p> </li> <li>Secret Creation</li> <li> <p>Inject secrets to a pod.</p> </li> <li> <p>Secret creation is achieved either imperatively or declaratively:</p> </li> <li>Declarative: Use a YAML definition file to \"declare\" the desired configuration</li> <li>Imperative: Use the <code>kubectl create secret</code> command to \"imply\" Kubernetes to create a secret, and let Kubernetes figure out / guestimate the configuration desired.</li> </ul>"},{"location":"certifications/CKA/05_Application-Lifecycle-Management.html#imperative-secret-creation","title":"Imperative Secret Creation","text":"<ul> <li> <p><code>kubectl create secret generic &lt;secret name&gt; --from-literal=&lt;key&gt;=&lt;value&gt;</code></p> </li> <li> <p>As with ConfigMaps, data can be specified from the CLI in key-value-pairs via the <code>--from-literal</code> flag multiple times.</p> </li> <li> <p>Example: <code>kubectl create secret generic app-secret --from-literal=DB_HOST=mysql --from-literal=DB_USER=root --from-literal=DB_PASSWORD=password</code></p> </li> <li> <p>For larger amounts of secrets, the data can be imported from a file, achieved by using the <code>--from-file</code> flag.</p> </li> <li> <p>Example: <code>kubectl create secret generic app-secret --from-file=app-secret.properties</code></p> </li> </ul>"},{"location":"certifications/CKA/05_Application-Lifecycle-Management.html#declarative-secret-creation","title":"Declarative Secret Creation","text":"<ul> <li>Using a YAML definition file:</li> </ul> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: app-secret\ndata:\n  DB_HOST: mysql\n  DB_USER: root\n  DB_PASSWORD: password\n</code></pre> <ul> <li>As discussed, the secrets should not be stored in plaintext string format. Typically, Kubernetes secrets are stored in Base64 encrypted format.</li> <li>To convert: <code>echo -n '&lt;secret plaintext value&gt;' | base64</code></li> <li>Create the secret via <code>kubectl create -f ....</code> as normal</li> <li>Secrets can be viewed via: <code>kubectl get secrets</code></li> <li> <p>Detailed information viewed via: <code>kubectl describe secrets &lt;secret name&gt;</code></p> </li> <li> <p>To view secret in more detail: <code>kubectl get secret &lt;secret name&gt; -o yaml</code></p> </li> <li> <p>To decode secret: <code>echo -n '&lt;secret base64 value&gt;' | base64 --decode</code></p> </li> </ul>"},{"location":"certifications/CKA/05_Application-Lifecycle-Management.html#secrets-in-pods","title":"Secrets in Pods","text":"<ul> <li>With both a pod and secret YAML file, the secret data can be injected as environment variables:</li> </ul> <pre><code>spec:\n  containers:\n  - envFrom:\n    - secretRef:\n        name: &lt;secret name&gt;\n</code></pre> <ul> <li> <p>When <code>kubectl create -f ...</code> is run, the secret data is available as environment variables in the pod.</p> </li> <li> <p>As before, one can inject secrets to pods via environment variables (above) OR a single environment variable (below):</p> </li> </ul> <pre><code>spec:\n  containers:\n  - env:\n    - name: DB_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: app-secret\n          key: DB_PASSWORD\n</code></pre>"},{"location":"certifications/CKA/05_Application-Lifecycle-Management.html#secrets-in-volumes","title":"Secrets in Volumes","text":"<ul> <li>Secrets can also be added as volumes attached to pods:</li> </ul> <pre><code>volumes:\n- name: app-secret-volume\n  secret:\n    secretName: app-secret\n</code></pre> <ul> <li>If mounting the secret as a volume, each attribute in the secret is created as a file, with the value being the content.</li> </ul>"},{"location":"certifications/CKA/05_Application-Lifecycle-Management.html#57-scale-applications","title":"5.7 - Scale Applications","text":"<p>Refer to Rolling Updates and Rollbacks - Section 5.1</p>"},{"location":"certifications/CKA/05_Application-Lifecycle-Management.html#58-multi-container-pods","title":"5.8 - Multi-Container Pods","text":"<ul> <li>Multiple patterns are availablem such as:</li> <li>Ambassador</li> <li>Adapter</li> <li> <p>Sidecar</p> </li> <li> <p>In general, it's advised to decouple a monolithic (single-tiered) application into a series of smaller components -&gt; microservices</p> </li> <li> <p>Allows ability to independently develop and deploy sets of small reusable code</p> </li> <li> <p>Allows easier scalability and independent modification.</p> </li> <li> <p>In some cases, may need services to interact with one another, whilst still being identifiable as separate services</p> </li> <li>Example: web server and logging agent</li> <li> <p>1 agent service would be required per web server, not merging them together.</p> </li> <li> <p>Only the 2 functionalities (or more) need to work together that can be scaled as required:</p> </li> <li> <p>Multi-container pods required</p> </li> <li> <p>Multi-container pods contain multiple containers running different services, sharing aspects such asL</p> </li> <li>Network: Can refer to each other via localhost</li> <li>Storage: No need for additional volume setup / integration</li> <li> <p>Lifecycle: Resources are created and destroyed together</p> </li> <li> <p>To create a multi-container pod, add the additional container details to the pod's spec in a similar manner to the following:</p> </li> </ul> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: simple-webapp\n  labels:\n    name: simple-webapp\nspec:\n  containers:\n  - name: simple-webapp\n    image: simple-webapp\n    ports:\n    - containerPort: 8080\n\n  - name: log-agent\n    image: log-agent\n</code></pre>"},{"location":"certifications/CKA/05_Application-Lifecycle-Management.html#59-multi-container-pod-design-patterns","title":"5.9 - Multi-Container Pod Design Patterns","text":"<ul> <li>3 Design patterns available:</li> <li>Sidecar</li> <li>Adapter</li> <li>Ambassador</li> </ul>"},{"location":"certifications/CKA/05_Application-Lifecycle-Management.html#sidecar","title":"Sidecar","text":"<ul> <li>The most common design pattern</li> <li>Uses a \"helper\" container to assist or improve the functionalut of a primary container</li> <li>Example: Log agent with a web server</li> </ul>"},{"location":"certifications/CKA/05_Application-Lifecycle-Management.html#adapter","title":"Adapter","text":"<ul> <li>Used to assist in standardising communications between resources</li> <li>Processes that transmit data e.g. logs will be formatted in the same manner</li> <li>All data stored in centralised location</li> </ul>"},{"location":"certifications/CKA/05_Application-Lifecycle-Management.html#ambassador","title":"Ambassador","text":"<ul> <li>Responsible for handling proxy for other parts of the system or services</li> <li>Used when wanting microservices to interact with one another</li> <li>Services to be identified by name only via service discovery such as DNS or at an application level.</li> </ul> <ul> <li>Whilst the design patterns differ, their implementation is the same, adding containers to the pod definition file spec where required,</li> </ul>"},{"location":"certifications/CKA/05_Application-Lifecycle-Management.html#510-initcontainers","title":"5.10 - InitContainers","text":"<ul> <li>In multi-container pods, each container will run a process that stays alive for the duration of the pod's lifecycle</li> <li>Example: Consider a multi-container pod that runs a web application in one container, and a logging agent in another</li> <li>Both containers are expected to stay running at all times (the log agent runs as long as the web application is running)</li> <li>If either fails, the pod stops</li> <li>In some scenarios, would want to run a process that runs to completion in a container e.g. a process that pulls a code or binary from a repository to be used by the main application</li> <li>Processes like this are to be ran only one time when the pod is first created, or to wait for an external service or database to be up and running before the application starts</li> <li>Containers like this are initContainers</li> <li>InitContainers are configured in the exact same way as regular containers in a pod's spec, they are just a separate section to containers</li> <li>When the POD is first created, the initContainer runs its progress to completion before the main application starts</li> <li>Multiple initContainers can be configured in a similar manner to multi-container pods</li> <li>If this is the case, each initContainer will run one at a time</li> <li>In the event any of the initContainers fail, the pod will repeatedly restart until they all succeed</li> </ul>"},{"location":"certifications/CKA/05_Application-Lifecycle-Management.html#511-self-healing-applications","title":"5.11 - Self-Healing Applications","text":"<ul> <li>Self-healing applications are supported through the use of ReplicaSets and Replication Controllers</li> <li>ReplicationControllers help in ensuring a pod is recreated automatically when the application within crashes; thus ensuring enough replicas of the application are running at all times</li> <li>Additional support to check the health of applications running within pods and take necessary actions when they're unhealthy, this is done through Readiness and Liveness Probes.</li> <li>For more information, see section 5.0-5.2 of the written CKAD notes</li> </ul>"},{"location":"certifications/CKA/06_Cluster-Maintenance.html","title":"6.0 - Cluster Maintenance","text":""},{"location":"certifications/CKA/06_Cluster-Maintenance.html#61-os-upgrades","title":"6.1 - OS Upgrades","text":"<ul> <li>Suppose you have a cluster with a few nodes and pods serving applications; what happens if one of these nodes goes down?</li> <li>Associated pods are rendered inaccessible</li> <li>Depending on the deployment method of these PODs, users may be impacted</li> <li>If multiple replicas of the pod are spread across the cluster, users are uninterrupted as it's still accessible</li> <li>Any pods running ONLY on that node however will experience downtime</li> <li>Kubernetes will automatically try and restart the node</li> <li>If it comes back on immediately, kubectl restarts and the pods restart</li> <li>If after 5 mins and it's not back online, Kubernetes considers the pods as dead and terminates them from the node<ul> <li>If part of a replicaset, the pods will be recreated on other nodes</li> </ul> </li> <li>The time it takes for a pod to come back online is the pod eviction timeout</li> <li>Can be set on the controller manager via: <code>kube-controller-manager --pod-eviction-timeout=xmys</code><ul> <li>X,y = integer values</li> </ul> </li> <li>If the node comes back online after the timeout it restarts as a blank node, any pods that were on it and not part of a replicaset will remain \"gone\"</li> <li>Therefore, if maintenance is required on a node that is likely to come back within 5 minutes, and workloads on it are also available on other nodes, it's fine for it to be temporarily taken down for upgrades</li> <li>There is no guarantee that it'll reboot within the 5 minutes</li> <li>Nodes can be \"drained\", a process where they are gracefully terminated and deployed on other nodes</li> <li>Done so via: <code>kubectl drain &lt;node name&gt;</code></li> <li>Node cordoned and made unschedulable</li> <li>To uncordon node: <code>kubectl uncordon &lt;nodename&gt;</code></li> <li>To mark the node as unschedulable, run: <code>kubectl cordon &lt;nodename&gt;</code></li> <li>Doesn't terminate any preexisting pods, just stops any more from being scheduled</li> <li>Note: May need the flag <code>--ignore-daemonsets</code> and or <code>--force</code></li> </ul>"},{"location":"certifications/CKA/06_Cluster-Maintenance.html#62-kubernetes-software-versions","title":"6.2 - Kubernetes Software Versions","text":"<ul> <li>When installing a kubernetes cluster, a specific version of kubernetes is installed alongside</li> <li>Can be viewed via <code>kubectl get nodes</code> in the version column</li> <li>Release versions follow the process major.minor.patch</li> <li>Kubernetes is regularly updated with new minor versions every couple of months</li> <li>Alpha and beta versions also available</li> <li>Alpha - Features disabled by default, likely to be buggy</li> <li>Beta - Code tested, new features enabled by default</li> <li>Stable release - Code tested, bugs fixed</li> <li>Kubernetes releases found in a tarball file in Github; contains all required executables of the same version</li> <li>Note: Some components within the control plane will not have the same version numbers and are released as separate files; ETCD cluster and CoreDNS servers being the main examples</li> </ul>"},{"location":"certifications/CKA/06_Cluster-Maintenance.html#63-cluster-upgrade-process","title":"6.3 - Cluster Upgrade Process","text":"<ul> <li>The kubernetes components don't all have to be at the same versions</li> <li>No component should be at a version higher than the kube-api server</li> <li>If Kube-API Server is version X (a minor release), then the following ranges apply for the other components for support level:<ul> <li>Controller manager: X-1</li> <li>Kube-Scheduler: X-1</li> <li>Kubelet: X-2</li> <li>Kube-Proxy: X-2</li> <li>Kubectl: X-1 - X+1</li> </ul> </li> <li>At any point, Kubernetes only supports the 3 most recent minor releases e.g. 1.19 - 1.17</li> <li>It's better to upgrade iteratively over minor processes e.g. 1.17 - 1.18 and so on</li> <li>Upgrade process = Cluster-Dependent</li> <li>If on a cloud provider, built-in functionality available</li> <li>If on kubeadm/manually created cluster, must use commands:</li> <li><code>kubeadm upgrade plan</code></li> <li><code>kubeadm upgrade apply</code></li> <li>Cluster upgrades involve two steps:</li> <li>Upgrade the master node<ul> <li>All management components go down temporarily during the processes</li> <li>Doesn't impact the current node workloads (only if you try to do anything with them)</li> </ul> </li> <li>Upgrade the worker nodes<ul> <li>Can be done all at once - Results in downtime</li> <li>Can be done iteratively - Minimal downtime by draining nodes as they get upgraded one after another</li> <li>Could also add new nodes with the most recent software versions</li> </ul> </li> <li>Proves especially inconvenient when on a cloud provider</li> <li>Upgrading via Kubeadm:</li> <li><code>kubeadm upgrade plan</code><ul> <li>Lists latest versions available</li> <li>Components that must be upgraded manually</li> <li>Command to upgrade kubeadm</li> </ul> </li> <li>Note: kubeadm itself must be upgraded first: <code>apt-get upgrade -y kubeadm=major.minor-patch_min-patch_max</code></li> <li>Check upgrade success based on CLI output and kubectl get nodes</li> <li>If Kubelet is running on Master node, this must be upgraded next the master node and restart the service:</li> <li><code>apt-get upgrade -y kubelet=1.12.0-00</code></li> <li><code>systemctl restart kubelet</code></li> <li>Upgrading the worker nodes:</li> <li>Use the drain command to stop and transfer the current workloads to other nodes, then upgrade the following for each node (ssh into each one):<ul> <li>Kubeadm</li> <li>Kubelet</li> <li>Node config: <code>kubeadm upgrade node config --kubelet-version major.minor.patch</code></li> </ul> </li> <li>Restart the service: <code>systemctl restart kubelet</code></li> <li>Make sure to uncordon each node after each upgrade!</li> </ul>"},{"location":"certifications/CKA/06_Cluster-Maintenance.html#64-backup-and-restore-methods","title":"6.4 - Backup and Restore Methods","text":"<ul> <li>It's good practice to save resource configuration definition files</li> <li>Kube-api server can be used to query all resources to get yaml files for each</li> <li>E.g. <code>kubectl get all --all-namespaces -o yaml &gt; filename.yaml</code></li> <li>The etcd cluster stores information about the state of the cluster e.g. what nodes are on it and what applications are they running</li> <li>When configuring the etcd, you can configure the data directory for the etcd data store via the <code>--data-dir</code> flag</li> <li>You can take a snapshot of the etcd database using the etcdctl utility</li> <li>To restore the cluster from the backup:</li> <li>Service kube-apiserver stop</li> <li><code>etcdctl snapshot restore snapshot.db --options</code></li> <li>New data store directory created</li> <li>The etcd service file must then be reconfigured for the new cluster token and data directory</li> <li>Reload the daemon and restart the service</li> <li>Backup candidates:</li> <li>Kube-API Server query - Generally the more common method</li> <li>ETCD Server</li> </ul>"},{"location":"certifications/CKA/06_Cluster-Maintenance.html#disaster-recovery-with-etcd-in-kubernetes","title":"Disaster Recovery with ETCD in Kubernetes","text":"<p>Assuming ETCDCTL is installed, use it to take a snapshot, make sure to specify the flags, which can all be found via examining the etcd pod and ARE MANDATORY for authentication:</p> <pre><code>ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379\n--cacert=/etc/kubernetes/pki/etcd/ca.crt \\\n--cert=/etc/kubernetes/pki/etcd/server.crt\n--key=/etc/kubernetes/pki/etcd/server.key \\\nsnapshot save PATH/TO/BACKUP/BACKUP.db\n</code></pre> <ul> <li>Suppose disaster happens:</li> <li>Restore the snapshot to a new folder:</li> </ul> <pre><code>ETCDCTL_API=3 etcdctl --data-dir /var/lib/etcd-from-backup \\\nsnapshot restore PATH/TO/BACKUP/BACKUP.db\n</code></pre> <ul> <li>Update the etcd pod's volume hostpath and mount paths for etcd-data to be <code>/var/lib/etcd-from-backup</code> etc as appropriate by updating the yaml file at <code>/etc/kubernetes/manifests/etcd.yaml</code></li> <li>The etcd pod should automatically restart once this update is done, bringing back the pods stored in the backup along with it. (Use <code>watch \"docker ps | grep etcd\"</code> to track)</li> </ul>"},{"location":"certifications/CKA/06_Cluster-Maintenance.html#working-with-etcdctl","title":"Working with ETCDCTL","text":"<ul> <li>For backup and restore purposes, make sure to set the ETCDCTL API to 3: <code>export ETCDCTL_API=3</code></li> <li>For taking a snapshot of the etcd cluster: etcdctl snapshot save -h and keep a note of the mandatory global options.</li> <li>For a TLS-Enabled ETCD Database, the following are mandatory:</li> <li><code>--cacert</code></li> <li><code>--cert</code></li> <li><code>--endpoints[IP:PORT]</code></li> <li><code>--key</code></li> <li>Use the snapshot restore option for backup: <code>etcdctl snapshot restore -h</code></li> <li>Note options available and apply as appropriate</li> </ul>"},{"location":"certifications/CKA/07_Security.html","title":"7.0 - Security","text":""},{"location":"certifications/CKA/07_Security.html#71-kubernetes-security-primitives","title":"7.1 - Kubernetes Security Primitives","text":"<ul> <li>Controlling access to API Server is the top priority</li> <li>Need to define who can access the API Server through and what they can do</li> <li>Could use any of:</li> <li>Files</li> <li>Certificates</li> <li>External authentication providers</li> <li>Service Accounts</li> <li>By default, all pods within a cluster can access one another</li> <li>This can be restricted via the introduction of network policies</li> </ul>"},{"location":"certifications/CKA/07_Security.html#72-authentication","title":"7.2 - Authentication","text":"<ul> <li>Authentication Mechanisms Available in Kubernetes for the API Server:</li> <li>Static Password File</li> <li>Static Token File</li> <li>Certificates</li> <li>Identity Services e.g. <code>LDAP</code></li> <li>Suppose you have the user details in a file, you can pass this file as an option for authentication in the kube-apiserver.service file adding the flag: <code>--basic-auth-file=user-details.csv</code></li> <li>Restart the service after this change is done</li> <li>If the cluster is setup using Kubeadm, edit the yaml file and add the same option</li> <li>Kubernetes will automatically update the apiserver once the change is made</li> <li>To authenticate using the credentials specified in this manner run a curl command similar to: <code>curl -v -k https://master-node-ip:6443/api/v1/pods -u \"username:password\"</code></li> <li>Could also have a token file, specify using <code>--token-auth-file=user-details.csv</code></li> <li>Note: These are not recommended authentication mechanisms</li> <li>Should consider a volume mount when providing the auth file in a kubeadm setup</li> <li>Setup RBAC for new users</li> <li>Could also setup RBAC using YAML files to create rolebindings for each user</li> </ul>"},{"location":"certifications/CKA/07_Security.html#73-tls-basics","title":"7.3 - TLS Basics","text":"<ul> <li>Certificates used to guarantee trust between two parties communicating with one another, leading to a secure and encrypted connection</li> <li>Data involved in transmission should be encrypted via the use of encryption keys</li> <li>Encryption Methods:</li> <li>Symmetric: Same key used for encryption and decryption</li> <li>Asymmetric encryption: A public and private key are used for encryption and decryption specifically \u25a0 Private key can only be used for decryption</li> <li>SSH Assymetric Encryption: <code>run ssh-keygen</code></li> <li>Creates <code>id_rsa</code> and <code>id_rsa.pub</code> (public and private keys)</li> <li>Servers can be secured by adding public key to authorized key file at <code>~/.ssh/authorized_keys</code></li> <li>Access to the server is then allowed via ssh -i id_rsa username@server</li> <li>For the same user, can copy the public key to any other servers</li> <li>To securely transfer the key to the server, use asymmetric encryption</li> <li>Can generate keys with: <code>openssl genrsa -out &lt;name&gt;.key 1024</code></li> <li>Can create public variant with: <code>openssl rsa -in &lt;name&gt;.key -pubout &gt; &lt;name&gt;.pem</code></li> <li>When the user first accesses the web server via HTTPS, they get the public key from the server</li> <li>Hacker also gets a copy of it</li> <li>The users browser encrypted the symmetric key using the public key</li> <li>Hacker gets copy</li> <li>Server uses private key to decrypt symmetric key</li> <li>Hacker doesn't have access to the private key, and therefore cannot encrypt it.</li> <li>For the hacker to gain access, they would have to create a similar website and route your requests</li> <li>As part of this, the hacker would have to create a certificate</li> <li>In general, certificates must be signed by a proper authority</li> <li>Any fake certificates made by hackers must be self-signed \u25a0 Web browsers have built-in functionalities to verify if a connection is secure</li> <li>To ensure certificates are valid, the Certificate Authorities (CAs) must sign and validate the certs.</li> <li>Examples: Symanteg, Digicert</li> <li>To validate a certificate, one must first generate a certificate validation request to be sent to the CA: <code>openssl req -new -key &lt;name&gt;.key -out &lt;&lt;name&gt;.csr -subk \"/C=US/ST=CA/O=MyOrg, Inc./CN=mybank.com\"</code></li> <li>CAs have a variety of techniques to ensure that the domain is owned by you</li> <li>How does the browser know what certificates are valid? CAs have a series of public and private keys built in to the web browser, the public key is then used for communication between the browser and CA to validate the certificates</li> <li>Note: The above are described for public domain websites</li> <li>For private websites, such as internal organisation websites, private CAs are generally required and can be installed on all instances of the web browser within the organisation</li> <li>Note:</li> <li>Certificates with a public key are named with the extension .crt or .pem, with the prefix of whatever it is being communicated with</li> <li>Private keys will have the extension of either <code>.key</code> or <code>-key.pem</code></li> </ul>"},{"location":"certifications/CKA/07_Security.html#74-tls-in-kubernetes","title":"7.4 - TLS In Kubernetes","text":"<ul> <li>In the previous section, three types of certificates were discussed, for the purposes of discussing them in Kubernetes, how they're referred to will change:</li> <li>Public and Private Keys used to secure connectivity between the likes of web browsers and servers: Server Certificates</li> <li>Certificate Authority Public and Private Keys for signing and validating certificates: Root Certificates</li> <li>Servers can request a client to verify themselves: Client Certificates</li> <li>Note:</li> <li>Certificates with a public key are named with the extension <code>.crt</code> or <code>.pem</code>, with the prefix of whatever it is being communicated with</li> <li>Private keys will have the extension of either <code>.key</code> or <code>-key.pem</code></li> <li>All communication within a Kubernetes cluster must be secure, be it pods interacting with one another, services with their associated clients, or accessing the APIServer using the Kubectl utility</li> <li>Secure TLS communication is a requirement</li> <li>Therefore, it is required that the following are implemented:</li> <li>Server Certificates for Servers</li> <li>Client Certificates for Clients</li> </ul>"},{"location":"certifications/CKA/07_Security.html#server-components","title":"Server Components","text":"<ul> <li>Kube-API Server \u25a0 Exposes an HTTPS service that other components and external users use to manage the Kubernetes cluster \u25a0 Requires certificates and a key pair</li> <li>apiserver.crt and apiserver.key</li> <li>ETCD Server \u25a0 Stores all information about the cluster \u25a0 Requires a certificate and key pair</li> <li>Etcdserver.crt and apiserver.key</li> <li>Kubelet server: \u25a0 Exposes HHTPS API Endpoint that the Kube-API Server uses to interact with the worker nodes</li> <li>Client Certificates:</li> <li>All of the following require access to the Kube-API Server</li> <li>Admin user \u25a0 Requires certificate and key pair to authenticate to the API Server \u25a0 admin.crt and admin.key</li> <li>Scheduler \u25a0 Client to the Kube-APIServer for object scheduling pods etc</li> <li>scheduler.crt and scheduler.key</li> <li>Kube-Controller: \u25a0 controller-manager.crt and controller-manager.key</li> <li>Kube-Proxy \u25a0 kube-proxy.crt and kube-proxy.key</li> <li>Note: The API Server is the only component that communicates with the ETCD server, which views the API server as a client</li> <li>The API server can use the same keys as before for serving itself as a service OR a new pair of certificates can be generated specifically for ETCD Server Authentication</li> <li>The same principle applies for the API Server connecting to the Kubelet service</li> <li>To verify the certificates, a CA is required. Kubernetes requires at least 1 CA to be present; which has its own certificate and key (ca.crt and ca.key)</li> </ul>"},{"location":"certifications/CKA/07_Security.html#75-tls-in-kubernetes-certificate-creation","title":"7.5 - TLS in Kubernetes: Certificate Creation","text":"<ul> <li>Tools available for certificate creation include:</li> <li>EASYRCA</li> <li>OPENSSL - The more common one</li> <li>CFSSL</li> <li>Steps - Server Certificates: CA Example</li> <li>Generate the keys: <code>openssl genrsa -out ca.key 2048</code> \u25a0 The number \"2048\" in the above command indicates the size of the private key. You can choose one of five sizes: 512, 758, 1024, 1536 or 2048 (these numbers represent bits). The larger sizes offer greater security, but this is offset by a penalty in CPU performance. We recommend the best practice size of 1024.</li> <li>Generate certificate signing request: <code>openssl req -new -key ca.key -subj \"/CN=KUBERNETES-CA\" -out ca.csr</code></li> <li>Sign certificates: <code>openssl x509 -req -in ca.csr -signkey ca.key -out ca.crt</code></li> <li>Client Certificate Generation Steps: Admin User Example</li> <li>Generate the keys: <code>openssl genrsa -out admin.key 2048</code></li> <li>Generate certificate signing request: <code>openssl req -new -key admin.key -subj \"/CN=kube-admin\" -out admin.csr</code></li> <li>Sign the certificate: <code>openssl x509 -req -in admin.csr -CAkey ca.key -out admin.crt</code> \u25a0 The CA key pair is used for signing the new certificate, thus proving its validity</li> <li>When the admin user attempts to authenticate to the cluster, it is the certificate admin.crt that will be used for this</li> <li>For non-admin users, need to add group details to the certificate signing request to signal this.</li> <li>Group called <code>SYSTEM:MASTERS</code> has administrative privileges, to specify this for an admin user, the signing request should be like: <code>openssl req -new -key admin.key -subj \"/CN=kube-admin/O=system:masters\" -out admin.csr</code></li> <li>The same procedure would be followed for client certificates for the Scheduler, Controller-Manager and Kubelet</li> <li>The certificates generated could be applied in different scenarios:</li> <li>Could use the certificate instead of usernames and password in a REST API call to the api server (via a curl request). \u25a0 To do so, specify the key and the certs involved as options following the request e.g. <code>--key admin.key</code> <code>--cert admin.crt</code> and <code>--cacert ca.crt</code></li> <li>Alternatively, all the parameters could be moved to the kube config yaml file, which acts as a centralized location to reference the certificates</li> <li>Note: For each of the Kubernetes components to verify one another, they need a copy of the CA's root certificate</li> <li>Server Certificate Example: ETCD Server</li> <li>As ETCD server can be deployed as a cluster across multiple servers, you must secure communication between the cluster members, or peers as they're commonly known as.</li> <li>Once generated, specify the certificates when starting the server</li> <li>In the etcd yaml file, there are options to specify the peer certificates</li> <li>Server Certificate Example: API Server:</li> <li>Same procedure involved, but due to the nature of the API Server, with essentially every operation running via it, and it being referred to by multiple names, requires a lot more information included; requiring an openssl config file \u25a0 Using the config file, specify DNS and IP aliases for the component \u25a0 When generating the signing request, you can reference the config file by appending: <code>--config &lt;config name&gt;.cnf</code> to the signing request command</li> <li>The location of all certificates are passed into the exec start file for the api server or the service's configuration file, specifically: \u25a0 Etcd:</li> <li>CA File</li> <li>ETCD Certificate</li> <li>ETCD Private Key File \u25a0 Kubelet</li> <li>CA File</li> <li>Client Certificate</li> <li>Client Key \u25a0 Client CA \u25a0 API Server Cert and Private Key</li> <li>Kubelet Server:</li> <li>Key-Certificate pair required for each worker node</li> <li>Named after each node</li> <li>Must be referenced in the kubelet config file for each node, specifically: \u25a0 Client CA file \u25a0 TLS Certificate file (<code>kubelet-node01.crt</code> for example) \u25a0 TLS Private Key File (<code>kubelet-node01.key</code> for example)</li> <li>Client certificates used to authenticated to the Kube API Server \u25a0 Naming convention should be <code>system:node:nodename</code> \u25a0 Nodes must also be added to <code>system:nodes</code> group for associated privileges</li> </ul>"},{"location":"certifications/CKA/07_Security.html#76-view-certificate-details","title":"7.6 - View Certificate Details","text":"<ul> <li>The generation of certificates depends on the cluster setup</li> <li>If setup manually, all certificates would have to be generated manually in a similar manner to that of the previous sections \u25a0 Components deployed as native services in this manner</li> <li>If setup using a tool such as kubeadm, this is all pre-generated \u25a0 Components deployed as pods in this manner</li> <li>For Kubeadm clusters:</li> <li>Component found in <code>/etc/kubernetes/manifests/</code> folder \u25a0 Certificate file paths located within component's yaml files \u25a0 Example: apiserver.crt \u25a0 Use <code>openssl x509 -in /path/to/.crt</code> file -text -noout</li> <li>Can check the certificate details such as name, alternate names, issuer and expiration dates</li> <li>Note: Additional details available in the documentation for certificates</li> <li>Use <code>kubectl logs &lt;podname&gt;</code> on kubeadm if any issues are found with the components</li> <li>If kubectl is unavailable, use Docker to get the logs of the associated container:</li> <li>Run <code>docker ps - a</code> to identify the container ID</li> <li>View the logs via <code>docker logs &lt;container ID&gt;</code></li> </ul>"},{"location":"certifications/CKA/07_Security.html#77-certificates-api","title":"7.7 - Certificates API","text":"<ul> <li>All certificates have an expiration date, whenever the expiry happens, keys and certificates must be re-generated</li> <li>As discussed, the signing of the certificates is handled by the CA Server</li> <li>The CA server in reality is just a pair of key and certificate files generated</li> <li>Whoever has access to these files can sign any certificate for the Kubernetes environment, create as many users they want and set their permissions</li> <li>Based on the previous point, it goes without saying these files need to be protected</li> <li>Place the files on a fully secure server</li> <li>The server that securely hosts these files becomes the \"CA Server\"</li> <li>Any time you want to sign a certificate, it is the CA server that must be logged onto/communicated with</li> <li>For smaller clusters, it's common for the CA server to actually be the master node</li> <li>The same applies for a kubeadm cluster, which creates a CA pair of files and stores that on the master node</li> <li>As clusters grow in users, it becomes important to automate the signing of certificate requests and renewing expired certificates; this is handled via the Certificates API</li> <li>When a certificate needs signing, a Certificate Signing Request is sent to Kubernetes directly via an API call</li> <li>Instead of an admin logging onto the node and manually signing the certificate, they create a Kubernetes API object called CertificateSigningRequest</li> <li>Once the API object is created, any requests like this can be seen by administrators across the cluster</li> <li>From here, the request can be reviewed and approved using kubectl, the resultant certificate can then be extracted and shared with the user</li> <li>Steps:</li> <li>User generates key: <code>openssl genrsa -out &lt;keyname&gt;.key 2048</code></li> <li>User generates certificate signing request and sends to administrator: <code>openssl req -new -key &lt;key&gt;.name -subk \"/CN=name\" -out name.csr</code></li> <li>Admin receives request and creates the API object using a manifest file, where the spec file includes the following: \u25a0 Groups - To set the permissions for the user \u25a0 Usages - What is the user able to do with keys with this certificate to be signed? \u25a0 Request - The certificate signing request associated with the user, which must be encoded in base64 language first i.e. <code>cat cert.crt | base64</code> \u25a0 Admins across the cluster can view certificate requests via: <code>kubectl | get csr</code> \u25a0 If all's right with the csr, any admin can approve the request with: <code>kubectl certificate approve &lt;name&gt;</code> \u25a0 You can view the CSR in a YAML form, like any Kubernetes object by appending <code>-o yaml</code> to the <code>kubectl get command</code></li> <li>Note: The certificate will still be in base64 code, so run: <code>echo \"CODED CERTIFICATE\" | base64 --decode</code></li> <li>Note: The controller manager is responsible for all operations associated with approval and management of CSR</li> <li>The controller manager's YAML file has options where you can specify the key and certificate to be used when signing certificate requests:</li> <li><code>--cluster-signing-cert-file</code></li> <li><code>--cluster-signing-key-file</code></li> </ul>"},{"location":"certifications/CKA/07_Security.html#78-kubeconfig","title":"7.8 - KubeConfig","text":"<ul> <li>Files containing information for different cluster configurations, such as:</li> <li>-<code>-server</code></li> <li><code>--client-key</code></li> <li><code>--client-certificate</code></li> <li><code>--certificate-authority</code></li> <li>The existence of this file removes the need to specify the option in the CLI</li> <li>File located at <code>$HOME/.kube/config</code></li> <li>KubeConfig Files contain 3 sections:</li> <li>Clusters - Any cluster that the user has access to, local or cloud-based</li> <li>Users - User accounts that have access to the clusters defined in the previous section, each with their own privileges</li> <li>Contexts - A merging of clusters and users, they define which user account can access which cluster</li> <li>These config files do not involve creating new users, it's simply configuring what existing users, given their current privileges, can access what cluster</li> <li>Removes the need to specify the user certificates and server addresses in each kubectl command</li> <li><code>--server</code> spec listed under clusters</li> <li>User keys and certificates listed in Users section</li> <li>Context created to specify that the user \"MyKubeAdmin\" is the user that is used to access the cluster \"MyKubeCluster\"</li> <li>Config file defined in YAML file</li> <li>ApiVersion = v1</li> <li>Kind = Config</li> <li>Spec includes the three sections defined previously, all of which are arrays</li> <li>Under clusters: specify the cluster name, the certificate authority associated and the server address</li> <li>Under users, specify username and associated key(s) and certificate(s)</li> <li>Under contexts:<ul> <li>Name format: username@clustername</li> <li>Under context specify cluster name and users</li> </ul> </li> <li>Repeat for all clusters and users associated</li> <li>The file is automatically read by the kubectl utility</li> <li>Use current-context field in the yaml file to set the current context</li> <li>CLI Commands:</li> <li>View current config file being used: <code>kubectl config view</code><ul> <li>Default file automatically used if not specified</li> <li>To view non-default config files, append: <code>--kubeconfig=/path/to/file</code></li> </ul> </li> <li>To update current context: <code>kubectl config use-context &lt;context-name&gt;</code></li> <li>Other commands available via <code>kubectl config -h</code></li> <li>Default namespaces for particular contexts can be added also</li> <li>Note: for certificates in the config file, use the full path to specify the location</li> <li>Alternatively use certificate-authority-data to list certificate in base64 format</li> </ul>"},{"location":"certifications/CKA/07_Security.html#79-api-groups","title":"7.9 - API Groups","text":"<ul> <li>API Server accessible at master node IP address at port 6443</li> <li>To get the version, append <code>/version</code> to a curl request to the above IP address</li> <li>To get a list of pods, append <code>/api/v1/pods</code></li> <li>Kubernetes' API is split into multiple groups depending on the group's purpose such as</li> <li><code>/api</code> - core functionalities e.g. pods, namespaces, secrets</li> <li><code>/version</code> - viewing the version of the cluster</li> <li><code>/metrics</code> - used for monitoring cluster health</li> <li><code>/logs</code> - for integration with 3rd-party logging applications</li> <li><code>/apis</code> - named functionalities added to kubernetes over time such as deployments, replicasets, extensions \u25a0 Each group has a version, resources, and actions associated with them</li> <li><code>/healthz</code> - used for monitoring cluster health</li> <li>Use <code>curl http://localhost:6443 -k</code> to view the api groups, then append the group and grep name to see the subgroups within</li> <li>Note: Need to provide certificates to access the api server or use <code>kubectl proxy</code> to view</li> <li>Note: <code>kubectl proxy</code> is not the same as kube proxy, the former is an http proxy service to access the api server</li> </ul>"},{"location":"certifications/CKA/07_Security.html#710-authorization","title":"7.10 - Authorization","text":"<ul> <li>When adding users, need to ensure their access levels are sufficiently configured, so they cannot make any unwanted changes to the cluster</li> <li>This applies to any physical users, like developers, or virtual users like applications e.g. Jenkins</li> <li>Additional measures must be taken when sharing clusters with organizations or teams, so that they are restricted to their specific namespaces</li> <li>Authorization mechanisms available are:</li> <li>Node-based</li> <li>Attribute-Based</li> <li>Rule-Based</li> <li>WebHook-based</li> <li>Node-Based:</li> <li>Requests to the kube-apiserver via users and the kubelet are handled via the Node Authorizer</li> <li>Kubelets should be part of the system:nodes group</li> <li>Any requests coming from a user with the name system-node and is aprt of the system nodes group is authorized and granted access to the apiserver</li> <li>ABAC - Attribute-Based</li> <li>For users wanting to access the cluster, you should create a policy in a JSON format to determine what privileges the user gets, such as namespace access, resource management and access, etc</li> <li>Repeat for each users</li> <li>Each policy must be edited manually for changes to be made, the kube apiserver must be restarted to make the changes take effect</li> <li>RBAC</li> <li>Instead of associating each user with a set of permissions, can create a role which outlines a particular set of permissions</li> <li>Assign users to the role</li> <li>If any changes are to be made, it is just the role configuration that needs to be changed</li> <li>Webhook</li> <li>Use of third-party tools to help with authorization</li> <li>If any requests are made to say the APIserver, the third party can verify if the request is valid or not</li> <li>Note: Additional authorization methods are available:</li> <li>AlwaysAllow - Allows all requests without checks</li> <li>AlwaysDeny - Denies all requests without checks</li> <li>Authorizations set by <code>--authorization</code> option in the apiserver's .service or .yaml file</li> <li>Can set modes for multiple-phase authorization, use --authorization-mode and list the authorization methods</li> </ul>"},{"location":"certifications/CKA/07_Security.html#711-rbac","title":"7.11 - RBAC","text":"<ul> <li>To create a role, create a YAML file</li> <li>Spec replaced with rules</li> <li>Covers apiGroups, resources and verbs</li> <li>Multiple rules added by - apiGroups for each</li> <li>Create the role using <code>kubectl create -f</code></li> <li>To link the user to the role, need to create a Role Binding</li> <li>Under <code>metadata</code>:</li> <li>Specify subjects - Users to be affected by the rolebinding, their associated apiGroup for authorization</li> <li>RoleRef - The role to be linked to the subject</li> <li>To view roles: <code>kubectl get roles</code></li> <li>To view rolebindings: <code>kubectl get rolebindings</code></li> <li>To get additional details: <code>kubectl describe role/rolebinding &lt;name&gt;</code></li> <li>To check access level: <code>kubectl auth can-i &lt;command/activity&gt;</code></li> <li>To check if a particular user can do an activity, append <code>--as &lt;username&gt;</code></li> <li>To check if an activity can be done via a user in a particular namespace, append <code>--namespace &lt;namespace&gt;</code></li> <li>Note: Can restrict access to particular resources by adding resourceNames: <code>[\"resource1\", \"resource2\", ...]</code> to the role yaml file</li> </ul>"},{"location":"certifications/CKA/07_Security.html#712-clusterroles-and-rolebindings","title":"7.12 - ClusterRoles and Rolebindings","text":"<ul> <li>Roles and role bindings are created for particular namespaces and control access to resources in that particular namespace</li> <li>By default, roles and role bindings are applied to the default namespace</li> <li>In general, resources such as pods, replicasets are namespaced</li> <li>Cluster-scoped resources are resources that cannot be associated to any particular namespace, such as:</li> <li>Persistentvolumes</li> <li>Nodes</li> <li>To switch view namespaced/cluster-scoped resources: <code>kubectl api-resources --namespaced=TRUE/FALSE</code></li> <li>To authorize users to cluster-scoped resources, use cluster-roles and cluster-rolebindings</li> <li>Could be used to configure node management across a cluster etc</li> <li>Cluster roles and role bindings are configured in the exact same manner as roles and rolebindings; the only difference is the kind</li> <li>Note: Cluster roles and rolebindings can be applied to namespaced resources</li> </ul>"},{"location":"certifications/CKA/07_Security.html#713-image-security","title":"7.13 - Image Security","text":"<ul> <li>Docker images follow the naming convention where <code>image: &lt;image name&gt;</code></li> <li>Image name = image / repository referenced</li> <li>i.e. library/image name<ul> <li>Library = default account where docker official images are stored</li> <li>If referencing from a particular account - swap library with account name</li> </ul> </li> <li>Images typically pulled from docker registry at docker.io by default</li> <li>Private repositories can also be referenced</li> <li>Requires login via <code>docker login &lt;registry name&gt;</code></li> <li>It can then be referenced via the full path in the private registry</li> <li>To facilitate the authentication - create a secret of type docker-registry i.e.:</li> </ul> <p><code>kubectl create secret docker-registry &lt;name&gt; --docker-server=&lt;registry name&gt; --docker-username=&lt;username&gt; --docker-password=&lt;password&gt; --docker-email=&lt;email&gt;</code></p> <p>Then, in the pod spec, add:</p> <pre><code>imagePullSecrets:\n- Name: &lt;secret name&gt;\n</code></pre>"},{"location":"certifications/CKA/07_Security.html#714-securitycontext","title":"7.14 - SecurityContext","text":"<ul> <li>When running docker containers, can specify security standards such as the ID of the user to run the container</li> <li>The same security standards can be applied to pods and their associated containers</li> <li>Configurations applied a pod level will apply to all containers within</li> <li>Any container-level security will override pod-level security</li> <li>To add security contexts, add securityContext to either or both the POD and Container specs; where user IDs and capabilities can be set</li> </ul>"},{"location":"certifications/CKA/07_Security.html#715-network-policy","title":"7.15 - Network Policy","text":""},{"location":"certifications/CKA/07_Security.html#traffic-example","title":"Traffic Example","text":"<ul> <li>Suppose we have the following setup of servers:</li> <li>Web</li> <li>API</li> <li>Database</li> <li>Network traffic will be flowing through each of these servers across particular ports, for example:</li> <li>Web user requests and receives content from the web server on port 80 for HTTP</li> <li>Web server makes a request to the API over port 5000</li> <li> <p>API requests for information from the database over port 3306 (e.g. if MySQL)</p> </li> <li> <p>2 Types of Network Traffic in this setup:</p> </li> <li>Ingress: Traffic to a resource</li> <li> <p>Egress: Traffic sent out from a resource</p> </li> <li> <p>For the setup above, we could control traffic by allowing ONLY the following traffic to and from each resource across particular ports:</p> </li> <li>Web Server:<ul> <li>Ingress: 80 (HTTP)</li> <li>Egress: 5000 (API port)</li> </ul> </li> <li>API Server:<ul> <li>Ingress: 5000</li> <li>Egress: 3306 (MySQL Database Port)</li> </ul> </li> <li> <p>Database Server:</p> <ul> <li>Ingress: 3306</li> </ul> </li> <li> <p>Considering this from a Kubernetes perspective:</p> </li> <li>Each node, pod and service within the cluster has its own IP address</li> <li>When working with networks in Kubernetes, it's expected that the pods should be able to communicate with one another, regardless of the olution to the project<ul> <li>No additional configuration required</li> </ul> </li> <li>By default, Kubernetes has an \"All-Allow\" rule, allowing communication between any pod in the cluster.</li> <li>This isn't best practice, particularly if working with resources that store very sensitive information e.g. databases.</li> <li>To restrict the traffic, one can implement a network policy.</li> </ul> <ul> <li>A network policy is a Kubernetes object allowing only certain methods of network traffic to and from resources. An example follows:</li> </ul> <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: network-policy\nspec:\n  podSelector:\n    matchLabels:\n      role: db\n  policyTypes:\n  - Ingress:\n  ingress:\n  - from:\n    - podSelector:\n        matchLabels:\n          name: api-pod\n    ports:\n    - protocol: TCP\n      port: 3306\n</code></pre> <ul> <li> <p>The policy can then be created via <code>kubectl create -f ....</code></p> </li> <li> <p>Network policies are enforced and supported by the network solution implemented on the cluster.</p> </li> <li>Solutions that support network policies include:</li> <li>kube-router</li> <li>calico</li> <li>romana</li> <li> <p>weave-net</p> </li> <li> <p>Flannel doesn't support Network Policies, they can still be created, but will not be enforced.</p> </li> </ul>"},{"location":"certifications/CKA/07_Security.html#716-developing-network-policies","title":"7.16 - Developing Network Policies","text":"<ul> <li>When developing network policies for pods, always consider communication from the pods perspective</li> <li>PolicyTypes Available:</li> <li>Ingress - Incoming traffic</li> <li>Egress - Outgoing traffic \u25a0 Both can be implemented if desired</li> <li>Each ingress rule has a from and ports field:</li> <li>From describes the pods which the pod affected by the policy can accept ingress communication</li> <li>For additional specification, can use podSelector and namespaceSelector or ipBlock to specify particular IP addresses</li> <li>Each rule start denoted by -</li> <li>Ports - Network ports communication can be received from</li> <li>For egress rules, the only difference is \"from\" is replaced with \"to\"</li> </ul>"},{"location":"certifications/CKA/08_Storage.html","title":"8.0 - Storage","text":"<p>For the following topics, please refer to the linked section of the CKAD course:</p> <ul> <li>Volumes (CKAD 8.1)</li> <li>Persistent Volumes (CKAD 8.2)</li> <li>Persistent Volume Claims (CKAD 8.3)</li> </ul>"},{"location":"certifications/CKA/08_Storage.html#81-storage-in-docker","title":"8.1 - Storage in Docker","text":"<ul> <li>For storage in Docker, must consider both Storage and Volume Drivers.</li> <li>Docker file system setup at /var/lib/docker</li> <li>Contains data relating to containers, images, volumes, etc.</li> <li>To ensure data in a container is stored, create a persistent volume:</li> <li><code>Docker volume create &lt;volume&gt;</code></li> <li>The volume can then be mounted into a container: <code>docker run -v data_volume:/path/to/volume &lt;container&gt;</code></li> <li>Note: if a volume hasn't been already created before this run command, docker will automatically create a volume of that name at the path specified</li> <li>For mounting a particular folder to a container, replace  or whatever named with the full path to the folder you want to mount <li>Alternative command: <code>--mount type=&lt;type&gt;,source=&lt;source&gt;,target=&lt;container volume path&gt; container</code></li> <li>Operations like this, maintaining a layered architecture etc. is handled by storage drivers such as AUFS, BTRFS, Overlay2, Device Mapper</li> <li>Docker chooses the best storage driver given the OS and application</li>"},{"location":"certifications/CKA/08_Storage.html#82-volume-driver-plugins-in-docker","title":"8.2 - Volume Driver Plugins in Docker","text":"<ul> <li>Default volume driver plugin = local</li> <li>Alternatives available include:</li> <li>Azure File Storage</li> <li>GCE-Docker</li> <li>VMware vSphere Storage</li> <li>Convoy</li> <li>To specify the volume driver, append <code>--volume-driver &lt;drivername&gt;</code> to the <code>docker run</code> command</li> </ul>"},{"location":"certifications/CKA/08_Storage.html#83-container-storage-interface-csi","title":"8.3 - Container Storage Interface (CSI)","text":"<ul> <li>CRI = Container Runtime Interface</li> <li>Configures how Kubernetes interacts with container runtimes, such as Docker</li> <li>CNI - Container Network Interface</li> <li>Sets predefined standards for networking solutions to work with Kubernetes</li> <li>CSI - Container Storage Interface</li> <li>Sets standards for storage drivers to be able to work with kubernetes</li> <li>Examples include Amazon EBS, Portworx</li> <li>All of the above allow any container orchestration to work with drivers available</li> </ul>"},{"location":"certifications/CKA/08_Storage.html#84-volumes","title":"8.4 - Volumes","text":"<ul> <li>In practice, Docker containers are ran on a \"need-to-use\" basis.</li> <li>They only exist for a short amount of time</li> <li> <p>Once the associated job or process is complete, they're taken down, along with any associated data.</p> </li> <li> <p>To persist data associated with a container, one can attach a volume.</p> </li> <li> <p>When working in Kubernetes, a similar process can be can be used:</p> </li> <li>Attach a volume to a Pod</li> <li>The volume(s) store data associated with the pod.</li> </ul>"},{"location":"certifications/CKA/08_Storage.html#example-pod-volume-integration","title":"Example Pod-Volume Integration","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: random-number-generator\nspec:\n  containers:\n  - image: alpine\n    name: alpine\n    command: [\"/bin/sh\", \"-c\"]\n    args: [\"shuf -i 0-100 -n  1 &gt;&gt; /opt/number.out\"]\n    volumeMounts:\n    - mountPath: /opt\n      name: data-volume\nvolumes:\n- name: data-volume\n  hostPath:\n    path: /data\n    type: Directory\n</code></pre> <ul> <li>This is ok for a single-node cluster, for multi-node clusters, the data would be persisted to the same directory on each node.</li> <li>One would expect them all to be the same and have the same data, but this is not the case as they're different servers.</li> <li>To work around this, one can use other storage solutions supported by Kubernetes e.g.:</li> <li>AWS Elastic Block Storage (EBS)</li> <li>Azure Disk</li> <li> <p>Google Persistent Disk</p> </li> <li> <p>When using one of these solutions, such as EBS, the volume definition changes:</p> </li> </ul> <pre><code>...\nvolumes:\n- name: data-volume\n  awsElasticBlockStore:\n    volumeID: &lt;volume ID&gt;\n      fsType: ext4\n</code></pre>"},{"location":"certifications/CKA/08_Storage.html#85-persistentvolumes","title":"8.5 - PersistentVolumes","text":"<ul> <li>When creating volumes in the prior section, volumes are created in definition file</li> <li>When working in a larger environment, where users are deploying lots of pods etc, this can cause issues.</li> <li>Each time a pod is to be deployed, storage needs to be configured.</li> <li>These changes would have to be applied to every pod individually</li> <li>To remove this problem and manage the storage centrally, persistent volumes can be leveraged.</li> </ul>"},{"location":"certifications/CKA/08_Storage.html#persistent-volumes","title":"Persistent Volumes","text":"<ul> <li>A cluster-wide pool of storage volumes that is configured by an admin.</li> <li> <p>Used by users deploying applications on the cluster.</p> </li> <li> <p>Users can select storage from this pool by persistent volume claims.</p> </li> <li> <p>Defining a PersistentVolume:</p> </li> </ul> <pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: pv-vol1\nspec:\n  accessModes:\n    - ReadWriteOnce / ReadOnlyMany / ReadWriteMany\n  capacity:\n    storage: 1Gi\n  hostPath:\n    path: /tmp/data\n</code></pre> <ul> <li> <p>To create the volume: <code>kubectl create -f &lt;file&gt;.yaml</code></p> </li> <li> <p>To view: <code>kubectl get persistentvolume</code></p> </li> <li> <p>For production environments, it's recommended to use a storage solution like AWS EBS.</p> </li> <li>Replace <code>hostPath</code> with the appropriate attributes in this case.</li> </ul>"},{"location":"certifications/CKA/08_Storage.html#86-persistentvolumeclaims","title":"8.6 - PersistentVolumeClaims","text":"<ul> <li>Kubernetes objects created by users to request access to a portion of a PersistentVolume.</li> <li>Once claims are created, Kubernetes binds the Persistent Volume to the claims</li> <li> <p>Binding determined based on request and properties set on the volume</p> </li> <li> <p>Note: Each persistent volume claim is bound to a single persistent vilume</p> </li> <li> <p>Kubernetes will always try to find a persistent volume with sufficient capacity as requested by the claim.</p> </li> <li> <p>Also considers storage class, access modes, etc.</p> </li> <li> <p>If there are multiple possible matches for a claim, and a particular volume is desired, labels and selectors can be utilised.</p> </li> <li> <p>It's possible for a smaller claim to be matched to a larger volume if the criteria is satisfied and no other option is available:</p> </li> <li>1-to-1 relationship between claims and volumes</li> <li> <p>No additional claims could utilise the remaining volume.</p> </li> <li> <p>If no opther volumes are available, the claim remains in a pending state</p> </li> <li> <p>Automatic assignment occurs when an applicable volume becomes available.</p> </li> <li> <p>To create a claim:</p> </li> </ul> <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: myclaim\nspec:\n  accessMode:\n  - ReadWriteOnce\n  resources:\n    requests:\n      storage: 500Mi\n</code></pre> <ul> <li>Creation via <code>kubectl create -f ...</code></li> <li> <p>To view claims: <code>kubectl get persistentvolumeclaim</code></p> </li> <li> <p>When claim is created, Kubernetes will look through all available volumes and binds appropriately</p> </li> <li> <p>Associated volume will be noted in a column as part of the <code>kubectl get</code> command above.</p> </li> <li> <p>To delete a PVC, <code>kubectl delete persistentvolumeclaim &lt;claim name&gt;</code></p> </li> <li> <p>One can choose to delete, retain or recycle the volume upon claim deletion</p> </li> <li>Determined via configuring the <code>persistentVolumeReclaimPolicy</code> attribute</li> </ul>"},{"location":"certifications/CKA/08_Storage.html#87-storage-class","title":"8.7 - Storage Class","text":"<ul> <li>Allows definition of a provisioner, so that storage can automatically be provisioned and attached to pods when a claim is made</li> <li>Make storage classes using yaml files to define a particular storage class</li> <li>To use the storage class, specify it in the pvc definition file</li> <li>Still creates a PV, BUT doesn't require a definition file</li> <li>Provisioners available include:</li> <li>AWS</li> <li>GCP</li> <li>Azure</li> <li>ScaleIO</li> <li>Additional configuration options available for different provisioners, so you could have multiple storage classes per provisioners</li> </ul>"},{"location":"certifications/CKA/09_Networking.html","title":"9.0 - Networking","text":""},{"location":"certifications/CKA/09_Networking.html#91-prerequisite-switching-routing","title":"9.1 - Prerequisite: Switching Routing","text":"<ul> <li>To connect two hosts to one another, need to connect them to a switch, which creates a network connection</li> <li>Need an interface on the host, viewable via <code>ip link</code></li> <li>Assign the system with IP addresses of the same network: <code>ip addr add &lt;IP&gt; &lt;namespace&gt; &lt;networkname&gt;</code></li> <li>For systems on other networks, need a router for inter-switch communication</li> <li>Has an IP address for each network that it can communicate with</li> <li>Gateway - Setup to help route requests to a particular location, view via route</li> <li>Add via <code>ip route add &lt;IP&gt; via &lt;IP&gt;</code></li> <li>For the internet, can set default gateway so any requests to a network outside of the current can be sent to the internet - <code>ip route add default via &lt;Router IP&gt;</code></li> <li>If multiple routers, entries required for each to setup gateway</li> <li><code>ip route add &lt;IP&gt; via &lt;IP&gt;</code></li> <li>To check connection - <code>ping &lt;IP&gt;</code></li> <li>Whether data is forwarded is defined by <code>/proc/sys/net/ipv4/ip_forward</code> (set to 1 by default)</li> <li><code>ip link</code> - List and modify interfaces on the host</li> <li><code>ip addr</code> - see ip addresses assigned to interfaces described in ip link</li> <li>Ip addr add - Add IP addresses to interface</li> <li>Note: Any changes made via these commands don't persist beyond a restart, to ensure they do, edit the <code>/etc/network/interfaces</code> file</li> <li>Ip route (or just route) - View routing table</li> <li>Ip route add - add entries into the ip routing table</li> </ul>"},{"location":"certifications/CKA/09_Networking.html#92-prerequisite-dns","title":"9.2 - Prerequisite: DNS","text":"<ul> <li>Used to assign text names to ip addresses, saving the need to remember manual ip addresses</li> <li>Can assign names in <code>/etc/hosts</code>, write the IP and name in key-value pairs</li> <li>Note: No checks would be done by the system when done in this manner to check the hostname</li> <li>As environments grow, modifying the <code>/etc/hosts</code> becomes impossible</li> <li>Moved to Domain Name Server for centralized management</li> <li>Host will point to the DNS server to resolve any names unknown to them</li> <li>For any changes that need to be made, just the one change needs to be made in the DNS server, all hosts will register it</li> <li>Note: custom entries can still be added in the <code>/etc/hosts</code> file, though this is better for local networking</li> <li>If both the DNS and the <code>/etc/hosts</code> file contains the same IP address for an entry, it looks in the <code>/etc/hosts</code> file first, then DNS, taking whichever one comes first</li> <li>Record types:</li> <li>A - Domain Name - IP address</li> <li>AAAA - Domain name to full Address</li> <li>CNAME - 1-to-1 name mapping for the same IP</li> <li>Dig - tool to test DNS resolution (<code>dig &lt;DNS NAME&gt;</code>)</li> </ul>"},{"location":"certifications/CKA/09_Networking.html#93-prerequisite-coredns","title":"9.3 - Prerequisite: CoreDNS","text":"<ul> <li>We are given a server dedicated as the DNS server, and a set of Ips to configure as entries in the server.</li> <li>There are many DNS server solutions out there, in this lecture we will focus on a particular one - CoreDNS.</li> <li>So how do you get core dns? CoreDNS binaries can be downloaded from their Github releases page or as a docker image.</li> <li>Let's go the traditional route. Download the binary using curl or wget. And extract it. You get the coredns executable.</li> <li>Run the executable to start a DNS server. It by default listens on port 53, which is the default port for a DNS server.</li> <li>Now we haven't specified the IP to hostname mappings. For that you need to provide some configurations. There are multiple ways to do that. We will look at one. First we put all of the entries into the DNS servers <code>/etc/hosts</code> file.</li> <li>And then we configure CoreDNS to use that file. CoreDNS loads it's configuration from a file named Corefile.</li> <li>Here is a simple configuration that instructs CoreDNS to fetch the IP to hostname mappings from the file <code>/etc/hosts</code>. When the DNS server is run, it now picks the Ips and names from the <code>/etc/hosts</code> file on the server.</li> <li>CoreDNS also supports other ways of configuring DNS entries through plugins. We will look at the plugin that it uses for Kubernetes in a later section.</li> </ul>"},{"location":"certifications/CKA/09_Networking.html#94-prerequisite-network-namespaces","title":"9.4 - Prerequisite: Network Namespaces","text":"<ul> <li>Used to implement network isolation</li> <li>Resources within a namespace can only access other resources within their namespace</li> <li>Want containers to remain isolated when running a process; run in a namespace</li> <li>Underlying host sees all processes associated with other containers</li> <li>Creating a new namespace: <code>ip netns add &lt;namespace name&gt;</code></li> <li>To view namespaces: <code>ip netns</code></li> <li>To execute a command in a namespace: <code>ip netns exec &lt;namespace&gt; &lt;command&gt;</code></li> <li>While testing the Network Namespaces, if you come across issues where you can't ping one namespace from the other, make sure you set the <code>NETMASK</code> while setting IP Address. ie: 192.168.1.10/24</li> </ul> <pre><code>ip -n red addr add 192.168.1.10/24 dev veth-red\n</code></pre> <ul> <li>Another thing to check is FirewallD/IP Table rules. Either add rules to IP Tables to allow traffic from one namespace to another. Or disable IP Tables all together (Only in a learning environment!).</li> </ul>"},{"location":"certifications/CKA/09_Networking.html#95-prerequisite-docker-networking","title":"9.5 - Prerequisite: Docker Networking","text":"<ul> <li>Run a docker container without attaching it to a network (specified by none parameter) - <code>docker run --network none &lt;containername&gt;</code></li> <li>Attach a container to a host's network - <code>docker run --network host &lt;containername&gt;</code></li> <li>For whatever the port the container runs on, it will be available on the same port at the hosts IP address (localhost)</li> <li>Setup a private internal network which the docker host and containers attach themselves to: <code>docker run &lt;containername&gt;</code></li> <li>When docker's installed it creates a default network called bridge (when viewed by docker) and <code>docker0</code> (when viewed via <code>ip link</code>)</li> <li>Whenever <code>docker run &lt;containername&gt;</code> is ran, it creates its own private namespace (viewable via ip netns) and <code>docker inspect &lt;namespace&gt;</code></li> <li>Port mapping:</li> <li>For a container within the private network on the host, only the containers within the network can view it</li> <li>To allow external access, docker provides a port mapping option: appending <code>-p &lt;hostport&gt;:&lt;containerport&gt;</code> to the docker run command</li> </ul>"},{"location":"certifications/CKA/09_Networking.html#36-prerequisite-cni","title":"3.6 - Prerequisite: CNI","text":"<ul> <li>A single program encompassing all the steps required to setup a particular network type, for example <code>bridge add &lt;container ns&gt; /path/</code></li> <li>CNI Defines a set of standards that define how programs should be developed to solve and perform network operations with containers</li> <li>Any variants developed in line with the CNI are plugins</li> <li>Docker doesn't use CNI, instead adopting CNM (container network model)</li> <li>Can't use certain CNI plugins with Docker instantly, instead would have to create a none network container, then manually configure CNI features</li> </ul>"},{"location":"certifications/CKA/09_Networking.html#37-cluster-networking","title":"3.7 - Cluster Networking","text":"<ul> <li>Each node within a cluster must have at least 1 interface connected to a network</li> <li>Each node's interface must have an IP address configured</li> <li>Hosts must each have a unique hostname and unique MAC address</li> <li>Particularly important if cloning VMs</li> <li>Ports need to be opened:</li> <li>APIServer (Master Node) - Port 6443</li> <li>Kubelet (Master and Worker) - Port 10250</li> <li>Kube-Scheduler (Master) - Port 10251</li> <li>Kube-Controller-Manager - Port 10252</li> <li>ETCD - Port 2379</li> <li>Note: 2380 In addition for the case where there are multiple master nodes (allows ETCD Clients to communicate with each other</li> </ul>"},{"location":"certifications/CKA/09_Networking.html#note-on-cni-and-the-cka-exam","title":"Note on CNI and the CKA Exam","text":"<ul> <li>In the upcoming labs, we will work with Network Addons. - This includes installing a network plugin in the cluster.</li> <li> <p>While we have used weave-net as an example, please bear in mind that you can use any of the plugins which are described here:</p> </li> <li> <p>https://kubernetes.io/docs/concepts/cluster-administration/addons/ -https://kubernetes.io/docs/concepts/cluster-administration/networking/#how-to-implement-the-kubernetes-networking-model</p> </li> <li> <p>In the CKA exam, for a question that requires you to deploy a network addon, unless specifically directed, you may use any of the solutions described in the link above.</p> </li> <li>However, the documentation currently does not contain a direct reference to the exact command to be used to deploy a third party network addon.</li> <li>The links above redirect to third party/ vendor sites or GitHub repositories which cannot be used in the exam.</li> <li>This has been intentionally done to keep the content in the Kubernetes documentation vendor-neutral.</li> <li>At this moment in time, there is still one place within the documentation where you can find the exact command to deploy weave network addon:</li> <li>https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/#steps-for-the-first-control-plane-node</li> </ul>"},{"location":"certifications/CKA/09_Networking.html#38-pod-networking","title":"3.8 - Pod Networking","text":"<ul> <li>Inter-pod communication is hugely important in a fully operational environment for Kubernetes</li> <li>At the time of writing, there is no built-in solution in Kubernetes for this, but the requirements have been clearly identified:</li> <li>Each pod should have an IP address</li> <li>Each pod should be able to communicate with every other pod on the same node</li> <li>Every pod should be able to communicate with pods on other nodes without NAT</li> <li>Solutions available include weaveworks, VMware etc</li> </ul>"},{"location":"certifications/CKA/09_Networking.html#example-configuring-pod-networking","title":"Example - Configuring Pod Networking","text":"<ul> <li>Consider a cluster containing 3 identical nodes. The nodes are part of an external network and have IP addresses in the 192.168.1 series (11, 12 and 13).</li> <li>When containers are created on pods, Kubernetes creates network namespaces for each of them, to enable communication between containers, can create a bridge network and the containers to it.</li> <li>Running <code>ip link add v-net-0 type bridge</code> on each node, then bring them up with <code>ip link set dev v-net-0 up</code></li> <li>IP addresses can then be assigned to each of the bridge interfaces of networks. In this case, suppose we want each personal network to be on its own subnet (/24).</li> <li>The Ip address for the bridge interface can be set from here via <code>ip addr add 10.244.1.1/24 dev v-net-0</code> etc</li> <li>The remaining steps can be summarised in a script that is to be ran every time a new container is created.</li> </ul> <pre><code>##create veth pair\nip link add ....\n##attach veth pair\nIp link set ....\nIp link set .....\n##assign IP address\nIp -n &lt;namespace&gt; addr add ....\nIp -n &lt;namespace&gt; addr add ....\n##bring up the interface\nIp -n &lt;namespace&gt; link set\n</code></pre> <ul> <li>This script is run for the second container involved in the pair, with its respective information applied; allowing the two containers to communicate with one another.</li> <li>The script is then copied and run on the other nodes; assigning IP addresses and connecting their containers to their own internal networks.</li> <li>This solves the first problem, all pods get their own IP address and can communicate with each other within their own nodes.</li> <li> <p>To extend communication across nodes in the cluster, create an ip route to each nodes' internal network via the nodes' IP address i.e. on each node, run: <code>ip route add &lt;pod network ip&gt; via &lt;node IP&gt;</code></p> </li> <li> <p>For larger, more complex network, it's better to configure these routes via a central router, which is then used as a default gateway.</p> </li> <li>Additionally, we don't have to run the script manually for each pod, this can automatically be done via the CNI as it sets out predefined standards and how the script/operations look.</li> <li>The script needs a bit of tweaking to consider container creation and deletion.</li> </ul> <p><code>shell   --cni-conf-dir=/etc/cni/net.d   --cni-bin-dir=/etc/cni/bin   ./net-script.sh add &lt;container&gt; &lt;namespace&gt;</code></p>"},{"location":"certifications/CKA/09_Networking.html#39-cni-in-kubernetes","title":"3.9 - CNI in Kubernetes","text":"<ul> <li>CNI Defines the best practices and standards that should be followed when networking containers and the container runtime</li> <li>Responsibilities include:</li> <li>Creating namespaces</li> <li>Identifying the network a container should attach to</li> <li>Invoke the associated network plugin (bridge) when a container is added and deleted</li> <li>Maintain the network configuration in a JSON format</li> <li>CNI Must be invoked by the Kubernetes component responsible for container creation, therefore its configuration is determined by the kubelet server</li> <li>Configuration parameters for CNI in Kubelet (kubelet.service):</li> <li><code>--network-plugin=cni</code></li> <li><code>--cni-bin-dir=/opt/cni/bin/</code></li> <li><code>--cni-conf-dir=/etc/cni/net.d/</code></li> <li>Can see these options by viewing the kubelet process</li> <li>CNI bin contains associated network plugins e.g. bridge, flannel</li> <li>Conf dir contains config files to determine the most suitable one</li> <li>If multiple files, considerations made in alphabetical order</li> <li>IPAM section in conf considers subnets, IPs and routes etc</li> </ul>"},{"location":"certifications/CKA/09_Networking.html#310-cni-weave","title":"3.10 - CNI Weave","text":"<ul> <li>Becomes important when significant numbers of routes are available</li> <li>Deploys an agent or service on each node and communicates with other nodes agent</li> <li>Weave creates its own inter-node network, each agent knows the configuration and location of each node on the network, helping route the packages from one node to another, which can then be sent to the correct pod</li> <li>Weave can be deployed manually as a daemonset or via pods</li> <li><code>kubectl apply -f \"https://cloud.weave.works/k8s...</code>.</li> <li>Weave peers deployed as daemonsets</li> </ul>"},{"location":"certifications/CKA/09_Networking.html#911-ip-address-management-weave","title":"9.11 - IP Address Management - Weave","text":"<ul> <li>How are the virtual networks, nodes and pods assigned IPs?</li> <li>How do you avoid duplicate IPs</li> <li>The CNI plugin is responsible for assigning IPs</li> <li>To manage the IPs, Kubernetes isn't bothered how they're managed</li> <li>Could do it via referencing a list</li> <li>CNI comes with 2 built in plugins to leverage this, the host_local plugin or dynamic</li> <li>CNI conf has sections determining the plugins, routes and subnet used</li> <li>Various network solutions have different approaches</li> <li>Weave by default allocates the range <code>10.32.0.0/12</code> =&gt; 10.32.0.0 - 10.47.0.0, ~1 million IPs available, each node gets a subrange of this range defined</li> </ul>"},{"location":"certifications/CKA/09_Networking.html#912-service-networking","title":"9.12 - Service Networking","text":"<ul> <li>Don't want to make each pod communicate with one another, can use services to leverage this</li> <li>Each service runs at a particular IP address and is accessible by any pod from any node, they aren't bound to a particular node</li> <li>ClusterIP = Service exposed to particular cluster only</li> <li>NodePort = Runs on a particular port on all nodes, with its own IP</li> <li>How are these services allocated IP addresses, made available to users, etc</li> <li>Each kubelet server watches for cluster changes via the api-server, each time a pod is to be created, it creates the pod and invokes the CNI plugin to configure the networking for it</li> <li>Kube-proxy watches for any changes, any time a new service is created, it's invoked, however these are virtual objects.</li> <li>Services are assigned an IP from a predefined range, associated forwarding rules are assigned to it via the <code>kube-proxy</code></li> <li>The kube-proxy creates the forwarding rules via:</li> <li>Listening on a port for each service and proxies connections to pods (userspace)</li> <li>Creating ipvs rules</li> <li>Use IP tables (default setting)</li> <li>Proxy mode configured via: <code>kube-proxy --proxy-mode &lt;proxy mode&gt;</code></li> <li>When a service is created, kubernetes will assign an IP address to it, the range is set by the <code>kube-api server option --service-cluster-ip-range ipNet</code></li> <li>By default, set to <code>10.0.0.0/24</code></li> <li>Network ranges for services, pods etc. should never overlap as this causes conflicts</li> <li><code>iptables -L -t net | grep &lt;service&gt;</code></li> <li>Displays rules created by kube-proxy for service</li> </ul>"},{"location":"certifications/CKA/09_Networking.html#913-dns-in-kubernetes","title":"9.13 - DNS in Kubernetes","text":"<ul> <li>Consider a 3-node cluster with pods and services</li> <li>Nodenames and IP addresses stored in DNS server</li> <li>Want to consider cluster-specific DNS</li> <li>Kubernetes by default deploys a Cluster-DNS server</li> <li>Manual setup required otherwise</li> <li>Consider 2 pods with one running as a service, each pod being on different nodes</li> <li>Kubernetes DNS service creates a DNS record for any service created (name and IP address)</li> <li>If in same namespace, just need <code>curl http://service</code></li> <li>In different namespaces: <code>curl http://service.namespace</code></li> <li>For each namespace, the kubernetes service creates a subdomain, with further subdomains for services</li> <li>The full hierarchy would follow:</li> <li>Cluster.local (root domain)</li> <li>Svc</li> <li>Namespace</li> <li>Service name</li> <li>So to access the fill service, can run: <code>curl http://service.namespace.svc.cluster.local</code></li> <li>Note: DNS records aren't created for pods by default, though this can be enabled (next section)</li> <li>Once enabled, records are created for pods in the DNS server, however the pod name is rewritten, replacing the dots in the pods IP address with dashes</li> <li>Pod can then be accessed via: <code>curl https://&lt;pod hostname&gt;.namespace.pod.cluster.local</code></li> </ul>"},{"location":"certifications/CKA/09_Networking.html#914-coredns-in-kubernetes","title":"9.14 - CoreDNS in Kubernetes","text":"<ul> <li>How does Kubernetes implement DNS?</li> <li>Could add entries into /etc/hosts file -&gt; not suitable for large scale</li> <li>Move to central dns server and specify the nameserver located at <code>/etc/resolv.conf</code></li> <li>This works for services, but for pods it works differently</li> <li>Pod hostnames are the pod IP addresses rewritten with - instead of .</li> <li>Recommended DNS server = CoreDNS</li> <li>DNS Server deployed within the cluster as a pod in the kube-system namespace</li> <li>Deployed as a replicaset</li> <li>Runs the coredns executable</li> <li>Requires a config file named Corefile at <code>/etc/coredns/</code></li> <li>Details numerous plugins for handling errors, monitoring metrics, etc</li> <li>Cluster.local defined by kubernetes plugin</li> <li>Options here determine whether pods have records</li> <li>Set pods insecure -&gt; pods secure</li> <li>Coredns config deployed as configmap -&gt; edit this to make changes</li> <li>Kube-DNS deployed as a service by default, IP address configured as the nameserver of the pod</li> <li>IP address to look at for DNS server configured via Kubelet.</li> </ul>"},{"location":"certifications/CKA/09_Networking.html#915-ingress","title":"9.15 - Ingress","text":"<ul> <li>To understand the importance of Ingress, consider the following example:</li> <li>Suppose you build an application into a Docker image and deploy it as a pod via Kubernetes.</li> <li>Due to the application's nature, set up a MySQL database and deploy a clusterIP service -&gt; allows app-database communications.</li> <li>To expose the app or external access, one needs to create a NodePort service.</li> <li>App can then be accessed via the Node's IP and the port defined.</li> <li>To access the URL, users need to go to <code>http://&lt;node ip&gt;:&lt;node port&gt;</code></li> <li> <p>This is fine for small non-production apps, it should be noted that as demand increases, the replicaSet and service configuration can be altered to support load balancing.</p> </li> <li> <p>For production, users wouldn't want to have to enter an IP and port number every time, typically a DNS entry would be created to map to the port and IP.</p> </li> <li>As service node ports can only allocate high numbered ports (<code>&gt; 30000</code>):</li> <li> <p>Introduce a proxy server between DNS cluster and point it to the DNS server.</p> </li> <li> <p>The above steps are applicable if hosting an app in an on-premise datacenter.</p> </li> <li>If working with a public cloud application, NodePort can be replaced by <code>LoadBalancer</code></li> <li> <p>Kubernetes still performs NodePort's functionality AND sends an additional request to the platform to provision a network load balancer.</p> </li> <li> <p>The cloud platform automatically deploys a load balancer configured to route traffic to the service ports of all the nodes.</p> </li> <li> <p>The cloud provider's load balancer would have its own external IP</p> </li> <li> <p>User request access via this IP.</p> </li> <li> <p>Suppose as the application grows and a new service is to be added, it's to be accessed via a new URL.</p> </li> <li>For the new application to share the cluster resource, release it as a separate deployment.</li> <li> <p>Engineers could create a new load balancer for this app, monitoring a new port</p> <ul> <li>Kubernetes automatically configures a new load balancer on the cloud platform of a new IP.</li> </ul> </li> <li> <p>To map the URLs between the 2 new services, one would have to implement a new proxy server on top of those associated with the service.</p> </li> <li> <p>This proxy service would have to be configured and SSL communications would have to be enabled.</p> </li> <li> <p>This final proxy could be configured on a team-by-team basis, however would likely lead to issues.</p> </li> </ul> <ul> <li>The whole process outlined above has issues, on top of having additional proxies to manage per service, one must also consider:</li> <li>Cost: Each additional Load Balancer adds more expense.</li> <li> <p>Difficulty of Management: For each service introduced, additional configuration is required for both firewalls and proxies</p> <ul> <li>Different teams required as well as time and \"person\" power.</li> </ul> </li> <li> <p>To work around this and collectively manage all these aspects within the cluster, one can use Kubernetes Ingress:</p> </li> <li>Allows users access via a single URL</li> <li>URL can be configured to route different services depending on the URL paths.</li> <li>SSL security may automatically be implemented via Ingress</li> <li> <p>Ingress can act as a layer 7 load balancer built-in to Kubernetes clusters</p> <ul> <li>Can be configured to act like a normal Kubernetes Object.</li> </ul> </li> <li> <p>Note: Even with Ingress in place, one still needs to expose the application via a NodePort or Load Balancer -&gt; this would be a 1-time configuration.</p> </li> <li> <p>Once exposed, all load balancing authenticaiton, SSL and URL routing configrations are manageable and viewable via an Ingress Cotnroller.</p> </li> <li> <p>Ingress controllers aren't set up by default in Kubernetes, example solutions that can be deployed include:</p> </li> <li>GCE</li> <li>NGINX</li> <li> <p>Traefik</p> </li> <li> <p>Load balancers aren't the only component of an Ingress controller, additionaly functionalities are available for monitoring the cluster for new Ingress resources or definitions.</p> </li> <li> <p>To create, write a definition file:</p> </li> </ul> <pre><code>apiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: nginx-ingress-controller\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: nginx-ingress\n  template:\n    metadata:\n      labels:\n        name: nginx-ingress\n    spec:\n      containers:\n      - name: nginx-ingress-controller\n        image: &lt;nginx ingress controller url&gt;\n        args:\n        - /nginx-ingress-controller\n        - --configmap=$(POD_NAMESPACE)/nginx-configuration\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - name: http\n          containerPort: 80\n        - name: https\n          containerPort: 443\n</code></pre> <ul> <li>Note: As working with Nginx, need to configure options such as log paths, SSL settings, etc.</li> <li> <p>To decouple this from the controller image, write a separate config map definition file to be referenced:</p> <ul> <li>Allows easier modification rather than editing one huge file.</li> </ul> </li> <li> <p>An ingress service definition file is also required to support external communicationL</p> </li> </ul> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-ingress\nspec:\n  type: NodePort\n  ports:\n  - port: 80\n    targetPort: 80\n    protocol: TCP\n    name: http\n  - port: 443\n    targetPort: 443\n    protocol: TCP\n    name: https\n  selector:\n    name: nginx-ingress\n</code></pre> <ul> <li> <p>The service NodePort definition above links the service to the deployment.</p> </li> <li> <p>As mentioned, Ingress controllers have additional functionality available for monitoring the cluster for ingress resources, and apply configurations when changes are made</p> </li> <li> <p>For the controller to do this, a service account must be associated with it:</p> </li> </ul> <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: nginx-ingress-serviceaccount\n</code></pre> <ul> <li> <p>The service account must have the correct roles and role-bindings to work.</p> </li> <li> <p>To summarise, for an ingress controller, the following resources are needed:</p> </li> <li>Deployment</li> <li>Service</li> <li>ConfigMap</li> <li> <p>ServiceAccount</p> </li> <li> <p>Once an ingress controller is in place, one can create ingress resources:</p> </li> <li> <p>Ingress resources are a set of rules and configurations applied to an ingress controller, linking it to other Kubernetes objects.</p> </li> <li> <p>For example, one could configure a rule to forward all traffic to one application, or to a different set of applications based on a URL.</p> </li> <li>Alternatively, could route based on DNS.</li> <li>As per, ingress resources are configured via a destination file</li> </ul> <pre><code>apiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: ingress-cluster\nspec:\n  backend:\n    serviceName: wear-service\n    servicePort: 80\n</code></pre> <ul> <li>Note: For a single backend like above, no additional rules are required.</li> <li> <p>The ingress resource can be created via standard means i.e. <code>kubectl create -f ....</code></p> </li> <li> <p>To view ingress resource: <code>kubectl get ingress</code></p> </li> <li>To route traffic in a conditional form, use ingress rules e.g. routing based on DNS</li> <li> <p>Within each rule, can configure additional paths to route to additional services or applications.</p> </li> <li> <p>To implement, adhere to the principles outlined in the following 2-service example:</p> </li> </ul> <pre><code>apiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: ingress-cluster\nspec:\n  rules:\n  - http:\n      paths:\n      - path: /wear\n        backend:\n            serviceName: wear-service\n            servicePort: 80\n      - path: /watch\n        backend:\n            serviceName: watch-service\n            servicePort: 80\n</code></pre> <ul> <li> <p>Create the ingress resource using <code>kubectl create -f ...</code> as per usual.</p> </li> <li> <p>To view the ingress's detailed information: <code>kubectl describe ingress &lt;ingress name&gt;</code></p> </li> <li> <p>Note: In the description, a default backend is described.  In the event a user enters a path not matching any of the rules, they will be redirected to that backend service (which must exist!).</p> </li> <li> <p>If wanting to split traffic via domain name, a definition file can be filled out as normal, but in the spec, the rules can be updated to point to specific hosts instead of paths:</p> </li> </ul> <pre><code>...\nrules:\n- host: &lt;url 1&gt;\n  http:\n    paths:\n    - backend:\n        serviceName: &lt;service name 1&gt;\n        servicePort: &lt;port 1&gt;\n- host: &lt;url 2&gt;\n  http:\n    paths:\n    - backend:\n        serviceName: &lt;service name 2&gt;\n        servicePort: &lt;port 2&gt;\n...\n</code></pre> <ul> <li>When splitting by URL, had 1 rule and split the traffic by 2 paths</li> <li> <p>When splitting by hostname, used 2 rules with a path for each.</p> </li> <li> <p>Note: If not specifying the host field, it'll assume it to be a <code>*</code> and / or accept all incoming traffic without matching the hostname</p> </li> <li>Acceptable for a single backend</li> </ul>"},{"location":"certifications/CKA/10_Design-a-Kubernetes-Cluster.html","title":"10.0 - Design a Kubernetes Cluster","text":""},{"location":"certifications/CKA/10_Design-a-Kubernetes-Cluster.html#design-a-kubernetes-cluster","title":"Design a Kubernetes Cluster","text":"<ul> <li>Considerations must be made when designing a cluster, such as:</li> <li>Purpose</li> <li>Workload to be hosted<ul> <li>Cloud or On-Prem</li> </ul> </li> <li>For production clusters, should consider a high availability setup</li> <li>Kubeadm or GCP or other supported platforms</li> <li>Multiple nodes and master nodes</li> <li>For cloud hosted kubernetes clusters, resource requirements are predefined</li> <li>For on-prem, could use the figures from previous for reference</li> <li>Kops - A popular tool for deploying Kubernetes on AWS</li> <li>Minimum of 4 nodes required (1 master, 3 workers)</li> <li>Nodes must use the Linux x64 OS</li> <li>Could separate ETCD cluster to own node, separate from the master</li> </ul>"},{"location":"certifications/CKA/10_Design-a-Kubernetes-Cluster.html#choosing-kubernetes-infrastructure","title":"Choosing Kubernetes Infrastructure","text":"<ul> <li>For Kubernetes on a laptop, multiple options available:</li> <li>For Linux, could install binaries - Tedious, but worth it</li> <li>For Windows, no native support available, Hyper-V, Virtualbox etc required to run Linux in a virtualised manner</li> <li>Minikube creates a single-node cluster easily, good for beginners practicing</li> <li>Kubeadm - Can be used to quickly deploy a multi-node cluster, though the host must be configured beforehand</li> <li>Turnkey solutions - Solutions where VMs required are manually configured and maintained</li> <li>OpenShift as an example - Built on top of Kubernetes and easily integratable with CI/CD 90</li> <li>CloudFoundry - Helps deploy and manage highly available kubernetes clusters</li> <li>VMWare cloud PKS</li> <li>Vagrant - Providers scripts to deploy kubernetes clusters on different cloud providers</li> <li>Hosted Solutions (Managed) - Kubernetes-as-a-service solution</li> <li>VMs maintained and provisioned by cloud provider</li> <li>Examples:<ul> <li>Google Container Engine (GKE)</li> <li>Openshift Online</li> <li>Azure Kubernetes Service</li> <li>Amazon Elastic Container Service for Kubernetes</li> </ul> </li> <li>For the purposes of education, Virtualbox is probably the best place to start</li> </ul>"},{"location":"certifications/CKA/10_Design-a-Kubernetes-Cluster.html#configure-high-availability","title":"Configure High-Availability","text":"<ul> <li>As long as the worker nodes are available and nothing is going wrong, the applications on worker nodes will run even if the master node is unavailable</li> <li>Multiple master nodes are recommended for high-availability clusters; even if one master node goes down, it's all good.</li> <li>For multiple master nodes, it's better to have a load balancer to split traffic between the two api servers, nginx, ha-proxy are all good examples</li> <li>Other components like scheduler and kube-controller can't run at the same time across multiple master nodes</li> <li>Can leverage a leader-elect approach for an active-standby approach<ul> <li>If one receives the request first, it becomes the leader-elect</li> <li>Configure using the following options:   <code>- --leader-elect true</code></li> <li><code>--leader-elect-lease-duration &lt;x&gt;s</code> (how long does the non-leader wait until attempting to become the leader again)</li> <li><code>--leader-elect-renew-deadline &lt;x&gt;s</code> (interval between acting master attempting to renew a leadership slot before it stops leading (must be equal or less than lease duration)</li> <li><code>--leader-elect-retry-period &lt;x&gt;s</code> (The duration the clients should wait between attempting acquisition and renewal of a leadership)</li> </ul> </li> <li>If ETCD is part of the master nodes: Stacked topology</li> <li>Easy setup and management</li> <li>Fewer servers involved</li> <li>Poses a risk when failures occur</li> <li>External ETCD Topology - ETCD setup on a separate node</li> <li>Less risky</li> <li>Harder to setup</li> <li>Where the etcd is setup can be determined by the --etcd-servers option on the apiservers configuration</li> </ul>"},{"location":"certifications/CKA/10_Design-a-Kubernetes-Cluster.html#etcd-in-high-availability","title":"ETCD In High-Availability","text":"<ul> <li>ETCD Previously deployed as one server, but can run multiple instances, each containing the same data as a fault-tolerant measure</li> <li>To allow this, ETCD needs to ensure that all instances are consistent in terms of what data they store, such that you can write to and read data from any of the instances</li> <li>In the event multiple write requests come in to an ETCD, the leader-elect processes the write request, which then transfers a copy to the other nodes</li> <li>Leaders decided using RAFT algorithm, using random timers for initiating requests</li> <li>The first to finish the request becomes the leader</li> <li>Sends out continuous notifications from then on saying \"i'm continuing as leader\"</li> <li>If no notifications received e.g. the leader goes down, reelection occurs</li> <li>A write will only be considered if the transfer is completed to the majority of the Nodes or the Quorum</li> <li>Quorum = N/2 + 1; where N = Node number (for .5 numbers, round down)</li> <li>Quorum = Minimum number of nodes in an N-node cluster that need to be running for a cluster to operate as expected. Fault Tolerance = Instances Number - Quorum</li> <li>Odd number of master nodes recommended</li> <li>To install etcd, download the binaries from the Github repo and configure the certificates (See previous sections) and configure in <code>etcd.service</code></li> <li>Etcdctl can be used to backup the data</li> <li>ETCDCTL_API default version = 2, 3 COMMONLY USED</li> <li>For ETCD, the following number of nodes are recommended: 3, 5 or 7</li> </ul>"},{"location":"certifications/CKA/11_Install-Kubernetes-the-kubeadm-way.html","title":"11.0 - Install Kubernetes the Kubeadm Way","text":"<ol> <li>Decide on Configuration - Master vs Worker</li> <li>Install a container runtime (Docker, etc.)</li> <li>Install Kubeadm</li> <li>Initialise the Master Node</li> <li>Pod Network Setup</li> <li>Join worker nodes to the master node</li> </ol>"},{"location":"certifications/CKA/11_Install-Kubernetes-the-kubeadm-way.html#resources","title":"Resources","text":"<ul> <li>Github Repo</li> <li>Kubernetes Documentation</li> </ul>"},{"location":"certifications/CKA/11_Install-Kubernetes-the-kubeadm-way.html#deploy-with-kubeadm-provision-vms-with-vagrant","title":"Deploy with Kubeadm - Provision VMs with Vagrant","text":"<ul> <li>Using links above, and ensuring Virtualbox and vagrant are installed, clone the repo and run <code>vagrant up</code></li> <li>This will pull the images for the vms defined and kubernetes, creating a kubernetes master node and 2 workers</li> <li>To access any, <code>vagrant ssh &lt;vm name&gt;</code> (can run commands to test things)</li> <li>Check status with <code>vagrant status</code></li> </ul>"},{"location":"certifications/CKA/11_Install-Kubernetes-the-kubeadm-way.html#demo-deploy-with-kubeadm","title":"Demo - Deploy with Kubeadm","text":"<ul> <li>Ssh into kubemaster: <code>vagrant ssh kubemaster</code></li> <li>Check <code>br_nefilter</code> is deployed: <code>lsmod | grep br_netfilter</code></li> <li>Allows iptables to can see bridged traffic</li> <li>Load the kernel module: <code>sudo modprobe br_netfilter</code></li> <li> <p>Run the previous <code>lsmod</code> command for verification and repeat for each node.</p> </li> <li> <p>Load the relevant modules and iptables settings into a <code>k8s.conf</code> file:</p> </li> </ul> <pre><code>cat &lt;&lt;EOF | sudo tee /etc/modules-load.d/k8s.conf\nbr_netfilter\nEOF\n\ncat &lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.conf\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-iptables = 1\nEOF\n\nsudo sysctl --system\n</code></pre> <ul> <li>Run the rest as sudo by default: <code>sudo -i</code></li> <li>Install container runtime e.g. Docker via supporting documentation.</li> <li> <p>Install kubeadm, kubectl and kubelet via supporting documentation.</p> </li> <li> <p>Note: If not using Docker as the container runtime, additional configuration may be needed for the CGroup</p> </li> <li> <p>Creating the cluster starts with initializing the master node: <code>kubadm init --pod-network-cidrs &lt;range&gt; --apiserver-advertise-address &lt;masternode address&gt;</code></p> </li> <li> <p>Command sample: <code>kubeadm init --pod-network-cidr 10.244.0.0/16 --apiserver-advertise-address=192.168.56.2</code></p> </li> <li> <p>Install a pod network addon upon initialization e.g. Flannel, Weave, Calico; each has their own supporting documentation for this.</p> </li> <li> <p>Once complete, instructions are provided on how to start using the cluster, including setting the kubeconfig location and ownership.</p> </li> <li> <p>Worker nodes can be added to the cluster by running the provided command upon initialisation: <code>sudo kubeadm join .... &lt;parameters&gt;</code></p> </li> </ul>"},{"location":"certifications/CKA/13_Troubleshooting.html","title":"13.0 - Troubleshooting Overview","text":""},{"location":"certifications/CKA/13_Troubleshooting.html#application-failure","title":"Application Failure","text":"<ul> <li>Consider a 2-tier application, composed of a database and web server</li> <li>It's good practice to draw out the usual data flow for the application to help visualise the failure</li> <li>E.g. suppose a user is complaining about accessibility issues</li> <li>Check accessibility from the frontend first:</li> <li>Check if the web server is accessible on the IP of the node port using curl:<ul> <li><code>curl http://&lt;service-IP&gt;:node-port</code></li> </ul> </li> <li>Check the service to see if it has discovered the endpoints for the web pod<ul> <li><code>kubectl describe service &lt;Service-name&gt;</code></li> </ul> </li> <li>If the service didn't find the endpoints for the pod, check the service-to-pod-discovery by comparing the selectors configured on the service and the pod respectively</li> <li>Check the pod to ensure in a running state: kubectl get pods<ul> <li>Check running, restarts, logs and events</li> </ul> </li> <li>Repeat for db service and pods</li> <li>Additional tips available via the Kubernetes documentation</li> </ul>"},{"location":"certifications/CKA/13_Troubleshooting.html#control-plane-failure","title":"Control Plane Failure","text":"<ul> <li>Check node status</li> <li>Check pod status on nodes</li> <li>Check kube-system pods</li> <li>Check services: <code>service &lt;servicename&gt; status</code></li> <li>Check logs for each pod: <code>kubectl logs &lt;podname&gt; -n kube-system</code></li> </ul>"},{"location":"certifications/CKA/13_Troubleshooting.html#worker-node-failure","title":"Worker Node Failure","text":"<ul> <li>Check node status and details (Describe)</li> <li>See lastheartbeat field for last online time</li> <li>Check resource consumptions: kubectl top nodes</li> <li>Check the kubelet status: service kubelet status</li> <li>Check certs: <code>openssl x500 -in /var/lib/kubelet/&lt;cert&gt; -text</code></li> <li>See if they're issued by the correct ca</li> </ul>"},{"location":"certifications/CKA/14_JSONpath-introduction.html","title":"14.1 - Introduction to JSONPath","text":""},{"location":"certifications/CKA/14_JSONpath-introduction.html#1411-introduction-to-yaml","title":"14.1.1 - Introduction to YAML","text":"<ul> <li>Ansible playbooks format similar to XML and JSON</li> <li>Used to express data</li> <li>Data in YAML files, at its most basic form is a series of Key-Value pairs, separated by a colon</li> <li>For an array:</li> </ul> <pre><code>- entry1: value1\n- entry2: value2\n- entry3: value3\n</code></pre> <ul> <li>For a dictionary:</li> </ul> <pre><code>item:\n  property1: value1\n  property2: value2\n</code></pre> <ul> <li>Note: 2 spaces determine what properties come under what iteM</li> <li>Can have lists containing dictionaries containing lists</li> <li>To store information about different properties of a single item, use a dictionary</li> <li>If the properties need further segregation, can use dictionaries within dictionary items</li> <li>For multiple items of the same type, use an array</li> <li> <p>To store information for multiple items, expand each array item to a the dictionary for each</p> </li> <li> <p>Key Notes:</p> </li> <li>Dictionary = Unordered</li> <li>Lists / Arrays = Ordered</li> <li>Comments added via #</li> </ul>"},{"location":"certifications/CKA/14_JSONpath-introduction.html#1412-introduction-to-jsonpath","title":"14.1.2 - Introduction to JSONpath","text":"<ul> <li>Yaml vs Json:</li> <li>Data can be expressed via both</li> <li>To segregate data, methods differ:<ul> <li>YAML - Indentations and -'s</li> <li>JSON - Indentation and {} for dictionaries, [] for array items</li> </ul> </li> <li>JSON data can be queried via JSON Path</li> <li>To select an item, specify it as <code>&lt;item&gt;</code></li> <li>For a dictionary property: <code>&lt;item&gt;.&lt;property&gt;</code></li> <li>Note: Anything within { } denotes a dictionary</li> <li>The top-level dictionary, which isn't named, is denoted by a $</li> <li>A typical query is <code>$.item.property</code> etc</li> <li>Any output from a JSONpath query is an array []</li> <li>To query an array/list, use square brackets to reference the position, with positions starting at [0].</li> <li>E.g. 1st element: <code>$.[0]</code></li> <li>For dictionaries in lists, combine the query use for all</li> <li>For criteria: <code>$.[CRITERIA]</code> e.g.:</li> <li><code>$.[?( @ &gt; 40 )]</code></li> <li>In this case, ?() signals to use a filter, the @ symbol signifies \"each item\"</li> <li>Queries could be used to accommodate for changing positions</li> <li>E.g. <code>$.car.wheels[?(@.location == \"rear-right\")].model</code><ul> <li>Only entries that satisfy the citeria \"location = rear-right\" will be returned.</li> </ul> </li> </ul>"},{"location":"certifications/CKA/14_JSONpath-introduction.html#1413-jsonpath-wildcard","title":"14.1.3 - JSONPath: Wildcard","text":"<ul> <li>Denoted by *, meaning \"any\", can be used to retrieve all/any properties of a particular dictionary</li> <li>Can swap as a value when referencing an array position i.e. []</li> </ul>"},{"location":"certifications/CKA/14_JSONpath-introduction.html#1414-jsonpath-advanced-list-queries","title":"14.1.4 - JSONPath: Advanced List Queries","text":"<ul> <li>To get all names in an array's particular range, add [x:y], where x is the first element's position, y is the end position of the range +1</li> <li>To iterate over a step, insert: [x:y:z], where z is the step rate</li> <li>To get the last item: [-1:0]</li> </ul>"},{"location":"certifications/CKA/14_JSONpath-introduction.html#1415-advanced-kubectl-commands-kubectl-and-jsonpath","title":"14.1.5 - Advanced Kubectl Commands: Kubectl and JSONPath","text":"<ul> <li>Prerequisites:</li> <li>JSONPath for beginners</li> <li>JSONPath Practice tests - Kodekloud: General use and for Kubernetes</li> <li>Why JSON Path?</li> <li>Large data sets involved with production-grade clusters<ul> <li>100s of nodes</li> <li>1000s of PODS, Deployments, ReplicaSets etc</li> </ul> </li> <li>More often than not, will want to quickly print information for large numbers of resources and particular information</li> <li>Kubectl commands invokes the APIServer, which obtains the information requested in a JSON format and is redisplayed in a readable format by Kubectl</li> <li>Particularly noticeable in kubectl get commands</li> <li>For additional information, add <code>-o</code> wide flag</li> <li>Example:</li> <li>Suppose we want to see the following:<ul> <li>CPU count</li> <li>Taints and Tolerations</li> <li>Pod name and Images</li> </ul> </li> <li>There is no built-in kubectl command for this, but we can use kubectl and JSON path in combination to get the particular fields</li> <li>To use JSON Path in Kubectl, consider the 4 steps:</li> <li>Identify the kubectl command required e.g. kubectl get pods</li> <li>Familiarize yourself with the JSON format output: add the <code>-o</code> json flag</li> <li>From the JSON output, figure out the custom query you'd want to apply, e.g. for container images of a particular pod: <code>.items[*].spec.containers[*].image</code></li> <li>Combine the kubectl command with the JSON query i.e. <code>kubectl get pods -o=jsonpath='{JSON_PATH_QUERY}'</code></li> <li>Note:</li> <li>For multiple queries, within the '', encompass each query by {}</li> <li>To format this, use any of the following:<ul> <li><code>{\"\\n\"}</code> - New line (Add in between queries)</li> <li><code>{\"\\t\"}</code> - Tab</li> </ul> </li> <li>Looping through ranges: <code>'{range .items[*]} {Queries} {end}'</code></li> <li>Can print custom columns via: <code>-o=custom-columns=&lt;COLUMN_NAME&gt;:&lt;JSON PATH&gt;, &lt;COLUMN&gt;:&lt;JSON PATH&gt;</code></li> <li>Recommended to view full query first then forming the JSON query</li> <li>Use <code>--sort-by</code> property where necessary e.g. <code>--sort-by=.metadata.name</code></li> </ul>"},{"location":"certifications/CKA/Mocks/KodeKloud_Mock_3/02_Mock-Exam-Notes-3.html","title":"15.2 - Mock Exam 3","text":""},{"location":"certifications/CKA/Mocks/KodeKloud_Mock_3/02_Mock-Exam-Notes-3.html#pvviewer","title":"PVViewer","text":"<ul> <li><code>kubectl create serviceaccount</code></li> <li><code>kubectl create clusterrole -&lt;name&gt; --resource=persistentvolumes --verb=list</code></li> <li><code>kubectl create clusterrolebinding pvviewer-role-binding --clusterrole=&lt;clusterrole&gt; --serviceaccount=namespace.serviceaccount</code></li> </ul>"},{"location":"certifications/CKA/Mocks/KodeKloud_Mock_3/02_Mock-Exam-Notes-3.html#multi-pod","title":"Multi-Pod","text":"<ul> <li><code>Name: name</code></li> <li><code>Value: alpha/beta</code></li> <li><code>Beta command = sleep 4800</code></li> </ul>"},{"location":"certifications/CKA/Mocks/KodeKloud_Mock_3/02_Mock-Exam-Notes-3.html#kubeconfig","title":"Kubeconfig","text":"<ul> <li>Run <code>kubectl cluster-info --kubeconfig=/path/to/file</code></li> <li>Analyse output</li> <li>Edit API Server port to 6443</li> </ul>"},{"location":"certifications/CKA/Mocks/KodeKloud_Mock_3/02_Mock-Exam-Notes-3.html#replicas","title":"Replicas","text":"<ul> <li>Edit kube-controller yaml file and edit typos</li> </ul>"},{"location":"certifications/CKA/Mocks/Kodekloud_Mock_2/01_Mock-Exam-Notes-2.html","title":"15.1 - Mock Exam 02","text":""},{"location":"certifications/CKA/Mocks/Kodekloud_Mock_2/01_Mock-Exam-Notes-2.html#etcd-backup","title":"ETCD Backup","text":"<ul> <li>Best bet is to use the Kubernetes Documentation</li> <li>Check the version: <code>ETCDCTL_API=3 etcdctl version</code></li> <li>Navigate to <code>/etc/kubernetes/manifests</code></li> <li>Check the etcd YAML and find the following values or paths to be used in the command:</li> <li>Endpoints</li> <li>Cacert</li> <li>Cert</li> <li> <p>Key</p> </li> <li> <p>Run the command: <code>ETCDCTL_API=3 etcdctl --endpoints=&lt;&gt; --cacert=&lt;&gt; --cert=&lt;&gt; --key=&lt;&gt; snapshot save &lt;filepath to backup&gt;</code></p> </li> </ul>"},{"location":"certifications/CKA/Mocks/Kodekloud_Mock_2/01_Mock-Exam-Notes-2.html#use-pv-question","title":"Use-PV Question","text":"<ul> <li>Create a PersistentVolumeClaim</li> <li>10Mi</li> <li>Ensure correct access mode</li> <li>No storage class needs to be specified</li> <li>Specify the PVC and VolumeMount as required</li> </ul>"},{"location":"certifications/CKA/Mocks/Kodekloud_Mock_2/01_Mock-Exam-Notes-2.html#record-annotations","title":"Record Annotations","text":"<ul> <li><code>kubectl run &lt;parameters&gt; --record</code></li> <li><code>kubectl set image &lt;parameters&gt; --record</code></li> <li>Use <code>kubectl rollout history</code> and <code>kubectl rollout status</code> where appropriate.</li> </ul>"},{"location":"certifications/CKA/Mocks/Kodekloud_Mock_2/01_Mock-Exam-Notes-2.html#certificate-signing-request-csr","title":"Certificate Signing Request (CSR)","text":"<ul> <li>Use manage TLS certificates task in Kubernetes documentation</li> <li>Creatue using spec provided in YAML file</li> <li>Encode <code>.csr</code> file in base64 as appropriate</li> <li>Create the CSR</li> <li>Approve the CSR</li> <li>Create role with appropriate spec via <code>kubectl create</code> - separate the verbs with commas</li> <li>Create rolebinding -&gt; developer-role-binding --role=developer --user=john --namespace=developer via <code>kubectl create</code></li> <li>Check permissions with <code>kubectl auth can-i</code> with appropriate options</li> </ul>"},{"location":"certifications/CKA/Mocks/Kodekloud_Mock_2/01_Mock-Exam-Notes-2.html#nginx-resolver","title":"Nginx-Resolver","text":"<ul> <li>Use port 80</li> <li>Type=ClusterIP</li> <li>Test DNS Lookup with Busybox Pod: <code>--rm -it --nslookup &lt;service&gt;</code></li> <li>Record as appropriate</li> <li><code>kubectl expose pod nginx-resolver --port=80 --target-port=80</code></li> <li><code>kubectl describe svc</code> -&gt; get IP and endpoing</li> <li><code>kubectl run test nslookup --image=busybox:1.28 --rm -it -- nslookup nginx-resolver-service &gt; /root/nginx.svc</code></li> <li><code>kubectl run test-nslookup --image=busybox:1.28 --rm-it -- nslookup &lt;pod IP address&gt; &gt; filepath</code></li> </ul>"},{"location":"certifications/CKAD/01_Introduction.html","title":"1.0 - Introduction","text":""},{"location":"certifications/CKAD/01_Introduction.html#11-introduction","title":"1.1 - Introduction","text":""},{"location":"certifications/CKAD/01_Introduction.html#course-structure-and-notes","title":"Course Structure and Notes","text":"<ul> <li>Exam is 100% Practical</li> <li>Prerequisites Tools and Knowledge:</li> <li>Lab environment</li> <li>Kubernetes Architecture</li> <li>Master and Worker Node Functionality</li> <li>Pods, ReplicaSets, and Deployments</li> <li>Kubernetes CLI (<code>kubectl</code>)</li> <li>Understand YAML</li> <li>Services</li> <li>Namespaces</li> </ul>"},{"location":"certifications/CKAD/01_Introduction.html#course-objectives","title":"Course Objectives","text":"<ol> <li>Core Concepts</li> <li>Configuration</li> <li>Multi-container pods</li> <li>Observability</li> <li>Pod Design</li> <li>Services and Networking</li> <li> <p>State Persistence</p> </li> <li> <p>In general, aim to gain practice in the above and resolve associated configuration issues.</p> </li> </ol>"},{"location":"certifications/CKAD/01_Introduction.html#14-certification-details","title":"1.4 - Certification Details","text":"<ul> <li>Kubernetes is a very fast-growing technology, as such, more engineers need to be trained in it.</li> <li>Becoming certified allows for one to stand out from the crowd by demonstrating the capabilities needed to design and build cloud-native apps.</li> <li>Exam cost: ~$300</li> <li>Prerequisites outlined in candidate handbook</li> <li>Fully practical, 2-hour exam</li> <li>The Kubernetes documentation can be referred to at any point as 1 additional tab.</li> </ul>"},{"location":"certifications/CKAD/02_Core-Concepts.html","title":"2.0 - Core Concepts","text":""},{"location":"certifications/CKAD/02_Core-Concepts.html#21-recap-kubernetes-architecture","title":"2.1 - Recap: Kubernetes Architecture","text":""},{"location":"certifications/CKAD/02_Core-Concepts.html#node","title":"Node","text":"<ul> <li>A physical or virtual machine where Kubernetes is installed</li> <li>Containers are deployed to these nodes via the Kubernetes CLI.</li> <li>To avoid applications failing to run on a node, it's advised to have multiple nodes together or multiple replicase of an app.</li> <li>This can support high-availability and fault-tolerance.</li> </ul>"},{"location":"certifications/CKAD/02_Core-Concepts.html#cluster","title":"Cluster","text":"<ul> <li>A set of nodes grouped together</li> <li>In the event of one node or app failing, users can be redirected to another node; maintaining accessibility.</li> <li>Clusters therefore allow load balancing to be supported in Kubernetes.</li> </ul>"},{"location":"certifications/CKAD/02_Core-Concepts.html#master-node","title":"Master Node","text":"<ul> <li>Watches over the worker nodes and orchestrates containers within the cluster.</li> <li>Other responsibilities include:</li> <li>Cluster management</li> <li>Storing information around cluster members, etc.</li> <li>Monitoring node status</li> <li>Managing per-node workloads.</li> </ul>"},{"location":"certifications/CKAD/02_Core-Concepts.html#cluster-components","title":"Cluster Components","text":""},{"location":"certifications/CKAD/02_Core-Concepts.html#api-server","title":"API Server","text":"<ul> <li>Acts as the frontend for Kubernetes</li> <li>User Management devices and CLI tools all go through the API server when interacting with the cluster.</li> </ul>"},{"location":"certifications/CKAD/02_Core-Concepts.html#etcd-server","title":"ETCD Server","text":"<ul> <li>Acts as a key store service</li> <li>Stores all data used to manage the cluster</li> <li>Responsible for implementing logs within the cluster -&gt; Avoids master-master conflicts</li> </ul>"},{"location":"certifications/CKAD/02_Core-Concepts.html#scheduler","title":"Scheduler","text":"<ul> <li>Distributes workloads or containers across nodes</li> <li>Looks for newly-created containers and assigns them to nodes accordingly</li> </ul>"},{"location":"certifications/CKAD/02_Core-Concepts.html#controller","title":"Controller","text":"<ul> <li>The primary orchestrators</li> <li>Responsible for noticing and responding to node/container failure, etc.</li> <li>Makes decisions to bring up new containers to replace those that have vailed (or another appropriate action)</li> </ul>"},{"location":"certifications/CKAD/02_Core-Concepts.html#container-runtime","title":"Container Runtime","text":"<ul> <li>The underlying software used to run containers.</li> </ul>"},{"location":"certifications/CKAD/02_Core-Concepts.html#kubelet","title":"Kubelet","text":"<ul> <li>The agent running on each node.</li> <li>Responsible for ensuring containers run on nodes as expected.</li> </ul>"},{"location":"certifications/CKAD/02_Core-Concepts.html#master-vs-worker-node","title":"Master vs Worker Node","text":"<ul> <li>Worker nodes host containers and other associated resources.</li> <li>Requires container runtime to be installed (typically Docker)</li> <li>Master Node has Kube-API server running</li> <li>Worker nodes need the Kubelet agent to interact with the API server:</li> <li>Provides info regarding worker node status</li> <li>Allows ability to carry out interactions / tasks requested by the master node</li> <li>ETCD key-value store found only on the master node for security purposes</li> <li>Controller and scheduler also found on master node only - the master node handles all orchestration and workload allocation tasks.</li> </ul> Master Node Worker Node kube-API server kubelet etcd key-value store container runtime (e.g. Docker) Controllers Scheduler"},{"location":"certifications/CKAD/02_Core-Concepts.html#kubectl","title":"Kubectl","text":"<ul> <li>Tool used to deploy and manage resources on a Kubernetes cluster</li> <li>Common command examples:</li> </ul> <pre><code>\n## Deploy application onto cluster\n\nkubectl run hello-minikube\n\n## View cluster-related information\n\nkubectl cluster-info\n\n## Get / display information about the nodes in a cluster\n\nkubectl get nodes\n\n</code></pre>"},{"location":"certifications/CKAD/02_Core-Concepts.html#22-docker-vs-containerd","title":"2.2 - Docker vs ContainerD","text":""},{"location":"certifications/CKAD/02_Core-Concepts.html#background","title":"Background","text":"<ul> <li>Began as the primary container runtime based on its enhanced user experience, Kubernetes was then introduced to orchestrate Docker containers ONLY.</li> <li>As Kubernetes grew in popularity, other container runtimes wanted to be able to work with Kubernetes.</li> <li>This led to the introduction of the Container Runtime Interface (CRI).</li> <li>CRI allowed any vendor to work as a Container Runtime for Kubernetes, so long as they adhered to the Open Container Initiative (OCI) standards:</li> <li>imagespec - standards for how a particular image is built.</li> <li>runtimespec - standards for how a particular runtime should be developed.</li> <li>Docker wasn't built to support the CRI standards, to work around this, dockershim was introduced to support it as a container runtime interface in Kubernetes.</li> <li>Docker consists of multiple components in addition to the runtime, runc including:</li> <li>API</li> <li>CLI</li> <li>VOLUMES</li> <li>The runtime for Docker, runc, ran by the daemon containerd, IS CRI compatible, and can be used outside of Docker on its own.</li> <li>Given this, maintaining Dockershim was deemed unnecessary, and Kubernetes support for it was therefore dropped from v1.24.</li> </ul>"},{"location":"certifications/CKAD/02_Core-Concepts.html#containerd","title":"ContainerD","text":"<ul> <li>A standalone container runtime that can be installed without Docker.</li> <li>Comes with its own CLI tool <code>ctr</code> - advised only for debugging containerD and not much else.</li> <li>Example commands:</li> <li><code>ctr images pull docker.io/library/redis:alpine</code></li> <li><code>ctr run &lt;image url&gt;:&lt;tag&gt; &lt;container name&gt;</code></li> </ul>"},{"location":"certifications/CKAD/02_Core-Concepts.html#nerdctl","title":"NerdCTL","text":"<ul> <li>Provides a Docker-like CLI for containerD, supporting docker-compose and the latest features in containerD such as:</li> <li>Encrypted images</li> <li>Lazy Pulling</li> <li>P2P Image Distribution</li> <li>Image signing and verifying</li> <li> <p>Namespaces in Kubernetes</p> </li> <li> <p>Comparing Docker and NerdCTL Commands:</p> </li> </ul> Command Goal Docker Command NerdCTL Command Run a container <code>docker run --name redis redis:alpine</code> <code>nerdctl run --name redis redis:alpine</code> Run a container with typical options <code>docker run --name webserver -p 80:80 -d nginx</code> <code>nerdctl run --name webserver -p 80:80 -d nginx</code>"},{"location":"certifications/CKAD/02_Core-Concepts.html#crictl","title":"CRIctl","text":"<ul> <li>Provides a CLI for CRI-compatible container runtimes, installed separately to a given runtime.</li> <li>Used to inspect and debug container runtimes only, it does not do anything with running containers.</li> <li> <p>Applicable to multiple runtimes.</p> </li> <li> <p>Example commands:</p> </li> <li>Pull an image: <code>crictl pull busybox</code></li> <li>List images: <code>crictl images</code></li> <li>Exec into a container: <code>crictl exec -i -t &lt;container id&gt; &lt;command&gt;</code></li> <li>List the logs of a container: <code>crictl logs &lt;container id&gt;</code></li> <li> <p>Get Kubernetes pods: <code>crictl pods</code></p> </li> <li> <p>Container runtime endpoints are called in the following priority by <code>crictl</code>:</p> </li> <li><code>unix:///run/containerd/containerd.sock</code></li> <li><code>unix:///run/crio/crio.sock</code></li> <li><code>unix:///var/run/cri-dockerd.sock</code></li> </ul>"},{"location":"certifications/CKAD/02_Core-Concepts.html#22-recap-pods","title":"2.2 - Recap: Pods","text":"<ul> <li>Kubernetes doesn't deploy containers directly to nodes, they're encapsulated into Pods; a Kubernetes object.</li> <li>Pod: A single instance of an application</li> <li> <p>A pod is the smallest possible object in Kubernetes</p> </li> <li> <p>Suppose a containersied app is running on a single pod in a single node. If the user demand increases, how is the load balanced?</p> </li> <li>One cannot have multiple containers to a pod</li> <li>Instead, a new pod will be required with a new instance of the application</li> <li> <p>If the user demand increases further, but no pods are available on the node; a new node has to be created.</p> </li> <li> <p>In general, pods and containers have a 1-to-1 relationship.</p> </li> </ul>"},{"location":"certifications/CKAD/02_Core-Concepts.html#multi-container-pods","title":"Multi-Container Pods","text":"<ul> <li>A single pod can contain more than 1 container, however it cannot be running the same application.</li> <li>In some cases, one may have a \"helper\" container running alongside the primary application</li> <li>The helper container usually runs support processes such as:<ul> <li>Process user-entered data</li> <li>Carry out initial configuration</li> <li>Process uploaded files</li> </ul> </li> <li>When new pod is created, an additional helper-container will automatically be created alongside it.</li> <li>App and helper container communicate and share resources across a shared network</li> <li>The two containers have a 1-to-1 relationship.</li> </ul>"},{"location":"certifications/CKAD/02_Core-Concepts.html#example-kubectl-commands","title":"Example Kubectl Commands","text":"<ul> <li><code>kubectl run &lt;container name&gt;</code></li> <li>Runs docker container by creating a pod</li> <li>To specify image, append <code>--image &lt;image name&gt;:&lt;image tag&gt;</code></li> <li> <p>Image will then be pulled from DockerHub</p> </li> <li> <p><code>kubectl get pods</code></p> </li> <li>Return a list detailing the pods in the default namespace</li> <li>Append <code>--namespace &lt;namespace&gt;</code> to specify a namespace.</li> </ul>"},{"location":"certifications/CKAD/02_Core-Concepts.html#24-recap-pods-with-yaml","title":"2.4 - Recap: Pods with YAML","text":"<ul> <li>Kubernetes uses YAML files as inputs for object creation e.g. pods, deployments, services.</li> <li>These YAML files always contain 4 key fields:</li> <li>apiVersion:<ul> <li>Version of Kubernetes API used to create the object</li> <li>Correct api version required for varying objects e.g. <code>v1</code> for Pods and Services, <code>apps/v1</code> for Deployments</li> </ul> </li> <li>kind:<ul> <li>type of object being created</li> </ul> </li> <li>metadata:<ul> <li>data referring to specifics of the object</li> <li>Expressed as a dictionary</li> <li>Labels: Children of metadata</li> <li>Indents denote what metadata is related toa  child of a property</li> <li>Used to differentiate pods</li> <li>Any key-value pairs allowed in labels</li> </ul> </li> <li> <p>spec:</p> <ul> <li>specification containing additional information around the object</li> <li>Written in a dictionary</li> <li><code>-</code> denotes first item in a dictionary</li> </ul> </li> <li> <p>To create a resource from YAML: <code>kubectl create -f &lt;definition&gt;.yaml</code></p> </li> <li> <p>To view pods: <code>kubectl get pods</code></p> </li> <li> <p>To view detailed info of a particular pod: <code>kubectl describe pod &lt;pod name&gt;</code></p> </li> </ul>"},{"location":"certifications/CKAD/02_Core-Concepts.html#25-creating-pods-with-yaml-demo","title":"2.5 - Creating Pods with YAML: Demo","text":"<ul> <li>To create YAML files, any editor will suffice</li> <li>All files end with <code>.yml</code> or <code>.yaml</code></li> <li>Example definition:</li> </ul> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: myapp-pod\n  labels:\n    app: myapp\nspec:\n  containers:\n  - name: nginx-container\n    image: nginx\n</code></pre> <ul> <li>To deploy: <code>kubectl create -f &lt;pod definition&gt;.yaml</code></li> <li>To verify deployment: <code>kubectl get pods</code></li> </ul>"},{"location":"certifications/CKAD/02_Core-Concepts.html#29-editing-existing-pods","title":"2.9 - Editing Existing Pods","text":"<ul> <li>Given a pod definition file, one can edit it and use it create a new pod.</li> <li>If not given a pod definition file, one can extract it by  <code>kubectl get pod &lt;pod name&gt; -o yaml &gt; file.yaml</code></li> <li>The extracted YAML file can then be edited and applied, either by deleting the pod and recreating it, or using <code>kubectl apply</code></li> <li>Alternatively, one can use <code>kubectl edit &lt;pod name&gt;</code> to edit the live pod's properties</li> <li>Some properties cannot be edited on live deployments - in this case it is advisable to delete and recreate the resource.</li> </ul>"},{"location":"certifications/CKAD/02_Core-Concepts.html#210-replicasets-recap","title":"2.10 - ReplicaSets Recap","text":"<ul> <li>Controllers monitor Kubernetes objects and respond accordingly</li> <li>A key one used is the replication controller</li> <li>Consider a single pod running an application:</li> <li>If this pod crashes, the app becomes inaccessible</li> <li>To prevent this, it'd be better to have multiple instances of the same app running simultaneously</li> <li>Replication controller allows the running of multiple instances of the same pod in the cluster; leading to higher availability</li> <li>Note: Even if there is a single pod, the replication controller will automatically replace it in the event of failure - this leans into the idea of the \"desired state\"; Kubernetes will ensure that the desired amount of replicas are available.</li> </ul>"},{"location":"certifications/CKAD/02_Core-Concepts.html#load-balancing-and-scaling","title":"Load Balancing and Scaling","text":"<ul> <li>The replication controller is needed to create replicas of the same pod and share the load across it.</li> <li>Consider a single pod serving a single user:</li> <li>If a new user wants to acces the service, the controller automatically deploys an additional pod(s) to balance the load</li> <li> <p>If demand exceeds node space, the controller will create additional pods on other available node(s) in the cluster automatically</p> </li> <li> <p>One can therefore see the replication controller spans multiple nodes</p> </li> <li> <p>It helps to balance the load across multiple pods on different nodes and supports scalability.</p> </li> <li> <p>In terms of the replication controller, 2 terms are considered:</p> </li> <li>Replication Controller</li> <li> <p>Replica Set</p> </li> <li> <p>ReplicaSet is the newer technology for the role of Replication Controller</p> </li> <li> <p>Replication controllers are defined in YAML format similar to the following:</p> </li> </ul> <pre><code>apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: myapp-rc\n  labels:\n    app: myapp\n    type: frontend\nspec:\n  replicas: &lt;replica number&gt;\n  template:\n    metadata:\n      &lt;pod metadata&gt;\n    spec:\n      &lt;pod spec&gt;\n</code></pre> <ul> <li>There are 2 definition files posted, the replication controller's definition being a \"parent\" of the pod's definition file.</li> <li>The replication controller is created in standard practice via <code>kubectl create -f &lt;filename&gt;.yaml</code></li> <li>To view the RC: <code>kubectl get replicationcontroller</code></li> <li>Displays the number of desired, currently available, and ready pods for associated replication controllers.</li> <li> <p>Pods are still viewable via <code>kubectl get pods</code></p> </li> <li> <p>ReplicaSet example definition:</p> </li> </ul> <pre><code>apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: myapp-replicaset\n  labels:\n    app: myapp\n    type: frontend\nspec:\n  template:\n    metadata:\n      &lt;pod metadata&gt;\n    spec:\n      &lt;pod spec&gt;\n  selector:\n    matchLabels:\n      type: frontend\n</code></pre> <ul> <li>Selectors ghelp replicasets determine what pods to focus on</li> <li>This is required as replicasets can also manage pods not created or associated with the replicaset - so long as the labels match the selectr.</li> <li>Selectors are the main difference between ReplicaSets and ReplicationControllers</li> <li>If not defined, it will assume the same lable provided in the pod definition file</li> <li>Creation done via <code>kubectl create -f &lt;filename&gt;.yaml</code> as per usual</li> <li>Pods can be checked via <code>kubectl get pods</code></li> </ul>"},{"location":"certifications/CKAD/02_Core-Concepts.html#labels-and-selectors","title":"Labels and Selectors","text":"<ul> <li>Consider a deployment of an application with 3 pods:</li> <li>To create a replication controller or replicaset, one can ensure that at any given point, 3 pods will be running.</li> <li>If the pods weren't created, the ReplicaSet will automatically create them.</li> <li> <p>ReplicaSet monitors the pods and deploys the replacements in the event of failure.</p> </li> <li> <p>The ReplicaSet knows what pods to monitor via labels</p> </li> <li> <p>In the <code>matchLabels</code> parameter, the label entered denotes the pods the replicaset should manage.</p> </li> <li> <p>The template section is required such that the pod can be redeployed based on the template defined.</p> </li> </ul>"},{"location":"certifications/CKAD/02_Core-Concepts.html#scaling","title":"Scaling","text":"<ul> <li>To scale a Kubernetes deployment, one can update the <code>replicas</code> number in the <code>.yaml</code> file associated, and run <code>kubectl replace -f &lt;definition&gt;.yaml</code></li> <li>Alternatively: <code>kubectl scale --replicas &lt;new amount&gt; &lt;defintion&gt;.yaml</code></li> <li>Alternatively: <code>kubectl scale --replicas=&lt;number&gt; --replicaset &lt;replicaset name&gt;</code></li> <li>This method does not update the YAML file.</li> </ul>"},{"location":"certifications/CKAD/02_Core-Concepts.html#command-summary","title":"Command Summary","text":"<ul> <li>Create a ReplicaSet or object in KuberneteS: <code>kubectl create -f &lt;definition&gt;.yaml</code></li> <li>List ReplicaSets: <code>kubectl get replicasets</code></li> <li>Delete a replicaset and its underlying pods: <code>kubectl delete replicaset &lt;replicaset name&gt;</code></li> <li>Replace or update the replicaset: <code>kubectl replace -f &lt;replicaset definition&gt;.yaml</code></li> <li>Scale a replicaset: <code>kubectl scale --replicas=&lt;number&gt; -f &lt;defintiion&gt;.yaml</code></li> </ul>"},{"location":"certifications/CKAD/02_Core-Concepts.html#213-deployments-recap","title":"2.13 - Deployments Recap","text":"<ul> <li>When deploying an application in a production environment, like a web server:</li> <li>Many instances of the web server could be needed</li> <li>Need to be able to upgrade the instances seamlessly one-after-another (rolling updates)</li> <li> <p>Need to avoid simultaneous updates as this could impact user accessibility</p> </li> <li> <p>In the event of update failure, one should be able to rollback upgrades to a previously working iteration</p> </li> <li> <p>If wanting to make multiple changes to the environment, can pause each environment to make the changes, and resume when updates are in effect.</p> </li> <li> <p>These capabilities are provided via Kubernetes Deployments.</p> </li> <li>These are objects higher in the hierarchy than a ReplicaSet</li> <li> <p>Provides capabilities to:</p> <ul> <li>Upgrade underlying instances seamlessly</li> <li>Utilise rolling updates</li> <li>Rollback changes during failure</li> <li>Pause and resume environments to allow changes to take place.</li> </ul> </li> <li> <p>As usual, Deployments can be defined by YAML definitions:</p> </li> </ul> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp-deployment\n  labels:\n    app: myapp\n    type: frontend\nspec:\n  template:\n    metadata:\n      name: myapp-prod\n      labels:\n        app: myapp\n        type: frontend\n    spec:\n      containers:\n      - name: nginx-controller\n        image: nginx\n  replicas: 3\n  selector:\n    matchLabels:\n      type: frontend\n</code></pre> <ul> <li>To create deployment: <code>kubectl create -f &lt;deployment&gt;.yaml</code></li> <li> <p>View deployments: <code>kubectl get deployments</code></p> </li> <li> <p>Other commands: <code>kubectl get all</code> -&gt; Display all Kubernetes objects</p> </li> </ul>"},{"location":"certifications/CKAD/02_Core-Concepts.html#217-namespaces","title":"2.17 - Namespaces","text":"<ul> <li>A namespace is automatically created when a cluster is created</li> <li> <p>They serve to isolate the cluster resources such that they aren't accidentally maniuplated.</p> </li> <li> <p>Example:</p> </li> <li> <p>When developing an application, one can create a <code>Dev</code> and <code>Prod</code> namespace to keep resources isolated</p> </li> <li> <p>Each namespace can then have their own policies, detailing user access and controlm etc.</p> </li> <li>Resource limits may also be namespace-scoped.</li> </ul>"},{"location":"certifications/CKAD/02_Core-Concepts.html#dns","title":"DNS","text":"<ul> <li>For objects communicating in their namespace, they simply refer to the other object by their name.</li> <li>Example, for a web application pod connecting to a database service titled <code>db-service</code>, you would specify: <code>mysql.connect(\"db-service\")</code>.</li> <li>For objects communicating outside of their namespace, need to append the name of the namespace to access and communicate.</li> <li>Example: <code>mysql.connect(\"db-service.dev.svc.cluster.local\")</code></li> <li>In general format followed: <code>&lt;service name&gt;.&lt;namespace&gt;.svc.cluster.local</code></li> <li>This can be done as when a service is created, a DNS entry is added automatically in this format.</li> <li><code>cluster.local</code> is the default cluster's domain name.</li> <li><code>svc</code> = subdomain for service.</li> <li>List all pods in default namespace: <code>kubectl get pods</code></li> <li>List all pods in specific namespace: <code>kubectl get pods --namespace &lt;namespace&gt;</code></li> <li>When creating a pod via a definition file, it will automatically be added to the default namespace if no namespace is specified.</li> <li> <p>To add to a particular namespace: <code>kubectl create -f &lt;definition&gt;.yaml --namespace=&lt;namespace name&gt;</code></p> </li> <li> <p>To set default namespace of a pod, add <code>namespace: &lt;namespace name&gt;</code> to metadata in the definition file.</p> </li> <li> <p>Namespaces can be created via YAML definitions:</p> </li> </ul> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: namespace-name\n</code></pre> <ul> <li>To create: <code>kubectl create -f &lt;namespace&gt;.yaml</code></li> <li>Alternatively: <code>kubectl create namespace &lt;namespace name&gt;</code></li> <li>To switch context: <code>kubectl config set-context $(kubectl config current-context) --namespace=&lt;namespace&gt;</code></li> <li>To view all pods in each namespace add <code>--all-namespaces</code> to the <code>get pods</code> commands.</li> </ul>"},{"location":"certifications/CKAD/02_Core-Concepts.html#resource-quota","title":"Resource Quota","text":"<ul> <li>Creates limitations on resources for namespaces</li> <li>Created via definition file</li> <li>Kind: ResourceQuota</li> <li>Spec: must specify variables such as:</li> <li>Pod numbers</li> <li>Memory limits</li> <li>CPU limits</li> <li> <p>Minimum requested/required CPU and Memory</p> </li> <li> <p>Example:</p> </li> </ul> <pre><code>apiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: compute-quota\n  namespace: dev\nspec:\n  hard:\n    pods: \"10\"\n    requests.cpu: \"4\"\n    requests.memory: 5Gi\n    limits.cpu: \"10\"\n</code></pre>"},{"location":"certifications/CKAD/03_Configuration.html","title":"3.0 - Configuration","text":""},{"location":"certifications/CKAD/03_Configuration.html#31-prerequisites-commands-and-arguments-in-docker","title":"3.1 - Prerequisites: Commands and Arguments in Docker","text":"<ul> <li> <p>Note: This is not a requirement for the CKAN curriculum.</p> </li> <li> <p>Consider a simple scenario:</p> </li> <li>Run a docker container via an ubuntu image: <code>docker run ubuntu</code></li> <li> <p>Runs an instance of the Ubuntu image and exits immediately, noted upon execution of <code>docker ps -a</code></p> </li> <li> <p>This occurs because containers aren't designed to host an OS, but instead to run a specific task or process.</p> </li> <li>Example: host a web server or database</li> <li>So long as that process stays active, so does the container.</li> <li> <p>If the service stops or crashes, the container exits.</p> </li> <li> <p>A Dockerfile with <code>CMD [\"bash\"]</code> defined doesn't work as this is not a command, but a CLI instead.</p> </li> <li>When the container runs, it runs Ubuntu and launches bash</li> <li>In general, Docker doesn't attach a terminal to a container when it's ran.</li> <li> <p>Bash cannot find a terminal and the container exits as the process finishes/fails.</p> </li> <li> <p>To solve, one can append commands to the <code>docker run</code> command e.g.  <code>docker run ubuntu sleep 5</code></p> </li> <li> <p>Similarly, in a Dockerfile  <code>CMD &lt;command&gt; &lt;param1&gt;</code>  or in JSON:  <code>CMD [\"command\", \"parameter\"]</code></p> </li> <li> <p>To build new image and run: <code>docker build -t &lt;image name&gt; .</code></p> </li> <li> <p>To run: <code>docker run &lt;image name&gt;</code></p> </li> <li> <p>To use the command but with a parameter value subject to change, change <code>CMD</code> to <code>ENTRYPOINT</code> i.e.:  <code>ENTRYPOINT [\"command\"]</code></p> </li> <li> <p>Any parameters specified on the CLI will automatically be appended to the entrypoint command.</p> </li> <li> <p>If using entrpoint and a command parameter isn't specified, an error is likely to occur, a default value should therefore be provided.</p> </li> <li> <p>Therefore, <code>ENTRYPOINT</code> and <code>CMD</code> should be used together.</p> </li> <li> <p>Example:</p> </li> </ul> <pre><code>ENTRYPOINT [\"command\"]\n\nCMD [\"parameter\"]\n</code></pre> <ul> <li>From this configuration, if no additional parameter(s) is provided, the <code>CMD</code> parameter will be provided.</li> <li> <p>Any parameter on the CLI will override the <code>CMD</code> parameter</p> </li> <li> <p>To override the entrypoint: <code>docker run --entrypoint &lt;new command&gt; &lt;image name&gt;</code></p> </li> <li> <p>Note: <code>ENTRYPOINT</code> and <code>CMD</code> values should be expressed in a JSON format.</p> </li> </ul>"},{"location":"certifications/CKAD/03_Configuration.html#32-commands-and-arguments-in-kubernetes","title":"3.2 - Commands and Arguments in Kubernetes","text":"<ul> <li>The Ubuntu sleeper image can be defined in a YAML file for Kubernetes similar to the following:</li> </ul> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: &lt;pod name&gt;\nspec:\n  containers:\n  - name: &lt;container name&gt;\n    image: &lt;image name&gt;:&lt;image tag&gt;\n    command: [\"&lt;command&gt;\"]\n    args: [\"&lt;arg1&gt;\"]\n</code></pre> <ul> <li>To add anything to be appended to the <code>docker run</code> command, one adds the a<code>args</code> attribute to the container spec.</li> <li>The pod can then be created through standard means such as <code>kubectl create -f &lt;filenmame&gt;.yaml</code></li> <li> <p>The Dockerfile's entrypoint is overwritten by the <code>command</code> atribute.</p> </li> <li> <p>Note: You cannot edit specifications of a pre-existing pod aside from:</p> </li> <li><code>containers</code></li> <li><code>initContainers</code></li> <li><code>activeDeadlineSections</code></li> <li> <p><code>tolerations</code></p> </li> <li> <p>Environmental variables, in general, cannot be edited, as well as service accounts and resource limits.</p> </li> <li>If editing is required of these parameters, 2 methods are advised:</li> <li><code>kubectl edit pod &lt;pod name&gt;</code><ul> <li>Edit the properties desired</li> <li>As outlined above, certain properties cannot be edited whilst a pod is \"live\" - if this happens, the requested changes to the YAML will be saved as a temporary definition.</li> <li>Delete the original pod <code>kubectl delete pod &lt;pod name&gt;</code></li> <li>Recreate the pod from the temp definition file: <code>kubectl create -f &lt;temp filename&gt;.yaml</code></li> </ul> </li> <li> <p>Extract the YAML of the pod via <code>kubectl get pod &lt;pod name&gt; -o yaml &gt; &lt;file name&gt;.yaml</code></p> <ul> <li>Open the extracted YAML and edit as appropriate e.g. <code>vim &lt;filename&gt;.yaml</code></li> <li>Delete the original pod and recreate similar to the latter 2 steps of method 1.</li> </ul> </li> <li> <p>Note: If part of a deployment, change any instance of <code>pod</code> in the commands above to <code>deployment</code>.</p> </li> <li>Any changes to deployments are automatically applied to the pods within.</li> </ul>"},{"location":"certifications/CKAD/03_Configuration.html#35-environment-variables","title":"3.5 - Environment Variables","text":"<ul> <li>For a given definition file, one can set environment variables via the <code>env:</code> field in containers spec.</li> <li>Each environment variable is an array entry, with a name and value associated:</li> </ul> <pre><code>env:\n- name: &lt;ENV NAME&gt;\n  value: &lt;VALUE&gt;\n</code></pre> <ul> <li>Environment variables may be set via 1 of 3 ways (primarily):</li> <li>Key-value pairs (above)</li> <li>ConfigMaps</li> <li> <p>Secrets</p> </li> <li> <p>The latter two are implemented in a similar manner to the following respective examples:</p> </li> </ul> <pre><code>env:\n- name: &lt;ENV NAME&gt;\n  valueFrom:\n    configMapKeyRef:\n</code></pre> <pre><code>env:\n- name: &lt;ENV NAME&gt;\n  valueFrom:\n    secretKeyRef:\n</code></pre>"},{"location":"certifications/CKAD/03_Configuration.html#36-configmaps","title":"3.6 - ConfigMaps","text":"<ul> <li>When there are multiple pod definition files, it becomes difficult to manage environment data.</li> <li>This information can be removed from the definition files and managed centrally via ConfigMaps</li> <li>Used to pass configuration data as key-value pairs in Kubernetes</li> <li>When a pod is created, one can inject the ConfigMap into the pod.</li> <li> <p>The key-value pairs are available to the pod as environment variables for the application within the pod.</p> </li> <li> <p>Configuring ConfigMaps involves 2 phases:</p> </li> <li>Create the ConfigMap</li> <li> <p>Inject it to the pod.</p> </li> <li> <p>Creation is achieved through standard means: <code>kubectl create configmap &lt;configmap name&gt;</code></p> </li> <li> <p>Or if a YAML file exists: <code>kubectl create -f &lt;filename&gt;.yaml</code></p> </li> <li> <p>Via the first method, one can automatically pass in key-value pairs:  <code>kubectl create configmap &lt;configmap name&gt; --from-literal=&lt;key&gt;=&lt;value&gt; --from-literal=&lt;key2&gt;=&lt;value2&gt;</code></p> </li> <li> <p>Multiple uses of the <code>--from-literal=&lt;key&gt;=&lt;value&gt;</code> allows multiple variables to be added.</p> </li> <li>Note: This becomes difficult when too many config items are present.</li> <li>One can also create ConfigMaps from a file e.g.  <code>kubectl create configmap &lt;configmap name&gt; --from-file=/path/to/file</code></li> </ul>"},{"location":"certifications/CKAD/03_Configuration.html#creating-a-configmap-via-declaration","title":"Creating a ConfigMap via Declaration","text":"<ul> <li>Create a definition file:</li> </ul> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: app-config\ndata:\n  APP_COLOR: blue\n  APP_MODE: prod\n</code></pre> <ul> <li> <p>To create from the above: <code>kubectl create -f &lt;filename&gt;.yaml</code></p> </li> <li> <p>Can create as many ConfigMaps and required, just ensure they are named appropriately.</p> </li> <li> <p>View ConfigMaps via <code>kubectl get configmaps</code></p> </li> <li> <p>Get detailed information of a ConfigMap via <code>kubectl describe configmap &lt;configmap name&gt;</code></p> </li> <li> <p>Configuring a pod with a ConfigMap:</p> </li> <li>In a pod definition file, under the containers in spec, add <code>envFrom:</code> list property.</li> <li> <p>Each item in the resultant list corresponds to a ConfigMap item.</p> </li> <li> <p>Example usage:</p> </li> </ul> <pre><code>envFrom:\n- configMapRef:\n    name: app-config\n</code></pre> <ul> <li>Can apply the config file and pod definitions with the <code>kubectl create -f &lt;pod definition&gt;.yaml</code></li> </ul>"},{"location":"certifications/CKAD/03_Configuration.html#summary","title":"Summary","text":"<ul> <li>ConfigMaps can be used to inject environmental variables into pods</li> <li>Could also inject the data as a file or via a volume</li> </ul>"},{"location":"certifications/CKAD/03_Configuration.html#env","title":"env","text":"<pre><code>envFrom:\n- configMapRef:\n    name: &lt;configmap key name&gt;\n</code></pre>"},{"location":"certifications/CKAD/03_Configuration.html#single-env","title":"single env","text":"<pre><code>env:\n- name: &lt;env name&gt;\n  valueFrom:\n    configMapKeyRef:\n      name: &lt;configmap name&gt;\n      key: &lt;configmap key name&gt;\n</code></pre>"},{"location":"certifications/CKAD/03_Configuration.html#volumes","title":"Volumes","text":"<pre><code>volumes:\n- name: app-config-volume\n  configMap:\n    name: app-config\n</code></pre>"},{"location":"certifications/CKAD/03_Configuration.html#38-secrets","title":"3.8 - Secrets","text":"<ul> <li>Considering a simple python server:</li> <li>The hostname username and passwords are hardcoded in bad practice =&gt; high security risk.</li> <li>It would be better to store this data as a ConfigMap based on previous discussion - the problem though is that ConfigMap data is stored in a plaintext format.</li> <li> <p>Not applicable for sensitive info like passwords</p> </li> <li> <p>Variables like username and passwords are better stored as <code>secrets</code> in Kubernetes.</p> </li> <li> <p>These are similar to ConfigMaps, but the values are stored in encrypted format.</p> </li> <li> <p>Analogous to ConfigMaps, there are 2 steps:</p> </li> <li>Secret Creation</li> <li> <p>Inject secrets to a pod.</p> </li> <li> <p>Secret creation is achieved either imperatively or declaratively:</p> </li> <li>Declarative: Use a YAML definition file to \"declare\" the desired configuration</li> <li>Imperative: Use the <code>kubectl create secret</code> command to \"imply\" Kubernetes to create a secret, and let Kubernetes figure out the configuration desired.</li> </ul>"},{"location":"certifications/CKAD/03_Configuration.html#imperative-secret-creation","title":"Imperative Secret Creation","text":"<ul> <li> <p><code>kubectl create secret generic &lt;secret name&gt; --from-literal=&lt;key&gt;=&lt;value&gt;</code></p> </li> <li> <p>As with ConfigMaps, data can be specified from the CLI in key-value-pairs via the <code>--from-literal</code> flag multiple times.</p> </li> <li> <p>Example: <code>kubectl create secret generic app-secret --from-literal=DB_HOST=mysql --from-literal=DB_USER=root --from-literal=DB_PASSWORD=password</code></p> </li> <li> <p>For larger amounts of secrets, the data can be imported from a file, achieved by using the <code>--from-file</code> flag.</p> </li> <li> <p>Example: <code>kubectl create secret generic app-secret --from-file=app-secret.properties</code></p> </li> </ul>"},{"location":"certifications/CKAD/03_Configuration.html#declarative-secret-creation","title":"Declarative Secret Creation","text":"<ul> <li>Using a YAML definition file:</li> </ul> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: app-secret\ndata:\n  DB_HOST: mysql\n  DB_USER: root\n  DB_PASSWORD: password\n</code></pre> <ul> <li>As discussed, the secrets should not be stored in plaintext string format. Typically, Kubernetes secrets are stored in Base64 encrypted format.</li> <li>To convert: <code>echo -n '&lt;secret plaintext value&gt;' | base64</code></li> <li>Create the secret via <code>kubectl create -f ....</code> as normal</li> <li>Secrets can be viewed via: <code>kubectl get secrets</code></li> <li> <p>Detailed information viewed via: <code>kubectl describe secrets &lt;secret name&gt;</code></p> </li> <li> <p>To view secret in more detail: <code>kubectl get secret &lt;secret name&gt; -o yaml</code></p> </li> <li> <p>To decode secret: <code>echo -n '&lt;secret base64 value&gt;' | base64 --decode</code></p> </li> </ul>"},{"location":"certifications/CKAD/03_Configuration.html#secrets-in-pods","title":"Secrets in Pods","text":"<ul> <li>With both a pod and secret YAML file, the secret data can be injected as environment variables:</li> </ul> <pre><code>spec:\n  containers:\n  - envFrom:\n    - secretRef:\n        name: &lt;secret name&gt;\n</code></pre> <ul> <li> <p>When <code>kubectl create -f ...</code> is run, the secret data is available as environment variables in the pod.</p> </li> <li> <p>As before, one can inject secrets to pods via environment variables (above) OR a single environment variable (below):</p> </li> </ul> <pre><code>spec:\n  containers:\n  - env:\n    - name: DB_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: app-secret\n          key: DB_PASSWORD\n</code></pre>"},{"location":"certifications/CKAD/03_Configuration.html#secrets-in-volumes","title":"Secrets in Volumes","text":"<ul> <li>Secrets can also be added as volumes attached to pods:</li> </ul> <pre><code>volumes:\n- name: app-secret-volume\n  secret:\n    secretName: app-secret\n</code></pre> <ul> <li>If mounting the secret as a volume, each attribute in the secret is created as a file, with the value being the content.</li> </ul>"},{"location":"certifications/CKAD/03_Configuration.html#additional-notes","title":"Additional Notes","text":"<ul> <li>Secrets are encoded in base64 format -&gt; this can easily be decoded as they're not encrypted!</li> <li> <p>It's thought that secrets are a safer option, but this is only in the sense that \"it's better than plaintext\".</p> </li> <li> <p>It's the practices regarding secret storage that makes them safer, including:</p> </li> <li>Not checking-in secret object definition files to source code repositories</li> <li> <p>Enabling encryption-at-rest for secrets</p> </li> <li> <p>Kubernetes takes some actions to ensure safe handling of secrets:</p> </li> <li>A secret is only sent to a node if a pod on said node requires it</li> <li>Kubelet stores the secret into a temporary file storage so it's not persisted to a disk.</li> <li> <p>Once a pod is deleted, any local copies of secrets used by that pod are deleted.</p> </li> <li> <p>For further improved safety regarding secrets, one could also use tools such as Helm Secrets and HashiCorp Vault.</p> </li> </ul>"},{"location":"certifications/CKAD/03_Configuration.html#encrypting-secrets-at-rest","title":"Encrypting Secrets at Rest","text":"<ul> <li>Additional guidance can be found in the Kubernetes documentation.</li> <li>Encryption at rest is determined by configuring the <code>kube-apiserver</code> and the etcd server.</li> </ul>"},{"location":"certifications/CKAD/03_Configuration.html#secret-storage-in-etcd","title":"Secret Storage in ETCD","text":"<ul> <li>Creating a sample secret in Kubernetes: <code>kubectl create secret generic secret1 -n default --from-literal=mykey=mydata</code></li> <li>The secret can be read from <code>etcd</code> via the <code>etcdctl</code> utility:</li> </ul> <pre><code>ETCDCTL_API=3 etcdctl \\\n   --cacert=/etc/kubernetes/pki/etcd/ca.crt   \\\n   --cert=/etc/kubernetes/pki/etcd/server.crt \\\n   --key=/etc/kubernetes/pki/etcd/server.key  \\\n   get /registry/secrets/default/secret1 | hexdump -C\n</code></pre> <ul> <li>Without encryption at rest, the above command would return the secret value in plaintext format, this is a huge security risk.</li> </ul>"},{"location":"certifications/CKAD/03_Configuration.html#enabling-encryption-at-rest","title":"Enabling Encryption at Rest","text":"<ul> <li>The <code>kube-apiserver</code> process accepts the flag <code>--encryption-provider-config</code> to determine API data encryption in ETCD.</li> <li>To check if it's enabled: <code>ps -aux | grep kube-api | grep \"encryption-provider-config\"</code> OR examine the <code>kube-apiserver.yaml</code> manifest for the same flag.</li> <li>If not enabled, one can define an <code>EncryptionConfiguration</code> YAML manifest to attach to this flag.</li> <li>An example (not suitable for production) from the documentation follows:</li> </ul> <pre><code>---\n##\n## CAUTION: this is an example configuration.\n##          Do not use this for your own cluster!\n##\napiVersion: apiserver.config.k8s.io/v1\nkind: EncryptionConfiguration\nresources:\n  - resources:\n      - secrets\n      - configmaps\n      - pandas.awesome.bears.example # a custom resource API\n    providers:\n      # This configuration does not provide data confidentiality. The first\n      # configured provider is specifying the \"identity\" mechanism, which\n      # stores resources as plain text.\n      #\n      - identity: {} # plain text, in other words NO encryption\n      - aesgcm:\n          keys:\n            - name: key1\n              secret: c2VjcmV0IGlzIHNlY3VyZQ==\n            - name: key2\n              secret: dGhpcyBpcyBwYXNzd29yZA==\n      - aescbc:\n          keys:\n            - name: key1\n              secret: c2VjcmV0IGlzIHNlY3VyZQ==\n            - name: key2\n              secret: dGhpcyBpcyBwYXNzd29yZA==\n      - secretbox:\n          keys:\n            - name: key1\n              secret: YWJjZGVmZ2hpamtsbW5vcHFyc3R1dnd4eXoxMjM0NTY=\n  - resources:\n      - events\n    providers:\n      - identity: {} # do not encrypt Events even though *.* is specified below\n  - resources:\n      - '*.apps' # wildcard match requires Kubernetes 1.27 or later\n    providers:\n      - aescbc:\n          keys:\n          - name: key2\n            secret: c2VjcmV0IGlzIHNlY3VyZSwgb3IgaXMgaXQ/Cg==\n  - resources:\n      - '*.*' # wildcard match requires Kubernetes 1.27 or later\n    providers:\n      - aescbc:\n          keys:\n          - name: key3\n            secret: c2VjcmV0IGlzIHNlY3VyZSwgSSB0aGluaw==\n</code></pre> <ul> <li>As per usual, the first 2 lines determine the <code>apiVersion</code> and <code>kind</code>. From here, the general format is to list in arrays the <code>resources</code> and the <code>provider(s)</code> to be used in association with encryption and decryption.</li> <li>Under <code>providers</code>, the first provider is for encryption only, any providers listed after that for the given resources are suitable for decryption only.</li> <li> <p>There are multiple providers available, such as <code>identity</code> (no encryption), <code>kms</code>, <code>secretbox</code>, etc.</p> </li> <li> <p>Creating an example encryption configuration file, which leverages the <code>aesbc</code> encryption provider:</p> </li> </ul> <pre><code>---\napiVersion: apiserver.config.k8s.io/v1\nkind: EncryptionConfiguration\nresources:\n  - resources:\n      - secrets\n      - configmaps\n      - pandas.awesome.bears.example\n    providers:\n      - aescbc:\n          keys:\n            - name: key1\n              # See the following text for more details about the secret value\n              secret: &lt;BASE 64 ENCODED SECRET&gt;\n      - identity: {} # this fallback allows reading unencrypted secrets;\n                     # for example, during initial migration\n</code></pre> <ul> <li>The secret for the <code>aescbc</code> provider can be a random 32-byte key encoded in base64 - <code>head -c 32 /dev/urandom | base64</code>.</li> <li>Saving the encryption config file to a given path, it will then need to be mounted to the <code>kube-apiserver</code> static pod, achieved by editing the <code>kube-apiserver</code> manifest, adding the <code>--encryption-provider-config=/path/to/config</code> flag, and mounting the config's location as a volume.</li> </ul> <pre><code>---\n##\n## This is a fragment of a manifest for a static Pod.\n## Check whether this is correct for your cluster and for your API server.\n##\napiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 10.20.30.40:443\n  creationTimestamp: null\n  labels:\n    app.kubernetes.io/component: kube-apiserver\n    tier: control-plane\n  name: kube-apiserver\n  namespace: kube-system\nspec:\n  containers:\n  - command:\n    - kube-apiserver\n    ...\n    - --encryption-provider-config=/etc/kubernetes/enc/enc.yaml  # add this line\n    volumeMounts:\n    ...\n    - name: enc                           # add this line\n      mountPath: /etc/kubernetes/enc      # add this line\n      readOnly: true                      # add this line\n    ...\n  volumes:\n  ...\n  - name: enc                             # add this line\n    hostPath:                             # add this line\n      path: /etc/kubernetes/enc           # add this line\n      type: DirectoryOrCreate             # add this line\n  ...\n</code></pre> <ul> <li>As a static pod is being edited, the API Server should restart automatically, if it fails to do so, troubleshoot accordingly.</li> <li>These steps should be repeated per control plane node, and for verification, the original steps with <code>etcdctl</code> can be repeated.</li> <li>Note: To ensure all relevant data is encrypted, including that which is already stored, run the following command to update them with the relevant config as an administrator: <code>kubectl get secrets --all-namespaces -o json | kubectl replace -f -</code></li> </ul>"},{"location":"certifications/CKAD/03_Configuration.html#311-docker-security","title":"3.11 - Docker Security","text":"<ul> <li>Consider a host with Docker running on it.</li> <li>The host will be running processes such as the Docker Daemon</li> <li>Containers aren't completely isolated from their host -&gt; they share the same kernel</li> <li>In general, containers are separated by namespaces (Linux)</li> <li>All processes run by a container are run by the host, just in their own namespace</li> <li> <p>The container can only see its own process via <code>ps aux</code></p> </li> <li> <p>In the host, all processes are visible, container processes have differing IDs depending on their namespace</p> </li> </ul>"},{"location":"certifications/CKAD/03_Configuration.html#security-users","title":"Security: Users","text":"<ul> <li>Docker hosts can have root users as well as non-root users</li> <li>By default, Docker runs processes in containers as the root user.</li> <li> <p>True within and outside the container</p> </li> <li> <p>To edit the default user for the container, use <code>docker run</code> in a similar manner to: <code>docker run --user=&lt;username&gt; &lt;container&gt; &lt;command&gt;</code></p> </li> <li> <p>To enforce security, one can add <code>USER</code> value to the Dockerfile e.g. <code>USER 1000</code></p> </li> <li>This automatically defines the user when the container is built and run.</li> <li>When running a container that defaults to the root user, Docker takes measures to prevent the root user from taking unnecessary actions via Linux Capabilities.</li> </ul>"},{"location":"certifications/CKAD/03_Configuration.html#linux-capabilities","title":"Linux Capabilities","text":"<ul> <li>Listed in <code>/usr/include/linux/capability.h</code></li> <li>In containers, Docker applies limited capabilities by default</li> <li>To override, add <code>--cap-add &lt;CAPABILITY NAME&gt;</code> to the <code>docker run</code> command</li> <li>To remove: <code>--cap-drop &lt;CAPABILITY NAME&gt;</code> in a similar manner</li> <li>For all capabilities, add <code>--privileged</code> -&gt; not recommended!</li> </ul>"},{"location":"certifications/CKAD/03_Configuration.html#312-security-contexts","title":"3.12 - Security Contexts","text":"<ul> <li>Container security may be configured by adding or specifying users and their associated capabilities in the <code>docker run</code> command.</li> <li> <p>Similar settings may also be handled via Kubernetes.</p> </li> <li> <p>As containers are hosted within pods on Kubernetes, one can either configure security at the pod or container level.</p> </li> <li>If configured at pod level, any changes will automatically be applied to the containers within.</li> <li>If configured at container level, these settings will override anything defined at pod level.</li> </ul>"},{"location":"certifications/CKAD/03_Configuration.html#security-context","title":"Security Context","text":"<ul> <li>To configure security in the definition file, add the <code>securityContext:</code> attribute</li> <li>User is set by <code>runAsUser: &lt;user ID&gt;</code></li> <li>To configure at container-level, add the same fields to the containers list</li> <li>To add capabilities, add <code>capabilities:</code>, then in a dictionary, add <code>add: [\"&lt;CAPABILITY ID&gt;\", .... ]</code>.</li> </ul> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod\nspec:\n  #securityContext:\n  #    runAsUser: 1000\n  containers:\n  - name: ubuntu\n    image: ubuntu\n    command: [\"sleep\", \"3600\"]\n    securityContext:\n      runAsUser: 1000\n      capabilities:\n        add: [\"MAC_ADMIN\"]\n</code></pre> <ul> <li>Note: Capabilities are only supported at container-level, not pod-level.</li> </ul>"},{"location":"certifications/CKAD/03_Configuration.html#314-service-account","title":"3.14 - Service Account","text":"<ul> <li>A service account links to securIty concepts such as authorization and RBAC, etc.</li> <li>One of 2 account types in Kubernetes, the other being a user account:</li> <li>User accounts are used by humans e.g. development accounts.</li> <li> <p>Service accounts are those used by applications to interact with Kubernetes.</p> </li> <li> <p>For an app to use/interact with the Kubernetes API, it needs to authenticate via service accounts.</p> </li> <li> <p>Creation via: <code>kubectl create serviceaccount &lt;serviceaccount name&gt;</code></p> </li> <li>View service accounts via <code>kubectl get serviceaccount</code></li> <li> <p>When a `serviceaccount`` is created, it automatically creates a service token to be used for authentication.</p> </li> <li> <p>Token can be viewed (along with other details) via <code>kubectl describe serviceaccount &lt;serviceaccount name&gt;</code></p> </li> <li> <p>Token is stored as a Kubernetes secret by default, it can be viewed via <code>kubectl describe secret &lt;secret ID&gt;</code></p> </li> <li> <p>Suppose the app using the service account is already part of a Kubernetes cluster.</p> </li> <li>One can mount the service token secret as a volume inside the pod</li> <li> <p>This allows it to be easily accessible by the application</p> </li> <li> <p>The default service account and its corresponding token is automatically mounted as a volume to the pod.</p> </li> <li>In the path of the mount, 3 files are stored, detailing:</li> <li><code>Namespace</code></li> <li><code>Token</code></li> <li><code>ca.crt</code></li> <li> <p>The above can be viewed for a given service by <code>kubectl exec -it &lt;service id&gt; ls /var/run/secrets/kubernetes.io/&lt;serviceaccount&gt;</code></p> </li> <li> <p>Note: The default service account is restricted to basic operations.</p> </li> <li>Automatically created and mounted.</li> <li>If you wish to switch it, edit the pod definition file to add <code>serviceAccount:</code> under the <code>spec:</code>, then delete and recreate the pod.</li> <li> <p>Changes like this are automatically applied if editing a deployment.</p> </li> <li> <p>Note: One can avoid automatic service account association via the addition of <code>automountServiceAccountToken: false</code></p> </li> <li> <p>Example:</p> </li> </ul> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  serviceAccountName: build-robot\n  automountServiceAccountToken: false\n</code></pre>"},{"location":"certifications/CKAD/03_Configuration.html#service-accounts-updates-122124","title":"Service Accounts Updates - 1.22/1.24","text":"<ul> <li>All namespaces have a default service account with its own secret, when a pod is created, this service account is automatically associated with the pod, and the secret is mounted to a given location.</li> <li>As a result of this, a process within the pod can query the Kubernetes API using the mounted token.</li> <li>Checking the location where the secret is mounted, three files are found:</li> <li><code>ca.crt</code></li> <li><code>namespace</code></li> <li><code>token</code> - the ServiceAccount token.</li> <li>The token can be decoded via <code>jq</code> (or some other means e.g. jwt.io) via the following command: <code>jq -R 'split(\".\") | select(length &gt; 0) | .[0],.[1] | @base64d | fromjson' &lt;&lt;&lt; &lt; TOKEN&gt;</code></li> <li>The output shows that the token has no expiry date defined in the payload section - this poses problems.</li> </ul>"},{"location":"certifications/CKAD/03_Configuration.html#v122-notes-on-bound-service-account-tokens","title":"v1.22 Notes on Bound Service Account Tokens","text":"<ul> <li> <p>Kubernetes already provisions JWTs to workloads, a functionality that is enabled by default, and therefore widely deployed - leading to the following problems:</p> </li> <li> <p>These JWTs are not audience-bound -&gt; Anyone with the JWT can pretend to be an authenticated user</p> </li> <li>The current model of storing the service account token in a secret delivered to nodes provides a large attack surface to the control plane nodes.</li> <li>The JWTs are not time-bound, any JWT Compromised via the previous 2 points are valid so long as the service account exists.</li> <li> <p>Each JWT requires a Kubernetes secret -&gt; Not Scalable</p> </li> <li> <p>To overcome this, the <code>TokenRequestAPI</code> was introduced, this generates tokens which are Audience, Time and Object bound.</p> </li> <li>Since its introduction, when a new pod is created, it no longer requires on the token from the service account, instead, a token with a particular lifetime is created by the TokenRequestAPI via the ServiceAccount Admission controller.</li> <li>This token is then mounted as a projected volume into the pod.</li> </ul> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\n  namespace: default\nspec:\n  containers:\n  - image: nginx\n    name: nginx\n    volumeMounts:\n    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount\n      name: kube-api-access-&lt;random string&gt;\n  volumes:\n  - name: kube-api-access-&lt;random string&gt;\n    projected:\n      defaultMode: 420\n      sources:\n      - serviceAccountToken:\n          expirationSeconds: 3607\n          path: token\n      - configMap:\n          items:\n          - key: ca.crt\n            path: ca.crt\n          name: kube-root-ca.crt\n      - downnwardAPI:\n          items:\n          - fieldRed:\n              apiVersion: v1\n</code></pre>"},{"location":"certifications/CKAD/03_Configuration.html#v124-enhancements","title":"v1.24 Enhancements","text":"<ul> <li>This version included changes to reduce the amount of secret-based service account tokens.</li> <li>Previously, upon <code>serviceaccount</code> creation, a secret token that had no expiry and was unbound, was automatically created and mounted into pods upon their creation.</li> <li>In <code>v1.24</code>, the <code>serviceaccount</code> token creation is no longer automatic, instead it must be created separately after serviceaccount creation i.e.:</li> <li><code>kubectl create serviceaccount &lt;serviceaccount name&gt;</code></li> <li><code>kubectl create token &lt;serviceaccount name&gt;</code></li> <li>If this token is decoded via means defined previously, an expiry date can be seen in the payload.</li> <li>The secrets can still be created with no expiry if desired, however this is REALLY not advised:</li> </ul> <pre><code>apiVersion: v1\nkind: Secret\ntype: kubernetes.io/service-account-token\nmetadata:\n  name: &lt;secret name&gt;\n  annotations:\n    kubernetes.io/service-account.name: &lt;service account name&gt;\n</code></pre>"},{"location":"certifications/CKAD/03_Configuration.html#317-resource-requirements","title":"3.17 - Resource Requirements","text":"<ul> <li>Consider a 3-node setup, each has a set amount of resources available i.e.:</li> <li>CPU</li> <li>Memory</li> <li> <p>Disk Space</p> </li> <li> <p>The Kubernetes scheduler is responsible for allocating pods to nodes</p> </li> <li>To do so, it takes into account the node's current resource allocation and the resources requested by the pod.</li> <li>If no resources are available, the scheduler will hold the pod back for release</li> <li>Kubernetes automatically assumes a pod or container within a pod will require at least:</li> <li><code>0.5</code> CPU Units</li> <li> <p><code>256Mi</code> Memory</p> </li> <li> <p>If the pod or container requires more resources than allocated above, one can configure the pod definition file's spec, in particular, add the following under the <code>containers</code> list:</p> </li> </ul> <pre><code>resources:\n  requests:\n    memory: \"1Gi\"\n    cpu: 1\n</code></pre>"},{"location":"certifications/CKAD/03_Configuration.html#resource-cpu","title":"Resource - CPU","text":"<ul> <li>Can be set from <code>1m</code> (1 micro) to as high as required / supported by the host system.</li> <li>1 CPU = 1 AWS vCPU = 1 GCP Core = 1 Azure Core = 1 Hyperthread</li> </ul>"},{"location":"certifications/CKAD/03_Configuration.html#resources-memory","title":"Resources - Memory","text":"<ul> <li>Allocate within any of the following suffix for the givne purpose and the system's capabilities:</li> </ul> Memory Metric Shorthand Notation Equivalency Gigagbyte G 1000M Megabyte M 1000K Kilobyte K 1000 Bytes Gigibyte Gi 1024Mi Mebibyte Mi 1024Ki Kilibyte Ki 1024 Bytes <ul> <li>Docker containers have no limit to the resources they can consume</li> <li>When only running on a node, it can only use a maximum of 1vCPU unit - if the limits need changing, update the pod definition file:</li> </ul> <pre><code>resources:\n  requests:\n    memory: &lt;value and unit&gt;\n    cpu: &lt;value and limit&gt;\n    ....\n  limits:\n    memory: &lt; value and unit&gt;\n    cpu: &lt;number&gt;\n</code></pre> <ul> <li>The limits and requests can be set for each pod and container</li> <li>If CPU overload occurs, CPU usage is \"throttled\" on the node so it does not go beyond the limit.</li> <li>If repeated memory use is exceeded for an extended period of time, the pod is terminated with an <code>OOM</code> (Out of Memory) error.</li> </ul>"},{"location":"certifications/CKAD/03_Configuration.html#default-behavior","title":"Default Behavior","text":"<ul> <li> <p>By default, Kubernetes doesn't set CPU or Memory limits, pods would be able to use as much of these resources as they like until it stops other pods from functioning.</p> </li> <li> <p>Rather than just putting blanket limits or requests in place, it's often advised to tailor the requests and limits per workload.</p> </li> <li>Alternatively, you could put in requests without limits, ensuring that each pod is guaranteed the minimum resources required, but this doesn't prevent resource quota throttling.</li> </ul>"},{"location":"certifications/CKAD/03_Configuration.html#limit-ranges","title":"Limit Ranges","text":"<ul> <li>To ensure all pods are created with particular limits by default, one can create a <code>LimitRange</code> object, both CPU and Memory constraints can be defined by a single LimitRange:</li> </ul> <pre><code>apiVersion: v1\nkind: LimitRange\nmetadata:\n  name: cpu-resource-constraint\nspec:\n  limits:\n  - default:\n      cpu: 500m\n    defaultRequest:\n      cpu: 500m\n    max:\n      cpu: \"1\"\n    min:\n      cpu: 100m\n    type: Container\n</code></pre> <ul> <li>Quotas can be set for a namespace itself by creating a <code>ResourceQuota</code> object - these are namespace-scoped:</li> </ul> <pre><code>apiVersion: v1\nkind:  ResourceQuota\nmetadata:\n  name: my-resource-quota\nspec:\n  hard:\n    requests.cpu: 4\n    requests.memory: 4Gi\n    limits.cpu: 10\n    limits.memory: 10Gi\n</code></pre>"},{"location":"certifications/CKAD/03_Configuration.html#319-taints-and-tolerations","title":"3.19 - Taints and Tolerations","text":"<ul> <li>Used to set restrictions regarding what pods can be scheduled on a node.</li> <li>Consider a cluster of 3 nodes with 4 pods preparing for launch:</li> <li> <p>The scheduler will place the pods across all nodes equally if no restriction applies</p> </li> <li> <p>Suppose now only 1 node has resources available to run a particular application:</p> </li> <li>A taint can be applied to the node in question; preventing any unwanted pods from being scheduled on it.</li> <li> <p>Tolerations then need to be applied to the pod(s) to specifically run on node 1</p> </li> <li> <p>Pods can only run on a node if their tolerations match the taint applied to the node.</p> </li> <li> <p>Taints and tolerations allow the scheduler to allocate pods to required nodes, such that all resources are used and allocated accordingly.</p> </li> <li> <p>Note: By default, no tolerations are applied to pods.</p> </li> </ul>"},{"location":"certifications/CKAD/03_Configuration.html#taints-node","title":"Taints - Node","text":"<ul> <li>To apply a taint: <code>kubectl taint nodes &lt;nodename&gt; key=value:&lt;taint-effect&gt;</code></li> <li>The key-value pair defined could match labels defined for resources e.g <code>app=frontend</code></li> <li>The taint effect determines what happens to pods that are intolerant to the taint, 1 of 3 possibilities can be specified:</li> <li><code>NoSchedule</code> - Pods won't be scheduled.</li> <li><code>PreferNoSchedule</code> - Try to avoid scheduling if possible.</li> <li><code>NoExecute</code> - New pods won't be scheduled, and any pre-existing pods intolerant to the taint are stopped and evicted.</li> </ul>"},{"location":"certifications/CKAD/03_Configuration.html#tolerations-pod","title":"Tolerations - Pod","text":"<ul> <li>To apply a toleration to a pod, one can look at the definition file</li> <li>In the spec section, add similar to the following:</li> </ul> <pre><code>...\nspec:\n  containers:\n  ...\n  tolerations:\n  - key: app\n    operator: \"Equal\"\n    value: \"blue\"\n    effect: \"NoSchedule\"\n</code></pre> <ul> <li>Be sure to apply the same values used when applying the taint to the node.</li> <li>All values added need to be enclosed in \" \".</li> </ul>"},{"location":"certifications/CKAD/03_Configuration.html#taint-noexecute","title":"Taint - NoExecute","text":"<ul> <li>Suppose Node1 is to be used for a particular application:</li> <li>Apply a taint to node 1 with the app name and add a toleration to the pod running the app.</li> <li> <p>Setting the taint effect to <code>NoExecute</code> causes existing pods on the node that are intolerant to be stopped and evicted.</p> </li> <li> <p>Taints and tolerations are only used to restrict pod access to nodes.</p> </li> <li>As there are no restrictions / taints applied to the other pods, there's a chance the app could still be placed on a different node(s).</li> <li> <p>If wanting the pod to go to a particular node, one can utilize node affinity.</p> </li> <li> <p>Note: A taint is automatically applied to the master node, such that no pods can be scheduled to it.</p> </li> <li>View it via <code>kubectl describe node kubemaster | grep Taint</code></li> </ul>"},{"location":"certifications/CKAD/03_Configuration.html#320-node-selectors","title":"3.20 - Node Selectors","text":"<ul> <li>Consider a 3-node cluster, with 1 node having a larger resource configuration:</li> <li>In this scenario, one would like the task/process requiring more resources to go to the larger node.</li> <li>To solve, can place limitations on pods</li> <li>This can be done via the <code>nodeSelector</code> property in the definition file:</li> </ul> <pre><code>nodeSelector:\n  size: node-label\n</code></pre> <ul> <li> <p>NodeSelectors require the node to be labelled: <code>kubectl label nodes &lt;node name&gt; &lt;label key&gt;=&lt;key value&gt;</code></p> </li> <li> <p>When pod is created, it should be assigned to the labelled node so long as the resources allow it.</p> </li> </ul>"},{"location":"certifications/CKAD/03_Configuration.html#limitations-of-nodeselectors","title":"Limitations of NodeSelectors","text":"<ul> <li>NodeSelectors are beneficial for simple allocation tasks, but if more complex allocation is needed, Node Affinity is recommended, e.g. \"go to either 1 of 2 nodes\".</li> </ul>"},{"location":"certifications/CKAD/03_Configuration.html#322-node-affinity","title":"3.22 - Node Affinity","text":"<ul> <li>Node affinity looks to ensure that pods are hosted on the desired nodes</li> <li> <p>Can ensure high-resource consumption jobs are allocated to high-resource nodes</p> </li> <li> <p>Node affinity allows more complex capabilities regarding pod-node limitation.</p> </li> <li> <p>To specify, in the spec section of a pod definition file add in a new field:</p> </li> </ul> <pre><code>affinity:\n  nodeAffinity:\n    requiredDuringSchedulingIgnoredDuringExecution:\n      matchExpressions:\n      - key: size\n        operator: In\n        values:\n        - Large\n</code></pre> <ul> <li>Note: For the example above, the <code>NotIn</code> operator could also be used to avoid particular nodes.</li> <li> <p>Note: If just needing a pod to go to any node with a particular label, regardless of value, use the <code>Exists</code> operator -&gt; no values are required in this case.</p> </li> <li> <p>Additional operators are available, with further details provided in the documentation.</p> </li> <li>In the event that a node cannot be allocated due to a label fault, the resulting action is dependent upon the NodeAffinityType set.</li> </ul>"},{"location":"certifications/CKAD/03_Configuration.html#node-affinity-types","title":"Node Affinity Types","text":"<ul> <li> <p>Defines the scheduler's behavior regarding Node Affinity and pod lifecycle stages</p> </li> <li> <p>2 main types available:</p> </li> <li><code>RequireDuringSchedulingIgnoredDuringExecution</code></li> <li> <p><code>PreferredDuringSchedulingIgnoredDuringExecution</code></p> </li> <li> <p>Other types are to be released such as <code>requiredDuringSchedulingRequiredDuringExecution</code></p> </li> <li> <p>Considering the 2 available types, can break it down into the 2 stages of a pod lifecycle:</p> </li> <li>DuringScheduling -&gt; The pod has been created for the first time and not deployed</li> <li> <p>DuringExecution</p> </li> <li> <p>If the node isn't available according to the NodeAffinity, the resultant action is dependent upon the NodeAffinity type:</p> </li> <li> <p>Required:</p> </li> <li>Pod must be placed on a node that satisfies the node affinity criteria</li> <li>If no node satisfies the criteria, the pod won't be scheduled</li> <li> <p>Generally used when the node placement is crucial</p> </li> <li> <p>Preferred:</p> </li> <li>Used if the pod placement is less important than the need for running the task</li> <li>If a matching node not found, the scheduler ignores the NodeAffinity</li> <li> <p>Pod placed on any available node</p> </li> <li> <p>Suppose a pod has been running and a change is made to the Node Affinity:</p> </li> <li>The response is determined by the prefix of <code>DuringExecution</code>:<ul> <li>Ignored:</li> <li>Pods continue to run</li> <li>Any changes in Node Affinity will have no effect once scheduled.</li> <li>Required:</li> <li>When applied, if any current pods that don't meet the NodeAffinity requirements are evicted.</li> </ul> </li> </ul>"},{"location":"certifications/CKAD/03_Configuration.html#taints-and-tolerations-vs-node-affinity","title":"Taints and Tolerations vs Node Affinity","text":"<ul> <li>Consider a 5-cluster setup:</li> <li>Blue Node: Runs the blue pod</li> <li>Red Node: Runs the red pod</li> <li>Green Node: runs the green pod</li> <li>Node 1: To run the grey pod</li> <li> <p>Node 2: \" \"</p> </li> <li> <p>Applying a taint to each of the colored nodes to accept their respective pod</p> </li> <li> <p>Tolerances are then are applied to the pods</p> </li> <li> <p>Need to apply a taint to node 1 and node 2 as the colored pods can still be allocated to nodes where they're not wanted.</p> </li> <li> <p>To overcome, use Node Affinity:</p> </li> <li>Label nodes with respective colors</li> <li> <p>Pods end up in the correct nodes via use of Node Selector.</p> </li> <li> <p>There's a chance that the unwanted pods could still be allocated e.g. the grey pods could still be scheduled on the colored nodes.</p> </li> <li> <p>A combination of taints and tolerations, and node affinity must be used.</p> </li> <li>Apply taints and tolerations to present unwanted pod placement on nodes</li> <li>Use node affinity to prevent the correct pods from being placed on incorrect nodes.</li> </ul>"},{"location":"certifications/CKAD/04_Multi-Container-Pods.html","title":"4.0 - Multi-Container Pods","text":""},{"location":"certifications/CKAD/04_Multi-Container-Pods.html#41-overview","title":"4.1 - Overview","text":"<ul> <li>Multiple patterns are available, such as:</li> <li>Ambassador</li> <li>Adapter</li> <li> <p>Sidecar</p> </li> <li> <p>In general, it's advised to decouple a monolithic (single-tiered) application into a series of smaller components -&gt; microservices</p> </li> <li> <p>Allows ability to independently develop and deploy sets of small reusable code</p> </li> <li> <p>Allows easier scalability and independent modification.</p> </li> <li> <p>In some cases, may need services to interact with one another, whilst still being identifiable as separate services</p> </li> <li>Example: web server and logging agent</li> <li> <p>1 agent service would be required per web server, not merging them together.</p> </li> <li> <p>Only the 2 functionalities (or more) need to work together that can be scaled as required:</p> </li> <li> <p>Multi-container pods required</p> </li> <li> <p>Multi-container pods contain multiple containers running different services, sharing aspects such as:</p> </li> <li>Network: Can refer to each other via localhost</li> <li>Storage: No need for additional volume setup / integration</li> <li> <p>Lifecycle: Resources are created and destroyed together</p> </li> <li> <p>To create a multi-container pod, add the additional container details to the pod's spec in a similar manner to the following:</p> </li> </ul> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: simple-webapp\n  labels:\n    name: simple-webapp\nspec:\n  containers:\n  - name: simple-webapp\n    image: simple-webapp\n    ports:\n    - containerPort: 8080\n\n  - name: log-agent\n    image: log-agent\n</code></pre>"},{"location":"certifications/CKAD/04_Multi-Container-Pods.html#design-patterns","title":"Design Patterns","text":"<ul> <li>3 Design patterns available:</li> <li>Sidecar</li> <li>Adapter</li> <li>Ambassador</li> </ul>"},{"location":"certifications/CKAD/04_Multi-Container-Pods.html#sidecar","title":"Sidecar","text":"<ul> <li>The most common design pattern</li> <li>Uses a \"helper\" container to assist or improve the functionality of a primary container</li> <li>Example: Log agent with a web server</li> </ul>"},{"location":"certifications/CKAD/04_Multi-Container-Pods.html#adapter","title":"Adapter","text":"<ul> <li>Used to assist in standardizing communications between resources</li> <li>Processes that transmit data e.g. logs will be formatted in the same manner</li> <li>All data stored in centralized location</li> </ul>"},{"location":"certifications/CKAD/04_Multi-Container-Pods.html#ambassador","title":"Ambassador","text":"<ul> <li>Responsible for handling proxy for other parts of the system or services</li> <li>Used when wanting microservices to interact with one another</li> <li>Services to be identified by name only via service discovery such as DNS or at an application level.</li> </ul> <ul> <li>Whilst the design patterns differ, their implementation is the same, adding containers to the pod definition file spec where required,</li> </ul>"},{"location":"certifications/CKAD/04_Multi-Container-Pods.html#initcontainers","title":"InitContainers","text":"<ul> <li>In multi-container pods, each container is expected to run a process that stays alive for the Pod's lifecycle.</li> <li>Sometimes, you may wish to run a process that runs to completion in a pod's container, such as carry out initial configuration or pull some additional code in a one-time task before the main application starts.</li> <li>This is achieved through the use of <code>initContainers</code>.</li> </ul> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: myapp-pod\n  labels:\n    app: myapp\nspec:\n  containers:\n  - name: myapp-container\n    image: busybox:1.28\n    command: ['sh', '-c', 'echo The app is running! &amp;&amp; sleep 3600']\n  initContainers:\n  - name: init-myservice\n    image: busybox\n    command: ['sh', '-c', 'git clone &lt;some-repository-that-will-be-used-by-application&gt; ;']\n</code></pre> <ul> <li>When the pod's first created, the process in the <code>initContainer</code> must run to completion before the main container starts.</li> <li>Multiple <code>InitContainers</code> can be configured, however they will run in the sequence defined in their YAML manifest.</li> </ul>"},{"location":"certifications/CKAD/05_Observability.html","title":"5.0 - Observability","text":""},{"location":"certifications/CKAD/05_Observability.html#51-readiness-and-liveness-probes","title":"5.1 - Readiness and Liveness Probes","text":"<ul> <li>Pod lifecycles are defined by 2 parameters:</li> <li><code>Status</code></li> <li> <p><code>Conditions</code></p> </li> <li> <p>Status determines Pod lifecycle stage</p> </li> <li><code>Pending</code> (Until Scheduled)</li> <li><code>ContainerCreating</code> (Once Scheduled)</li> <li> <p><code>Running</code> (Once Container Running)</p> </li> <li> <p>Status may be viewed by <code>kubectl get pods</code></p> </li> <li>For additional information, consider Pod conditions:</li> <li> <p>A set of True/False conditions to determine lifecycle stage:</p> <ul> <li><code>PodScheduled</code></li> <li><code>Initialized</code></li> <li><code>ContainersReady</code></li> <li><code>Ready</code></li> </ul> </li> <li> <p>In some scenarios, the service provided by a container may take additional time to load beyond the pod being declared \"Ready\".</p> </li> <li>This can cause issues as if a service isn't fully ready, but Kubernetes deems it to be, Kubernetes will automatically route traffic to the pod.</li> <li>By default, Kubernetes assumes services to be ready as soon as the associated container is ready, not the associated services.</li> <li>If the application isn't ready, the users will be requesting to an unavailable pod.</li> <li> <p>To fix, one needs to change the readiness condition to suit the application within the container, this is defined by Readiness Probes</p> </li> <li> <p>Various types of readiness probes can be configured:</p> </li> <li>HTTP Test e.g <code>/api/ready</code> (Web Server)</li> <li>TCP Test - Port 3306 (Check connection to MySQL Database)</li> <li> <p>Exec command in container =&gt; Exit if successful</p> </li> <li> <p>To configure a readiness probe, add to the container's spec in the pod in a similar manner to:</p> </li> </ul> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: simple-webapp\n  labels:\n    name: simple-webapp\nspec:\n  containers:\n  - name: simple-webapp\n    image: simple-webapp\n    ports:\n    - containerPort: 8080\n    readinessProbe:\n      httpGet:\n        path: /api/ready\n        port: 8080\n</code></pre> <ul> <li> <p>Now, when the pod is deployed and the container is created, the pod's \"READY\" status is determined as to whether the HTTP call to the defined path returns a positive response.</p> </li> <li> <p>Sample TCP readiness probe:</p> </li> </ul> <pre><code>readinessProbe:\n  tcpSocket:\n    port: 3306\n</code></pre> <ul> <li>For a command execution:</li> </ul> <pre><code>readinessProbe:\n  exec:\n    command:\n    - cat\n    - /app/is_ready\n</code></pre> <ul> <li> <p>Note: To allow for additional time before the test occurs, add as a sibling to the test type: <code>initialDelaySeconds: 10</code></p> </li> <li> <p>Note: To configure the time period between checks: <code>periodSeconds: 5</code></p> </li> <li> <p>By default,the probe will stop after 3 attempts. To configure: <code>failureThreshold: 5</code></p> </li> <li>Readiness probes become beneficial when using a multi-pod configuration</li> <li>If demand increases, additional pods will be needed</li> <li>If pods don't use readiness probes, false deployment occurs -&gt; leading to service disruption.</li> </ul>"},{"location":"certifications/CKAD/05_Observability.html#52-liveness-probes","title":"5.2 - Liveness Probes","text":"<ul> <li>Suppose you run an <code>nginx</code> docker image and is serving users:</li> <li>In the event of this container failing, the service will stop, and will remain stopped until restarted; as Docker is not an orchestration tool.</li> <li>This problem can be fixed via Kubernetes' orchestration. During application failure, Kubernetes will always try to restart failed containers to minimize user downtime.</li> <li>This will work fine so long as it's not an application-level issue.</li> <li> <p>If the container is working but the issue is at an application level, Kubernetes will see no issue. This is not good for user experience.</p> </li> <li> <p>To work around this, Liveness Probes can be leveraged to periodically test the application's health.</p> </li> <li> <p>If the test fails, the container is destroyed and recreated</p> </li> <li>Test could be:</li> <li>HTTP Test: Check a given route e.g. <code>/api/healthy</code></li> <li>TCP Test: Check connection to a given port e.g. 3306 (MySQL)</li> <li> <p>Exec a command and check the result</p> </li> <li> <p>As with readiness probe, configure the liveness probe in the pod's definition file for the particular container:</p> </li> </ul> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: simple-webapp\n  labels:\n    name: simple-webapp\nspec:\n  containers:\n  - name: simple-webapp\n    image: simple-webapp\n    ports:\n    - containerPort: 8080\n    livenessProbe:\n      httpGet:\n        path: /api/ready\n        port: 8080\n</code></pre> <ul> <li>Sample TCP liveness probe:</li> </ul> <pre><code>livenessProbe:\n  tcpSocket:\n    port: 3306\n</code></pre> <ul> <li>For a command execution:</li> </ul> <pre><code>livenessProbe:\n  exec:\n    command:\n    - cat\n    - /app/is_ready\n</code></pre> <ul> <li>Additionally, as with readiness probes, one can add parameters such as:</li> <li>How long the test should wait before starting: <code>initialDelaySeconds</code></li> <li>How often should the test run: <code>periodSeconds</code></li> <li>How many successive failures are allowed: <code>failureThreshold</code></li> </ul>"},{"location":"certifications/CKAD/05_Observability.html#54-container-logging","title":"5.4 - Container Logging","text":""},{"location":"certifications/CKAD/05_Observability.html#logs-docker","title":"Logs - Docker","text":"<ul> <li> <p>When running a container there are 2 main options:</p> </li> <li> <p>Live / Forefront</p> <ul> <li><code>docker run &lt;image&gt;</code></li> <li>Displays live logs and processes associated with the container.</li> <li>Good for testing standalone containers</li> </ul> </li> <li> <p>Detached / Background</p> <ul> <li>Recommended</li> <li><code>docker run -d &lt;image&gt;</code></li> </ul> </li> <li> <p>To view the logs associated with the detached container and stream them: <code>docker logs -f &lt;container id&gt;</code></p> </li> </ul>"},{"location":"certifications/CKAD/05_Observability.html#logs-kubernetes","title":"Logs - Kubernetes","text":"<ul> <li>When running a pod in Kubernetes, you can view the container's logs via: <code>kubectl logs -f &lt;pod name&gt;</code></li> <li>This only works for 1 container, what about 2?</li> <li>Error will occur</li> <li>To view specific container logs within a pod, run the above command and append the container name i.e. <code>kubectl logs -f &lt;pod name&gt; &lt;container name&gt;</code>.</li> </ul>"},{"location":"certifications/CKAD/05_Observability.html#56-monitor-and-debug-applications","title":"5.6 - Monitor and Debug Applications","text":"<ul> <li>When monitoring a cluster, one would like to monitor metrics such as:</li> <li>Node metrics:<ul> <li>No. of nodes</li> <li>How many nodes are healthy</li> </ul> </li> <li>Performance metrics:<ul> <li>CPU Usage</li> <li>Memory Usage</li> <li>Disk Utilization</li> </ul> </li> <li> <p>Pod metric levels:</p> <ul> <li>Pod numbers</li> <li>Pod performance levels</li> </ul> </li> <li> <p>Need to monitor, store and analyze these metrics.</p> </li> <li> <p>This is not automatically done by Kubernetes, it lacks the functionality.</p> </li> <li> <p>Additional tools are available for this, such as Prometheus Metrics Server.</p> </li> <li>Note: For this course, only a minimum knowledge of these tools is required:</li> <li>Consider metrics server for now.</li> </ul>"},{"location":"certifications/CKAD/05_Observability.html#heapster-vs-metrics","title":"Heapster vs Metrics","text":""},{"location":"certifications/CKAD/05_Observability.html#heapster","title":"Heapster","text":"<ul> <li>One of the original projects for Kubernetes monitoring and analysis</li> <li>Now deprecated.</li> </ul>"},{"location":"certifications/CKAD/05_Observability.html#metrics-server","title":"Metrics-Server","text":"<ul> <li>A slimmed-down version of Heapster</li> </ul> <ul> <li>1 metrics server can be deployed per Kubernetes cluster</li> <li>It retrieves metrics from cluster nodes and pods, storing them in-memory</li> <li>Metrics server is therefore an in-memory monitoring solution.</li> <li> <p>Any data stored isn't stored on-disk</p> <ul> <li>One cannot view historical data, another tool is required e.g. Prometheus</li> </ul> </li> <li> <p>Kubernetes runs an agent on each node called Kubelet</p> </li> <li>Kubelet is responsible for receiving instructions from the Kubernetes API on the Master Node</li> <li> <p>It is also responsible for running pods on nodes.</p> </li> <li> <p>Kubelet contains sub-components called cAdvisor (container advisor)</p> </li> <li>Responsible for retrieving performance metrics from pods</li> <li> <p>The metrics are exposed through the Kubelet API to be accessed via the Metrics Server</p> </li> <li> <p>If running minikube, enable via <code>minikube addons enable metrics-server</code></p> </li> <li> <p>If using another environment:  <code>git clone &lt;link to metrics server repo&gt;</code> <code>kubectl create -f /path/to/deployments</code></p> </li> <li> <p>Note: The create command above deploys a set of resources to enable metric collation by the metrics server.</p> </li> <li>Once the data is processed, view analytics via either:</li> <li><code>kubectl top node</code> (Node Metrics)</li> <li><code>kubectl top pod</code> (Pod Metrics)</li> </ul>"},{"location":"certifications/CKAD/06_Pod-Design.html","title":"6.0 - Pod Design","text":""},{"location":"certifications/CKAD/06_Pod-Design.html#61-lablels-selectors-and-annotations","title":"6.1 - Lablels, Selectors and Annotations","text":"<ul> <li>Labels and selectors allow standardized categorization of resources or objects</li> <li> <p>Objects can then be filtered by the categories selected</p> </li> <li> <p>Labels: Properties attached to each item</p> </li> <li>Selectors: Filter criteria for items based on labels</li> <li>E.g. Kind: Pod</li> <li>Over time one could end up with thousands of objects in a cluster, including Pods, nodes, etc.</li> <li>Need to filter and categorize them.</li> <li>Could group objects via:</li> <li>Type</li> <li>Associated application</li> <li> <p>Functionality</p> </li> <li> <p>For each object, a label(s) can be associated.</p> </li> <li> <p>Appropriate selectors can be applied to filter objects</p> </li> <li> <p>To apply labels, add them under the object's <code>metadata</code> list as another list, in a similar manner to the following:</p> </li> </ul> <pre><code>....\nmetadata:\n  name: simple-webapp\n  labels:\n    app: app1\n    function: frontend\n....\n</code></pre> <ul> <li> <p>To select a pod of appropriate label: <code>kubectl get pods --selector &lt;key&gt;=&lt;value&gt;</code></p> </li> <li> <p>Kubernetes also uses labels and selectors internally to connect different objects.</p> </li> <li>Example: for a replicaset, one needs to configure the replicaset to match labels for a particular key-value pair<ul> <li>If matched correctly, the replicaset is created and manages the desired pods.</li> <li>This is by supplying the desired labels under <code>selector</code> in the ReplicaSet spec, and the metadata labels in the ReplicaSet <code>template</code>.</li> </ul> </li> </ul>"},{"location":"certifications/CKAD/06_Pod-Design.html#annotations","title":"Annotations","text":"<ul> <li>Used to record details for informatory purposes, such as:</li> <li>Build version</li> <li>IDs for integrations</li> <li>Added under metadata in a similar manner to that of labels.</li> </ul> <pre><code>metadata:\n  name: annotations-demo\n  annotations:\n    imageregistry: \"https://hub.docker.com/\"\n</code></pre>"},{"location":"certifications/CKAD/06_Pod-Design.html#63-rolling-updates-and-rollbacks-in-deployments","title":"6.3 - Rolling Updates and Rollbacks in Deployments","text":""},{"location":"certifications/CKAD/06_Pod-Design.html#rollout-and-versioning","title":"Rollout and Versioning","text":"<ul> <li>When first creating a deployment, a rollout is triggered</li> <li>Rollout = Deployment Revision</li> <li>When future upgrades occur, a new rollout and therefore a new revision of the deployment is created.</li> <li>This functionality allows tracking of deployment changes</li> <li>Allows rollback capability in the event of application failure.</li> </ul>"},{"location":"certifications/CKAD/06_Pod-Design.html#rollout-commands","title":"Rollout Commands","text":"<ul> <li>View rollout status: <code>kubectl rollout status &lt;deployment name&gt;</code></li> <li>View rollout history and versioning: <code>kubectl rollout history &lt;deployment name&gt;</code></li> </ul>"},{"location":"certifications/CKAD/06_Pod-Design.html#deployment-strategy","title":"Deployment Strategy","text":"<ul> <li>There are multiple deployment strategies available for applications, such as:</li> <li> <p>Recreate Strategy:</p> <ul> <li>When a new version of an application is ready, tear down all current instances at once</li> <li>Deploy new version once 'current' version is unavailable</li> <li>This method causes significant downtime</li> </ul> </li> <li> <p>Rolling Updates:</p> <ul> <li>Destroy current instance and upgrade with new version one at a time</li> <li>Leads to a higher application availability</li> <li>Upgrade appears to be \"seamless\"</li> </ul> </li> </ul>"},{"location":"certifications/CKAD/06_Pod-Design.html#kubectl-apply","title":"Kubectl Apply","text":"<ul> <li>To update a deployment, apply the appropriate changes to the associated definition file e.g. could update the image used, labels, etc.</li> <li> <p>Apply the changes: <code>kubectl apply -f &lt;definition&gt;.yaml</code></p> </li> <li> <p>Changes can also be applied via the CLI directly, though this would not update the base definition YAML: <code>kubectl set image deployment/&lt;deployment name&gt; &lt;image name&gt;=&lt;image name&gt;:&lt;new tag&gt;</code></p> </li> <li> <p>Deployment strategies can be viewed in detail for a given deployment via <code>kubectl describe deployment &lt;deployment name&gt;</code></p> </li> <li>For recreate strategy, during the upgrade process, one will see the deployment is gradually scaled down to 0 instances and back again.</li> <li>For rolling update:</li> <li>Scaled down individually, old version is scaled down, then new version is brought up, this repeats per replicas.</li> </ul>"},{"location":"certifications/CKAD/06_Pod-Design.html#upgrades","title":"Upgrades","text":"<ul> <li>When a new deployment's created, a new replicaset is automatically created, hosting the desired number of replica pods.</li> <li>During an upgrade, a new replicaset is created</li> <li>New pods with new application version added sequentially</li> <li> <p>At the same time, the \"old\" pods are gradually taken down</p> </li> <li> <p>Once upgraded, if a rollback is required: <code>kubectl rollout undo &lt;deployment name&gt;</code></p> </li> </ul>"},{"location":"certifications/CKAD/06_Pod-Design.html#kubectl-run","title":"Kubectl Run","text":"<ul> <li>The command <code>kubectl run &lt;deployment name&gt; --image &lt;image name&gt;</code> will create a deployment</li> <li>This method acts as an alternative to using a definition file</li> <li>Using this method, the required replicaset and pods auto-created in the backend.</li> <li>Note: It's highly recommended to use a definition file for deployments for file editing and versioning.</li> </ul>"},{"location":"certifications/CKAD/06_Pod-Design.html#command-summary","title":"Command Summary","text":"<ul> <li>Create: <code>kubectl create -f &lt;filename&gt;.yaml</code></li> <li>Get: <code>kubectl get deployments</code></li> <li> <p>Update a deployment: <code>kubectl apply -f &lt;filename&gt;.yaml</code> or <code>kubectl set image &lt;deployment&gt; &lt;image ID&gt;=&lt;image&gt;:&lt;tag&gt;</code></p> </li> <li> <p>Apply an update via changes in a definition file or use CLI flags. The latter doesn't update any associated definition files stored on the system.</p> </li> <li> <p>Rollout Status: <code>kubectl rollout status &lt;deploymment name&gt;</code></p> </li> <li> <p>Rollout History: <code>kubectl rollout history &lt;deployment name&gt;</code></p> </li> <li> <p>Rollback: <code>kubectl rollout undo &lt;deployment name&gt;</code></p> </li> </ul>"},{"location":"certifications/CKAD/06_Pod-Design.html#additional-notes","title":"Additional Notes","text":"<ul> <li>Get the history of a particular deployment revision: <code>kubectl rollout history deployment &lt;name&gt; --revision=&lt;revision number&gt;</code></li> <li>When updating a deployment revision, append the <code>--record</code> flag to save the command used to cause the revision update, this will be stored in the <code>change-cause</code> field of the above command's output.</li> <li>To rollback to a specific revision: <code>kubectl rollout undo deployment &lt;name&gt; --to-revision=&lt;revision number&gt;</code></li> </ul>"},{"location":"certifications/CKAD/06_Pod-Design.html#65-jobs","title":"6.5 - Jobs","text":"<ul> <li>Containers are created with the aim of running a particular task or workload. Common examples include:</li> <li>Web apps</li> <li>Databases</li> <li>Analytics</li> <li> <p>Image processing</p> </li> <li> <p>When creating a container to perform a particular job, Kubernetes will want the container to \"live forever\", default behavior for a replicaSet.</p> </li> <li>When a container completes its job it'll exit and its state will become \"completed\"</li> <li> <p>By default, Kubernetes will continuously restart it as it views there to be a problem, despite this not being the case.</p> </li> <li> <p>To work around, one can configure the <code>restartPolicy</code> property of the pod</p> </li> <li> <p>By default, this is set to <code>always</code> but can be changed to <code>never</code> or <code>on failure</code></p> </li> <li> <p>Example Usage:</p> </li> </ul> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: math-pod\nspec:\n  containers:\n  - name: math-add\n    image: ubuntu\n    command: ['expr', '3', '+', '2']\n  restartPolicy: Never\n</code></pre> <ul> <li>In practice, there may be more than one pod processing the data simultaneously.</li> <li> <p>All pods need to complete their task and exit.</p> </li> <li> <p>ReplicaSets could be used to ensure identical sets of pods are created.</p> </li> <li> <p>Use a job to run a set of pods to complete a common goal</p> </li> <li> <p>As usual, a pod definition file to describe the job is needed.</p> </li> <li>Additionally, need a job definition file to create it:</li> </ul> <pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: math-add-job\nspec:\n  template:\n    spec:\n      containers:\n      - name: math-add\n        image: ubuntu\n        command: ['expr', '3', '+', '2']\n    restartPolicy: Never\n</code></pre> <ul> <li> <p>As with replicasets and deployments, the spec of the job is defined by a template containing the pod definition's spec.</p> </li> <li> <p>To create: <code>kubectl create -f &lt;job definition&gt;.yaml</code></p> </li> <li> <p>To view jobs and status: <code>kubectl get jobs</code></p> </li> <li> <p>Displays the number of desired and successful jobs</p> </li> <li> <p>To view pods associated with jobs: <code>kubectl get pods</code></p> </li> <li> <p>Status will show <code>completed</code> as Kubernetes didn't try to restart the pod</p> </li> <li> <p>To verify the output, use <code>kubectl logs &lt;job id&gt;</code></p> </li> <li> <p>To delete the job and associated pods: <code>kubectl delete job &lt;job name&gt;</code></p> </li> <li> <p>Note: In reality, the jobs included would be far more complex</p> </li> <li> <p>To run multiple instances / pods to complete the job, add the property <code>completions</code> should be added as a sibling to <code>template</code>.</p> </li> <li> <p>Pods will be created sequentially as each are completed.</p> </li> <li> <p>To create jobs in parallel rather than sequentially, add <code>parralellism</code> property as a sibling to <code>completions</code>.</p> </li> <li> <p>Creates <code>x</code> pods simultaneously (so long as the system can handle it)</p> </li> <li> <p>More recommended to use <code>parralellism</code> rather than sequential creation as this could create more pods than necessary.</p> </li> </ul>"},{"location":"certifications/CKAD/06_Pod-Design.html#66-cronjobs","title":"6.6 - CronJobs","text":"<ul> <li>A job that can be scheduled.</li> <li>Usually if a job was created via <code>kubectl create</code>, the job will run instantly</li> <li> <p>Not applicable for certain jobs e.g. logging</p> </li> <li> <p>To create a CronJob, can create a definition file similar to:</p> </li> </ul> <pre><code>apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: reporting-cron-job\nspec:\n  schedule: \"*/1 * * * *\"\n  jobTemplate:\n    spec:\n      completions: 3\n      parralellism: 3\n      template:\n        spec:\n          containers:\n          - name: reporting-tool\n            image: reporting-tool\n      restartPolicy: Never\n</code></pre> <ul> <li>Note: The format for the schedule property should adhere to:  <code>minute (0-59) hour (0-23) day (0-31) month (1-12) day (week, 0-6)</code></li> <li> <p>One can use <code>*</code> to act as a wildcard / \"every\" option.</p> </li> <li> <p>To create a CronJob: <code>kubectl create -f &lt;cronjob&gt;.yaml</code></p> </li> <li> <p>To view CronJob: <code>kubectl get cronjob</code></p> </li> </ul>"},{"location":"certifications/CKAD/07_Services-and-Networking.html","title":"7.0 - Services and Networking","text":""},{"location":"certifications/CKAD/07_Services-and-Networking.html#71-services","title":"7.1 - Services","text":"<ul> <li>Kubernetes services allows inter-component communication both within and outside Kubernetes.</li> <li>Additionally, services allow connections of applications with users or other applications.</li> </ul>"},{"location":"certifications/CKAD/07_Services-and-Networking.html#example","title":"Example","text":"<ul> <li>Suppose an application has a group of pods running various aspects of the app</li> <li>One for serving frontend users</li> <li>Another for backend etc</li> <li> <p>Another for connecting to an external data source</p> </li> <li> <p>All these groups of pods are connected by use of services</p> </li> <li>Additionally, services allow frontend accessibility to users</li> <li> <p>Allows front-to-back pod communication and external data source communication</p> </li> <li> <p>One of the key services used in Kubernetes is the facilitation of external communication:</p> </li> <li>Suppose a pod has been deployed and is running a web app:</li> <li>The Kubernetes node's IP address is in the same network as the local machine</li> <li>The pod's network is separate</li> <li>To access the container's contents, could either use a <code>curl</code> request to the IP or access via local browser</li> <li>In practice, wouldn't want to have to ssh into the node to access the container's content, you'd want to access it as an \"external\" user.</li> <li>Kubernetes service(s) can be introduced to map the request from a local machine -&gt; node -&gt; pod</li> <li>Kubernetes services are treated as objects in Kubernetes like Pods, ReplicaSets, etc.</li> <li> <p>To facilitate external communications, one can use NodePort:</p> <ul> <li>Listens to a port on the node</li> <li>Forwards requests to the required pods</li> </ul> </li> <li> <p>Primary service types include:</p> </li> <li>NodePort: Makes an internal pod accessible via a port on a node</li> <li>ClusterIP:<ul> <li>Creates a virtual IP inside the cluster</li> <li>Enables communication between services e.g. frontend &lt;--&gt; backend</li> </ul> </li> <li>Load Balancer:<ul> <li>Provisions a load balancer for application support</li> <li>Typically cloud-provider only.</li> </ul> </li> </ul>"},{"location":"certifications/CKAD/07_Services-and-Networking.html#nodeport","title":"NodePort","text":"<ul> <li>Maps a port on the cluster node to a port on the pod to allow accessibility</li> <li>On closer inspection, this service type can b e broken down into 3 parts:</li> <li>The port of the application on the pod it's running from -&gt; <code>targetPort</code></li> <li>The port on the service itself -&gt; <code>port</code></li> <li> <p>Port on the node used to access the application externally -&gt; <code>NodePort</code></p> </li> <li> <p>Note: NodePort range only available for <code>30000 - 32767</code></p> </li> <li> <p>To create the service, create a definition file similar to the following:</p> </li> </ul> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: myapp-service\nspec:\n  type: NodePort\n  ports:\n  - targetPort: 80\n    port: 80\n    nodePort: 30080\n</code></pre> <ul> <li>Note: The prior definition file is acceptable for using only one pod on the cluster.</li> <li>If there are multiple pods with a <code>targetPort</code> of say 80 in this case, this will cause issues, the service needs to only focus on certain pods</li> <li> <p>This can be worked around via the use of selectors</p> </li> <li> <p>Under <code>selector</code>, add any labels associated with the pod definition file e.g.:</p> </li> </ul> <pre><code>spec:\n  ...\n  selector:\n    app: myapp\n    type: frontend\n  ...\n</code></pre> <ul> <li> <p>The service can then be created using <code>kubectl create -f &lt;filename&gt;.yaml</code> as per usual.</p> </li> <li> <p>To view services: <code>kubectl get services</code></p> </li> <li> <p>To access the web service: <code>curl &lt;node IP&gt;:&lt;node Port&gt;</code></p> </li> <li> <p>Suppose you're in a production environment:</p> </li> <li>Multiple pods or instances running in the same application</li> <li> <p>Allows high availability and load balancing</p> </li> <li> <p>If all pods considered share the same labels, the selector will automatically assign the pods as the service endpoints -&gt; no additional configuration is required.</p> </li> <li> <p>If the pods are distributed across multiple nodes:</p> </li> <li> <p>Without any additional configuration Kubernetes automatically creates the service to span the entire set of nodes in the cluster.</p> <ul> <li>Maps the target port to the same node port for each node.</li> <li>The application can be accessed through the IP of any of the nodes in the cluster, but via the same port.</li> </ul> </li> <li> <p>Regardless of the number of pods or nodes involved, the service creation method is exactly the same, no additional steps are required.</p> </li> <li> <p>Note: When pods are removed or added, the service will automatically be updated =&gt; offering higher flexibility and adaptability.</p> </li> </ul>"},{"location":"certifications/CKAD/07_Services-and-Networking.html#72-clusterip-service","title":"7.2 - ClusterIP Service","text":"<ul> <li>In general, a fill stack will comprise of groups of pods, hosting different parts of an application, such as:</li> <li>Frontend</li> <li>Backend</li> <li> <p>Key-value store</p> </li> <li> <p>Each of these groups of pods need to be able to interact with one another for the application to fully function.</p> </li> <li> <p>Each pod will automatically be assigned its own IP address</p> </li> <li>Not static</li> <li>Pods could be removed or added at any given point</li> <li> <p>One cannot therefore rely on these IP addresses for inter-service communication</p> </li> <li> <p>Kubernetes ClusterIP services can be used to group the pods together by functionality and provide a single interface to access them.</p> </li> <li> <p>Any requests to that group is assigned randomly to one of the pods within.</p> </li> <li> <p>This provides an easy and effective deployment of a microservice-based application on a Kubernetes cluster.</p> </li> <li> <p>Each layer or group gets assigned its own IP address and name within the cluster</p> </li> <li>To be used by other pods to access the service.</li> <li> <p>Each layer can scale up or down without impacting service-service communications</p> </li> <li> <p>To create, write a definition file:</p> </li> </ul> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: backend\nspec:\n  type: ClusterIP\n  ports:\n  - targetPort: 80\n    port: 80\n  selector:\n    app: myapp\n    type: backend\n</code></pre> <ul> <li> <p>To create the service: <code>kubectl create -f &lt;service file&gt;.yaml</code> or <code>kubectl expose &lt;deployment or pod name&gt; --port=&lt;port&gt; --target-port=&lt;port&gt; --type=clusterIP</code></p> </li> <li> <p>View services via <code>kubectl get services</code></p> </li> <li> <p>From here, the services can be accessed via the ClusterIP or service name.</p> </li> </ul>"},{"location":"certifications/CKAD/07_Services-and-Networking.html#74-ingress-networking","title":"7.4 - Ingress Networking","text":"<ul> <li>To understand the importance of Ingress, consider the following example:</li> <li>Suppose you build an application into a Docker image and deploy it as a pod via Kubernetes.</li> <li>Due to the application's nature, set up a MySQL database and deploy a clusterIP service -&gt; allows app-database communications.</li> <li>To expose the app or external access, one needs to create a NodePort service.</li> <li>App can then be accessed via the Node's IP and the port defined.</li> <li>To access the URL, users need to go to <code>http://&lt;node ip&gt;:&lt;node port&gt;</code></li> <li> <p>This is fine for small non-production apps, it should be noted that as demand increases, the replicaSet and service configuration can be altered to support load balancing.</p> </li> <li> <p>For production, users wouldn't want to have to enter an IP and port number every time, typically a DNS entry would be created to map to the port and IP.</p> </li> <li>As service node ports can only allocate high numbered ports (<code>&gt; 30000</code>):</li> <li> <p>Introduce a proxy server between DNS cluster and point it to the DNS server.</p> </li> <li> <p>The above steps are applicable if hosting an app in an on-premise datacenter.</p> </li> <li>If working with a public cloud application, NodePort can be replaced by <code>LoadBalancer</code></li> <li> <p>Kubernetes still performs NodePort's functionality AND sends an additional request to the platform to provision a network load balancer.</p> </li> <li> <p>The cloud platform automatically deploys a load balancer configured to route traffic to the service ports of all the nodes.</p> </li> <li> <p>The cloud provider's load balancer would have its own external IP</p> </li> <li> <p>User request access via this IP.</p> </li> <li> <p>Suppose as the application grows and a new service is to be added, it's to be accessed via a new URL.</p> </li> <li>For the new application to share the cluster resource, release it as a separate deployment.</li> <li> <p>Engineers could create a new load balancer for this app, monitoring a new port</p> <ul> <li>Kubernetes automatically configures a new load balancer on the cloud platform of a new IP.</li> </ul> </li> <li> <p>To map the URLs between the 2 new services, one would have to implement a new proxy server on top of those associated with the service.</p> </li> <li> <p>This proxy service would have to be configured and SSL communications would have to be enabled.</p> </li> <li> <p>This final proxy could be configured on a team-by-team basis, however would likely lead to issues.</p> </li> </ul> <ul> <li>The whole process outlined above has issues, on top of having additional proxies to manage per service, one must also consider:</li> <li>Cost: Each additional Load Balancer adds more expense.</li> <li> <p>Difficulty of Management: For each service introduced, additional configuration is required for both firewalls and proxies</p> <ul> <li>Different teams required as well as time and \"person\" power.</li> </ul> </li> <li> <p>To work around this and collectively manage all these aspects within the cluster, one can use Kubernetes Ingress:</p> </li> <li>Allows users access via a single URL</li> <li>URL can be configured to route different services depending on the URL paths.</li> <li>SSL security may automatically be implemented via Ingress</li> <li> <p>Ingress can act as a layer 7 load balancer built-in to Kubernetes clusters</p> <ul> <li>Can be configured to act like a normal Kubernetes Object.</li> </ul> </li> <li> <p>Note: Even with Ingress in place, one still needs to expose the application via a NodePort or Load Balancer -&gt; this would be a 1-time configuration.</p> </li> <li> <p>Once exposed, all load balancing authentication, SSL and URL routing configurations are manageable and viewable via an Ingress Controller.</p> </li> <li> <p>Ingress controllers aren't set up by default in Kubernetes, example solutions that can be deployed include:</p> </li> <li>GCE</li> <li>NGINX</li> <li> <p>Traefik</p> </li> <li> <p>Load balancers aren't the only component of an Ingress controller, additionaly functionalities are available for monitoring the cluster for new Ingress resources or definitions.</p> </li> <li> <p>To create, write a definition file:</p> </li> </ul> <pre><code>apiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: nginx-ingress-controller\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: nginx-ingress\n  template:\n    metadata:\n      labels:\n        name: nginx-ingress\n    spec:\n      containers:\n      - name: nginx-ingress-controller\n        image: &lt;nginx ingress controller url&gt;\n        args:\n        - /nginx-ingress-controller\n        - --configmap=$(POD_NAMESPACE)/nginx-configuration\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - name: http\n          containerPort: 80\n        - name: https\n          containerPort: 443\n</code></pre> <ul> <li>Note: As working with Nginx, need to configure options such as log paths, SSL settings, etc.</li> <li> <p>To decouple this from the controller image, write a separate config map definition file to be referenced:</p> <ul> <li>Allows easier modification rather than editing one huge file.</li> </ul> </li> <li> <p>An ingress service definition file is also required to support external communicationL</p> </li> </ul> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-ingress\nspec:\n  type: NodePort\n  ports:\n  - port: 80\n    targetPort: 80\n    protocol: TCP\n    name: http\n  - port: 443\n    targetPort: 443\n    protocol: TCP\n    name: https\n  selector:\n    name: nginx-ingress\n</code></pre> <ul> <li> <p>The service NodePort definition above links the service to the deployment.</p> </li> <li> <p>As mentioned, Ingress controllers have additional functionality available for monitoring the cluster for ingress resources, and apply configurations when changes are made</p> </li> <li> <p>For the controller to do this, a service account must be associated with it:</p> </li> </ul> <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: nginx-ingress-serviceaccount\n</code></pre> <ul> <li> <p>The service account must have the correct roles and role-bindings to work.</p> </li> <li> <p>To summarise, for an ingress controller, the following resources are needed:</p> </li> <li>Deployment</li> <li>Service</li> <li>ConfigMap</li> <li> <p>ServiceAccount</p> </li> <li> <p>Once an ingress controller is in place, one can create ingress resources:</p> </li> <li> <p>Ingress resources are a set of rules and configurations applied to an ingress controller, linking it to other Kubernetes objects.</p> </li> <li> <p>For example, one could configure a rule to forward all traffic to one application, or to a different set of applications based on a URL.</p> </li> <li>Alternatively, could route based on DNS.</li> <li>As per, ingress resources are configured via a destination file</li> </ul> <pre><code>apiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: ingress-cluster\nspec:\n  backend:\n    serviceName: wear-service\n    servicePort: 80\n</code></pre> <ul> <li>Note: For a single backend like above, no additional rules are required.</li> <li> <p>The ingress resource can be created via standard means i.e. <code>kubectl create -f ....</code></p> </li> <li> <p>To view ingress resource: <code>kubectl get ingress</code></p> </li> <li>To route traffic in a conditional form, use ingress rules e.g. routing based on DNS</li> <li> <p>Within each rule, can configure additional paths to route to additional services or applications.</p> </li> <li> <p>To implement, adhere to the principles outlined in the following 2-service example:</p> </li> </ul> <pre><code>apiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: ingress-cluster\nspec:\n  rules:\n  - http:\n      paths:\n      - path: /wear\n        backend:\n            serviceName: wear-service\n            servicePort: 80\n      - path: /watch\n        backend:\n            serviceName: watch-service\n            servicePort: 80\n</code></pre> <ul> <li> <p>Create the ingress resource using <code>kubectl create -f ...</code> as per usual.</p> </li> <li> <p>To view the ingress's detailed information: <code>kubectl describe ingress &lt;ingress name&gt;</code></p> </li> <li> <p>Note: In the description, a default backend is described.  In the event a user enters a path not matching any of the rules, they will be redirected to that backend service (which must exist!).</p> </li> <li> <p>If wanting to split traffic via domain name, a definition file can be filled out as normal, but in the spec, the rules can be updated to point to specific hosts instead of paths:</p> </li> </ul> <pre><code>...\nrules:\n- host: &lt;url 1&gt;\n  http:\n    paths:\n    - backend:\n        serviceName: &lt;service name 1&gt;\n        servicePort: &lt;port 1&gt;\n- host: &lt;url 2&gt;\n  http:\n    paths:\n    - backend:\n        serviceName: &lt;service name 2&gt;\n        servicePort: &lt;port 2&gt;\n...\n</code></pre> <ul> <li>When splitting by URL, had 1 rule and split the traffic by 2 paths</li> <li> <p>When splitting by hostname, used 2 rules with a path for each.</p> </li> <li> <p>Note: If not specifying the host field, it'll assume it to be a <code>*</code> and / or accept all incoming traffic without matching the hostname</p> </li> <li>Acceptable for a single backend</li> </ul>"},{"location":"certifications/CKAD/07_Services-and-Networking.html#updates-and-additional-notes","title":"Updates and Additional Notes","text":"<ul> <li>As of Kubernetes versions 1.20+, Ingress resources are defined a little differently, in particular the <code>apiVersion</code> and <code>service</code> parameters. An example follows:</li> </ul> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: ingress-cluster\nspec:\n  rules:\n  - http:\n      paths:\n      - path: /wear\n        backend:\n          service:\n            name: wear-service\n            port:\n              number: 80\n      - path: /watch\n        backend:\n          service:\n            name: watch-service\n            port:\n              number: 80\n</code></pre> <ul> <li>Ingress resources can be created imperatively via commands similar to: <code>kubectl create ingress &lt;ingress-name&gt; --rule=\"&lt;host&gt;/&lt;path&gt;=service:port\"</code></li> </ul>"},{"location":"certifications/CKAD/07_Services-and-Networking.html#rewrite-target-option","title":"Rewrite-Target Option","text":"<ul> <li>Different ingress controllers come with particular customization options. For example, NGINX has the <code>rewrite-target</code> option.</li> <li>To understand this, consider two services to be linked via the same ingress, accessible at the following urls for standalone services and via ingress respectively:</li> <li><code>http://&lt;service 1&gt;:&lt;port&gt;/</code> &amp; <code>http://&lt;service 2&gt;:&lt;port&gt;/</code></li> <li> <p><code>http://&lt;ingress-service&gt;:&lt;ingress-port&gt;/&lt;service 1 path&gt;</code> &amp; <code>http://&lt;ingress-service&gt;:&lt;ingress-port&gt;/&lt;service 2 path&gt;</code></p> </li> <li> <p>The two applications will not have the paths defined in the ingress urls configured on them, without the <code>rewrite-target</code> configuration, the following journey would occur for each given service:</p> </li> <li><code>http://&lt;ingress-service&gt;:&lt;ingress-port&gt;/&lt;service 1 path&gt;</code> -&gt; <code>http://&lt;service 1&gt;:&lt;port&gt;/&lt;service 1 path&gt;</code></li> <li>As the service paths aren't configured for the applications, this would throw an unexpected error. To avoid, one can use the <code>rewrite-target</code> option to rewrite the URL upon the ingress URL being called.</li> <li>This will replace the content under <code>path</code> for the given <code>paths</code> with a value defined in a <code>rewrite-target</code> annotation. This is analogous to a <code>replace</code> function.</li> <li>This is implemented in a similar manner to below:</li> </ul> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: ingress-cluster\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /\nspec:\n  rules:\n  - http:\n      paths:\n      - path: /wear\n        backend:\n          service:\n            name: wear-service\n            port:\n              number: 80\n      - path: /watch\n        backend:\n          service:\n            name: watch-service\n            port:\n              number: 80\n</code></pre>"},{"location":"certifications/CKAD/07_Services-and-Networking.html#77-network-policies","title":"7.7 - Network Policies","text":""},{"location":"certifications/CKAD/07_Services-and-Networking.html#traffic-example","title":"Traffic Example","text":"<ul> <li>Suppose we have the following setup of servers:</li> <li>Web</li> <li>API</li> <li>Database</li> <li>Network traffic will be flowing through each of these servers across particular ports, for example:</li> <li>Web user requests and receives content from the web server on port 80 for HTTP</li> <li>Web server makes a request to the API over port 5000</li> <li> <p>API requests for information from the database over port 3306 (e.g. if MySQL)</p> </li> <li> <p>2 Types of Network Traffic in this setup:</p> </li> <li>Ingress: Traffic to a resource</li> <li> <p>Egress: Traffic sent out from a resource</p> </li> <li> <p>For the setup above, we could control traffic by allowing ONLY the following traffic to and from each resource across particular ports:</p> </li> <li>Web Server:<ul> <li>Ingress: 80 (HTTP)</li> <li>Egress: 5000 (API port)</li> </ul> </li> <li>API Server:<ul> <li>Ingress: 5000</li> <li>Egress: 3306 (MySQL Database Port)</li> </ul> </li> <li> <p>Database Server:</p> <ul> <li>Ingress: 3306</li> </ul> </li> <li> <p>Considering this from a Kubernetes perspective:</p> </li> <li>Each node, pod and service within the cluster has its own IP address</li> <li>When working with networks in Kubernetes, it's expected that the pods should be able to communicate with one another, regardless of the olution to the project<ul> <li>No additional configuration required</li> </ul> </li> <li>By default, Kubernetes has an \"All-Allow\" rule, allowing communication between any pod in the cluster.</li> <li>This isn't best practice, particularly if working with resources that store very sensitive information e.g. databases.</li> <li>To restrict the traffic, one can implement a network policy.</li> </ul> <ul> <li>A network policy is a Kubernetes object allowing only certain methods of network traffic to and from resources. An example follows:</li> </ul> <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: network-policy\nspec:\n  podSelector:\n    matchLabels:\n      role: db\n  policyTypes:\n  - Ingress\n  - Egress\n  ingress:\n  - from:\n    - podSelector: # what pods can communicate with this pod?\n        matchLabels:\n          name: api-pod\n      namespaceSelector: # what namespaced resources can communicate with this pod?\n        matchLabels:\n          name: prod\n    - ipBlock: # what IP range(s) are allowed?\n        cidr: 192.168.5.10/32\n    ports:\n    - protocol: TCP\n      port: 3306\n  egress:\n  - to:\n    - ipBlock: # what IP range(s) are allowed?\n        cidr: 192.168.5.10/32\n\n</code></pre> <ul> <li> <p>The policy can then be created via <code>kubectl create -f ....</code></p> </li> <li> <p>Network policies are enforced and supported by the network solution implemented on the cluster.</p> </li> <li>Solutions that support network policies include:</li> <li><code>kube-router</code></li> <li><code>calico</code></li> <li><code>romana</code></li> <li> <p><code>weave-net</code></p> </li> <li> <p>Flannel doesn't support Network Policies, they can still be created, but will not be enforced.</p> </li> <li>Rules are separated by <code>-</code> in order of listing, in the example above, the conditions for <code>podSelector</code> and <code>namespaceSelector</code> must be met. If this isn't met, then the <code>ipBlock</code> condition would be checked.</li> <li>Similar syntax for <code>egress</code> policies are used with some slight tweaks e.g. replace <code>from</code> with <code>to</code>.</li> </ul>"},{"location":"certifications/CKAD/08_Persistent-Volumes.html","title":"8.0 - Persistent Volumes","text":""},{"location":"certifications/CKAD/08_Persistent-Volumes.html#81-volumes","title":"8.1 - Volumes","text":"<ul> <li>In practice, Docker containers are ran on a \"need-to-use\" basis.</li> <li>They only exist for a short amount of time / are ephemeral.</li> <li> <p>Once the associated job or process is complete, they're taken down, along with any associated data.</p> </li> <li> <p>To persist data associated with a container, one can attach a volume.</p> </li> <li> <p>When working in Kubernetes, a similar process can be can be used:</p> </li> <li>Attach a volume to a Pod</li> <li>The volume(s) store data associated with the pod.</li> </ul>"},{"location":"certifications/CKAD/08_Persistent-Volumes.html#example-pod-volume-integration","title":"Example Pod-Volume Integration","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: random-number-generator\nspec:\n  containers:\n  - image: alpine\n    name: alpine\n    command: [\"/bin/sh\", \"-c\"]\n    args: [\"shuf -i 0-100 -n  1 &gt;&gt; /opt/number.out\"]\n    volumeMounts:\n    - mountPath: /opt\n      name: data-volume\nvolumes:\n- name: data-volume\n  hostPath:\n    path: /data\n    type: Directory\n</code></pre> <ul> <li>This is ok for a single-node cluster, for multi-node clusters, the data would be persisted to the same directory on each node.</li> <li>One would expect them all to be the same and have the same data, but this is not the case as they're different servers.</li> <li>To work around this, one can use other storage solutions supported by Kubernetes e.g.:</li> <li>AWS Elastic Block Storage (EBS)</li> <li>Azure Disk</li> <li> <p>Google Persistent Disk</p> </li> <li> <p>When using one of these solutions, such as EBS, the volume definition changes:</p> </li> </ul> <pre><code>...\nvolumes:\n- name: data-volume\n  awsElasticBlockStore:\n    volumeID: &lt;volume ID&gt;\n      fsType: ext4\n</code></pre>"},{"location":"certifications/CKAD/08_Persistent-Volumes.html#82-persistent-volumes","title":"8.2 - Persistent Volumes","text":"<ul> <li>When creating volumes in the prior section, volumes are created in definition file</li> <li>When working in a larger environment, where users are deploying lots of pods etc, this can cause issues.</li> <li>Each time a pod is to be deployed, storage needs to be configured.</li> <li>These changes would have to be applied to every pod individually</li> <li>To remove this problem and manage the storage centrally, persistent volumes can be leveraged.</li> </ul>"},{"location":"certifications/CKAD/08_Persistent-Volumes.html#persistent-volumes","title":"Persistent Volumes","text":"<ul> <li>A cluster-wide pool of storage volumes that is configured by an admin.</li> <li> <p>Used by users deploying applications on the cluster.</p> </li> <li> <p>Users can select storage from this pool by persistent volume claims.</p> </li> <li> <p>Defining a PersistentVolume:</p> </li> </ul> <pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: pv-vol1\nspec:\n  accessModes:\n    - ReadWriteOnce / ReadOnlyMany / ReadWriteMany # how a volume should be mounted on a host\n  capacity:\n    storage: 1Gi\n  hostPath:\n    path: /tmp/data\n</code></pre> <ul> <li> <p>To create the volume: <code>kubectl create -f &lt;file&gt;.yaml</code></p> </li> <li> <p>To view: <code>kubectl get persistentvolume</code></p> </li> <li> <p>For production environments, it's recommended to use a storage solution like AWS EBS.</p> </li> <li>Replace <code>hostPath</code> with the appropriate attributes in this case.</li> </ul>"},{"location":"certifications/CKAD/08_Persistent-Volumes.html#83-persistent-volume-claims","title":"8.3 - Persistent Volume Claims","text":"<ul> <li>Kubernetes objects created by users to request access to a portion of a PersistentVolume.</li> <li>Once claims are created, Kubernetes binds the Persistent Volume to the claims</li> <li> <p>Binding determined based on request and properties set on the volume</p> </li> <li> <p>Note: Each persistent volume claim is bound to a single persistent volume.</p> </li> <li> <p>Kubernetes will always try to find a persistent volume with sufficient capacity as requested by the claim.</p> </li> <li> <p>Also considers storage class, access modes, etc.</p> </li> <li> <p>If there are multiple possible matches for a claim, and a particular volume is desired, labels and selectors can be utilized.</p> </li> <li> <p>It's possible for a smaller claim to be matched to a larger volume if the criteria is satisfied and no other option is available:</p> </li> <li>1-to-1 relationship between claims and volumes</li> <li> <p>No additional claims could utilize the remaining volume.</p> </li> <li> <p>If no other volumes are available, the claim remains in a pending state</p> </li> <li> <p>Automatic assignment occurs when an applicable volume becomes available.</p> </li> <li> <p>To create a claim:</p> </li> </ul> <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: myclaim\nspec:\n  accessMode:\n  - ReadWriteOnce\n  resources:\n    requests:\n      storage: 500Mi\n</code></pre> <ul> <li>Creation via <code>kubectl create -f ...</code></li> <li> <p>To view claims: <code>kubectl get persistentvolumeclaim</code></p> </li> <li> <p>When claim is created, Kubernetes will look through all available volumes and bind it appropriately, checking for <code>accessMode</code>, <code>storage</code> parameters, etc.</p> </li> <li> <p>Associated volume will be noted in a column as part of the <code>kubectl get</code> command above.</p> </li> <li> <p>To delete a PVC, <code>kubectl delete persistentvolumeclaim &lt;claim name&gt;</code></p> </li> <li> <p>One can choose to delete, retain or recycle the volume upon claim deletion</p> </li> <li>Determined via configuring the <code>persistentVolumeReclaimPolicy</code> attribute. <code>Delete</code> ensures the volume is deleted upon claim deletion.</li> </ul>"},{"location":"certifications/CKAD/08_Persistent-Volumes.html#84-storage-classes","title":"8.4 - Storage Classes","text":""},{"location":"certifications/CKAD/08_Persistent-Volumes.html#static-provisioning","title":"Static Provisioning","text":"<ul> <li>When working with storage providers such as cloud platforms via Kubernetes, you have to ensure that the resources required are pre-provisioned.</li> <li>Using Google Cloud as an example: <code>gcloud beta compute disks create --size 1Gb --region us-east1 pd-disk</code></li> <li>The volume can then be referenced in YAML manifests:</li> </ul> <pre><code>apiVersion: v1\nkind: PersisentVolume\nmetadata:\n  name: pv-vol1\nspec:\n  accessModes:\n  - ReadWriteOnce\n  capacity:\n    storage: 500Mi\n  gcePersistentDisk:\n    pdName: pd-disk\n    fsType: ext4\n</code></pre> <ul> <li>This would have to be done every time, a tedious task known as Static Provisioning. Thankfully, there's alternatives!</li> </ul>"},{"location":"certifications/CKAD/08_Persistent-Volumes.html#dynamic-provisioning","title":"Dynamic Provisioning","text":"<ul> <li>In an ideal scenario, if we define a <code>PersistentVolume</code> manifest referencing a storage provider external to a cluster, the resources required from said provider should be created upon application of the manifest.</li> <li>This can be achieved by defining a <code>StorageClass</code> for the particular provider. For GCP, an example follows:</li> </ul> <pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: google-storage\nprovisioner: kubernetes.io/gce-pd\n</code></pre> <ul> <li>Upon application, the <code>StorageClass</code> can be referenced by any <code>PersistentVolumes</code> or <code>PersistentVolumeClaims</code> by adding the <code>storageClassName</code> parameter to the object's spec.</li> <li>When these objects are created, Kubernetes will reference the specified Provisioner and automatically provision the desired supporting resources.</li> <li>Provisioners exist for most platforms such as <code>Azure</code>, <code>AWS</code>, <code>vSphere</code>. Each will come with their own parameters for further configuration.</li> <li>Note: Multiple StorageClasses can be defined for the same provisioner with different parameters, so long as they're named differently.</li> </ul>"},{"location":"certifications/CKAD/08_Persistent-Volumes.html#85-statefulsets","title":"8.5 - StatefulSets","text":""},{"location":"certifications/CKAD/08_Persistent-Volumes.html#why-statefulsets","title":"Why StatefulSets?","text":"<ul> <li>Consider a standalone server that is to host a database. Once the database is setup, applications can connect to it.</li> <li>To support high-availability, the process is repeated on other servers, however the new databases are blank.</li> <li>In an ideal scenario, one would want the data stored to be replicated across all instances - how is this achieved?</li> <li>In a standard scenario, native replication features supported by the database would be utilised. This can be done in multiple scenarios:</li> </ul>"},{"location":"certifications/CKAD/08_Persistent-Volumes.html#master-worker-topology","title":"Master-Worker Topology","text":"<ul> <li>Master node is set up and is the only node that supports write operations</li> <li>Worker node is setup, and the master's data is cloned to the replica.</li> <li> <p>Continuous replication is enabled between master and worker.</p> </li> <li> <p>This is fine, but as it scales up, more and more resources are required from the master - this is not sustainable.</p> </li> <li>Each worker receives the data from the 1 master node, they have no knowledge of the other workers.</li> </ul>"},{"location":"certifications/CKAD/08_Persistent-Volumes.html#kubernetes-setup","title":"Kubernetes Setup","text":"<ul> <li>With deployments, the order of pod deployment cannot be guaranteed as pods all come up at the same time, so one could not set something up for worker 1 to come up before worker 2.</li> <li>Another way is required to differentiate master vs worker. The master requires a static hostname / identifier.</li> <li>Statefulsets aim to support this workflow / architecture.</li> <li>Similar to deployments, they create pods based on a template and are scalable, but pods are created in a sequential order. Pod 1 must be running before Pod 2 kicks off, etc.</li> <li>StatefulSets assign a unique name to each pod, based on the name of the StatefulSet and a numeric index starting from 0. E.g. <code>redis-0</code>, <code>redis-1</code>, ...</li> <li>Each of the \"worker\" pods can be configured to point to the specific parameters associated with the master pod.</li> </ul>"},{"location":"certifications/CKAD/08_Persistent-Volumes.html#introduction","title":"Introduction","text":"<ul> <li>StatefulSets aren't always required, it really depends on the application.</li> <li>As discussed previously, this may depend on situations such as:</li> <li>Instances need to come up in a particular order</li> <li> <p>Instances need to have specific names</p> </li> <li> <p>StatefulSets can be created effectively in the same manner as a Deployment, with the value of <code>kind:</code> being replaced accordingly, and the name of the headless service to be associated with the statefulset to be added against the <code>serviceName</code> parameter.</p> </li> <li>An example for Redis follows:</li> </ul> <pre><code>apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis-cluster\nspec:\n  serviceName: redis-cluster\n  replicas: 6\n  selector:\n    matchLabels:\n      app: redis-cluster\n  template:\n    metadata:\n      labels:\n        app: redis-cluster\n    spec:\n      containers:\n      - name: redis\n        image: redis:5.0.1-alpine\n        ports:\n        - containerPort: 6379\n          name: client\n        - containerPort: 16379\n          name: gossip\n        command: [\"/conf/update-node.sh\", \"redis-server\", \"/conf/redis.conf\"]\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        volumeMounts:\n        - name: conf\n          mountPath: /conf\n          readOnly: false\n        - name: data\n          mountPath: /data\n          readOnly: false\n      volumes:\n      - name: conf\n        configMap:\n          name: redis-cluster\n          defaultMode: 0755\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: 1Gi\n</code></pre> <ul> <li>Upon creation, each pod is created sequentially and gets a unique DNS record.</li> <li>This is ordered, graceful deployment and scaling.</li> <li>The same behaviour follows during deletion.</li> <li>The behaviour can be overwritten by setting the <code>podManagement</code> parameter in the <code>spec</code> to <code>Parallel</code>, such that the pods are not created sequentially.</li> </ul>"},{"location":"certifications/CKAD/08_Persistent-Volumes.html#headless-services","title":"Headless Services","text":"<ul> <li>Each pod in a StatefulSet is deployed, unless specified otherwise, in a sequential manner and has a unique name for identification.</li> <li>In the case of Kubernetes, to allow interaction with these pods and inter-pod communication, a Service is required.</li> <li>In a standard deployment, one could deploy a load balancer service to distribute traffic across the master and the workers.</li> <li> <p>In StatefulSets, this doesn't work 100%, as only write actions should be allowed to the master node. How is this achieved?</p> </li> <li> <p>The master node can be reached directly via its IP address, and the workers via their DNS (determined via IP addresses) - these cannot be used as the IP addresses are always subject to change.</p> </li> <li>What is needed, is a service that does not load balance requests, but gives a DNS entry to reach each pod - this is achieved by a <code>Headless</code> service.</li> <li>Headless services are created like standard services, but do not have their own IPs like ClusterIPs or balance loads, it simply creates DNS entries for each pod via the pod name and a subdomain.</li> <li>The DNS record is of the form <code>&lt;pod name&gt;.&lt;headless service name&gt;.&lt;namespace&gt;.svc.&lt;cluster-domain&gt;.example</code></li> <li>Headless services can be created via a standard YAML definition file for services, but setting <code>clusterIP: None</code></li> </ul> <pre><code>---\napiVersion: v1\nkind: Service\nmetadata:\n  name: redis-cluster-service\nspec:\n  ports:\n    - port: 6379\n      name: client\n      targetPort: 6379\n    - port: 16379\n      name: gossip\n      targetPort: 16379\n  selector:\n    app: redis-cluster\n  clusterIP: None\n</code></pre> <ul> <li>For pod DNS entries to be created, two optional fields must be added to the pod definition files:</li> <li><code>subdomain:</code> - The name of the headless service</li> <li> <p><code>hostname:</code> - The name to be assigned in the DNS record for the given pod.</p> </li> <li> <p>By default, if these aren't set, the headless service will not create a DNS record for the pods in a deployment.</p> </li> <li>In standard deployments, this still wouldn't work as all pods would have the same hostname if these parameters are added, StatefulSets work around this by previously defined features and including <code>serviceName</code> in the spec.</li> </ul>"},{"location":"certifications/CKAD/08_Persistent-Volumes.html#storage-in-statefulsets","title":"Storage in StatefulSets","text":"<ul> <li>With StatefulSets, if a PVC and storage is already created, all pods in the StatefulSet will try to use the same PVC.</li> <li>This may not always be desired, for example, in the MySQL scenario, each instance should have its own storage media, but the same data - each pod therefore needs its own PVC and PV.</li> <li>This is achieved using a <code>PersistentVolumeClaim</code> template in the <code>StatefulSet</code>'s specification. An example for Redis follows:</li> </ul> <pre><code>...\ntemplate:\n    metadata:\n      labels:\n        app: redis-cluster\n    spec:\n      containers:\n      - name: redis\n        image: redis:5.0.1-alpine\n        ports:\n        - containerPort: 6379\n          name: client\n        - containerPort: 16379\n          name: gossip\n        volumeMounts:\n        - name: conf\n          mountPath: /conf\n          readOnly: false\n        - name: data\n          mountPath: /data\n          readOnly: false\n      volumes:\n      - name: conf\n        configMap:\n          name: redis-cluster\n          defaultMode: 0755\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: 1Gi\n</code></pre> <ul> <li>These claim templates can refer to any given Storage Class created, so for each pvc created per replica, the associated resources are created e.g. PVC, PV, etc.</li> <li>In the event of failure for these pods,the PVCs and associated resources are not deleted automatically, once the pod is back online, the same storage objects are re-attached, ensuring stateful data.</li> </ul>"},{"location":"certifications/CKAD/09_2021-Updates.html","title":"9.0 - 2021 Updates","text":"<ul> <li>As of September 28th 2021, some changes have been made to the CKAD exam objectives. Certain exam objectives have been expanded upon and altered. In particular:</li> <li>Application Design and Build:<ul> <li>Design, build, and modify container images</li> </ul> </li> <li>Application Deployment:<ul> <li>Use Kubernetes Primitives to implement common deployment strategies e.g. blue/green, canary, etc.</li> <li>Use the Helm package manager to deploy existing packages</li> </ul> </li> <li>Application Observability and Maintenance:<ul> <li>Understand API Deprecations</li> </ul> </li> <li> <p>Application Environment, Configuration, and Security:</p> <ul> <li>Discover and use resources that extend Kubernetes (CRD)</li> <li>Understand authentication, authorization and admission control</li> </ul> </li> <li> <p>Some of these are already talked about in CKA, however for completions sake we'll do them here too.</p> </li> </ul>"},{"location":"certifications/CKAD/09_2021-Updates.html#91-define-build-and-modify-container-images","title":"9.1 - Define, Build, and Modify Container Images","text":"<ul> <li>When containerising an application, the image would typically follow the steps used to run the application locally e.g.:</li> <li>Define the OS / Base Image</li> <li>Install dependencies</li> <li>Setup source code</li> <li> <p>Initialise the App</p> </li> <li> <p>An example follows for a sample Flask application:</p> </li> </ul> <pre><code>FROM ubuntu\n\nRUN apt-get update\nRUN apt-get install python\n\nRUN pip install flask\nRUN pip install flask-mysql\n\nCOPY . /opt/source-code\n\nENTRYPOINT FLASK_APP=/opt/source-code/app.py flask run\n</code></pre> <ul> <li>The image can then be built locally: <code>docker build Dockerfole -t &lt;dockerhub Username&gt;/&lt;image name&gt;</code></li> <li> <p>The image can then be pushed to Dockerhub <code>docker push &lt;dockerhub username&gt;/&lt;image name&gt;</code></p> </li> <li> <p>The instructions defined in a Dockerfile are viewed as layered architecture, each layer only stores the changes defined from the previous. This is viewed during the Docker build as steps e.g. step 3/5.</p> </li> <li> <p>Each layer is cached by Docker, in the event of step failure, it will continue from the last working step once the issue is fixed. This saves time when building images.</p> </li> </ul>"},{"location":"certifications/CKAD/09_2021-Updates.html#92-authentication-authorization-and-admission-control","title":"9.2 - Authentication, Authorization and Admission Control","text":"<ul> <li>Controlling access to API Server is the top priority</li> <li>Need to define who can access the API Server through and what they can do</li> <li>Could use any of:</li> <li>Files</li> <li>Certificates</li> <li>External authentication providers</li> <li>Service Accounts</li> <li>By default, all pods within a cluster can access one another</li> <li>This can be restricted via the introduction of network policies.</li> </ul>"},{"location":"certifications/CKAD/09_2021-Updates.html#93-authentication","title":"9.3 - Authentication","text":"<ul> <li>Authentication Mechanisms Available in Kubernetes for the API Server:</li> <li>Static Password File</li> <li>Static Token File</li> <li>Certificates</li> <li>Identity Services e.g. <code>LDAP</code></li> <li>Suppose you have the user details in a file, you can pass this file as an option for authentication in the kube-apiserver.service file adding the flag: <code>--basic-auth-file=user-details.csv</code></li> <li>Restart the service after this change is done</li> <li>If the cluster is setup using Kubeadm, edit the yaml file and add the same option</li> <li>Kubernetes will automatically update the apiserver once the change is made</li> <li>To authenticate using the credentials specified in this manner run a curl command similar to: <code>curl -v -k https://master-node-ip:6443/api/v1/pods -u \"username:password\"</code></li> <li>Could also have a token file, specify using <code>--token-auth-file=user-details.csv</code></li> <li>Note: These are not recommended authentication mechanisms</li> <li>Should consider a volume mount when providing the auth file in a kubeadm setup</li> <li>Setup RBAC for new users</li> <li>Could also setup RBAC using YAML files to create rolebindings for each user</li> </ul>"},{"location":"certifications/CKAD/09_2021-Updates.html#94-kubeconfig","title":"9.4 - KubeConfig","text":"<ul> <li>Files containing information for different cluster configurations, such as:</li> <li>-<code>-server</code></li> <li><code>--client-key</code></li> <li><code>--client-certificate</code></li> <li><code>--certificate-authority</code></li> <li>The existence of this file removes the need to specify the option in the CLI</li> <li>File located at <code>$HOME/.kube/config</code></li> <li>KubeConfig Files contain 3 sections:</li> <li>Clusters - Any cluster that the user has access to, local or cloud-based</li> <li>Users - User accounts that have access to the clusters defined in the previous section, each with their own privileges</li> <li>Contexts - A merging of clusters and users, they define which user account can access which cluster</li> <li>These config files do not involve creating new users, it's simply configuring what existing users, given their current privileges, can access what cluster</li> <li>Removes the need to specify the user certificates and server addresses in each kubectl command</li> <li><code>--server</code> spec listed under clusters</li> <li>User keys and certificates listed in Users section</li> <li>Context created to specify that the user <code>MyKubeAdmin</code> is the user that is used to access the cluster <code>MyKubeCluster</code></li> <li>Config file defined in YAML file</li> <li><code>ApiVersion</code> = v1</li> <li><code>Kind</code> = Config</li> <li><code>Spec</code> includes the three sections defined previously, all of which are arrays</li> <li>Under clusters: specify the cluster name, the certificate authority associated and the server address</li> <li>Under <code>users</code>, specify username and associated key(s) and certificate(s)</li> <li>Under <code>contexts</code>:<ul> <li>Name format: <code>username@clustername</code></li> <li>Under context specify cluster name and users</li> </ul> </li> <li>Repeat for all clusters and users associated</li> <li>The file is automatically read by the kubectl utility</li> <li>Use current-context field in the yaml file to set the current context</li> <li>CLI Commands:</li> <li>View current config file being used: <code>kubectl config view</code><ul> <li>Default file automatically used if not specified</li> <li>To view non-default config files, append: <code>--kubeconfig=/path/to/file</code></li> </ul> </li> <li>To update current context: <code>kubectl config use-context &lt;context-name&gt;</code></li> <li>Other commands available via <code>kubectl config -h</code></li> <li>Default namespaces for particular contexts can be added also</li> <li>Note: for certificates in the config file, use the full path to specify the location</li> <li>Alternatively use <code>certificate-authority-data</code> to list certificate in base64 format</li> </ul>"},{"location":"certifications/CKAD/09_2021-Updates.html#95-api-groups","title":"9.5 - API Groups","text":"<ul> <li>API Server accessible at master node IP address at port 6443</li> <li>To get the version, append <code>/version</code> to a curl request to the above IP address</li> <li>To get a list of pods, append <code>/api/v1/pods</code></li> <li>Kubernetes' A:PI is split into multiple groups depending on the group's purpose such as</li> <li><code>/api</code> - core functionalities e.g. pods, namespaces, secrets</li> <li><code>/version</code> - viewing the version of the cluster</li> <li><code>/metrics</code> - used for monitoring cluster health</li> <li><code>/logs</code> - for integration with 3rd-party logging applications</li> <li><code>/apis</code> - named functionalities added to kubernetes over time such as deployments, replicasets, extensions<ul> <li>Each group has a version, resources, and actions associated with them</li> </ul> </li> <li><code>/healthz</code> - used for monitoring cluster health</li> <li>Use <code>curl http://localhost:6443 -k</code> to view the api groups, then append the group and grep name to see the subgroups within</li> <li>Note: Need to provide certificates to access the api server or use <code>kubectl proxy</code> to view</li> <li>Note: <code>kubectl proxy</code> is not the same as kube proxy, the former is an http proxy service to access the api server</li> </ul>"},{"location":"certifications/CKAD/09_2021-Updates.html#96-authorization","title":"9.6 - Authorization","text":"<ul> <li>When adding users, need to ensure their access levels are sufficiently configured, so they cannot make any unwanted changes to the cluster</li> <li>This applies to any physical users, like developers, or virtual users like applications e.g. Jenkins</li> <li>Additional measures must be taken when sharing clusters with organizations or teams, so that they are restricted to their specific namespaces</li> <li>Authorization mechanisms available are:</li> <li>Node-based</li> <li>Attribute-Based</li> <li>Rule-Based</li> <li>WebHook-based</li> <li>Node-Based:</li> <li>Requests to the kube-apiserver via users and the kubelet are handled via the Node Authorizer</li> <li>Kubelets should be part of the system:nodes group</li> <li>Any requests coming from a user with the name system-node and is aprt of the system nodes group is authorized and granted access to the apiserver</li> <li>ABAC - Attribute-Based</li> <li>For users wanting to access the cluster, you should create a policy in a JSON format to determine what privileges the user gets, such as namespace access, resource management and access, etc</li> <li>Repeat for each users</li> <li>Each policy must be edited manually for changes to be made, the kube apiserver must be restarted to make the changes take effect</li> <li>RBAC</li> <li>Instead of associating each user with a set of permissions, can create a role which outlines a particular set of permissions</li> <li>Assign users to the role</li> <li>If any changes are to be made, it is just the role configuration that needs to be changed</li> <li>Webhook</li> <li>Use of third-party tools to help with authorization</li> <li>If any requests are made to say the APIserver, the third party can verify if the request is valid or not</li> <li>Note: Additional authorization methods are available:</li> <li><code>AlwaysAllow</code> - Allows all requests without checks</li> <li><code>AlwaysDeny</code> - Denies all requests without checks</li> <li>Authorizations set by <code>--authorization</code> option in the apiserver's .service or .yaml file</li> <li>Can set modes for multiple-phase authorization, use <code>--authorization-mode</code> and list the authorization methods</li> </ul>"},{"location":"certifications/CKAD/09_2021-Updates.html#97-role-based-access-control-rbac","title":"9.7 - Role-Based Access Control (RBAC)","text":"<ul> <li>To create a role, create a YAML file</li> <li>Spec replaced with <code>rules</code></li> <li>Covers apiGroups, resources and verbs</li> <li>Multiple rules added by - <code>apiGroups</code> for each</li> <li>Create the role using <code>kubectl create -f</code></li> <li>To link the user to the role, need to create a Role Binding</li> <li>Under <code>metadata</code>:</li> <li>Specify <code>subjects</code> - Users to be affected by the <code>rolebinding</code>, their associated apiGroup for authorization</li> <li><code>RoleRef</code> - The role to be linked to the subject</li> <li>To view roles: <code>kubectl get roles</code></li> <li>To view rolebindings: <code>kubectl get rolebindings</code></li> <li>To get additional details: <code>kubectl describe role/rolebinding &lt;name&gt;</code></li> <li>To check access level: <code>kubectl auth can-i &lt;command/activity&gt;</code></li> <li>To check if a particular user can do an activity, append <code>--as &lt;username&gt;</code></li> <li>To check if an activity can be done via a user in a particular namespace, append <code>--namespace &lt;namespace&gt;</code></li> <li>Note: Can restrict access to particular resources by adding resourceNames: <code>[\"resource1\", \"resource2\", ...]</code> to the role yaml file</li> </ul>"},{"location":"certifications/CKAD/09_2021-Updates.html#98-cluster-roles","title":"9.8 - Cluster Roles","text":"<ul> <li>Roles and role bindings are created for particular namespaces and control access to resources in that particular namespace</li> <li>By default, roles and role bindings are applied to the default namespace</li> <li>In general, resources such as pods, replicasets are namespaced</li> <li>Cluster-scoped resources are resources that cannot be associated to any particular namespace, such as:</li> <li><code>Persistentvolumes</code></li> <li><code>Nodes</code></li> <li>To switch view namespaced/cluster-scoped resources: <code>kubectl api-resources --namespaced=TRUE/FALSE</code></li> <li>To authorize users to cluster-scoped resources, use cluster-roles and cluster-rolebindings</li> <li>Could be used to configure node management across a cluster etc</li> <li>Cluster roles and role bindings are configured in the exact same manner as roles and rolebindings; the only difference is the kind</li> <li>Note: Cluster roles and rolebindings can be applied to namespaced resources if desired, the user will then have access to the resources across all namespaces.</li> </ul>"},{"location":"certifications/CKAD/09_2021-Updates.html#99-admission-controllers","title":"9.9 - Admission Controllers","text":"<ul> <li>Commands are typically ran using the kubectl utility, these commands are sent to the kubeapi server and applied accordingly</li> <li>To determine the validity of the command, it goes through an authentication process via certificates</li> <li>Any kubectl commands come from users with a kubeconfig file containing the certificates required</li> <li>The determination of whether the process has permission to carry the task out is handled by RBAC authorization<ul> <li>Kubernetes Roles are used to support this.</li> </ul> </li> <li>With RBAC, restrictions can be placed on resources for:</li> <li>Operation-wide restrictions</li> <li>Specific operation restrictions e.g. create pod of specific names</li> <li>Namespace-scoped restrictions</li> <li>What happens if you want to go beyond this? For example:</li> <li>Allow images from a specific registry</li> <li>Only run as a particular user</li> <li>Allow specific capabilities</li> <li>Constrain the metadata to include specific information</li> <li>The above is handled by Admission controllers</li> <li>Various pre-built admission controllers come with K8s:<ul> <li><code>AlwaysPullImages</code></li> <li><code>DefaultStorageClass</code></li> <li><code>EventRateLimit</code></li> <li><code>NamespaceExists</code></li> </ul> </li> <li>Example - <code>NamespaceExists</code>:</li> <li>If creating a pod in a namespace that doesn't exist:<ul> <li>The request is authenticated and authorized</li> <li>The request is then denied as the admission controller acknowledges that the namespace doesn't exist -&gt; Request is denied</li> </ul> </li> <li>To view admission controllers enabled by default: <code>kube-apiserver -h | grep enable-admission-plugins</code></li> <li> <p>Note: For kubeadm setups, this must be run from the kube-apiserver control plane using kubectl</p> </li> <li> <p>Admission controllers can either be added to the .service file or the .yaml manifest depending on the setup:</p> </li> </ul> <pre><code>ExecStart=/usr/local/bin/kube-apiserver \\\\\n  --advertise-address=${INTERNAL_IP} \\\\\n  --allow-privileged=true \\\\\n  --apiserver-count=3 \\\\\n  --authorization-mode=Node,RBAC \\\\\n  --bind-address=0.0.0.0 \\\\\n  --enable-swagger-ui=true \\\\\n  --etcd-servers=https://127.0.0.1:2379 \\\\\n  --event-ttl=1h \\\\\n  --runtime-configmap=api/all \\\\\n  --service-cluster-ip-range=10.32.0.0/24 \\\\\n  --service-node-port-range=30000-32767 \\\\\n  --v=2\n  --enable-admission-plugins=NodeRestriction,NamespaceAutoProvision\n</code></pre> <pre><code>## /etc/kubernetes/manifests/kube-apiserver.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  creationTimestamp: null\n  name: kube-apiserver\n  namespace: kube-system\nspec:\n  containers:\n  - command:\n    - kube-apiserver\n    - --authorization-mode=Node,RBAC\n    - --advertise-address=172.17.0.107\n    - --allow-privileged=true\n    - --enabke-admission-plugins=NodeRestriction\n    - --enable-bootstrap-token-auth=true\n    image: k8s.gcr.io/kube-apiserver-amd64:v1.11.3\n    name: kube-apiserver\n</code></pre> <ul> <li>To disable a plugin use <code>--disable-admission-plugins=&lt;plugin1&gt;,&lt;plugin2&gt;, ....</code></li> <li>Note: NamespaceAutoProvision is not enabled by default - it can be enabled by the above menthods</li> </ul>"},{"location":"certifications/CKAD/09_2021-Updates.html#310-validating-and-mutating-admission-controllers","title":"3.10 - Validating and Mutating Admission Controllers","text":"<ul> <li>Validating Admission Controller - Allows or Denies a request depending on the controllers functionality/conditions</li> <li>Example: <code>NamespaceExists</code></li> <li>Mutating Admission Controller: If an object is to be created and a required parameter isn't specified, the object is modified to use the default value prior to creation</li> <li>Example: <code>DefaultStorageClass</code></li> <li>Note: Certain admission controllers can do both mutation and validation operations</li> <li>Typically, mutation admission controllers are called first, followed by validation controllers.</li> <li>Many admission controllers come pre-packaged with Kubernetes, but could also want custom controllers:</li> <li>To support custom admission controllers, Kubernetes has 2 available for use:<ul> <li>MutatingAdmissionWebhook</li> <li>ValidatingAdmissionWebhook</li> </ul> </li> <li>Webhooks can be configured to point to servers internal or external to the cluster<ul> <li>Servers will have their own admission controller webhook services running the custom logic</li> <li>Once all the built-in controllers are managed, the webhook is hit to call to the webhook server by passing a JSON object regarding the request</li> <li>The admission webhook server then responds with an admissionreview object detailing the response</li> </ul> </li> <li>To set up, the admission webhook server must be setup, then the admission controller should be setup via a webhook configuration object</li> <li>The server can be deployed as an api server in any programming language desired e.g. Go, Python, the only requirement is that it must be able to accept and handle the requests<ul> <li>Can have a validate and mutate call</li> </ul> </li> <li>Note: For exam purposes, need to only understand the functionality of the webhook server, not the actual code</li> <li>The webook server can be ran in whatever manner desired e.g. a server, or a deployment in kubernetes</li> <li>Latter requires it to be exposed as a service for access</li> <li>The webhook configuration object then needs to be created (validating example follows):</li> <li>Each configuration object contains the following:</li> <li>Name - id for server</li> <li>Clientconfig - determines how the webhook server should be contacted - via URL or service name<ul> <li>Note: for service-based configuration, communication needs to be authenticated via a CA, so a caBundle needs to be provided</li> </ul> </li> <li>Rules:<ul> <li>Determines when the webhook server needs to be called i.e. for what sort of requests should invoke the call to the webhook server</li> <li>Attributes detailed include API Groups, namespaces, and resources.</li> </ul> </li> </ul> <pre><code>apiVersion: admissionregistration.k8s.io/v1\nkind: ValidatingWebhookConfiguration\nmetadata:\n  name: \"pod-policy.example.com\"\nwebhooks:\n- name: \"pod-policy.example.com\"\n  clientConfig:\n    service:\n      namespace: \"webhook-namespace\"\n      name: \"webhook-service\"\n    caBundle: \"Ci0tLS0tQk.......tLS0K\"\n  rules:\n  - apiGroups: [\"\"]\n    apiVersions: [\"v1\"]\n    operations: [\"CREATE\"]\n    resources: [\"pods\"]\n    scope: \"Namespaced\"\n</code></pre>"},{"location":"certifications/CKAD/09_2021-Updates.html#311-api-versions-deprecations","title":"3.11 - API Versions &amp; Deprecations","text":""},{"location":"certifications/CKAD/09_2021-Updates.html#api-versions","title":"API Versions","text":"<ul> <li>Each API group has its own version.</li> <li>At <code>/v1</code>, the API group is in it's \"General Availability\" or \"stable\" version. Other versions are possible, a summary follows:</li> </ul> Alpha Beta GA (Stable) Version Name <code>vXalphaY</code> <code>vXbetaY</code> <code>vX</code> Enabled No, enabled via flags Yes, by default Yes, by default Tests May lack E2E Tests E2E Tests Conformance Tests Reliability May have bugs May have minor bugs Highly Reliable Support No committment, may be dropped Commits defined to complete the feature and progress to GA Expected to be present in many future releases Audience Expert users interested in providing early feedback Users interested in beta testing and providing feedback All users <ul> <li>Note: An API Group can support multiple versions at the same time e.g. you could create the same Deployment using <code>apps/v1beta</code> and <code>apps/v1</code>, but only one can be the preferred version.</li> <li>The preferred version is the <code>apiVersion</code> that is queried via <code>kubectl</code> commands and converted to for storage in the ETCD server.</li> <li>A preferred version is listed when viewing the <code>APIGroup</code> under <code>preferredVersion</code>.</li> <li>There is no way to see the <code>storageVersion</code> easily, except for querying the <code>ETCD</code> database directly.</li> <li>APIGroups that are enabled / disabled can be controlled by the flags for the <code>kube-apiserver</code> via the <code>--runtime-config=&lt;api version&gt;</code> flag in a comma-separated list.</li> </ul>"},{"location":"certifications/CKAD/09_2021-Updates.html#api-deprecations","title":"API Deprecations","text":"<ul> <li>A single API group can support multiple versions at the same time, but some versions may need to be shelved / have support dropped.</li> <li>There are rules put in place by Kubernetes' community to manage this, the API Deprecation Policy, some rules to highlight follow:</li> </ul>"},{"location":"certifications/CKAD/09_2021-Updates.html#rule-1","title":"Rule 1","text":"<ul> <li>API Elements may only be removed by incrementing the version of the API group.</li> <li>In an example scenario, suppose for a given API Group you have 2 components released as part of <code>v1alpha1</code>, component B proved unusable / unnecessary etc and was deemed suitable for removal.</li> <li>Component B cannot just be removed from <code>v1alpha1</code>, it may only be removed by removing it from <code>v1alpha2</code>, the next incremental version.</li> <li>In this scenario, YAML files would need to be changed to the new version, but the new release would need to support both versions.</li> <li>The preferred version could still be set to <code>v1alpha2</code> only.</li> </ul>"},{"location":"certifications/CKAD/09_2021-Updates.html#rule-2","title":"Rule 2","text":"<ul> <li>API objects must be able to round-trip between API versions in a given release without information loss, with the exception of whole REST resources that do not exist in some versions.</li> <li>Suppose a new field was added to a component in <code>v1alpha2</code>, an equivalent field must be added to <code>v1alpha1</code> should the user convert back to the older API Version from the newer version.</li> </ul>"},{"location":"certifications/CKAD/09_2021-Updates.html#rule-4a","title":"Rule 4a","text":"<ul> <li>As development progresses towards GA, older versions such as <code>v1alpha1</code> will need to be dropped.</li> <li>Suppose <code>v1alpha1</code> was first included with version <code>X</code> of Kubernetes, then <code>v1alpha2</code> in <code>X+1</code>, etc, what happens to <code>v1alpha1</code>?</li> <li>In the alpha phase there is no requirement to maintain support for a past release. Similar rules apply for <code>Beta</code> and <code>GA</code>.</li> <li>Rule 4a: Other than the most recent API Versions in each track, older API versions must be supported after their announced deprecation for a duration of no less than:</li> <li>GA: 12 months or 3 releases (whichever is longer)</li> <li>Beta: 9 months or 3 releases (whichever is longer)</li> <li> <p>Alpha: 0 Releases</p> </li> <li> <p>These deprecations must be mentioned in changelogs for each version update.</p> </li> <li>Similarly to above, when API version <code>v1beta1</code> is released with Kubernetes <code>X+2</code>, there is no requirement to keep the <code>v1alpha2</code> version support.</li> <li><code>v1beta1</code> must then stay supported for 3 Kubernetes releases (with a note on deprecation) when <code>v1beta2</code> is released in <code>X+3</code>.</li> <li>The preferred or storage version cannot change until Kubernetes <code>X+4</code> This is due to Rule 4b.</li> </ul>"},{"location":"certifications/CKAD/09_2021-Updates.html#rule-4b","title":"Rule 4b","text":"<ul> <li>The preferred API version and the storage version for a given group may not advance until after a release has been made that supports both the new version and the previous version</li> </ul>"},{"location":"certifications/CKAD/09_2021-Updates.html#rule-3","title":"Rule 3","text":"<ul> <li>An API Version in a given track may not be deprecated until a new API Version at least as stable is released.</li> <li>This means that GA can deprecate another GA version and Beta versions, Beta for other betas and alpha versions, etc.</li> </ul>"},{"location":"certifications/CKAD/09_2021-Updates.html#kubectl-convert","title":"Kubectl Convert","text":"<ul> <li>When clusters are upgraded, this is often packed with APIversion changes. Managing these changes can be very tedious for large amounts of manifest files.</li> <li>This process can be expedited by the <code>kubectl convert</code> plugin. Once installed, run <code>kubectl convert -f &lt;old-file&gt; --output-version &lt;new api version&gt;</code></li> <li>Example: <code>kubectl convert -f &lt;deployment.yaml&gt; --output-version apps/v1</code></li> <li>The plugin may not be installed by default, however it can be installed via instructions in the Kubernetes documentation.</li> </ul>"},{"location":"certifications/CKAD/09_2021-Updates.html#912-custom-resource-definition","title":"9.12 - Custom Resource Definition","text":"<ul> <li>Whenever an object is created in Kubernetes, information regarding it is stored in the ETCD database.</li> <li>The ETCD database can then be queried for the resource's information via the <code>kubectl</code> commands.</li> <li> <p>When it comes to actually creating and configuring the object, a <code>Controller</code> handles this responsibility.</p> </li> <li> <p>Controllers do not have to be created, some, like the DeploymentController, come pre-packaged with Kubernetes.</p> </li> <li> <p>They continuously monitor the ETCD database for changes to information for the resources they manage, and ensure that the changes are reflected in the cluster by applying or removing the configuration desired.</p> </li> <li> <p>Essentially all Kubernetes resources have an associated controller to support this process.</p> </li> <li>For new kinds of resources being created, custom controllers and custom resource definitions are required. You cannot create a new kind of resource without some form of definition for it in the Kubernetes API.</li> </ul>"},{"location":"certifications/CKAD/09_2021-Updates.html#custom-resource-definition","title":"Custom Resource Definition","text":"<pre><code>apiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  name: &lt;CRD Name&gt;\nspec:\n  scope: Namespaced # or not!\n  groups: &lt;api group to be used in resource&gt;\n  names:\n    kind: &lt;kind to be used&gt;\n    singular: &lt;CRD Singular Name&gt; # resource name to be called via `kubectl` commands\n    plural: &lt;CRD Plural Name&gt; # display name when `kubectl api-resources` is ran - plural of the singular\n    shortName:\n    - &lt;CRD shorthand&gt; # Like how deployments has deploy as shorthand\n  versions: # list each version supported\n  - name: v1\n    served: true\n    storage: true\n    schema: # the expected fields for the object's spec\n      openAPIV3Schema:\n        type: object\n        properties:\n          spec:\n            type: object\n            properties:\n              &lt;property1&gt;:\n                type: &lt;type&gt;\n                &lt;type parameter 1&gt;: &lt;value&gt;\n                &lt;type parameter 2&gt;: &lt;value&gt;\n                ...\n              &lt;property2&gt;:\n                type: &lt;type&gt;\n                &lt;type parameter 1&gt;: &lt;value&gt;\n                &lt;type parameter 2&gt;: &lt;value&gt;\n                ...\n              ...\n</code></pre> <ul> <li>Once the CRD is defined, it can be created via <code>kubectl create</code> commands as standard. For creating resources based off this CRD, a supporting controller must also be defined.</li> </ul>"},{"location":"certifications/CKAD/09_2021-Updates.html#913-custom-controllers","title":"9.13 - Custom Controllers","text":"<ul> <li>Controllers are any process or code that continuously monitors clusters for events associated with a specific type of Kubernetes objects, and can respond accordingly to said events to ensure the desired state determined by the ETCD database is maintained in the cluster.</li> <li>Kubernetes provides a base sample-controller repository to start off with.</li> <li>Most controllers are written in <code>Go</code>, so <code>controller.go</code> (or appropriate) is edited accordingly with the desired logic.</li> <li>The controller can be built by <code>go build -o &lt;controller name&gt; .</code></li> <li> <p>The controller can then be ran and pointed to the desired kubeconfig: <code>./&lt;go executable&gt; -kubeconfig=/path/to/kubeconfig</code></p> </li> <li> <p>You may also package the controller as a Go Docker image and run it within the Kubernetes cluster as a pod.</p> </li> <li>Expected questions in the exam may include defining a custom resource definition and WORKING WITH custom controllers.</li> </ul>"},{"location":"certifications/CKAD/09_2021-Updates.html#914-operator-framework","title":"9.14 - Operator Framework","text":"<ul> <li>CRD and Custom Controllers, up until now, have been separate entities created manually. However, they can be packaged together and deployed via the Operator Framework.</li> <li>One of the most common examples of the Operator Framework being utilised is ETCD - via the operator framework, a series of CRDs and Controllers are available:</li> </ul> CRD Custom Controller ETCDCluster ETCD Controller ETCDBackup Backup Operator ETCDRestpre Restore Operator <ul> <li>Operators often support additional tasks typically carried out via users e.g. Backup and Restore, as outline above.</li> <li>Operators are also available at OperatorHub.io. Many common apps and tools are available via this, such as Grafana, Istio, ArgoCD, etc.</li> <li>Each operator can be viewed individually for specific details on installation, etc.</li> <li>It is expected to require awareness regarding operators, CRDs are the more likely candidate for any exam questions.</li> </ul>"},{"location":"certifications/CKAD/09_2021-Updates.html#915-deployment-strategies","title":"9.15 - Deployment Strategies","text":"<ul> <li>Previously, the <code>recreate</code> and <code>rolling update</code> deployment strategies were considered.</li> <li><code>Recreate</code> poses problems as there is a period of time where the app cannot be accessed, as all replicas are torn down before the new versions are spun up,</li> <li><code>Rolling Update</code> is the default approach that mitigates this.</li> </ul>"},{"location":"certifications/CKAD/09_2021-Updates.html#9151-blue-green","title":"9.15.1 - Blue-Green","text":"<ul> <li>In this scenario, two sets of the application are deployed, one typically receives all the traffic at any given point.</li> <li>The version not receiving traffic is used as a staging environment of sorts to test out new changes to the application.</li> <li>When the new version is ready, all traffic is routed to the new version instead of the old.</li> <li> <p>The old version is then updated to the newer version, and the process repeats.</p> </li> <li> <p>This is commonly seen in conjunction with Service Mesh tools like Istio, but it can be achieved by Kubernetes alone.</p> </li> <li> <p>In Kubernetes, one can create two deployments, each labelled uniquely e.g. <code>version: v1</code>, <code>version: v2</code>.</p> </li> <li>One can create a service that filters traffic by sharing the selector <code>version: v1</code> to the one deployment.</li> <li>When the time comes for switchover, the selector for the service can be updated to match that of the \"newer\" deployment.</li> </ul>"},{"location":"certifications/CKAD/09_2021-Updates.html#9152-canary","title":"9.15.2 - Canary","text":"<ul> <li>In Canary deployments, the new version of the application is deployed alongside the old.</li> <li>A small percentage of traffic originally being routed to the old application is routed to the new one.</li> <li>This allows initial functionality tests to be ran.</li> <li> <p>If all looks good, the remaining application instances are upgraded, and the initial test instance is destroyed.</p> </li> <li> <p>In Kubernetes, this is achieved by having an initial deployment and a service, typically it will be labelled accordingly.</p> </li> <li>The \"Canary\" will be created as another deployment, this should be labelled accordingly to indicate the differing versions.</li> <li>When both of the deployments, traffic needs to be be routed to both of the versions, with a small percentage to the newer version.</li> <li>This can be achieved first by assigning a common label to the two deployments and updating the service's selector accordingly.</li> <li> <p>The actions above will result in traffic being distributed evenly, to make it a more canary deployment, simply reduce the amount of the pods on the secondary deployment to the minimum amount desired e.g. 5 primary, 1 canary.</p> </li> <li> <p>A caveat of this method is that there is limited control over how the traffic is split between the deployments.</p> </li> <li>Traffic split is solely determined by pod numbers as far as Kubernetes is concerned.</li> <li>Service Mesh tools like Istio do not view this as the case, and can fine-tune traffic splits via other methods.</li> </ul>"},{"location":"certifications/CKAD/09_2021-Updates.html#916-helm","title":"9.16 - Helm","text":""},{"location":"certifications/CKAD/09_2021-Updates.html#9161-helm-introduction","title":"9.16.1 - Helm Introduction","text":"<ul> <li>Kubernetes is a fantastic tool for managing complex infrastructure and container orchestration.</li> <li>However, applications can get complex very quickly, such as requiring deployments, storage, secrets, and services.</li> <li>This can become difficult to maintain, Helm aims to ease this difficulty by packaging up the YAML files required for the application to be deployed all at once.</li> <li>These pacakages can then be updated, versioned, and rolled back as required. Similar to how deployments work, but for Kubernetes applications as a whole.</li> </ul>"},{"location":"certifications/CKAD/09_2021-Updates.html#sample-commands","title":"Sample Commands","text":"<ul> <li>Install/Deploy a chart: <code>helm install &lt;chart name&gt; ...</code></li> <li>Upgrade a chart deployment: <code>helm upgrade &lt;chart name&gt; ...</code></li> <li>Rollback a chart deployment: <code>helm rollback &lt;chart name&gt; ...</code></li> <li>Uninstall a chart deployment: <code>helm uninstall &lt;chart name&gt; ...</code></li> </ul>"},{"location":"certifications/CKAD/09_2021-Updates.html#9162-install-helm","title":"9.16.2 - Install Helm","text":"<ul> <li>Helm installation requires a functional kubernetes cluster with <code>kubectl</code> enabled.</li> <li>Instructions are provided in the Helm documentation.</li> </ul>"},{"location":"certifications/CKAD/09_2021-Updates.html#9163-helm-concepts","title":"9.16.3 - Helm Concepts","text":"<ul> <li>Helm charts are comprised of the collective YAML definitions required for the separate Kubernetes resources.</li> <li>It's not a case of JUST merge the YAML manifests together, considerations must be made, in particular, for values that are subject to change e.g.:</li> <li>Passwords</li> <li>Secret values</li> <li>Storage parameters</li> <li>Image versions</li> <li>The YAML manifests can be converted to Helm templates by replacing the values as Jinja variables i.e. <code>{{ .Values.&lt;variable name&gt; }}</code></li> <li>The values themselves are stored in a <code>values.yaml</code> file e.g.:</li> </ul> <pre><code>variable1: value1\nvariable2: value2\n...\n</code></pre> <ul> <li>The combination of templates and values files define a chart.</li> <li>Additionally, one includes a <code>chart.yaml</code> file, which provides background information regarding the chart, such as name, versions, keyword tags, source code repository links, etc.</li> </ul>"},{"location":"certifications/CKAD/09_2021-Updates.html#helm-repositories","title":"Helm Repositories","text":"<ul> <li>Helm charts can be uploaded and viewed at artifacthub.io.</li> <li>One can search for Helm chart repositories via ArtifactHub or via <code>helm search hub &lt;chart name&gt;</code></li> <li>The repository(ies) desired can be addded via <code>helm repo add &lt;repo name&gt; &lt;repo url&gt;</code></li> <li>The repo can then be searched for versions via <code>helm search repo &lt;repo name&gt;</code></li> </ul>"},{"location":"certifications/CKAD/09_2021-Updates.html#installing-charts","title":"Installing Charts","text":"<ul> <li>Versions of charts can be installed as releases via <code>helm install &lt;release name&gt; &lt;chart name&gt;</code></li> <li>The value for <code>&lt;Release name&gt;</code> is defined by the user at command execution, the same version of a chart can be installed multiple times under different release names.</li> </ul>"},{"location":"certifications/CKAD/09_2021-Updates.html#additional-commands","title":"Additional Commands","text":"<ul> <li>List installed charts: <code>helm list</code></li> <li>Uninstall a release: <code>helm uninstall &lt;release name&gt;</code></li> <li>Download a chart: <code>helm pull --untar &lt;chart name&gt;</code></li> <li>The <code>--untar</code> is included as charts are packaged in tarballs.</li> <li>The chart can then be installed in a similar manner to the previous command, just replace chart name with the path to the downloaded chart tar.</li> </ul>"},{"location":"certifications/CKAD/13_Mock-Exams/01_Mock-Exam-1.html","title":"13.1 - Mock Exam 1","text":""},{"location":"certifications/CKAD/13_Mock-Exams/02_Mock-Exam-2.html","title":"13.2 - Mock Exam 2","text":""},{"location":"certifications/CKAD/13_Mock-Exams/03_Mock-Exam-3.html","title":"13.3 - Mock Exam 3","text":""},{"location":"certifications/CKS/01_Introduction.html","title":"1.0 - Introduction","text":""},{"location":"certifications/CKS/01_Introduction.html#11-introduction","title":"1.1 - Introduction","text":"<p>Kodekloud now a certified CNCF training partner</p> <ul> <li>Course structure:</li> <li>Lectures</li> <li>Demos</li> <li>Quizzes</li> <li>Hands-on-labs</li> <li>Slack Channel available</li> <li>Q&amp;A available</li> <li>Prerequisites:</li> <li>CKA Certification - Must be done as relies on many topics covered.</li> <li>Docker</li> <li>General DevOps and Linux knowledge</li> <li>Course Objectives</li> <li>Aligned with exam objectives</li> <li>Course structure not directly corresponding to CKS curriculum in terms of     order, but all content is included.</li> <li>Mock Exams are \"as close to the real exams as possible\" and can be attempted as many times as desired.</li> </ul>"},{"location":"certifications/CKS/02_The-Kubernetes-Attack-Surface.html","title":"2.0 - Understanding the Kubernetes Attack Surface","text":""},{"location":"certifications/CKS/02_The-Kubernetes-Attack-Surface.html#21-the-attack","title":"2.1 - The Attack","text":""},{"location":"certifications/CKS/02_The-Kubernetes-Attack-Surface.html#demonstration-of-attacks-on-a-kubernetes-environments","title":"Demonstration of attacks on a Kubernetes environments","text":"<ul> <li>Consider a Kubernetes application set running online, with two applications running in a domain each</li> <li>Suppose someone wanted to mess with these applications, the only initial information they have is the domain names, the architecture remains unknown</li> <li>Determining the IP addresses: <code>ping &lt;domain name&gt;</code></li> <li>The same IP address is returned for each, implying they are on the same server/infrastructure set</li> <li>Port scan of the IP address: <code>zsh port-scan.sh &lt;IP address&gt;</code></li> <li>All bar one port has a successful connection, Docker, implying that the applications are container-based</li> <li>Knowing the docker port is available, the next consideration is whether any authentication and authorization measures are in place within Docker</li> <li>This is not enabled by default, IT MUST BE PRE-CONFIGURED.</li> <li>Testing the waters for the Docker infrastructure:</li> <li><code>docker ps -H &lt;domain name&gt;</code><ul> <li>The -H flag is used to specify the host, if not specified it will default to localhost</li> </ul> </li> <li>As no Docker authentication and authorization measures are in place, this shows all the containers within the infrastructure</li> <li>Docker version can also be identified by running <code>docker -H &lt;domain name&gt; version</code></li> <li>To access the environment and other containers:</li> <li><code>docker -H &lt;domain name&gt; run --privileged -it ubuntu bash</code></li> <li>Runs a basic ubuntu container with escalated privileges within the infrastructure, allowing ssh'ing into other containers within the infrastructure</li> <li>Suppose the hacker already has a script capable of exploiting the infrastructure's vulnerability(ies), they should be able to download it to their privileged container without issue:</li> <li><code>Curl not found</code></li> <li><code>Wget not found</code></li> <li><code>Apt-get install &lt;curl&gt;</code></li> <li><code>Apt-get update</code></li> <li>Since the authentication isn't in place, the packages can be successfully installed and the hacker can run their script(s) to infiltrate the underlying infrastructure and mess around with the other applications, or learn additional information about the system e.g.:<ul> <li>Volume mounts: <code>df -h</code></li> <li>Username currently on system: <code>uname</code></li> <li>Host name: <code>hostname</code></li> <li>Additional containers: <code>sudo docker ps</code></li> </ul> </li> <li>Running these commands allows identification of a Kubernetes workload running in a container with Kubernetes-dashboard</li> <li>The Kubernetes dashboard must be running on a port somewhere, run <code>sudo iptables -L -t nat | grep kubernetes-dashboard</code></li> <li>Shows dashboard running on 30080</li> <li>Dashboard can be accessed and viewed easily if there's no authentication and security controls setup</li> <li>The dashboard provides information about pretty much everything within your Kubernetes cluster relating to storage, workloads, etc</li> <li>Using this, information can be identified for the database container such as the authentication parameters, in this case they're listed as environment variables</li> <li>From here the database container can be accessed and manipulated to the hacker's desire</li> <li>ALL OF THIS can be resolved via implementation of Kubernetes security measures discussed within this course.</li> </ul>"},{"location":"certifications/CKS/02_The-Kubernetes-Attack-Surface.html#22-the-4cs-of-cloud-native-security","title":"2.2 - The 4C's of Cloud Native Security","text":"<ul> <li>The 4C's of Cloud Native Security were all exploited in the previous demo:</li> <li>Cloud:<ul> <li>The infrastructure hosting the kubernetes cluster</li> <li>Not properly secured, allowed access to the pods on the cluster</li> <li>Could have been resolved by introducing network firewalls</li> </ul> </li> <li>Cluster<ul> <li>Relates to security via the Docker Daemon, Kubernetes API, etc</li> <li>Relates to security aspects such as:</li> <li>Authentication</li> <li>Authorization</li> <li>Admission</li> <li>Network Policy</li> </ul> </li> <li>Container<ul> <li>Relating to security at a container-level, restrictions can be put in place to secure containers from particular repos, user privileges, etc.</li> <li>Aspects covered include:</li> <li>Image restriction - Only able to run images from a particular repository</li> <li>Supply Chain</li> <li>Sandboxing</li> <li>Privileged - Certain activities should require escalated privileges</li> </ul> </li> <li>Code<ul> <li>The application code itself</li> <li>Sensitive data should not be exposed directly in the code</li> <li>Not covered directly in the course, though some practices such as utilising environment variables, key vaults such as Azure and HashiCorp vault, are touched on.</li> </ul> </li> </ul>"},{"location":"certifications/CKS/03_Cluster-Setup-and-Hardening.html","title":"3.0 - Cluster Setup and Hardening","text":""},{"location":"certifications/CKS/03_Cluster-Setup-and-Hardening.html#31-cis-benchmarks","title":"3.1 - CIS Benchmarks","text":"<ul> <li>Security Benchmark - Predefined standards and best practices that should be implemented for server (or other appropriate device) security.</li> <li>Areas of consideration include:</li> <li>Physical device configuration and limitation - USB ports that aren't expected to be used frequently / at all must have their access managed appropriately</li> <li>Access configurations - What user accounts need to be configured? Can users log in as root?<ul> <li>Recommended that root is disabled by default and admins use sudo where required</li> <li>Leads into only certain users having access to sudo, amongst other configurations</li> </ul> </li> <li>Network configuration<ul> <li>Firewall &amp; IP Tables</li> <li>Port Security</li> </ul> </li> <li>Service configuration<ul> <li>Ensure only certain services are allowed</li> </ul> </li> <li>Filesystem Configuration<ul> <li>All required permissions are set to the desired files</li> </ul> </li> <li>Auditing<ul> <li>Make sure all changes are logged for auditing purposes</li> </ul> </li> <li>Logging</li> <li>CIS - Centre for Internet Security</li> <li>Commonly used tool to check if security best practices are implemented</li> <li>Available on Linux, Windows, Cloud platforms, Mobile and many other platforms as well as Kubernetes.</li> <li>Can be downloaded from <code>https://www.cisecurity.org/cis-benchmarks/</code></li> <li>Guides come with predefined instructions for your associated platform(s) best practices and how to implement them (commands included)</li> <li>CIS Provide tools such as the CIS-CAT tool to automate the assessment of best practices implementation</li> <li>If any best practices aren't implemented, they are logged in the resultant HTML output report in a detailed manner.</li> </ul>"},{"location":"certifications/CKS/03_Cluster-Setup-and-Hardening.html#lab-run-cis-benchmark-assessment-tool-on-ubuntu","title":"Lab - Run CIS Benchmark Assessment Tool on Ubuntu","text":"<p>Q1: What is full form of CIS? A: Center for Internet Security</p> <p>Q2: What is not a function of CIS? A: Monitor Global Internet Traffic</p> <p>Q3: We have installed the CIS-CAT Pro Assessor tool called Assessor-CLI, under /root.</p> <p>Please run the assessment with the Assessor-CLI.sh script inside Assessor-CLI directory and generate a report called index.html in the output directory <code>/var/www/html/</code>.</p> <p>Once done, the report can be viewed using the Assessment Report tab located above the terminal.</p> <p>Run the test in interactive mode and use below settings:</p> <p>Benchmarks/Data-Stream Collections: : CIS Ubuntu Linux 18.04 LTS Benchmark v2.1.0 Profile : Level 1 - Server</p> <p>A:</p> <ul> <li>Run Assessor-CLI.sh</li> <li>Note options:</li> <li><code>-i (interactive)</code></li> <li><code>--rd &lt;reports dir&gt;</code></li> <li><code>--rp &lt;report prefix&gt;</code></li> <li>Run Assessor-CLI.sh with options for <code>/var/www/html/</code> and index.html respectively (and <code>--nts</code>) and <code>-i</code></li> <li>Note: Run options in order that they are displayed</li> <li>Apply conditions for benchmark setting and profiles</li> </ul> <p>Q4: How many tests failed for 1.1.1 Disable unused filesystems?</p> <p>A: View report via tab and note - 6</p> <p>Q5: How many tests passed for 2.1 Special Purpose Services?</p> <p>A: ditto - 18</p> <p>Q6: What parameters should we set to fix the failed test 5.3.10 Ensure SSH root login is disabled?</p> <p>A: PermitRootLogin no</p> <p>Q7: Fix the failed test - 1.7.6 Ensure local login warning banner is configured properly?</p> <p>A: Find in CIS Report, run the associated command</p> <p>Q8: Fix the failed test - 4.2.1.1 Ensure rsyslog is installed</p> <p>A: Find area in report and run command</p> <p>Q9: Fix the failed test - 5.1.2 Ensure permissions on /etc/crontab are configured</p> <p>A: Find area in report and run command</p> <p>Q10: In the previous questions we fixed the below 3 failed tests. Now run CIS-CAT tool test again and verify that all the below tests pass.</p> <ul> <li>1.7.6 Ensure local login warning banner is configured properly</li> <li>4.2.1.1 Ensure rsyslog is installed</li> <li>5.1.2 Ensure permissions on /etc/crontab are configured</li> </ul> <p>Run below command again to confirm that tests are passing now <code>sh ./Assessor-CLI.sh -i -rd /var/www/html/ -nts -rp index</code></p> <p>Use below setting while running tests Benchmarks/Data-Stream Collections: : CIS Ubuntu Linux 18.04 LTS Benchmark v2.1.0 Profile : Level 1 - Server A: Copy command and check associated tests pass - revisit questions if failed</p>"},{"location":"certifications/CKS/03_Cluster-Setup-and-Hardening.html#32-cis-benchmarks-in-kubernetes","title":"3.2 - CIS Benchmarks in Kubernetes","text":"<ul> <li>Kubernetes CIS benchmarks are readily available for download</li> <li>At the time of writing, the best practices are defined for versions 1.16-1.18</li> <li>The benchmarks are specifically aimed at system and application administrators, security specialists, auditors, help desk and platform deployment personnel who plan to develop, deploy, assess or secure solutions involving Kubernetes.</li> <li>Covers various security practices for master/control plane and worker nodes, such as permissions and configurations for API Server pods.</li> <li>Additional details provided to check recommended settings are in place and how to rectify any errors.</li> <li>CIS CAT Lite only supports certain OS's and benchmarks like Windows, Ubuntu, Kubernetes not included.</li> <li>CIS-CAT Pro includes Kubernetes support.</li> <li>CIS Benchmarks Link: https://www.cisecurity.org/cis-benchmarks/#kubernetes</li> <li>CIS CAT Tool: https://www.cisecurity.org/cybersecurity-tools/cis-cat-pro/cis-benchmarks-supported-by-cis-cat-pro/</li> </ul>"},{"location":"certifications/CKS/03_Cluster-Setup-and-Hardening.html#33-kube-bench","title":"3.3 - Kube-Bench","text":"<ul> <li>An open-source tool from Acqua security that can automate assessment of Kubernetes deployments in line with best practices</li> <li>The assessment occurs against the CIS Benchmarks</li> <li>To get started can either:</li> <li>Deploy via Docker Container</li> <li>Deploy as a pod in Kubernetes</li> <li>Install the associated binaries</li> <li>Compile from source</li> </ul>"},{"location":"certifications/CKS/03_Cluster-Setup-and-Hardening.html#lab-kube-bench","title":"Lab - Kube-Bench","text":"<p>Q1: Kube-Bench is a product of which company? A: Aqua Security</p> <p>Q2: What should Kube-Bench be used for? A: To check whether Kubernetes is deployed in line with best practices for security</p> <p>Q3: Install Kube Bench in /root - version 0.4.0, Download file: kube-bench_0.4.0_linux_amd64.tar.gz</p> <p>A: <code>curl -L https://github.com/aquasecurity/kube-bench/releases/download/v0.4.0/kube-bench_0.4.0_l inux_amd64.tar.gz -o kube-bench_0.4.0_linux_amd64.tar.gz tar -xvf kube-bench_0.4.0_linux_amd64.tar.gz</code></p> <p>Q4: Run a kube-bench test now and see the results Run below command to run kube bench</p> <p><code>./kube-bench --config-dir &lt;pwd&gt;/cfg --config &lt;pwd&gt;/cfg/config.yaml</code></p> <p>A: Follow instructions</p> <p>Q5: How many tests passed forEtcd Node Configuration?</p> <p>A: Review and report back - Section 2 and 7 pass</p> <p>Q6: How many tests failed for Control Plane Configuration? A: Review and report back - Section 3 and 0 failed</p> <p>Q7: Fix this failed test 1.3.1 Ensure that the <code>--terminated-pod-gc-threshold</code> argument is set as appropriate Follow exact commands given in Remediation of given test</p> <p>A: Find section in output and follow command</p> <p>Edit the Controller Manager pod specification file <code>/etc/kubernetes/manifests/kube-controller-manager.yaml</code> on the master node and set the <code>--terminated-pod-gc-threshold</code> to an appropriate threshold, for example: <code>--terminated-pod-gc-threshold=10</code></p> <p>Q8: Fix this failed test 1.3.6 Ensure that the RotateKubeletServerCertificate argument is set to true Follow exact commands given in Remediation of given test</p> <p>A: Find section and follow instructions: Edit the Controller Manager pod specification file <code>/etc/kubernetes/manifests/kube-controller-manager.yaml</code> on the master node and set the <code>--feature-gates</code> parameter to include RotateKubeletServerCertificate=true.</p> <p><code>--feature-gates=RotateKubeletServerCertificate=true</code></p> <p>Q9: Fix this failed test 1.4.1: Ensure that the --profiling argument is set to false</p> <p>A: Follow exact commands given in Remediation of given test</p> <p>Q10: Run the kube-bench test again and ensure that all tests for the fixes we implemented now pass</p> <ul> <li>1.3.1 Ensure that the --terminated-pod-gc-threshold argument is set as appropriate</li> <li>1.3.6 Ensure that the RotateKubeletServerCertificate argument is set to true</li> <li>1.4.1: Ensure that the --profiling argument is set to false</li> </ul> <p>A: Run command - ./kube-bench --config-dir <code>pwd</code>/cfg --config <code>pwd</code>/cfg/config.yaml</p> <p>And verify</p>"},{"location":"certifications/CKS/03_Cluster-Setup-and-Hardening.html#34-kubernetes-security-primitives","title":"3.4 - Kubernetes Security Primitives","text":"<ul> <li>Securing the hosts can be handled via methods such as disabling password authentication and allowing only SSH Key authentication</li> <li>Controlling access to API Server is the top priority - All Kubernetes operations depend upon this.</li> <li>Need to define:</li> <li>Who can access the API Server?</li> <li>What can they do with the API Server?</li> <li>For access, could use any of:</li> <li>Files</li> <li>Certificates</li> <li>External authentication providers</li> <li>Service Accounts</li> <li>For Authorization:</li> <li>RBAC - Role-based access control</li> <li>ABAC - Attribute-based access control</li> <li>Node Authorization</li> <li>By default, all pods within a cluster can access one another</li> <li>This can be restricted via the introduction of network policies</li> </ul>"},{"location":"certifications/CKS/03_Cluster-Setup-and-Hardening.html#35-authentication","title":"3.5 - Authentication","text":"<ul> <li>In general, there are two types of accounts available:</li> <li>User accounts - Admins, developers, etc</li> <li>Service accounts - machine-based accounts</li> <li>All access to Kubernetes is handled via the Kube-API Server</li> <li>Authentication Mechanisms Available in Kubernetes for the API Server:</li> <li>Static Password File - Not recommended</li> <li>Static Token File</li> <li>Certificates</li> <li>Identity Services (3rd Party services e.g. LDAT)</li> <li>Suppose you have the user details in a file, you can pass this file as an option for authentication in the kube-apiserver.service file adding the flag: <code>--basic-auth-file=user-details.csv</code></li> </ul> <pre><code>ExecStart=/usr/local/bin/kube-apiserver \\\\\n  --advertise-address=${INTERNAL_IP} \\\\\n  --allow-privileged=true \\\\\n  --apiserver-count=3 \\\\\n  --authorization-mode=Node,RBAC \\\\\n  --bind-address=0.0.0.0 \\\\\n  --enable-swagger-ui=true \\\\\n  --etcd-servers=https://127.0.0.1:2379 \\\\\n  --event-ttl=1h \\\\\n  --runtime-configmap=api/all \\\\\n  --service-cluster-ip-range=10.32.0.0/24 \\\\\n  --service-node-port-range=30000-32767 \\\\\n  --v=2\n</code></pre> <ul> <li>Restart the service / the server after the change is done.</li> <li>If the cluster is setup using Kubeadm, edit the yaml file and add the same options in a similar manner to:</li> </ul> <pre><code>## /etc/kubernetes/manifests/kube-apiserver.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  creationTimestamp: null\n  name: kube-apiserver\n  namespace: kube-system\nspec:\n  containers:\n  - command:\n    - kube-apiserver\n    - --authorization-mode=Node,RBAC\n    - --advertise-address=172.17.0.107\n    - --allow-privileged=true\n    - --enabke-admission-plugins=NodeRestriction\n    - --enable-bootstrap-token-auth=true\n    image: k8s.gcr.io/kube-apiserver-amd64:v1.11.3\n    name: kube-apiserver\n</code></pre> <ul> <li>Kubernetes will automatically update the apiserver once the change is made</li> <li>To authenticate using the credentials specified in this manner run a curl command similar to: <code>curl -v -k https://master-node-ip:6443/api/v1/pods -u \"username:password\"</code></li> <li>Could also have a token file, specify using --token-auth-file=user-details.csv in the api server .service or yaml file as appropriate</li> <li>Note: the token can also be included in the curl request via --header <code>\"Authorization: Bearer &lt;TOKEN&gt;\"</code></li> <li>Note: These are not recommended authentication mechanisms</li> <li>Should consider a volume mount when providing the auth file in a kubeadm setup</li> <li>Setup RBAC for new users</li> <li>Could also setup RBAC using YAML files to create rolebindings for each user</li> </ul>"},{"location":"certifications/CKS/03_Cluster-Setup-and-Hardening.html#36-service-accounts","title":"3.6 - Service Accounts","text":"<ul> <li>In Kubernetes there are two types of accounts:</li> <li>User - Used by Humans e.g. Administrators</li> <li>Service - Used by application services for various tasks e.g. Monitoring, CI/CD tools like Jenkins</li> <li>For an application to query the Kubernetes API, a service account is required</li> <li>Creation: <code>kubectl create serviceaccount &lt;serviceaccount name&gt;</code></li> <li>View: kubectl get serviceaccount</li> <li>For detailed information: <code>kubectl describe serviceaccount &lt;name&gt;</code></li> <li>When a serviceaccount is created, an authentication token is automatically created for it and stored in a Kubernetes secret.</li> <li>Serviceaccount token used for application authentication to the kube-api server</li> <li>The secret can be viewed via <code>kubectl describe secret &lt;secret name&gt;</code></li> <li>This token can then be passed as an Authorization Bearer token when making a curl request.</li> <li>E.g <code>curl https://&lt;IP-ADDRESS&gt; -insecure --header \"Authorization: Bearer &lt;token&gt;\"</code></li> <li>For 3rd party applications hosted on your own Kubernetes cluster:</li> <li>The exporting of the service account token is not required</li> <li>The service account secret can be mounted as a volume inside the pod hosting the application</li> <li>For every namespace, a default service account and its token are automatically assigned to a pod unless specified otherwise.</li> <li>Secret mounted as a volume for each pod - secret location viewable via <code>kubectl describe pod</code></li> <li>Note: The default service account can only run basic Kubernetes API queries</li> <li>To specify a particular service account for a pod, add in the pod's spec field: <code>serviceAccount: name</code></li> <li>You must delete and recreate a pod if you wish to edit the service account on a pod.</li> </ul>"},{"location":"certifications/CKS/03_Cluster-Setup-and-Hardening.html#37-tls-basics","title":"3.7- TLS Basics","text":"<ul> <li>Certificates used to guarantee trust between two parties communicating with one another, leading to a secure and encrypted connection</li> <li>Data involved in transmission should be encrypted via the use of encryption keys</li> <li>Encryption Methods:</li> <li>Symmetric: Same key used for encryption and decryption - insecure</li> <li>Asymmetric encryption: A public and private key are used for encryption and decryption specifically<ul> <li>Private key can only be used for decryption</li> </ul> </li> <li>SSH Assymetric Encryption: run ssh-keygen</li> <li>Creates <code>id_rsa</code> and <code>id_rsa.pub</code> (public and private keys)</li> <li>Servers can be secured by adding public key to authorized key file at <code>~/.ssh/authorized_keys</code></li> <li>Access to the server is then allowed via <code>ssh -i id_rsa username@server</code></li> <li>For the same user, can copy the public key to any other servers</li> <li>To securely transfer the key to the server, use asymmetric encryption</li> <li>Can generate keys with: <code>openssl genrsa -out &lt;name&gt;.key 1024</code></li> <li>Can create public variant with: <code>openssl rsa -in &lt;name&gt;.key -pubout &gt; &lt;name&gt;.pem</code></li> <li>When the user first accesses the web server via HTTPS, they get the public key from the server</li> <li>Hacker also gets a copy of it</li> <li>The users browser encrypts the servers symmetric key using their public key - securing the symmetric public key from the server</li> <li>Hacker gets copy</li> <li>Server uses private key to decrypt user's private key - allows user to access server securely</li> <li>Hackers don't have access to the servers private key, and therefore cannot encrypt it.</li> <li>For the hacker to gain access, they would have to create a similar website and route your requests</li> <li>As part of this, the hacker would have to create a certificate</li> <li>In general, certificates must be signed by a proper authority</li> <li>Any fake certificates made by hackers must be self-signed<ul> <li>Web browsers have built-in functionalities to verify if a connection is secure i.e. is certified</li> </ul> </li> <li>To ensure certificates are valid, the Certificate Authorities (CAs) must sign and validate the certs.</li> <li>Examples: Symanteg, Digicert, GlobalSign</li> <li>To validate a certificate, one must first generate a certificate signing request to be sent to the CA: <code>openssl req -new -key &lt;name&gt;.key -out &lt;name&gt;.csr -subj \"/C=US/ST=CA/O=MyOrg, Inc./CN=mybank.com\"</code></li> <li>CAs have a variety of techniques to ensure that the domain is owned by you</li> <li>How does the browser know what certificates are valid? E.g. if the certificate was assigned by a fake CA?</li> <li>CAs have a series of public and private keys built in to the web browser(s) used by the user,</li> <li>The public key is then used for communication between the browser and CA to validate the certificates</li> <li>Note: The above are described for public domain websites</li> <li>For private websites, such as internal organisation websites, private CAs are generally required and can be installed on all instances of the web browser within the organisation. Organizations previously outlined generally offer enterprise solutions.</li> <li>Note:</li> <li>Certificates with a public key are named with the extension .crt or .pem, with the prefix of whatever it is being communicated with</li> <li>Private keys will have the extension of either .key or -key.pem</li> <li>Summary:</li> <li>Public key used for encryption only</li> <li>Private keys used for decryption only</li> </ul>"},{"location":"certifications/CKS/03_Cluster-Setup-and-Hardening.html#38-tls-in-kubernetes","title":"3.8 - TLS In Kubernetes","text":"<ul> <li>In the previous section, three types of certificates were discussed, for the purposes of discussing them in Kubernetes, how they're referred to will change:</li> <li>Public and Private Keys used to secure connectivity between the likes of web browsers and servers: Server Certificates</li> <li>Certificate Authority Public and Private Keys for signing and validating certificates: Root Certificates</li> <li>Servers can request a client to verify themselves: Client Certificates</li> <li>Note:</li> <li>Certificates with a public key are named with the extension .crt or .pem, with the prefix of whatever it is being communicated with</li> <li>Private keys will have the extension of either <code>.key</code> or <code>-key.pem</code></li> <li>All communication within a Kubernetes cluster must be secure, be it:</li> <li>Pods interacting with one another</li> <li>Services with their associated clients</li> <li>Accessing the APIServer using the Kubectl utility</li> <li>Secure TLS communication is a requirement</li> <li>Therefore, it is required that the following are implemented:</li> <li>Server Certificates for Servers</li> <li>Client Certificates for Clients</li> <li>Server Components:</li> <li>Kube-API Server<ul> <li>Exposes an HTTPS service that other components and external users use to manage the Kubernetes cluster</li> <li>Requires certificates and a key pair to be generated</li> <li><code>apiserver.crt</code> (public) and <code>apiserver.key</code> (private key)</li> </ul> </li> <li>ETCD Server:<ul> <li>Stores all information about the cluster</li> <li>Requires a certificate and key pair</li> <li><code>etcdserver.crt</code> and <code>apiserver.key</code></li> </ul> </li> <li>Kubelet server:<ul> <li>Exposes HTTPS API Endpoint that the Kube-API Server uses to interact with the worker nodes</li> <li><code>kubelet.crt</code> and <code>kubelet.key</code></li> </ul> </li> <li>Client Certificates:</li> <li>All of the following require access to the Kube-API Server</li> <li>Admin user<ul> <li>Requires certificate and key pair to authenticate to the API Server</li> <li><code>admin.crt</code> and <code>admin.key</code></li> </ul> </li> <li>Scheduler<ul> <li>Client to the Kube-APIServer for object scheduling pods etc</li> <li><code>scheduler.crt</code> and <code>scheduler.key</code></li> </ul> </li> <li>Kube-Controller:<ul> <li><code>controller-manager.crt</code> and <code>controller-manager.key</code></li> </ul> </li> <li>Kube-Proxy<ul> <li>kube-proxy.crt and kube-proxy.key</li> </ul> </li> <li>Note: The API Server is the only component that communicates with the ETCD server, which views the API server as a client</li> <li>The API server can use the same keys as before for serving itself as a service OR a new pair of certificates can be generated specifically for ETCD Server Authentication</li> <li>The same principle applies for the API Server connecting to the Kubelet service</li> <li>To verify the certificates, a CA is required. Kubernetes requires at least 1 CA to be present; which has its own certificate and key (<code>ca.crt</code> and <code>ca.key</code>)</li> </ul>"},{"location":"certifications/CKS/03_Cluster-Setup-and-Hardening.html#39-tls-in-kubernets-certificates","title":"3.9 - TLS In Kubernets: Certificates","text":"<ul> <li>Tools available for certificate creation include:</li> <li>EASYRSA</li> <li>OPENSSL - The more common one (used in this example)</li> <li>CFSSL</li> <li>Steps - Server Certificates: CA Example</li> <li>Generate the private key: openssl genrsa -out ca.key 2048<ul> <li>The number \"2048\" in the above command indicates the size of the private key. You can choose one of five sizes: 512, 758, 1024, 1536 or 2048 (these numbers represent bits). The larger sizes offer greater security, but this is offset by a penalty in CPU performance. We recommend the best practice size of 1024.</li> </ul> </li> <li>Generate certificate signing request: <code>openssl req -new -key ca.key -subj \"/CN=KUBERNETES-CA\" -out ca.csr</code></li> <li>Note: <code>\"/CN=&lt;NAME&gt;\"</code> outlines the name of the component the certificate is for.</li> <li>Sign certificates, this is self-signed via the ca key pair: <code>openssl x509 -req -in ca.csr -signkey ca.key -out ca.crt</code><ul> <li>Results in CA having a private key and root certificate file</li> </ul> </li> <li>Client Certificate Generation Steps: Admin User Example</li> <li>Generate the keys: <code>openssl genrsa -out admin.key 2048</code></li> <li>Generate certificate signing request: <code>openssl req -new -key admin.key -subj \"/CN=kube-admin\" -out admin.csr</code></li> <li>Sign the certificate: <code>openssl x509 -req -in admin.csr -CAkey ca.key -out admin.crt</code><ul> <li>The CA key pair is used for signing the new certificate, thus proving its validity</li> </ul> </li> <li>When the admin user attempts to authenticate to the cluster, it is the certificate admin.crt that will be used for this</li> <li>For non-admin users, need to add group details to the certificate signing request to signal this.</li> <li>Group called SYSTEM:MASTERS has administrative privileges, to specify this for an admin user, the signing request should be like: <code>openssl req -new -key admin.key -subj \"/CN=kube-admin/O=system:masters\" -out admin.csr</code></li> <li>The same procedure would be followed for client certificates for the Scheduler, Controller-Manager and Kubelet - prefix with SYSTEM</li> <li>The certificates generated could be applied in different scenarios:</li> <li>Could use the certificate instead of usernames and password in a REST API call to the api server (via a curl request)<ul> <li>To do so, specify the key and the certs involved as options following the request e.g. <code>--key admin.key</code>, <code>--cert admin.crt</code>, and <code>--cacert ca.crt</code></li> </ul> </li> <li>Alternatively, all the parameters could be moved to the kube config yaml file, which acts as a centralized location to reference the certificates</li> <li>Note: For each of the Kubernetes components to verify one another, they need a copy of the CA's root certificate</li> <li>Server Certificate Example: ETCD Server</li> <li>As ETCD server can be deployed as a cluster across multiple servers, you must secure communication between the cluster members, or peers as they're commonly known as</li> <li>Once generated, specify the certificates when starting the server</li> <li>In the etcd yaml file, there are options to specify the peer certificates</li> </ul> <pre><code>- etcd\n  - --advertise-client-urls=https://127.0.0.1:2379\n  - --key-file=/path-to-certs/etcdserver.key\n  - --cert-file=/path-to-certs/etcdserver.crt\n  - --client-cert-auth=true\n  - --data-dir=/var/lib/etcd\n  - --initial-advertise-peer-urls=https://127.0.0.1:2380\n  - --initial-cluster=master=https://127.0.0.1:2380\n  - --listen-client-urls=https://127.0.0.1:2379\n  - --listen-peer-urls=https://127.0.0.1:2380\n  - --name=master\n  - --peer-cert-file=/path-to-certs/etcdpeer1.crt\n  - --peer-client-cert-auth=true\n  - --peer-key-file=/etc/kubernetes/pki/etc/peer.key\n  - --peer-truster-ca-file=/etc/kubernetes/pki/etcd/ca.crt\n  - --snapshot-count=10000\n  - --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt\n</code></pre> <ul> <li>Server Certificate Example: API Server</li> <li>Same procedure involved, but due to the nature of the API Server, with essentially every operation running via it, and it being referred to by multiple names, requires a lot more information included; requiring an openssl config file<ul> <li>Using the config file, specify DNS and IP aliases for the components</li> <li>Acceptable names:<ul> <li><code>kubernetes</code></li> <li><code>kubernetes.default</code></li> <li><code>kubernetes.default.svc</code></li> <li><code>kubernetes.default.svc.cluster.local</code></li> </ul> </li> <li>When generating the signing request, you can reference the config file by appending: <code>--config &lt;config name&gt;.cnf</code> to the signing request command</li> <li>From this, the certificate can be signed using the ca.crt and ca.key file as normal</li> </ul> </li> <li> <p>The location of all certificates are passed into the exec start file for the api server or the service's configuration file, specifically:</p> <p><code>shell ExecStart=/usr/local/bin/kube-apiserver \\\\   --advertise-address=${INTERNAL_IP} \\\\   --allow-privileged=true \\\\   --apiserver-count=3 \\\\   --authorization-mode=Node,RBAC \\\\   --bind-address=0.0.0.0 \\\\   --enable-swagger-ui=true \\\\   --etcd-cafile=/var/lib/kubernetes/ca.pem \\\\   --etcd-certfile=/var/lib/kubernetes/apiserver-etcd-client.crt \\\\   --etcd-keyfile=/var/lib/kubernetes/apiserver-etcd-client.key \\\\   --etcd-servers=https://127.0.0.1:2379 \\\\   --event-ttl=1h \\\\   --kubelet-certificate-authority=/var/lib/kubernetes/ca.pem \\\\   --kubelet-client-certificate=/var/lib/kubernetes/apiserver-kubelet-client.crt  \\\\   --kubelet-client-key=/var/lib/kubernetes/apiserver-kubelet-client.key   --kubelet-https=true \\\\   --runtime-configmap=api/all \\\\   --service-account-key-file=/var/lib/kubernetes/service-account.pem   --service-cluster-ip-range=10.32.0.0/24 \\\\   --service-node-port-range=30000-32767 \\\\   --client-ca-file=/var/lib/kubernetes/ca.pem \\\\   --tls-cert-file=/var/lib/kubernetes/apiserver.crt \\\\   --tls-private-key-file=/var/lib/kubernetes/apiserver.key \\\\   --v=2</code></p> <ul> <li>etcd::</li> <li>CA File = <code>--etcd-cafile</code></li> <li>ETCD Certificate = <code>--etchd-certfile</code></li> <li>ETCD Private Key File = <code>--etcd-keyfile</code></li> <li>kubelet:</li> <li>CA File = <code>--kubelet-certificate-authority</code></li> <li>Client Certificate = <code>--kubelet-client-certificate</code></li> <li>Client Key = <code>--kubelet-client-key</code></li> <li>Client CA = <code>--client-ca-file</code></li> <li>API Server Cert and Private Key = <code>--tls-cert-file</code>, <code>--tls-private-key-file</code></li> </ul> </li> <li> <p>Kubelet Server:</p> </li> <li>A HTTPS API server on each worker node to help manage the nodes</li> <li>Key-Certificate pair required for each worker node in the cluster</li> <li>Named after each node</li> <li>Must be referenced in the kubelet config file for each node, specifically:<ul> <li>Client CA file</li> <li>TLS Certificate file (kubelet-node01.crt for example)</li> <li>TLS Private Key File (kubelet-node01.key for example)</li> </ul> </li> <li>Client certificates used to authenticated to the Kube API Server<ul> <li>Naming convention should be <code>system:node:nodename</code></li> </ul> </li> <li>Nodes must also be added to <code>system:nodes</code> group for associated privileges</li> </ul>"},{"location":"certifications/CKS/03_Cluster-Setup-and-Hardening.html#310-view-certtificate-details","title":"3.10 - View Certtificate Details","text":"<ul> <li>The generation of certificates depends on the cluster setup</li> <li>If setup manually, all certificates would have to be generated manually in a similar manner to that of the previous sections<ul> <li>Components deployed as native services in this manner</li> </ul> </li> <li>If setup using a tool such as kubeadm, this is all pre-generated<ul> <li>Components deployed as pods in this manner</li> </ul> </li> <li>For Kubeadm clusters:</li> <li>Component found in <code>/etc/kubernetes/pki/</code> folder<ul> <li>Certificate file paths located within component's yaml files</li> <li>Example: <code>apiserver.crt</code></li> <li>Use <code>openssl x509 -in /path/to/.crt file -text -noout</code></li> </ul> </li> <li>Can check the certificate details such as name, alternate names, issuer and expiration dates</li> <li>Note: Additional details available in the documentation for certificates</li> <li>Use <code>kubectl logs &lt;podname&gt;</code> on kubeadm if any issues are found with the components</li> <li>If <code>kubectl</code> is unavailable, use Docker to get the logs of the associated container:</li> <li>Run <code>docker ps - a</code> to identify the container ID</li> <li>View the logs via <code>docker logs &lt;container ID&gt;</code></li> <li>A sample health check spreadsheet can be found here: https://github.com/mmumshad/kubernetes-the-hard-way/tree/master/tools</li> </ul>"},{"location":"certifications/CKS/03_Cluster-Setup-and-Hardening.html#311-certificates-api","title":"3.11 - Certificates API","text":"<ul> <li>All certificates have an expiration date, whenever the expiry happens, keys and certificates must be re-generated</li> <li>As discussed, the signing of the certificates is handled by the CA Server</li> <li>The CA server in reality is just a pair of key and certificate files generated</li> <li>Whoever has access to these files can sign any certificate for the Kubernetes environment, create as many users they want and set their permissions</li> <li>Based on the previous point, it goes without saying these files need to be protected</li> <li>Place the files on a fully secure server</li> <li>The server that securely hosts these files becomes the \"CA Server\"</li> <li>Any time you want to sign a certificate, it is the CA server that must be logged onto/communicated with</li> <li>For smaller clusters, it's common for the CA server to actually be the master node</li> <li>The same applies for a kubeadm cluster, which creates a CA pair of files and stores that on the master node</li> <li>As clusters grow in users, it becomes important to automate the signing of certificate requests and renewing expired certificates; this is handled via the Certificates API</li> <li>When a certificate needs signing, a Certificate Signing Request is sent to Kubernetes directly via an API call</li> <li>Instead of an admin logging onto the master node and manually signing the certificate, they create a Kubernetes API object called CertificateSigningRequest</li> <li>Once the API object is created, any requests like this can be seen by administrators across the cluster</li> <li>From here, the request can be reviewed and approved using kubectl, the resultant certificate can then be extracted and shared with the user</li> <li>Steps:</li> <li>User generates key: <code>openssl genrsa -out &lt;keyname&gt;.key 2048</code></li> <li>User generates certificate signing request and sends to administrator: <code>openssl req -new -key &lt;key&gt;.name -subk \"/CN=name\" -out name.csr</code></li> <li>Admin receives request and creates the API object using a manifest file, where the spec file includes the following:<ul> <li>Groups - To set the permissions for the user</li> <li>Usages - What is the user able to do with keys with this certificate to be signed?</li> <li>Request - The certificate signing request associated with the user, which must be encoded in base64 language first i.e. cat cert.crt | base64</li> <li>Admins across the cluster can view certificate requests via: <code>kubectl get csr</code></li> <li>If all's right with the csr, any admin can approve the request with: <code>kubectl certificate approve &lt;name&gt;</code></li> <li>You can view the CSR in a YAML form, like any Kubernetes object by appending -o yaml to the kubectl get command i.e. <code>kubectl get csr &lt;user&gt; -o yaml</code></li> </ul> </li> <li>Note: The certificate will still be in base64 code, so run: <code>echo \"CODED CERTIFICATE\" | base64 --decode</code></li> <li>Note: The controller manager is responsible for all operations associated with approval and management of CSR</li> <li>The controller manager's YAML file has options where you can specify the key and certificate to be used when signing certificate</li> <li><code>--cluster-signing-key-file</code></li> </ul> <pre><code>spec:\n  containers:\n  - command:\n  - kube-controller-manager\n  - --address=127.0.0.1\n  - --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt\n  - --cluster-signing-key-file=/etc/kubernetes/pki/ca.key\n  - --controllers=*,bootstrapsigner,tokencleaner\n  - --kubeconfig=/etc/kubernetes/controller-manager.conf\n  - --leader-elect=true\n  - --root-ca-file=/etc/kubernetes/pki/ca.crt\n  - --service-account-private-key-file=/etc/kubernetes/pki/sa.key\n  - --use-service-account-credentials=true\n</code></pre>"},{"location":"certifications/CKS/03_Cluster-Setup-and-Hardening.html#312-kubeconfig","title":"3.12 - KubeConfig","text":"<ul> <li>Files containing information for different cluster configurations, such as:</li> <li><code>--server</code></li> <li><code>--client-key</code></li> <li><code>--client-certificate</code></li> <li><code>--certificate-authority</code></li> <li>The existence of this file removes the need to specify the option in the CLI</li> <li>File located at <code>$HOME/.kube/config</code></li> <li>KubeConfig Files contain 3 sections:</li> <li>Clusters - Any cluster that the user has access to, local or cloud-based</li> <li>Users - User accounts that have access to the clusters defined in the previous section, each with their own privileges</li> <li>Contexts - A merging of clusters and users, they define which user account can access which cluster</li> <li>These config files do not involve creating new users, it's simply configuring what existing users, given their current privileges, can access what cluster</li> <li>This removes the need to specify the user certificates and server addresses in each kubectl command</li> <li><code>--server</code> spec listed under clusters</li> <li>User keys and certificates listed in Users section</li> <li>Context created to specify that the user \"MyKubeAdmin\" is the user that is used to access the cluster \"MyKubeCluster\"</li> <li>Config file defined in YAML file</li> <li>ApiVersion = v1</li> <li>Kind = Config</li> <li>Spec includes the three sections defined previously, all of which are arrays</li> <li>Under clusters: specify the cluster name, the certificate authority associated and the server address</li> <li>Under users, specify username and associated key(s) and certificate(s)</li> <li>Under contexts:<ul> <li>Name format: <code>username@clustername</code></li> <li>Under context specify cluster name and users</li> </ul> </li> <li>Repeat for all clusters and users associated</li> <li>The file is automatically read by the kubectl utility</li> <li>Use current-context field in the yaml file to set the current context</li> <li>CLI Commands:</li> <li>View current config file being used: <code>kubectl config view</code><ul> <li>Default file automatically used if not specified</li> <li>To view non-default config files, append: <code>--kubeconfig=/path/to/file</code></li> </ul> </li> <li>To update current context: <code>kubectl config use-context &lt;context-name&gt;</code></li> <li>Other commands available via <code>kubectl config -h</code></li> <li>Default namespaces for particular contexts can be added also under the context area in the Config file</li> <li>Note: for certificates in the config file, use the full/absolute path to specify the location</li> <li>Alternatively use certificate-authority-data to list certificate in base64 format</li> </ul>"},{"location":"certifications/CKS/03_Cluster-Setup-and-Hardening.html#313-api-groups","title":"3.13 - API Groups","text":"<ul> <li>API Server accessible at master node IP address at port 6443: <code>curl https://kube-master:6443/</code></li> <li>To get the version, append /version to a curl request to the above IP address</li> <li>To get a list of pods, append <code>/api/v1/pods</code></li> <li>Kubernetes' API is split into multiple groups depending on the group's purpose such as</li> <li><code>/api</code> - core functionalities e.g. pods, namespaces, secrets</li> <li><code>/version</code> - viewing the version of the cluster</li> <li><code>/metrics</code> - used for monitoring cluster health</li> <li><code>/logs</code> - for integration with 3rd-party logging applications</li> <li><code>/apis</code> - named functionalities added to kubernetes over time such as deployments, replicasets, extensions<ul> <li>Each group has a version, resources, and actions associated with them</li> </ul> </li> <li><code>/healthz</code> - used for monitoring cluster health</li> <li>Use <code>curl http://localhost:6443 -k</code> to view the api groups, then append the group and grep name to see the subgroups within</li> <li>Note: Need to provide certificates to access the api server or use kubectl proxy to view</li> <li>Note: kubectl proxy is not the same as kube proxy, the former is an http proxy service to access the api server.  Kube proxy is used to allow connectivity between kubernetes pods and services across nodes in a cluster.</li> </ul>"},{"location":"certifications/CKS/03_Cluster-Setup-and-Hardening.html#314-authorization","title":"3.14 - Authorization","text":"<ul> <li>When adding users, need to ensure their access levels are sufficiently configured, so they cannot make any unwanted changes to the cluster</li> <li>This applies to any physical users, like developers, or virtual users like applications e.g. Jenkins</li> <li>Additional measures must be taken when sharing clusters with organizations or teams, so that they are restricted to their specific namespaces</li> <li>Authorization mechanisms available are:</li> <li>Node-based</li> <li>Attribute-Based</li> <li>Rule-Based</li> <li>WebHook-based</li> <li>Node-Based:</li> <li>Requests to the kube-apiserver via users and the kubelet are handled  via the Node Authorizer</li> <li>Kubelets should be part of the system:nodes group i.e. <code>system:node:nodename</code></li> <li>Any requests coming from a user with the name system-node and is part of the system nodes group is authorized and granted access to the apiserver</li> <li>ABAC - Attribute-Based:</li> <li>For users wanting to access the cluster, you should create a policy in a JSON format to determine what privileges the user gets, such as namespace access, resource management and access, etc - this can then be passed to the API server for authorization</li> <li>Repeat for each users</li> <li>Each policy must be edited manually for changes to be made, the kube apiserver must be restarted to make the changes take effect</li> <li>Not generally used as difficult to manage</li> <li>RBAC:</li> <li>Instead of associating each user with a set of permissions, can create a role which outlines a particular set of permissions</li> <li>Assign users to the role</li> <li>If any changes are to be made, it is just the role configuration that needs to be changed, rather than restarting the entire api server again.</li> <li>Webhook:</li> <li>Use of third-party tools to help with authorization e.g. Open Policy Agent</li> <li>If any requests are made to say the APIserver, the third party can verify if the request is valid or not</li> <li>Note: Additional authorization methods are available:</li> <li>AlwaysAllow - Allows all requests without checks</li> <li>AlwaysDeny - Denies all requests without checks</li> <li>Authorizations set by <code>--authorization</code> option in the apiserver's <code>.service</code> or <code>.yaml</code> file</li> <li>Can set modes for multiple-phase authorization, use <code>--authorization-mode</code> and list the authorization methods</li> <li>Authorization will be done in the order of listing in this option</li> </ul>"},{"location":"certifications/CKS/03_Cluster-Setup-and-Hardening.html#315-rbac","title":"3.15 - RBAC","text":"<ul> <li>To create a role, create a YAML file</li> <li><code>apiVersion: rbac.authorization.k8s.io/v1</code></li> <li>Spec replaced with rules</li> <li>Covers apiGroups, resources and verbs</li> <li>Multiple rules added by <code>- apiGroups</code> for each</li> <li>Create the role using <code>kubectl create -f</code></li> <li>To link the user to the role, need to create a Role Binding</li> <li>Under metadata:</li> <li>Specify subjects - Users to be affected by the rolebinding, their associated apiGroup for authorization</li> <li>RoleRef - The role to be linked to the subject</li> <li>To view roles: <code>kubectl get roles</code></li> <li>To view rolebindings: kubectl get rolebindings</li> <li>To get additional details: <code>kubectl describe role/rolebinding &lt;name&gt;</code></li> <li>To check access level: <code>kubectl auth can-i &lt;command/activity&gt;</code></li> <li>To check if a particular user can do an activity, append <code>--as &lt;username&gt;</code></li> <li>To check if an activity can be done via a user in a particular namespace, append <code>--namespace &lt;namespace&gt;</code></li> <li>Note: Can restrict access to particular resources by adding <code>resourceNames: [\"resource1\", \"resource2\", ...]</code> to the role yaml file</li> </ul>"},{"location":"certifications/CKS/03_Cluster-Setup-and-Hardening.html#316-cluster-roles-and-rolebindings","title":"3.16 - Cluster Roles and Rolebindings","text":"<ul> <li>Roles and role bindings are created for particular namespaces and control access to resources in that particular namespace</li> <li>By default, roles and role bindings are applied to the default namespace</li> <li>In general, resources such as pods, replicasets are namespaced</li> <li>Cluster-scoped resources are resources that cannot be associated to any particular namespace, such as:</li> <li>PersistentVolumes</li> <li>Nodes</li> <li>To switch view namespaced/cluster-scoped resources: <code>kubectl api-resources --namespaced=TRUE/FALSE</code></li> <li>To authorize users to cluster-scoped resources, use cluster-roles and cluster-rolebindings</li> <li>Could be used to configure node management across a cluster, such as cluster or cluster storage administrator(s).</li> <li>Cluster roles and role bindings are configured in the exact same manner as roles and rolebindings; the only difference is the kind (ClusterRole and ClusterRoleBinding)</li> <li>Note: Cluster roles and role bindings can be applied to namespaced resources, this will allow users to have access to particular resources for anywhere in the cluster.</li> <li>Many cluster roles are created via Kubernetes by default.</li> </ul>"},{"location":"certifications/CKS/03_Cluster-Setup-and-Hardening.html#317-kubelet-security","title":"3.17 - Kubelet Security","text":"<ul> <li>Kubelet leads all the activities on a node to manage and maintain the node, carrying out actions such as:</li> <li>Loading or unloading containers based on the kube-schedulers demands</li> <li>Sending regular reports on worker node status to the api server</li> <li>Due to the importance of the kubelet, it's highly important to secure it and the communications between the kubernetes master node, api server, and worker nodes are secure.</li> <li>To refresh, the Kubelet, in worker nodes:</li> <li>Registers the node with the cluster</li> <li>Carries out instructions to run containers and container runtime</li> <li>Monitor node and pod status on a regular basis</li> <li>The kubelet can be installed as a binary file via a wget<ul> <li>Via kubeadm, it is automatically downloaded but not automatically deployed</li> </ul> </li> <li>The kubelet configuration file varies in appearance. Previously, it was viewable as a .service file:</li> </ul> <pre><code>ExecStart=/usr/local/bin/kubelet \\\\\n  --container-runtime=remote \\\\\n  --image-pull-progress-deadline=2m \\\\\n  --kubeconfig=/var/lib/kubelet/kubeconfig \\\\\n  --network-plugin=cni \\\\\n  --register-node=true \\\\\n  --v=2 \\\\\n  --cluster-domain=cluster.local \\\\\n  --file-check-frequency=0s \\\\\n  --healthz-port=10248 \\\\\n  --cluster-dns=10.96.0.10 \\\\\n  --http-check-frequency=0s \\\\\n  --sync-frequency=0s\n</code></pre> <ul> <li>Since kubernetes v1.10, options from --cluster-domain were moved to kubelet-config.yaml for ease of configuration and management</li> <li>In the kubelet.service file, the kubelet-config file path is passed via the --config flag.</li> <li>Note: any flags at CLI-level will override the .service file's value</li> <li>The kubeadm tool does not download or install the kubelet, but it can help manage the kubelet configuration</li> <li>Suppose there is a large number of worker nodes, rather than manually creating the config file in each of the nodes, the kubeadm tool can help automatically configure the kublet-config file associated with those nodes when joining them to the master node.</li> <li>Once kubelet is configured, there a number of frequent commands that can be used, including:</li> <li>View the kubelet options:<ul> <li><code>ps -aux | grep kubelet</code> Views the associated options for the kubelet</li> <li><code>cat /var/lib/kublet/config.yaml</code> View the config yaml for the kubelet</li> </ul> </li> <li>Securing the kubelet, at a high level, involves taking actions to ensure the kubelet responds only to the kube-apiserver's requests</li> <li>Kubelet serves on 2 ports:</li> <li>10250 - Serves API allowing full access</li> <li>10255 - Serves an API that allows unauthenticated read-only access</li> <li>By default, kubelet allows anonymous access to the api, e.g.</li> <li>Running <code>curl -sk htpps://localhost:10250/pods/</code> reproduces a list of all the pods</li> <li>Running <code>curl -sk https://localhost:10250/logs/syslogs</code> returns the system logs of the node that kubelet is running on</li> <li>For the service at 10255 - this provides access read-only access to metrics to any unauthorized clients</li> <li>The above two services pose significant security risks, as anyone knowing the host IP address for the node can identify information regarding the node and cause damage if desired based on the various API calls available.</li> <li>Securing the kubelet boils down to authentication and authorization</li> <li>Authentication determines whether the user can access the kubelet API</li> <li>Authorization determines whether the user has sufficient permissions to perform a particular task with the API</li> <li>Authentication:</li> <li>By default, the kubelet permits all requests without authentication</li> <li>Any requests are labelled to be from user groups \"anonymous\" part of an unauthenticated group.</li> <li>This can be disabled in either the kubelet.service file or the yaml file by setting <code>--anonymous-auth</code> to false, as shown below:</li> </ul> <pre><code>## kubelet.service\n\nExecStart=/usr/local/bin/kubelet \\\\\n    ...\n    --anonymous-auth=false \\\\\n    ...\n</code></pre> <pre><code>## kubelet-config.yaml\n\napiVersion: kubelet.config.k8s.io/v1beta1\nkinds: KubeletConfiguration\nauthentication:\n  anonymous:\n    enabled: false\n</code></pre> <ul> <li>Following the disabling of anonymous access, a recommended authentication method needs to be enabled. Generally there are two to choose from:</li> <li>Certificates (X509)</li> <li>API Bearer Tokens</li> <li>Following the creation of a pair of certificates, the ca file should be provided via the following option in the kubelet service file: <code>--client-ca-file=/path/to/ca.crt</code>  Or to the <code>kubelet-config.yaml</code> file as shown below:</li> </ul> <pre><code>apiVersion: kubelet.config.k8s.io/v1beta1\nkinds: KubeletConfiguration\nauthentication:\n  x509:\n    clientCAFile: /path/to/ca.crt\n</code></pre> <ul> <li>Now that the certificate is configured, the client certificates must be supplied in any curl commands made to the API i.e.: <code>curl -sk htpps://localhost:10250/pods/ -key kubelet-key.pem -cert kubelet-cert.pem</code></li> <li>As far as the kubelet is concerned, the kube-apiserver is a client, therefore the apiserver should also have the kubelet client certificate and key configured in the apiserver's service configuration, as shown below:</li> </ul> <pre><code>## /etc/systemd/system/kube-apiserver.service\n\n[Service]\nExecStart=/usr/local/bin/kube-apiserver \\\\\n  ...\n  --kubelet-client-certificate=/path/to/kubelet-cert.pem \\\\\n  --kubelet-client-key=/path/to/kubelet-key.pem \\\\\n</code></pre> <ul> <li>Remember: the kube-apiserver is itself a server and therefore has its own set of certificates, all other kubernetes certificates will also require sufficient configurations</li> <li>Note: Kubeadm also uses this approach when trying to secure the kubelet</li> <li>Note: If neither of the above authentication mechanisms explicitly reject a request, the default behaviour of kubelet is to allow the request under the username <code>system:anonymous</code> and group <code>system:unauthenticated</code></li> <li>Authorization:</li> <li>Once the user has access to the kubelet, what can they do with it?</li> <li> <p>By default, the mode AlwaysAllow is set, allowing all access to the API</p> <p>```shell</p> </li> <li> <p>To change / rectify this, the authorization mode can be set to Webhook - the kubelet makes a call to the API server to determine if the request can be granted or not.</p> </li> <li>Considering the metrics service running on 10255/metrics:</li> <li>By default, this runs due to the kubelet config or service file having the <code>--read-only-port</code> set to 10255</li> <li>If set to 0, the service is disabled and users cannot access it</li> <li>Summary:</li> <li>By default, the kubelet allows anonymous authentication</li> <li>To prevent this, you can set the <code>anonymous</code> flag to false - this can be done either in the <code>kubelet.service</code> or <code>kubelet-config.yaml</code> files</li> <li>Kubelet supports two authentication methods:<ul> <li>Certification-based authentication</li> <li>API Bearer Tokens</li> </ul> </li> <li>By default, the authorization mode is set to \"always allow\" - this can be prevented by changing the mode to webhook to authorize via webhook calls to the kube-api server</li> <li>By default, the readonly port is set to 10255, allowing unauthorized access to critical kubernetes metrics, this can be disabled by setting the <code>--read-only-port</code> to 0.</li> </ul>"},{"location":"certifications/CKS/03_Cluster-Setup-and-Hardening.html#kubeletservice","title":"kubelet.service","text":"<p>ExecStart=/usr/local/bin/kubelet \\     ...     --authorization-mode=AlwaysAllow \\     ... ```</p> <p><code>yaml apiVersion: kubelet.config.k8s.io/v1beta1 kinds: KubeletConfiguration authorization: mode: AlwaysAllow</code></p>"},{"location":"certifications/CKS/03_Cluster-Setup-and-Hardening.html#318-kubectl-proxy-and-port-forward","title":"3.18 - Kubectl Proxy and Port-Forward","text":"<ul> <li>In the CKA Course, it was learned that the kubectl tool can be used to interact with the Kubernetes API server</li> <li>No API authentication required as it was applied in the kube config file</li> <li>Kubectl can be found anywhere e.g. master node, a personal laptop (with a cluster in a VM)</li> <li>No matter where the cluster is, so long as the kubeconfig is appropriately configured with the security credentials, the kubectl tool can be used to manage it.</li> <li>Note: the api server could also be accessed via a curl command to the IP address and port 6443</li> <li>If done via this method, would also need to supply the certificate and key files, such that the curl command would be similar to:  <code>curl http://&lt;kube-api-server-ip&gt;:6443 -k --key admin.key --cert admin.crt --cacert car.crt</code></li> <li>Alternatively, one can start a proxy client using kubectl Kubectl proxy -&gt; starts to serve on <code>localhost:8001</code></li> <li>Launches a proxy service and uses the credentials from the config file to access the API server</li> <li>Proxy only runs on laptop and is only accessible from this device</li> <li>By default, accepts traffic from the loopback address localhost -&gt; not accessible from outside of the laptop.</li> <li>Can use kubectl proxy to make ANY request to the API server and services running within it.</li> <li>Consider an nginx pod exposed as a service only accessible within the cluster (if clusterIP service):</li> <li>Use kubectl proxy as part of a curl command: <code>curl http://localhost:8001/api/v1/namespaces/default/services/nginx/proxy/</code></li> <li>Allows access to remote clusters as if they were running locally</li> <li>Alternative: Port-Forward</li> <li>Takes a resource (pod, deployment, etc) as an argument and specifies a port on the host that you would like traffic to be forwarded to (and the service port)</li> <li>Example: <code>kubectl port-forward service/nginx 28080:80</code></li> <li>The service can then be accessed via curl http://localhost:28080/</li> <li>Allows remote access to any cluster service you have access to.</li> </ul>"},{"location":"certifications/CKS/03_Cluster-Setup-and-Hardening.html#319-kubernetes-dashboard","title":"3.19 - Kubernetes Dashboard","text":"<ul> <li>A kubernetes sub-project used for a variety of functions including:</li> <li>Get a graphical representation of your cluster</li> <li>Monitor resource performance</li> <li>Provision resources</li> <li>View and manage secrets or not easily viewable resources.</li> <li>Due to these functionalities, it's important to ensure that the dashboard is secured appropriately.</li> <li>In earlier releases, access control was limited - led to many dashboard cyberattacks e.g. Tesla and Cryptocurrency mining.</li> <li>Deployment:</li> <li>Can deploy by applying the recommended configuration from <code>https//&lt;path-to-kubernetes-dashboard&gt;/recommended.yaml</code></li> <li>Deploys:<ul> <li>Namespace - kubernetes-dashboard</li> <li>Service - kubernetes-dashboard</li> <li>Secrets - Certificates associated with the dashboard</li> </ul> </li> <li>Note: The service is not set to LoadBalancer by default, instead ClusterIP - this is in line with best practices so it is only accessible within the cluster VMs.</li> <li>Accessing the Dashboard:</li> <li>Would typically want to access from the users laptop / device separate to the cluster.</li> <li>Access will require the kubectl utility and kubeconfig file, as well as kube proxy.</li> <li>Run <code>kubectl proxy</code> - proxies all the requests to the api server on the cluster to the machine running the proxy<ul> <li>Can then access the dashboard via <code>localhost:8001/&lt;URL to Dashboard&gt;</code> e.g.: <code>https://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/htpps:kubernetes-dashboard:/proxy</code></li> </ul> </li> <li>Accessing the dashboard via a proxy is not applicable for teams requesting access to the dashboard.<ul> <li>Allowing access requires additional configuration to ensure only users of sufficient permissions have access - this is due to the ClusterIP service type.</li> <li>Possible solution: set service type to LoadBalancer - not recommended as would make the dashboard public.</li> <li>Possible solution: set service type to NodePort</li> <li>This allows the dashboard to be accessible via the ports on the node, more advised than the previous suggestion assuming a sufficiently secure network.</li> <li>Possible solution: Configure an Authentication Proxy e.g. OAuth2</li> <li>Out of scope for the course, but a highly recommended option.</li> <li>If users authenticate appropriately, traffic routed to Dashboard.</li> </ul> </li> </ul>"},{"location":"certifications/CKS/03_Cluster-Setup-and-Hardening.html#320-securing-the-kubernetes-dashboard","title":"3.20 - Securing the Kubernetes Dashboard","text":"<ul> <li>Various authentication methods available:</li> <li>Token:<ul> <li>Requires user creation with sufficient permissions via RBAC</li> <li>Kubernetes dashboard documentation provides instructions for this, but this is particularly geared towards cluster admins only.</li> <li>In general, requires creation of user service accounts, roles and role bindings.</li> <li>Once created, the token can be found by viewing the secret created relating to the service account.</li> </ul> </li> <li>Kubeconfig:<ul> <li>Requires passing of appropriate kubeconfig file that has the sufficient credentials to authenticate.</li> </ul> </li> </ul>"},{"location":"certifications/CKS/03_Cluster-Setup-and-Hardening.html#references","title":"References","text":"<ul> <li>https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/</li> <li>https://redlock.io/blog/cryptojacking-tesla</li> <li>https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/</li> <li>https://github.com/kubernetes/dashboardhttps://www.youtube.com/watch?v=od8TnIvuADg</li> <li>https://blog.heptio.com/on-securing-the-kubernetes-dashboard-16b09b1b7aca</li> <li>https://github.com/kubernetes/dashboard/blob/master/docs/user/access-control/creating-sample-user.md</li> </ul>"},{"location":"certifications/CKS/03_Cluster-Setup-and-Hardening.html#321-verify-platform-binaries-pre-deployment","title":"3.21 - Verify Platform Binaries Pre-Deployment","text":"<ul> <li>Kubernetes platform binaries are available via the Kubernetes Github repo's release</li> <li>As part of security best practices, it's important to ensure said binaries are safe for use</li> <li>This can be done by comparing the binary checksum with the checksum listed on the website.</li> <li>The reason for this check needing to occur is due to the possibility of the download being intercepted by attackers.</li> <li>Even the smallest of changes can cause a complete change to the hash</li> <li>Binaries downloaded by curl command i.e. <code>curl &lt;url&gt; -L -o &lt;filename&gt;</code></li> <li>Checksum can be viewed by using the shasum utility (Mac): <code>shasum -a 512 &lt;filename&gt;</code> or <code>sha512sum &lt;filename&gt;</code> (Linux)</li> <li>Checksum can then be compared against the release page checksum</li> <li>Reference Links:</li> <li>https://kubernetes.io/docs/setup/release/notes</li> <li>https://github.com/kubernetes/kubernetes/tree/master/CHANGELOG</li> </ul>"},{"location":"certifications/CKS/03_Cluster-Setup-and-Hardening.html#322-kubernetes-software-versions","title":"3.22 - Kubernetes Software Versions","text":"<ul> <li>API Versions</li> <li>When installing a Kubernetes cluster, a particular version of Kubernetes is installed</li> <li>Viewable via Kubectl get nodes</li> <li>Kubernetes versions are done via <code>&lt;Major&gt;.&lt;Minor&gt;.&lt;Patches&gt;</code></li> <li>This is the standard software release pattern.</li> <li>Alpha and Beta versions used to test and integrate features into main stable releases.</li> <li>View Kubernetes GitHub repo's release page for details.</li> <li>Packages contain all the required components of the same version.</li> <li> <p>For components like CoreDNS and ETCD, details on supported versions are provided as they are separate projects.</p> </li> <li> <p>Reference Links</p> </li> <li>https://github.com/kubernetes/kubernetes/releases</li> <li>https://github.com/kubernetes/community/blob/master/contributors/design-proposals/release/versioning.md</li> <li>https://github.com/kubernetes/community/blob/master/contributors/design-proposals/api-machinery/api-group.md</li> <li>https://blog.risingstack.com/the-history-of-kubernetes/</li> <li>https://kubernetes.io/docs/setup/version-skew-policy</li> </ul>"},{"location":"certifications/CKS/03_Cluster-Setup-and-Hardening.html#323-cluster-upgrade-process","title":"3.23 - Cluster Upgrade Process","text":"<ul> <li>The kubernetes components don't all have to be at the same versions</li> <li>No component should be at a version higher than the kube-api server</li> <li>If Kube-API Server is version X (a minor release), then the following ranges apply for the other components for support level:<ul> <li>Controller manager: X-1</li> <li>Kube-Scheduler: X-1</li> <li>Kubelet: X-2</li> <li>Kube-Proxy: X-2</li> <li>Kubectl: X-1 - X+1</li> </ul> </li> <li>At any point, Kubernetes only supports the 3 most recent minor releases e.g. 1.19 - 1.17</li> <li>It's better to upgrade iteratively over minor releases e.g. 1.17 - 1.18 and so on</li> <li>Upgrade process = Cluster-Dependent</li> <li>If on a cloud provider, built-in functionality available</li> <li>If on kubeadm/manually created cluster, must use commands:</li> <li><code>kubeadm upgrade plan</code></li> <li><code>kubeadm upgrade apply</code></li> <li>Cluster upgrades involve two steps:</li> <li>Upgrade the master node<ul> <li>All management components go down temporarily during the processes</li> <li>Doesn't impact the current node workloads (only if you try to do anything with them)</li> </ul> </li> <li>Upgrade the worker nodes<ul> <li>Can be done all at once - Results in downtime</li> <li>Can be done iteratively - Minimal downtime by draining nodes as they get upgraded one after another</li> <li>Could also add new nodes with the most recent software versions</li> <li>Proves especially inconvenient when on a cloud provider</li> </ul> </li> <li>Upgrading via Kubeadm:</li> <li><code>kubeadm upgrade plan</code><ul> <li>Lists latest versions available</li> <li>Components that must be upgraded manually</li> <li>Command to upgrade kubeadm</li> </ul> </li> <li>Note: kubeadm itself must be upgraded first: <code>apt-get upgrade -y kubeadm=major.minor.patch_min-patch_max</code></li> <li>Check upgrade success based on CLI output and kubectl get nodes</li> <li>If Kubelet is running on Master node, this must be upgraded next the master node and restart the service:</li> <li><code>apt-get upgrade -y kubelet=1.12.0-00</code></li> <li><code>systemctl restart kubelet</code></li> <li>Upgrading the worker nodes:</li> <li>Use the drain command to stop and transfer the current workloads to other nodes: <code>kubectl drain &lt;node&gt;</code>, then upgrade the following for each node (ssh into each one):<ul> <li>Kubeadm - <code>apt-get upgrade -y kubeadm=major.minor.patch_min-patch_max</code></li> <li>Kubelet - <code>apt-get upgrade -y kubelet=major.minor.patch_min-patch_max</code></li> <li>Node config: - <code>kubeadm upgrade node config --kubelet-version major.minor.patch</code></li> </ul> </li> <li>Restart the service: <code>systemctl restart kubelet</code></li> <li>Make sure to uncordon each node after each upgrade! <code>kubectl uncordon &lt;node&gt;</code></li> <li>Instructions Available Here: Upgrade A Cluster | Kubernetes</li> </ul>"},{"location":"certifications/CKS/03_Cluster-Setup-and-Hardening.html#324-network-policies","title":"3.24 - Network Policies","text":""},{"location":"certifications/CKS/03_Cluster-Setup-and-Hardening.html#traffic-example","title":"Traffic Example","text":"<ul> <li>Suppose we have the following setup of servers:</li> <li>Web</li> <li>API</li> <li>Database</li> <li>Network traffic will be flowing through each of these servers across particular ports, for example:</li> <li>Web user requests and receives content from the web server on port 80 for HTTP</li> <li>Web server makes a request to the API over port 5000</li> <li> <p>API requests for information from the database over port 3306 (e.g. if MySQL)</p> </li> <li> <p>2 Types of Network Traffic in this setup:</p> </li> <li>Ingress: Traffic to a resource</li> <li> <p>Egress: Traffic sent out from a resource</p> </li> <li> <p>For the setup above, we could control traffic by allowing ONLY the following traffic to and from each resource across particular ports:</p> </li> <li>Web Server:<ul> <li>Ingress: 80 (HTTP)</li> <li>Egress: 5000 (API port)</li> </ul> </li> <li>API Server:<ul> <li>Ingress: 5000</li> <li>Egress: 3306 (MySQL Database Port)</li> </ul> </li> <li> <p>Database Server:</p> <ul> <li>Ingress: 3306</li> </ul> </li> <li> <p>Considering this from a Kubernetes perspective:</p> </li> <li>Each node, pod and service within the cluster has its own IP address</li> <li>When working with networks in Kubernetes, it's expected that the pods should be able to communicate with one another, regardless of the olution to the project<ul> <li>No additional configuration required</li> </ul> </li> <li>By default, Kubernetes has an \"All-Allow\" rule, allowing communication between any pod in the cluster.</li> <li>This isn't best practice, particularly if working with resources that store very sensitive information e.g. databases.</li> <li>To restrict the traffic, one can implement a network policy.</li> </ul> <ul> <li>A network policy is a Kubernetes object allowing only certain methods of network traffic to and from resources. An example follows:</li> </ul> <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: network-policy\nspec:\n  podSelector:\n    matchLabels:\n      role: db\n  policyTypes:\n  - Ingress:\n  ingress:\n  - from:\n    - podSelector:\n        matchLabels:\n          name: api-pod\n    ports:\n    - protocol: TCP\n      port: 3306\n</code></pre> <ul> <li> <p>The policy can then be created via <code>kubectl create -f ....</code></p> </li> <li> <p>Network policies are enforced and supported by the network solution implemented on the cluster.</p> </li> <li>Solutions that support network policies include:</li> <li>kube-router</li> <li>calico</li> <li>romana</li> <li> <p>weave-net</p> </li> <li> <p>Flannel doesn't support Network Policies, they can still be created, but will not be enforced.</p> </li> <li> <p>Selectors available:</p> </li> <li>podSelector</li> <li>namespaceSelector</li> <li>ipBlock</li> </ul>"},{"location":"certifications/CKS/03_Cluster-Setup-and-Hardening.html#324-ingress","title":"3.24 - Ingress","text":"<ul> <li>To understand the importance of Ingress, consider the following example:</li> <li>Suppose you build an application into a Docker image and deploy it as a pod via Kubernetes.</li> <li>Due to the application's nature, set up a MySQL database and deploy a clusterIP service -&gt; allows app-database communications.</li> <li>To expose the app or external access, one needs to create a NodePort service.</li> <li>App can then be accessed via the Node's IP and the port defined.</li> <li>To access the URL, users need to go to <code>http://&lt;node ip&gt;:&lt;node port&gt;</code></li> <li> <p>This is fine for small non-production apps, it should be noted that as demand increases, the replicaSet and service configuration can be altered to support load balancing.</p> </li> <li> <p>For production, users wouldn't want to have to enter an IP and port number every time, typically a DNS entry would be created to map to the port and IP.</p> </li> <li>As service node ports can only allocate high numbered ports (<code>&gt; 30000</code>):</li> <li> <p>Introduce a proxy server between DNS cluster and point it to the DNS server.</p> </li> <li> <p>The above steps are applicable if hosting an app in an on-premise datacenter.</p> </li> <li>If working with a public cloud application, NodePort can be replaced by <code>LoadBalancer</code></li> <li> <p>Kubernetes still performs NodePort's functionality AND sends an additional request to the platform to provision a network load balancer.</p> </li> <li> <p>The cloud platform automatically deploys a load balancer configured to route traffic to the service ports of all the nodes.</p> </li> <li> <p>The cloud provider's load balancer would have its own external IP</p> </li> <li> <p>User request access via this IP.</p> </li> <li> <p>Suppose as the application grows and a new service is to be added, it's to be accessed via a new URL.</p> </li> <li>For the new application to share the cluster resource, release it as a separate deployment.</li> <li> <p>Engineers could create a new load balancer for this app, monitoring a new port</p> <ul> <li>Kubernetes automatically configures a new load balancer on the cloud platform of a new IP.</li> </ul> </li> <li> <p>To map the URLs between the 2 new services, one would have to implement a new proxy server on top of those associated with the service.</p> </li> <li> <p>This proxy service would have to be configured and SSL communications would have to be enabled.</p> </li> <li> <p>This final proxy could be configured on a team-by-team basis, however would likely lead to issues.</p> </li> </ul> <ul> <li>The whole process outlined above has issues, on top of having additional proxies to manage per service, one must also consider:</li> <li>Cost: Each additional Load Balancer adds more expense.</li> <li> <p>Difficulty of Management: For each service introduced, additional configuration is required for both firewalls and proxies</p> <ul> <li>Different teams required as well as time and \"person\" power.</li> </ul> </li> <li> <p>To work around this and collectively manage all these aspects within the cluster, one can use Kubernetes Ingress:</p> </li> <li>Allows users access via a single URL</li> <li>URL can be configured to route different services depending on the URL paths.</li> <li>SSL security may automatically be implemented via Ingress</li> <li> <p>Ingress can act as a layer 7 load balancer built-in to Kubernetes clusters</p> <ul> <li>Can be configured to act like a normal Kubernetes Object.</li> </ul> </li> <li> <p>Note: Even with Ingress in place, one still needs to expose the application via a NodePort or Load Balancer -&gt; this would be a 1-time configuration.</p> </li> <li> <p>Once exposed, all load balancing authenticaiton, SSL and URL routing configrations are manageable and viewable via an Ingress Cotnroller.</p> </li> <li> <p>Ingress controllers aren't set up by default in Kubernetes, example solutions that can be deployed include:</p> </li> <li>GCE</li> <li>NGINX</li> <li> <p>Traefik</p> </li> <li> <p>Load balancers aren't the only component of an Ingress controller, additionaly functionalities are available for monitoring the cluster for new Ingress resources or definitions.</p> </li> <li> <p>To create, write a definition file:</p> </li> </ul> <pre><code>apiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: nginx-ingress-controller\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: nginx-ingress\n  template:\n    metadata:\n      labels:\n        name: nginx-ingress\n    spec:\n      containers:\n      - name: nginx-ingress-controller\n        image: &lt;nginx ingress controller url&gt;\n        args:\n        - /nginx-ingress-controller\n        - --configmap=$(POD_NAMESPACE)/nginx-configuration\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - name: http\n          containerPort: 80\n        - name: https\n          containerPort: 443\n</code></pre> <ul> <li>Note: As working with Nginx, need to configure options such as log paths, SSL settings, etc.</li> <li> <p>To decouple this from the controller image, write a separate config map definition file to be referenced:</p> <ul> <li>Allows easier modification rather than editing one huge file.</li> </ul> </li> <li> <p>An ingress service definition file is also required to support external communicationL</p> </li> </ul> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-ingress\nspec:\n  type: NodePort\n  ports:\n  - port: 80\n    targetPort: 80\n    protocol: TCP\n    name: http\n  - port: 443\n    targetPort: 443\n    protocol: TCP\n    name: https\n  selector:\n    name: nginx-ingress\n</code></pre> <ul> <li> <p>The service NodePort definition above links the service to the deployment.</p> </li> <li> <p>As mentioned, Ingress controllers have additional functionality available for monitoring the cluster for ingress resources, and apply configurations when changes are made</p> </li> <li> <p>For the controller to do this, a service account must be associated with it:</p> </li> </ul> <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: nginx-ingress-serviceaccount\n</code></pre> <ul> <li> <p>The service account must have the correct roles and role-bindings to work.</p> </li> <li> <p>To summarise, for an ingress controller, the following resources are needed:</p> </li> <li>Deployment</li> <li>Service</li> <li>ConfigMap</li> <li> <p>ServiceAccount</p> </li> <li> <p>Once an ingress controller is in place, one can create ingress resources:</p> </li> <li> <p>Ingress resources are a set of rules and configurations applied to an ingress controller, linking it to other Kubernetes objects.</p> </li> <li> <p>For example, one could configure a rule to forward all traffic to one application, or to a different set of applications based on a URL.</p> </li> <li>Alternatively, could route based on DNS.</li> <li>As per, ingress resources are configured via a destination file</li> </ul> <pre><code>apiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: ingress-cluster\nspec:\n  backend:\n    serviceName: wear-service\n    servicePort: 80\n</code></pre> <ul> <li>Note: For a single backend like above, no additional rules are required.</li> <li> <p>The ingress resource can be created via standard means i.e. <code>kubectl create -f ....</code></p> </li> <li> <p>To view ingress resource: <code>kubectl get ingress</code></p> </li> <li>To route traffic in a conditional form, use ingress rules e.g. routing based on DNS</li> <li> <p>Within each rule, can configure additional paths to route to additional services or applications.</p> </li> <li> <p>To implement, adhere to the principles outlined in the following 2-service example:</p> </li> </ul> <pre><code>apiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: ingress-cluster\nspec:\n  rules:\n  - http:\n      paths:\n      - path: /wear\n        backend:\n            serviceName: wear-service\n            servicePort: 80\n      - path: /watch\n        backend:\n            serviceName: watch-service\n            servicePort: 80\n</code></pre> <ul> <li> <p>Create the ingress resource using <code>kubectl create -f ...</code> as per usual.</p> </li> <li> <p>To view the ingress's detailed information: <code>kubectl describe ingress &lt;ingress name&gt;</code></p> </li> <li> <p>Note: In the description, a default backend is described.  In the event a user enters a path not matching any of the rules, they will be redirected to that backend service (which must exist!).</p> </li> <li> <p>If wanting to split traffic via domain name, a definition file can be filled out as normal, but in the spec, the rules can be updated to point to specific hosts instead of paths:</p> </li> </ul> <pre><code>...\nrules:\n- host: &lt;url 1&gt;\n  http:\n    paths:\n    - backend:\n        serviceName: &lt;service name 1&gt;\n        servicePort: &lt;port 1&gt;\n- host: &lt;url 2&gt;\n  http:\n    paths:\n    - backend:\n        serviceName: &lt;service name 2&gt;\n        servicePort: &lt;port 2&gt;\n...\n</code></pre> <ul> <li>When splitting by URL, had 1 rule and split the traffic by 2 paths</li> <li> <p>When splitting by hostname, used 2 rules with a path for each.</p> </li> <li> <p>Note: If not specifying the host field, it'll assume it to be a <code>*</code> and / or accept all incoming traffic without matching the hostname</p> </li> <li>Acceptable for a single backend</li> </ul>"},{"location":"certifications/CKS/03_Cluster-Setup-and-Hardening.html#additional-reading-latest-kubernetes-versions","title":"Additional Reading - Latest Kubernetes Versions","text":"<ul> <li>in k8s version 1.20+ we can create an Ingress resource from the imperative way like this:-</li> <li>Format - <code>kubectl create ingress &lt;ingress-name&gt; --rule=\"host/path=service:port\"</code></li> <li>Example - <code>kubectl create ingress ingress-test --rule=\"wear.my-online-store.com/wear*=wear-service:80\"</code></li> <li>Find more information and examples in the below reference link:<ul> <li>https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#-em-ingress-em-</li> </ul> </li> <li>References:</li> <li>https://kubernetes.io/docs/concepts/services-networking/ingress</li> <li>https://kubernetes.io/docs/concepts/services-networking/ingress/#path-type</li> </ul>"},{"location":"certifications/CKS/03_Cluster-Setup-and-Hardening.html#ingress-annotations-and-rewrite-target","title":"Ingress - Annotations and Rewrite-Target","text":"<ul> <li>Different ingress controllers have different options that can be used to customise the way it works. NGINX Ingress controller has many options that can be seen here.</li> <li> <p>I would like to explain one such option that we will use in our labs. TheRewrite target option.</p> </li> <li> <p>Our watch app displays the video streaming webpage at <code>http://&lt;watch-service&gt;:&lt;port&gt;/</code></p> </li> <li>Our wear app displays the apparel webpage at <code>http://&lt;wear-service&gt;:&lt;port&gt;/</code></li> <li>We must configure Ingress to achieve the below. When user visits the URL on the left, his request should be forwarded internally to the URL on the right.</li> <li>Note that the <code>/watch</code> and <code>/wear</code> URL path are what we configure on the ingress controller so we can forwarded users to the appropriate application in the backend.</li> <li> <p>The applications don't have this URL/Path configured on them:  <code>http://&lt;ingress-service&gt;:&lt;ingress-port&gt;/watch -&gt; http://&lt;watch-service&gt;:&lt;port&gt;/</code> <code>http://&lt;ingress-service&gt;:&lt;ingress-port&gt;/wear -&gt; http://&lt;wear-service&gt;:&lt;port&gt;/</code></p> </li> <li> <p>Without the rewrite-target option, this is what would happen:</p> </li> <li><code>http://&lt;ingress-service&gt;:&lt;ingress-port&gt;/watch</code> -&gt; <code>http://&lt;watch-service&gt;:&lt;port&gt;/watch</code></li> <li><code>http://&lt;ingress-service&gt;:&lt;ingress-port&gt;/wear -&gt; http://&lt;wear-service&gt;:&lt;port&gt;/wear</code></li> <li>Notice watch and wear at the end of the target URLs. The target applications are not configured with /watch or /wear paths.</li> <li>They are different applications built specifically for their purpose, so they don't expect <code>/watch</code> or <code>/wear</code> in the URLs.<ul> <li>As such the requests would fail and throw a 404 not found error.</li> </ul> </li> <li>To fix that we want to \"ReWrite\" the URL when the request is passed on to the watch or wear applications.</li> <li>We don't want to pass in the same path that user typed in. So we specify the <code>rewrite-target</code> option.</li> <li>This rewrites the URL by replacing whatever is under <code>rules-&gt;http-&gt;paths-&gt;path</code> which happens to be <code>/pay</code> in this case with the value in rewrite-target. This works just like a search and replace function.</li> <li>For example: replace(path, rewrite-target)</li> <li>In our case: replace(<code>\"/path\"</code>,<code>\"/\"</code>)</li> </ul> <pre><code>apiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: test-ingress\n  namespace: critical-space\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /\nspec:\n  rules:\n  - http:\n    paths:\n    - path: /pay\n      backend:\n        serviceName: pay-service\n        servicePort: 8282\n</code></pre> <p>In another example given here, this could also be: <code>replace(\"/something(/|$)(.*)\", \"/$2\")</code></p> <pre><code>apiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /$2\n  name: rewrite\n  namespace: default\nspec:\n  rules:\n  - host: rewrite.bar.com\n    http:\n      paths:\n      - backend:\n          serviceName: http-svc\n          servicePort: 80\n        path: /something(/|$)(.*)\n</code></pre>"},{"location":"certifications/CKS/03_Cluster-Setup-and-Hardening.html#326-docker-service-configuration","title":"3.26 - Docker Service Configuration","text":"<ul> <li>Docker service can be managed via the systemctl command:</li> <li>Start - start the service</li> <li>Status - check the service status</li> <li>Stop - stop the service</li> <li>The Docker Daemon can be started manually via running dockerd</li> <li>Usually done for troubleshooting or debugging purposes</li> <li>Prints dockerd logs</li> <li>Additional logs printable via the <code>--debug</code> flag</li> <li>The docker daemon starts and listens on an internal unix socket at <code>/var/run/docker.sock</code></li> <li>Unix Socket = Inter-Process Communication (IPC) mechanism used for communication between different processes on the same host</li> <li>Implies the docker daemon is only intractable within the same host and docker CLI is only configured to interact with the docker daemon on this host.</li> <li>In the event you want to establish connection to the docker daemon from another host running the Docker CLI e.g. a Cloud VM running docker daemon.</li> <li>Not set up by default.</li> <li>One can instruct the daemon to listen on a TCP interface by adding a flag to the dockerd command: <code>--host=tcp://&lt;IP&gt;:2375</code></li> <li>The host at IP can now interact with the Docker Daemon using the Docker CLI, first by setting the environment variable <code>DOCKER_HOST=\"tcp://&lt;IP&gt;:2375\"</code></li> <li>Note: This is disabled by default - For good reason! This is because by making the API server available on the internet, anyone can create and manage containers on the Daemon host - no security is set up by default.</li> <li>Docker Daemon sets unencrypted communication by default, to set:<ul> <li>Create a pair of TLS certificates</li> <li>Add the following flags to the dockerd command:</li> <li><code>--tls=true</code></li> <li><code>--tlscert=/var/docker/server.pem</code> (path to certificate)</li> <li><code>--tlskey=/var/docker/serverkey.pem</code>(path to private key)</li> </ul> </li> <li>With TLS enabled, the standard port becomes 2376 - encrypted traffic; 2375 remains for unencrypted traffic.</li> <li>All the options specified in this section can be added to a configuration file for ease of use; typically located at <code>/etc/docker/daemon.json</code></li> <li>This must be created if it doesn't already exist; it's not included by default</li> <li>Note: Hosts is an array i.e. <code>[\"host 1\", \"host 2\", .... ]</code></li> <li>This configuration can be referenced when using the systemctl utility to start the Docker server.</li> </ul>"},{"location":"certifications/CKS/03_Cluster-Setup-and-Hardening.html#327-securing-the-docker-daemon","title":"3.27 - Securing the Docker Daemon","text":"<ul> <li>The Docker Daemon / Engine / API Service needs to be secured appropriately, otherwise unauthorized users could:</li> <li>Delete existing containers hosting applications</li> <li>Delete volumes storing data</li> <li>Run containers of their own e.g. bitcoin mining</li> <li>Gain root access to the host system via a privileged container<ul> <li>Can lead to targeting of other systems in the infrastructure</li> </ul> </li> <li>Reminder:</li> <li>The Docker Daemon is by default exposed on the host only on a UNIX socket at <code>/var/run/docker.sock</code></li> <li>No one outside of the host can access the docker daemon by default</li> <li>Note: The applications can still be accessed so long as the ports are published</li> <li>First Area of Security Consideration - Host:</li> <li>Actions that can be taken include:<ul> <li>Disable Password-based authentication</li> <li>Enable SSH key-based authentication</li> <li>Determine which users need access to the servers</li> <li>Disabling unused ports</li> </ul> </li> <li>External access needs to be configured only if absolutely necessary - achievable by adding to the \"hosts\" array in the docker daemon json configuration file.<ul> <li>Any hosts added must be private and only accessible within your organisation</li> <li>TLS Certificates must also be configured for the server in this scenario, requiring:</li> <li>A certificate authority</li> <li>A TLS certificate</li> <li>A TLS Key</li> <li>TLS Configuration requires setting the port value for hosts in the config file to 2376 for encrypted TLS communication; as well as tls = true.</li> <li>On the host accessing the docker daemon, the following must also be set as environment variables:</li> <li><code>DOCKER_TLS=true</code></li> <li><code>DOCKER_HOST=\"tcp://&lt;IP&gt;:2376\"</code></li> </ul> </li> <li>The configurations so far are acceptable but no authentication is enforced. To do so, certificate-based authentication can be enabled by copying the certificate authority to the docker daemon server.</li> <li>The docker daemon config file can have the following added:<ul> <li><code>tlsverify = true</code> - enforces the need for clients to authenticate</li> <li><code>tlsecacert = \"path/to/cacert.pem\"</code></li> </ul> </li> <li>To authenticate, clients need to have their own certificates signed by the CA, generated to give client.pem and clientkey.pem</li> <li>Sharing these with the cacert on the host</li> <li>On the client side, these can be passed via <code>--tlscert</code>, <code>--tlskey</code>, <code>--tlscacert</code> flags or added to the <code>./docker</code> folder on the system.</li> <li>Summary:</li> <li>TLS alone only enforces encryption, TLS verify enforces authentication</li> </ul>"},{"location":"certifications/CKS/04_System-Hardening.html","title":"4.0 - System Hardening","text":""},{"location":"certifications/CKS/04_System-Hardening.html#41-the-principle-of-least-privilege","title":"4.1 - The Principle of Least Privilege","text":"<ul> <li>Airport Analogy</li> <li>Consider an airport passenger, what procedure must they go through?<ul> <li>Baggage drop-off and ticket validation</li> <li>Travel document verification</li> <li>Security / Carry on check</li> <li>Move to a specific departure gate; users can access other areas during this time, but some restrictions still apply.</li> </ul> </li> <li>This suggests that in security management for a system, there are multiple entities involved; each with their own permissions and privileges</li> <li>Considering the privileges of each of the entities:<ul> <li>Traveller - Unrestricted access to public spaces in the airport after check-in and the designated gate</li> <li>Baggage - Has access to the travellers documents and the airlines information</li> <li>Security officers - can inspect the belongings of travellers in the area</li> <li>Store employees - public and restricted areas</li> <li>Boarding gate - access to the documents</li> <li>Cleaners and cargo workers - access to public and certain restricted areas</li> <li>Pilots / Flight crew - complete aircraft access and certain areas of the airport</li> </ul> </li> <li>They have access only to the plane(s) of their airlines</li> <li>In this case, each entity has the least / minimum amount of privileges required to perform their role.</li> <li>This principle is applicable to computer systems and K8s clusters.</li> <li>Kubernetes cluster security, via the least privilege principle, can be enforced via multiple options including:</li> </ul>"},{"location":"certifications/CKS/04_System-Hardening.html#42-minimize-host-os-footprint-intro","title":"4.2 - Minimize Host OS Footprint Intro","text":"<ul> <li>Reducing the attack service</li> <li>Achievable by keeping all systems in the cluster in a simple and consistent state</li> <li>Any inconsistent or weak nodes could be attacked<ul> <li>Consistency can be enforced by Least Privilege Principle</li> </ul> </li> </ul>"},{"location":"certifications/CKS/04_System-Hardening.html#43-limit-node-access","title":"4.3 - Limit Node Access","text":"<ul> <li>As standard practice, exposure to the internet of the control plane and nodes should be limited.</li> <li>For self-hosted clusters or managed K8s clusters, this can be achieved via a private network; access can then be achieved by a VPN</li> <li>If running in the cloud, control plane access may be unlikely</li> <li>If implementing a VPN solution is impossible, one can enable authorized networks via the infrastructure firewall</li> <li>Allows/denies access from particular IP addresses</li> <li>Note: Attacks may not come from external sources, entities with network access within the cluster may allow internal attacks.</li> <li>One must therefore consider restricting access within the cluster.</li> <li>Example - SSH Access:<ul> <li>Administrators require it</li> <li>Developers do not require it usually and therefore they should be restricted.</li> <li>End users shouldn't have access too.</li> </ul> </li> <li>Account Management - There are 4 types of accounts:</li> <li>User accounts - Any individuals needing access to the system e.g. developers system administrators</li> <li>Superuser account - Root Account, has complete access and control over the system</li> <li>System Accounts - Created during the system development, used by software such as SSH and Mail</li> <li>Service Accounts - Similar to system accounts, created when services are installed on Linux e.g. Nginx</li> <li>Viewing user details - Commands:</li> <li>Id - provides details regarding user and group id</li> <li>Who - lists who is currently logged in</li> <li>Last - last time users logged into the system</li> <li>Access control files:</li> <li>All stored under <code>/etc/</code> folder<ul> <li><code>/passwd</code> - contains basic information about system users, including user id and default directory</li> <li><code>/shadow</code> - contains the passwords for users, stored in hash format</li> <li><code>/group</code> - stores information about all user groups in the system</li> </ul> </li> <li>The above commands and configuration files should be used to investigate user permissions and access; limiting as appropriate based on least privilege principle.</li> <li>Disable user account by setting default shell to a nologin shell: <code>usermod -s /bin/nologin &lt;username&gt;</code></li> <li>Delete user - <code>userdel &lt;username&gt;</code></li> <li>Delete user and remove from group - <code>userdel &lt;username&gt; &lt;groupname&gt;</code></li> </ul>"},{"location":"certifications/CKS/04_System-Hardening.html#44-ssh-hardening","title":"4.4 - SSH Hardening","text":"<ul> <li>SSH used for logging into and executing commands for remote servers</li> <li>General access via any of the following commands:</li> <li><code>ssh &lt;hostname or IP address&gt;</code></li> <li><code>ssh &lt;user&gt;@&lt;hostname or IP address&gt;</code></li> <li><code>ssh -l &lt;user&gt; &lt;hostname or IP address&gt;</code></li> <li>Remote server must have an SSH service running and port 22 opened for communication</li> <li>Additionally, a valid username and password for the user should be created on the remote server; or an SSH key.</li> <li>To maximise security for access:</li> <li>Generate key pair (public and private key) on the client system</li> <li>Public key shared with remote server</li> <li>Creation: <code>ssh-keygen -t rsa</code><ul> <li>Enter passphrase = optional, but enhances security</li> <li>Public key and private key stored at <code>/home/user/.ssh/id_rsa.pub</code> and <code>/home/user/.ssh/id_rsa respectively</code></li> </ul> </li> <li>Copy public key to remote server: <code>ssh-copy-id &lt;username&gt;@&lt;hostname&gt;</code><ul> <li>Password required for authentication on the server; shouldn't be required after key copying</li> </ul> </li> <li>On the remote server, the keys should be stored at <code>/home/user/.ssh/authorized_keys</code></li> <li>Hardening the SSH service:</li> <li>On the remote server; can configure the SSH service to enhance security.</li> <li>Possible actions:<ul> <li>Disable SSH for root account - no one can log into the system via the root account, only their personal user or system accounts</li> </ul> </li> <li>In line with the principle of least privilege</li> <li>Requires updating of the ssh config file, located at <code>/etc/ssh/sshd_config</code> -&gt; set PermitRootLogin to no</li> <li>Could also disable password-based services if using SSH key based authentication -&gt; set <code>PasswordAuthentication</code> to no</li> <li>Once changes are applied, restart the service: systemctl restart sshd</li> <li>Additional information available via section 5.2 of the CIS Benchmarks https://www.cisecurity.org/cis-benchmarks/</li> </ul>"},{"location":"certifications/CKS/04_System-Hardening.html#45-privilege-escalation-in-linux","title":"4.5 - Privilege Escalation in Linux","text":"<ul> <li>Following disabling of root user access via ssh, need to consider when users may need root / administrative access credentials; this is provided by the <code>sudo</code> functionality (prefixed before any command)</li> <li>Default configuration for sudo is under <code>/etc/sudoers</code>; it can only be edited via <code>visudo</code></li> <li>The sudoers file defines user privileges for <code>sudo</code></li> </ul> <ul> <li>Administrators can give granular permissions</li> <li>User mark has all permissions</li> <li>User sarah has sudo permissions only for restarting the system</li> <li>Any sudo commands ran are executed in the users local shell rather than root shell -&gt; can eliminate the need for ever logging in as a root user directly</li> <li>This can be done by editing the <code>/etc/passwd</code> for root user like below</li> </ul> <ul> <li>Sudoers syntax</li> <li>Anything beginning with <code># or $</code> is a comment</li> <li>Content split into fields:<ul> <li>User or group the privilege(s) are to be granted to,</li> <li>Groups are prefixed with <code>%</code></li> <li>Host the user can make use of the privileges on</li> <li>By default, set to all, applies to localhost in most cases</li> <li>Users and Groups the user in field 1 can run the command as</li> <li>Command(s) runnable by the user(s) via <code>sudo</code></li> </ul> </li> </ul>"},{"location":"certifications/CKS/04_System-Hardening.html#46-remove-obsolete-packages-and-services","title":"4.6 - Remove Obsolete Packages and Services","text":"<ul> <li>In general, systems should be kept as lean as possible, making sure that only the necessary software is installed and they are kept updated to address security fixes</li> <li>This can be applied for standalone systems, Kubernetes systems, etc.</li> <li>Any software that isn't used should be uninstalled for general reasons including:</li> <li>Increased complexity of the system - one more package to maintain</li> <li>It may remain unused and take up unwanted space</li> <li>If not maintained, additional security vulnerabilities may arise that could be taken advantage of</li> <li>Removing services:</li> <li>Services start applications upon Linux system booting</li> <li>Managed by the Systemd tool and the systemctl utility</li> <li>E.g. <code>systemctl status &lt;service&gt;</code><ul> <li>Provides details regarding the service, including where its configuration file(s) are located</li> </ul> </li> <li>As with packages, only necessary services should be kept running or available on the system<ul> <li>To view the system services: systemctl list-units --type service</li> <li>To remove a service: <code>systemctl stop &lt;service&gt;</code></li> </ul> </li> <li>Once services for unwanted packages are stopped, one can remove the associated package(s) via <code>apt remove &lt;package name&gt;</code></li> <li>Additional information available in section 2 of the CIS Benchmarks.</li> </ul>"},{"location":"certifications/CKS/04_System-Hardening.html#47-restrict-kernel-modules","title":"4.7 - Restrict Kernel Modules","text":"<ul> <li>Linux kernel modules has a modular design - its functionality can be extended via the addition of extra modules</li> <li>Example - Hardware support for video drivers via graphics cards</li> <li>Modules can be added manually - <code>modprobe &lt;module name&gt;</code></li> <li>List modules in the Kernel - lsmod</li> <li>Note: When kubernetes workloads are running, non-root processes on pods may cause network-related modules to be installed on the Kernel - This can lead to vulnerabilities exploited by attackers.</li> <li>To mitigate, modules on cluster nodes can be blacklisted - achievable by creating a <code>.conf</code> file under <code>/etc/modprobe.d</code></li> <li>Any modules to be blacklisted should be included as an entry via: <code>blacklist &lt;modulename&gt;</code></li> <li>Upon entry addition, the system should be restarted and tests should be ran to see if the module is still running: <code>shutdown -r now</code> <code>lsmod | grep &lt;modulename&gt;</code></li> <li>Example modules that should be blacklisted - <code>sctp</code> and <code>dccp</code></li> <li>Additional information available in section 3.4 of the CIS Benchmarks</li> </ul>"},{"location":"certifications/CKS/04_System-Hardening.html#48-identify-and-disable-open-ports","title":"4.8 - Identify and Disable Open Ports","text":"<ul> <li>Several processes and services bind themselves to a network port on the system - an addressable location in the OS to allow for segregation of network traffic.</li> <li>Example - TCP port 22 is only used for SSH processes</li> <li>To understand if a port is in used and listening for a connection request:</li> <li><code>natstat -an | grep -w LISTEN</code></li> <li>To understand what each service or port is being used for, can check in the services file under <code>/etc</code></li> <li>Example: <code>cat /etc/services | grep -w 53</code></li> <li>This begs the question, what ports should be open for nodes on the Kubenetes cluster? Answer: Review the documentation!</li> <li>https://kubernetes/doc.setup/production-environment/tools/production-environment/tools/kubeadm/install-kubeadm/#check-required-ports</li> </ul>"},{"location":"certifications/CKS/04_System-Hardening.html#49-minimize-iam-roles","title":"4.9 - Minimize IAM Roles","text":"<ul> <li>As discussed in the section Limit Node Access, it's never a good idea to use the Root account to perform daily tasks, SSH hardening can prevent root access and force users to using their own user accounts.</li> <li>Root accounts are equivalent to Admin accounts in Windows and Root Accounts in public cloud platforms e.g. AWS</li> <li>Note: AWS used as an example for this section but topics apply</li> <li>1st account (Root account) created - User can log into management credentials<ul> <li>Any and all functions can be carried out by this account on the management account</li> </ul> </li> <li>Root account, in line with the least privilege principle, should be used to create new users and assign them the appropriate permissions</li> <li>The credentials for the root account should be saved and secured as appropriate</li> <li>When a new user is created, the least privilege is assigned depending on the associated IAM (Identity Access Management) policy.</li> <li>E.g. developers have ability to create EC2 instances, but can only view the S3 Buckets</li> <li>For further ease of assignment, users could be added to particular role groups or IAM groups - an IAM policy can then be attached to the group and assigned automatically.</li> <li>For resources and services, by default, no permissions are allowed. This cannot be achieved via IAM policies, roles must be developed.</li> <li>Allows secure access to an AWS Resource(s)</li> <li>Example - Allow EC2 Instance Access to an S3 Bucket.</li> <li>Note: Programmatic access can be configured, but this is typically less secure than via IAM methods.</li> </ul>"},{"location":"certifications/CKS/04_System-Hardening.html#410-minimize-external-access-to-the-network","title":"4.10 - Minimize External Access to the Network","text":"<ul> <li>To view the ports that are open and listening for requests on your system, utilize the netstat command: netstat -an | grep -w LISTEN</li> <li>By default, many of these ports can accept connections from any other device on the network - undesirable via the principle of least privilege</li> <li>In production environments, with multiple clients and servers involved, it's beyond important to implement network security to only allow or restrict access to various services</li> <li>Security can be provided via network-wide or external firewalls e.g.</li> <li>Cisco ASA</li> <li>Juniper NGFW</li> <li>Barracuda NGFW</li> <li>Fortinet</li> <li>Alternatively, firewalls can be applied per server.</li> </ul>"},{"location":"certifications/CKS/04_System-Hardening.html#411-ufw-firewall-basics","title":"4.11 - UFW Firewall Basics","text":"<ul> <li>UFW = Uncomplicated Firewall</li> <li>Considering an application setup where one needs to restrict access to an Ubuntu Server - call it app01</li> <li>The server should only accept SSH connections on Port 22 from a particular IP address</li> <li>Additionally, the server runs a web server on http port 80</li> <li>All other users in a particular IP range should be allowed</li> <li>Any other unused ports should be closed</li> <li>To help with this, can use the netfilter CLI tool and IPTables</li> <li>IPTables - Most common for Linux, but requires a steep learning curve</li> <li>UFW = Alternative that provides a frontend for IPTables</li> <li>On the machine: <code>netstat -an | grep -w LISTEN</code></li> <li>The UFW tool can be installed via the following:</li> <li><code>apt-get update</code></li> <li><code>apt-get install ufw</code></li> <li><code>systemctl enable ufw</code></li> <li><code>systemctl start ufw</code></li> <li>UFW Rules:</li> <li>Check the UFW Status: ufw status</li> <li>In example, want to allow 80 and 22 from particular sources, but no outgoing ports need closing:<ul> <li>Allow outgoing by default:</li> <li><code>ufw allow outgoing</code></li> <li>Deny ingress by default (before making exceptions)</li> <li><code>ufw default deny incoming</code></li> <li>Allow from particular IP address on particular port e.g. SSH 22</li> <li><code>ufw allow from 172.16.238.5 to any port 22 proto tcp</code><ul> <li>Allows inbound connections from the IP address defined to port 22 on any reachable IP address on app01</li> </ul> </li> <li><code>ufw allow from 172.16.238.5 to any port 80 proto tcp</code><ul> <li>Allows connections on port 80 from IP address</li> </ul> </li> <li><code>ufw allow from 172.16.100.0/28 to any port 80 proto tcp</code><ul> <li>Allows access on port 80 from IPs in any range</li> </ul> </li> <li><code>ufw deny 8080</code><ul> <li>Denies all access on port 8080 (not needed if denying access by default)</li> </ul> </li> <li>To enable: <code>ufw enable</code></li> <li><code>ufw status</code> to check implementation</li> </ul> </li> <li>To delete rule: <code>ufw delete &lt;rule&gt; or &lt;rule number&gt;</code></li> </ul>"},{"location":"certifications/CKS/04_System-Hardening.html#412-linux-syscalls","title":"4.12 - Linux Syscalls","text":"<ul> <li>To understand how Syscalls are made, need to understand how a process runs:</li> <li>Kernel - Major OS component that interfaces between hardware and processes</li> <li>Kernel can be split into Kernel and User Spaces<ul> <li>Kernel Space - OS-related applications and software e.g. drivers</li> <li>User Space - Where applications are ran</li> </ul> </li> <li>When a program runs in the user space, suppose one that wants to write data to a file:<ul> <li>Applications make system calls to the Kernel Space for tasks e.g.:</li> <li>Open a file</li> <li>Write to a file</li> <li>Define a variable</li> <li>open()</li> <li>close()</li> <li>Tracing Syscalls:</li> <li><code>which strace</code>:<ul> <li>Installed by default</li> <li>Used to inspect / provide output of the syscalls made when running a command</li> </ul> </li> <li>Output presented in form of:<ul> <li><code>syscall(&lt;path to executable file&gt;, [arg 1, arg 2, ...], ...)</code></li> </ul> </li> <li>To trace the syscalls of a running process:<ul> <li>Get process ID (PID) - <code>pid &lt;process&gt;</code></li> <li>Get Syscalls: <code>strace -p PID</code></li> </ul> </li> <li>Note: the <code>-c</code> flag can be used for more detailed outputs.</li> </ul> </li> </ul>"},{"location":"certifications/CKS/04_System-Hardening.html#413-aquasec-tracee","title":"4.13 - AquaSec Tracee","text":"<ul> <li>Open-source tool used for tracing container syscalls</li> <li>Makes use of EBPF to trace the system at runtime without interfering with the Kernel:</li> <li>extended Berkeley Packet filter</li> <li>Tracee can be easily ran as a container:</li> <li>Prerequisites:</li> <li>As ran as a container, it runs the EBFP program and stores it at /tmp/tracee by default</li> <li> <p>To ensure built once and usable for successive runs, a set number of bind mounts are required:</p> Mount Purpose <code>/tmp/tracee</code> Default Workspace <code>/lib/modules</code> Kernel Header Access <code>/usr/src</code> Kernel Header Access </li> <li> <p>Addtionally, due to tracing syscalls, Tracee requires privileged capabilities:</p> </li> <li> <p>Using Tracee:</p> </li> <li> <p>To observe syscalls generated by a single command: </p> <p><code>shell docker run --name tracee --rm --privileged --pid=host \\ -v /lib/modules/:/lib/modules/:ro -v /usr/src:/usr/src:ro \\ -v /tmp/tracee:/tmp/tracee aqasec/tracee:0.4.0 --trace comm=ls</code></p> </li> <li> <p>Returns syscalls made during the run of the ls command</p> </li> <li> <p>To observe syscalls for all new processes on the host:</p> <p><code>shell docker run --name tracee --rm --privileged --pid=host \\ -v /lib/modules/:/lib/modules/:ro -v /usr/src:/usr/src:ro \\ -v /tmp/tracee:/tmp/tracee aqasec/tracee:0.4.0 --trace pid=new</code></p> </li> <li> <p>For new containers:</p> <p><code>shell docker run --name tracee --rm --privileged --pid=host \\ -v /lib/modules/:/lib/modules/:ro -v /usr/src:/usr/src:ro \\ -v /tmp/tracee:/tmp/tracee aqasec/tracee:0.4.0 --trace container=new</code></p> </li> </ul>"},{"location":"certifications/CKS/04_System-Hardening.html#414-restricting-syscalls-using-seccomp","title":"4.14 - Restricting Syscalls Using Seccomp","text":"<ul> <li>Previously seen how to view syscalls in an os.</li> <li>In practice, ~435 syscalls are available in Linux, all can be used by applications in the user space.</li> <li>In reality, applications wouldn't need to use anywhere near as many syscalls - application should only be able to use the required syscalls for their application</li> <li>By default, the Linux Kernel allows any syscall to be invoked by programs in the user space - increasing attack surface</li> <li>To resolve, can utilise seccomp</li> <li>Secure Computing - A linux kernel-native feature designed to constrain applications to only make the required syscalls</li> <li>To check if host supports it, look in the boot config file: <code>grep -i seccomp /boot/config-$(uname -r)</code></li> <li>If <code>CONFIG_SECCOMP = y</code> -&gt; seccomp is supported</li> <li>To demonstrate how applications can be finetuned, consider running a container e.g. docker/whalesay:</li> <li><code>docker run docker/whalesay cowsay hello!</code></li> <li>Knowing the app works - can run the container and exec into it: <code>docker run -it --rm docker/whalesay /bin/sh</code><ul> <li>If users wanted to change the time for example, it would not be allowed</li> </ul> </li> <li>Process running is <code>shell/bin/sh</code></li> <li>Inspecting the process id -&gt; PID = 1</li> <li>Inspecting the PID can check via seccomp:</li> <li>Grep seccomp <code>/prc/1/status</code></li> <li>If value = 2 -&gt; seccomp enabled:</li> <li>Modes:<ul> <li>0 = Disabled</li> <li>1 = Strict -&gt; Blocks all syscalls except read, write, exit, rt_sigreturn</li> <li>2 =&gt; Selectively filters syscalls</li> <li>How was the seccomp filter applied? What restrictions has it got applied?</li> </ul> </li> <li>Docker has seccomp enabled by default and restricts ~60 of the 435 syscalls</li> <li>Defined by json file(s)</li> <li>Defines architectures for files</li> <li>Syscall arrays -&gt; names and actions (allow or deny)</li> <li>Default action -&gt; what to do with syscalls not defined in the syscall array</li> <li>Json files can act as whitelists or blacklists</li> <li>Whitelists can be very restrictive as any which you do want to run have to be added</li> <li>Blacklists -&gt; allows all by default bar any in the array; more open than whitelists but more susceptible to attacks<ul> <li>Default docker seccomp profiles block calls such as reboot, mount and unmount, clock time managements</li> <li>Default seccomp profiles are good but custom files are better for particular scenarios</li> </ul> </li> <li>To utilise a custom seccomp profile, can add associated flag to the <code>docker run</code> command i.e.:  <code>docker run -it --rm --security-opt seccomp=/path/to/file.json docker/whalesay /bin/sh</code></li> <li>Seccomp can be disabled for containers completely by setting the flag to <code>--security-opt seccomp=unconfined</code><ul> <li>SHOULD NEVER BE USED</li> </ul> </li> </ul>"},{"location":"certifications/CKS/04_System-Hardening.html#415-implementing-seccomp-in-kubernetes","title":"4.15 - Implementing Seccomp in Kubernetes","text":"<ul> <li>By default, Docker supplies its own default seccomp profile in containers in mode 2</li> <li>When running in kubernetes, the number of blocked syscalls and seccomp enablement may be different -&gt; seccomp may not be enabled by default in Kubernetes (as of v1.20)</li> <li>To implement seccomp - use a pod definition file, to apply, need to add an appropriate field in the securityContext area:</li> </ul> <p><code>yaml   securityContext:     seccompProfile:       type: RuntimeDefault</code></p> <ul> <li> <p>Note: When adding the profile in this manner, it is advised to add to the container's securityContext field a disabling of privilege escalation i.e.:</p> <p><code>yaml containers: - securityContext:     allowPrivilegeEscalation: false</code></p> </li> <li> <p>This helps the application run ONLY with the Syscalls it requires for the process</p> </li> <li>Note: Type can be set to Unconfined, but this is not recommended and is the default profile</li> <li>For custom profiles in the pod security context:</li> </ul> <p><code>yaml   securityContext:     seccompprofile:       type: Localhost       localhostProfile: &lt;path to file&gt;</code></p> <ul> <li>Note: The profile path must be relative to the default seccomp file in <code>/var/lib/kubelet/seccomp</code></li> <li>Example:</li> <li>Creating an audit seccomp profile with <code>defaultAction SCMP_ACT_LOG</code></li> <li>Syscalls from the container will be stored in <code>/var/log/syscall</code><ul> <li>Logs user id, process ID, etc about the processes used</li> </ul> </li> <li>To help map the syscall numbers to syscalls is to check the following: <code>grep -w 35 /usr/include/asm/unistd_64.sh</code></li> <li>Where 35 can be replaced with the PID</li> <li>Tracee can also be used for this as discussed previously</li> <li>Consider another seccomp profile that rejects by default (<code>defaultAction = SCMP_ACT_ERRNO</code>) =&gt; any pod created with this cannot run as all Syscalls are blocked by default</li> <li>Once the Syscalls are analysed and audited, users can identify the allowed syscalls and add them to a custom seccomp profile =&gt; recommended to block all by default and allow only ones needed.</li> <li>Note: A custom seccomp profile won't be required to be created from scratch, but a template one may need to be attached to a pod/deployment etc.</li> </ul>"},{"location":"certifications/CKS/04_System-Hardening.html#416-apparmor","title":"4.16 - AppArmor","text":"<ul> <li>Although Seccomp works well to restrict the syscalls a container makes (or pods/objects in Kubernetes), it cannot be used to restrict a programs access to objects like files and directories.</li> <li>Apparmor = Linux Security module, used to confine a program to a limited set of resources</li> <li>Installed by default for most distributions</li> <li>Use systemctl status apparmor to check status</li> <li>To use, the associated Kernel module must first be loaded on all nodes where containers can run<ul> <li>Check enabled file under: <code>/sys/module/apparmor/parameters/enabled</code>  Shows Y or N depending on enablement</li> </ul> </li> <li>Similar to Seccomp, apparmor is applied via profiles, which must be loaded into the Kernel; checkable via <code>/sys/kernel/security/apparmor/profiles</code></li> <li> <p>Profiles are simple text files that determines what capabilities are allowed or restricted</p> </li> <li> <p>Example:</p> </li> </ul> <p></p> <ul> <li>Above contains two rules:</li> <li>File -&gt; allow file system -&gt; allows complete access to filesystem</li> <li> <p>Deny rule -&gt; denies write access to all files under root system including subdirectories</p> </li> <li> <p>Example 2: Deny Write Access to Only Files Under a Particular Subdirectory</p> </li> </ul> <p></p> <ul> <li>Apparmor profiles can be created via various tools</li> <li>The status of the profiles loaded can be checked using aa-status</li> <li>Profiles can be loaded in 3 modes:</li> <li>Enforce - Apparmor will enforce the rules on any application that fits the profile</li> <li>Complain - Apparmor will allow the actions without restrictions but the actions are logged as events</li> <li>Unconfined - Any tasks are allowed and no logging occurs</li> </ul>"},{"location":"certifications/CKS/04_System-Hardening.html#417-creating-apparmor-profiles","title":"4.17 - Creating AppArmor Profiles","text":"<ul> <li>Example Script used in lecture - creates a directory and writes a file to said directory, the event of creation is logged in \"create.log\"</li> </ul> <ul> <li>Tools provided by apparmor can be used to facilitate the creation of apparmor profiles</li> <li>To install the utilities for apparmor: <code>apt-get install -y apparmor-utils</code></li> <li>Once done, can generate a profile via: <code>aa-genprof /path/to/script.sh</code></li> <li>This prepares apparmor to develop an accurate profile by scanning the system during the application's runnin</li> <li>Once the application has run in a separate window, in the original window, enter \"S\" to scan the system for AppArmor events<ul> <li>This will generate a series of questions that the user must answer to help develop the profile</li> </ul> </li> <li>E.g. for the example above, need to think about allowing the script access to the mkdir command or the tee command</li> <li>Each event is reviewed on severity level of 1-10 (10 = most severe)</li> <li>Resultant actions can be allowed/denied/inherited etc as required</li> <li>The new profile can be viewed and validated by <code>aa-status</code></li> <li>The profiles are stored under <code>/etc/apparmor.d/</code></li> <li>It's often advised to rerun the application with slightly different parameters to check the enforcement of the apparmor profile.</li> <li>To load a profile:</li> <li><code>apparmor_parser path/to/profile</code><ul> <li>No output = success</li> </ul> </li> <li>Disable profile:</li> <li><code>apparmor_parser -R path/to/profile</code><ul> <li>No output = success</li> </ul> </li> <li>Need to create a symlink to the profile and the apparmor disable directory:<ul> <li><code>ln -s /path/to/profile /etc/apparmor.d/disable/</code></li> </ul> </li> </ul>"},{"location":"certifications/CKS/04_System-Hardening.html#418-apparmor-in-kubernetes","title":"4.18 - AppArmor in Kubernetes","text":"<ul> <li>AppArmor can also be used to restrict the functions of containers orchestrated by Kubernetes</li> <li>At the time of writing / when the recording was done, Kubernetes support for it was in beta</li> <li>Prerquisites:</li> <li>The AppArmor Kernel module must be enabled on all nodes in the K8s cluster where pods are expected to run</li> <li>AppArmor profiles must be loaded in the Kernel</li> <li>The container runtime should support AppArmor - Generally expected, Docker know to support.</li> <li>Example:</li> </ul> <ul> <li>Ensure that the profile is loaded on all the nodes with aa-status if the profile is listed in the required director</li> <li>As the support is only in beta, need to link it by annotations:</li> </ul> <p><code>yaml   ...   metadata:     annotations: container.apparmor.security.beta.kubernetes.io/&lt;container_name&gt;:localhost/&lt;profile name&gt;   ...</code></p> <ul> <li>When the pod is created, you can inspect and test the pods capabilities to test the profile enforcement.</li> </ul>"},{"location":"certifications/CKS/04_System-Hardening.html#419-linux-capabilities","title":"4.19 - Linux Capabilities","text":"<ul> <li>Previously seen that even when running a container with an unconfined seccomp profile, it couldn't be manipulated</li> <li>The same is applicable to Kubernetes even though it doesn't use seccomp by default</li> <li>To understand this, need to review how processes run in Linux:</li> <li>Processes are separated into privileged and unprivileged (for kernel versions less than 2.2)</li> <li>For 2.2 onwards, processes are split into capabilities:<ul> <li>Privileged processes possess a number of capabilities e.g.:</li> <li><code>CAP_CHOWN</code></li> <li><code>CAP_NETMODE</code></li> <li><code>CAP_SYS_BOOT</code> - Allows reboots</li> </ul> </li> <li>To check the required capabilities for a command:<ul> <li>getcap /usr/bin/ping for example</li> </ul> </li> <li>For a process:<ul> <li>Get the PID: <code>ps -ef | grep /usr/sbin/sshd | grep -v grep</code></li> <li><code>getpcaps &lt;PID&gt;</code></li> </ul> </li> <li>Pivoting back to the pod in the example, the reason the certain commands like the change time couldn't work, the operation wasn't permitted</li> <li>Even if ran as a root user, the container is only started with limited capabilities (14 if Docker = runtime)</li> <li>Capabilities for a container can be managed at the container's security context:</li> </ul> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: ubuntu-sleeper\nspec:\n  containers:\n  - name: ubuntu-sleeper\n    image: ubuntu\n    command: [\"sleep\", \"1000\"]\n    securityContext:\n      capabilities:\n        add: [\"SYS_TIME\"]\n</code></pre> <ul> <li>To drop a capability, add drop: <code>[\"Capability 1\", \"Capability 2\", ...]</code></li> </ul>"},{"location":"certifications/CKS/05_Minimize-Microservice-Vulnerabilities.html","title":"5.0 - Minimize Microservice Vulnerabilities","text":""},{"location":"certifications/CKS/05_Minimize-Microservice-Vulnerabilities.html#51-security-contexts","title":"5.1 - Security Contexts","text":"<ul> <li>When running docker containers, can specify security standards such as the ID of the user to run the container</li> <li>The same security standards can be applied to pods and their associated containers</li> <li>Configurations applied a pod level will apply to all containers within</li> <li>Any container-level security will override pod-level security</li> <li>To add security contexts, add securityContext to either or both the POD and Container specs; where user IDs and capabilities can be set</li> <li>Note: Capabilities are only supported at container level</li> </ul> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: web-pod\nspec:\n  containers:\n  - name: ubuntu\n    image: ubuntu\n    command: [\"sleep\", \"3600\"]\n    securityContext:\n      runAsUser: 1000\n      capabbilities:\n        add: [\"MAC_ADMIN\"]\n</code></pre>"},{"location":"certifications/CKS/05_Minimize-Microservice-Vulnerabilities.html#52-admission-controllers","title":"5.2 - Admission Controllers","text":"<ul> <li>Commands are typically ran using the kubectl utility, these commands are sent to the kubeapi server and applied accordingly</li> <li>To determine the validity of the command, it goes through an authentication process via certificates</li> <li>Any kubectl commands come from users with a kubeconfig file containing the certificates required</li> <li>The determination of whether the process has permission to carry the task out is handled by RBAC authorization<ul> <li>Kubernetes Roles are used to support this.</li> </ul> </li> <li>With RBAC, restrictions can be placed on resources for:</li> <li>Operation-wide restrictions</li> <li>Specific operation restrictions e.g. create pod of specific names</li> <li>Namespace-scoped restrictions</li> <li>What happens if you want to go beyond this? For example:</li> <li>Allow images from a specific registry</li> <li>Only run as a particular user</li> <li>Allow specific capabilities</li> <li>Constrain the metadata to include specific information</li> <li>The above is handled by Admission controllers</li> <li>Various pre-built admission controllers come with K8s:<ul> <li>AlwaysPullImages</li> <li>DefaultStorageClass</li> <li>EventRateLimit</li> <li>NamespaceExists</li> </ul> </li> <li>Example - NamespaceExists:</li> <li>If creating a pod in a namespace that doesn't exist:<ul> <li>The request is authenticated and authorized</li> <li>The request is then denied as the admission controller acknowledges that the namespace doesn't exist -&gt; Request is denied</li> </ul> </li> <li>To view admission controllers enabled by default: <code>kube-apiserver -h | grep enable-admission-plugins</code></li> <li> <p>Note: For kubeadm setups, this must be run from the kube-apiserver control plane using kubectl</p> </li> <li> <p>Admission controllers can either be added to the .service file or the .yaml manifest depending on the setup:</p> </li> </ul> <pre><code>ExecStart=/usr/local/bin/kube-apiserver \\\\\n  --advertise-address=${INTERNAL_IP} \\\\\n  --allow-privileged=true \\\\\n  --apiserver-count=3 \\\\\n  --authorization-mode=Node,RBAC \\\\\n  --bind-address=0.0.0.0 \\\\\n  --enable-swagger-ui=true \\\\\n  --etcd-servers=https://127.0.0.1:2379 \\\\\n  --event-ttl=1h \\\\\n  --runtime-configmap=api/all \\\\\n  --service-cluster-ip-range=10.32.0.0/24 \\\\\n  --service-node-port-range=30000-32767 \\\\\n  --v=2\n  --enable-admission-plugins=NodeRestriction,NamespaceAutoProvision\n</code></pre> <pre><code>## /etc/kubernetes/manifests/kube-apiserver.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  creationTimestamp: null\n  name: kube-apiserver\n  namespace: kube-system\nspec:\n  containers:\n  - command:\n    - kube-apiserver\n    - --authorization-mode=Node,RBAC\n    - --advertise-address=172.17.0.107\n    - --allow-privileged=true\n    - --enabke-admission-plugins=NodeRestriction\n    - --enable-bootstrap-token-auth=true\n    image: k8s.gcr.io/kube-apiserver-amd64:v1.11.3\n    name: kube-apiserver\n</code></pre> <ul> <li>To disable a plugin use <code>--disable-admission-plugins=&lt;plugin1&gt;,&lt;plugin2&gt;, ....</code></li> <li>Note: NamespaceAutoProvision is not enabled by default - it can be enabled by the above menthods</li> </ul>"},{"location":"certifications/CKS/05_Minimize-Microservice-Vulnerabilities.html#53-validating-and-mutating-admission-controllers","title":"5.3 - Validating and Mutating Admission Controllers","text":"<ul> <li>Validating Admission Controller - Allows or Denies a request depending on the controllers functionality/conditions</li> <li>Example: NamespaceExists</li> <li>Mutating Admission Controller: If an object is to be created and a required parameter isn't specified, the object is modified to use the default value prior to creation</li> <li>Example: DefaultStorageClass</li> <li>Note: Certain admission controllers can do both mutation and validation operations</li> <li>Typically, mutation admission controllers are called first, followed by validation controllers.</li> <li>Many admission controllers come pre-packaged with Kubernetes, but could also want custom controllers:</li> <li>To support custom admission controllers, Kubernetes has 2 available for use:<ul> <li>MutatingAdmissionWebhook</li> <li>ValidatingAdmissionWebhook</li> </ul> </li> <li>Webhooks can be configured to point to servers internal or external to the cluster<ul> <li>Servers will have their own admission controller webhook services running the custom logic</li> <li>Once all the built-in controllers are managed, the webhook is hit to call to the webhook server by passing a JSON object regarding the request</li> <li>The admission webhook server then responds with an admissionreview object detailing the response</li> </ul> </li> <li>To set up, the admission webhook server must be setup, then the admission controller should be setup via a webhook configuration object</li> <li>The server can be deployed as an api server in any programming language desired e.g. Go, Python, the only requirement is that it must be able to accept and handle the requests<ul> <li>Can have a validate and mutate call</li> </ul> </li> <li>Note: For exam purposes, need to only understand the functionality of the webhook server, not the actual code</li> <li>The webook server can be ran in whatever manner desired e.g. a server, or a deployment in kubernetes</li> <li>Latter requires it to be exposed as a service for access</li> <li>The webhook configuration object then needs to be created (validating example follows):</li> <li>Each configuration object contains the following:</li> <li>Name - id for server</li> <li>Clientconfig - determines how the webhook server should be contacted - via URL or service name<ul> <li>Note: for service-based configuration, communication needs to be authenticated via a CA, so a caBundle needs to be provided</li> </ul> </li> <li>Rules:<ul> <li>Determines when the webhook server needs to be called i.e. for what sort of requests should invoke the call to the webhook server</li> <li>Attributes detailed include API Groups, namespaces, and resources.</li> </ul> </li> </ul> <pre><code>apiVersion: admissionregistration.k8s.io/v1\nkind: ValidatingWebhookConfiguration\nmetadata:\n  name: \"pod-policy.example.com\"\nwebhooks:\n- name: \"pod-policy.example.com\"\n  clientConfig:\n    service:\n      namespace: \"webhook-namespace\"\n      name: \"webhook-service\"\n    caBundle: \"Ci0tLS0tQk.......tLS0K\"\n  rules:\n  - apiGroups: [\"\"]\n    apiVersions: [\"v1\"]\n    operations: [\"CREATE\"]\n    resources: [\"pods\"]\n    scope: \"Namespaced\"\n</code></pre>"},{"location":"certifications/CKS/05_Minimize-Microservice-Vulnerabilities.html#54-pod-security-policies","title":"5.4 - Pod Security Policies","text":"<ul> <li>When developing pods, there may be configurations you wish to prevent users from using / applying on the cluster</li> <li>Examples:<ul> <li>Prevent container from having root access to the underlying system</li> <li>Prevent running as root user</li> <li>Prevent certain capabilities</li> </ul> </li> <li>These restrictions can be enforced by Pod Security Policies</li> <li>There is a pod security policy Admission controller that comes as part of Kubernetes by default, though it is not enabled by default.</li> <li>Check if disabled by:: <code>kube-apiserver -h | grep enable-admission-plugins</code></li> <li>It can be enabled by updating the <code>--enable-admission-plugins</code> flag with the <code>kube-apiserver.service</code> or <code>kube-apiserver.yaml</code> files:</li> </ul> <pre><code>## /etc/kubernetes/manifests/kube-apiserver.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  creationTimestamp: null\n  name: kube-apiserver\n  namespace: kube-system\nspec:\n  containers:\n  - command:\n    - kube-apiserver\n    - --authorization-mode=Node,RBAC\n    - --advertise-address=172.17.0.107\n    - --allow-privileged=true\n    - --enabke-admission-plugins=NodeRestriction\n    - --enable-bootstrap-token-auth=true\n    - --enable-admission-plugins=PodSecurityPolicy\n    image: k8s.gcr.io/kube-apiserver-amd64:v1.11.3\n    name: kube-apiserver\n</code></pre> <ul> <li>Once enabled, one can create a Pod Security Policy Object, outlining the requirements / restrictions. An example of restricting running as privileged user follows:</li> </ul> <pre><code>apiVersion: policy/v1beta1\nkind: PodSecurityPolicy\nmetadata:\n  name: example-psp\nspec:\n  privileged: false\n  seLinux:\n    rule: RunAsAny\n  supplementalGroups:\n    rule: RunAsAny\n  runAsUser:\n    rule: RunAsAny\n  fsGroup:\n    rule: RunAsAny\n</code></pre> <ul> <li>Note:</li> <li>seLinux, supplementalGroups, runAsUser and fsGroup are all mandatory fields.</li> <li>Once deployed, any pod definition files will be checked against the pod security policy.</li> <li>Note: Need to ensure that the security policy api must be authenticated / authorized for the admission controller to work.</li> <li>This can be achieved via RBAC</li> <li>Every pod has a serviceAccount associated with it (default if not specified)<ul> <li>Can therefore create a role and bind the serviceAccount to allow access to the pod security policy api</li> </ul> </li> </ul> <pre><code>## psp-example-role.yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: psp-example-role\nrules:\n- apiGroups: [\"policy\"]\n  resources: [\"podsecuritypolicies\"]\n  resourceNames: [\"example-psp\"]\n  verbs: [\"use\"]\n</code></pre> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: psp-example-rolebinding\nsubjects:\n- kind: ServiceAccount\n  name: default\n  neamespace: default\nroleRef:\n  kind: Role\n  name: psp-example-role\n  apiGroup: rbac.authorization.k8s.io\n</code></pre> <ul> <li>In Pod Security Policies, one can:</li> <li>Force the user to run as \"non-root\"</li> <li>Force capabilities to be added or removed</li> <li>Restrict the types of volumes</li> </ul>"},{"location":"certifications/CKS/05_Minimize-Microservice-Vulnerabilities.html#55-open-policy-agent-opa","title":"5.5 - Open Policy Agent (OPA)","text":"<ul> <li>Consider a user logging into a web server, there are typically multiple steps for logging in:</li> <li>Authentication - Checking identity</li> <li>Authorization - Checking / restricting</li> <li>OPA exists to handle authorization requests / verification.</li> <li>Without OPA, any time a new service will be deployed, one will have to manually configure all the routes and authorization mechanisms / policies between it and the other services =&gt; VERY TEDIOUS!</li> <li>In practice, OPA is deployed within the environment and policies are configured within it.</li> <li>When a service wants to make a request to another service, the request first goes to OPA, which either allows or denies it.</li> <li>Step 1:</li> <li>Download from Github</li> <li>Execute from within directory: <code>./opa run -s</code></li> <li>Step 2: Load Policies</li> <li> <p>Policies defined in .rego language (Example follows):</p> <p>```rego package httpai.authz</p> </li> </ul>"},{"location":"certifications/CKS/05_Minimize-Microservice-Vulnerabilities.html#http-api-request","title":"HTTP API Request","text":"<p>import input</p> <p>default allow = false</p> <p>allow {     input.path == \"home\"     input.user == \"john\" } ```</p> <ul> <li>{} determines acceptance conditions</li> <li>Once ready, can load the policy via curl to use a PUT request e.g.: <code>curl -X PUT --data-binary @example.rego http://localhost:8181/v1/policies/example1</code></li> <li>To view the list of existing policies: <code>curl http://localhost:8181/v1/policies</code></li> <li>Policies can then be called within programs by making a post request to the API and passing the required input parameters -&gt; If allow conditions are met, OPA allows the request, if not, rejected.</li> <li>Note: A guide on utilising the rego language is available via the documentation and a \"playground\" for policy testing</li> </ul>"},{"location":"certifications/CKS/05_Minimize-Microservice-Vulnerabilities.html#56-opa-in-kubernetes","title":"5.6 - OPA in Kubernetes","text":"<ul> <li>Kubernetes currently has RBAC for various functions except:</li> <li>Only permit images from certain registries</li> <li>Forbid \"runAs Root User\"</li> <li>Only allow certain capabilities</li> <li>Enforce pod labelling</li> <li>Admission Controllers go some way to covering these failures, with many pre-packaged with Kubernetes and capable of mutation and validation operations however even they have their limitations.</li> <li>For custom admission controllers, would have to build a separate admission controller webhook server</li> <li>This need can be removed and centralized by using OPA in kubernetes.</li> <li>Assuming OPA is pre-installed in Kubernetes, one can create a ValidatingWebhookConfiguration:</li> <li>If OPA is not installed in the Cluster, set the ClientConfig URL to the server</li> <li>If installed on the server, one needs to define:<ul> <li>The OPA caBundle</li> <li>The OPA Kubernetes service details</li> </ul> </li> <li>As discussed, the webhook configuration sends an admission review request to the OPA endpoint - in this case, the request is checked against the .rego policy stored within OPA.</li> <li>This can be for operations such as checking image registries.</li> <li>For policies where information about other pods is required, need to utilise an import statement: Import data.kubernetes.pods</li> <li>To help OPA understand the state / definition of the Kubernetes cluster in OPA, one can use the kube-mgmt service</li> <li>Service deployed as a sidecar container alongside OPA</li> <li>Used to:<ul> <li>Replicate Kubernetes resources to OPA</li> <li>Load policies into OPA via Kubernetes</li> </ul> </li> <li>If creating an OPA policy, can create a ConfigMap with the following label: openpolicyagent.org/policy: rego And define the policy logic under the configMap's data field.</li> <li>How is OPA deployed on Kubernetes?</li> <li>Deployed with kube-mgmt as a deployment</li> <li>Roles and rolebindings deployed</li> <li>Service to expose OPA service to Kubernetes API Server</li> <li>Deployed in an OPA namespace.</li> <li>Within the cluster, one can now create a validating/mutating admission webhook and reference the OPA service</li> </ul> <pre><code>apiVersion: admissionregistration.k8s.io/v1beta1\nkind: ValidatingWebhookConfiguration\nmetadata:\n  name: opa-validating-webhook\nwebhooks:\n- name: validating-webhook.openpolicyagent.org\n  rules:\n  - operations: [\"CREATE\", \"UPDATE\"]\n    apiGroups: [\"*\"]\n    apiVersions: [\"*\"]\n    resources: [\"*\"]\n  clientConfig:\n    caBundle: $(cat car.crt | base64 | tr -d '\\n')\n    service:\n      namespace: opa\n      name: opa\n</code></pre> <ul> <li>Note: The above is the \"old\" way of implementing OPA to Kubernetes, nowadays a gatekeeper service is used.</li> </ul>"},{"location":"certifications/CKS/05_Minimize-Microservice-Vulnerabilities.html#57-manage-kubernetes-secrets","title":"5.7 - Manage Kubernetes Secrets","text":"<ul> <li>Used to store sensitive information e.g. passwords and keys</li> <li>Similar to configmaps but stored in hashed format</li> <li>Secrets must first be created before injection</li> <li>Creation via commands or a definition file:</li> <li>Imperative: <code>kubectl create secret generic &lt;secret name&gt; --from-literal=&lt;key&gt;=&lt;value&gt;, ...</code><ul> <li>Or use: <code>--from-file=/path/to/file</code></li> </ul> </li> <li>Declarative:<ul> <li>Create a <code>secret.yaml</code> file</li> <li>Under data - add secrets in key-value pairs</li> </ul> </li> <li>To encode secrets: <code>echo -n 'secret' | base64</code></li> <li>Secrets viewable and manageable via kubectl &lt; task &gt; secrets</li> <li>To view secrets values: <code>kubectl get secret &lt;secret&gt; -o yaml</code></li> <li>To decode, use the <code>| base64</code> command with <code>--decode</code> flag</li> <li>Secrets can then be injected into pods as environment variables using the envFrom field</li> <li>One can also reference the secret directly using env // valueFrom</li> <li>Alternatively, volumes can be used, but this isn't recommended.</li> </ul>"},{"location":"certifications/CKS/05_Minimize-Microservice-Vulnerabilities.html#58-container-sandboxing","title":"5.8 - Container Sandboxing","text":"<ul> <li>All containers running on a server share the same underlying kernel. From the host perspective, this is just another process isolated from the host and other containers running it.</li> <li>When running a container running a process, the process will have a different process ID within the container and on the host OS.</li> <li>This is process ID namespacing<ul> <li>Allows containers to isolate processes from one another</li> <li>Note: If process killed on the host, the container would also be stopped</li> </ul> </li> <li>For containers to run in the user space, they need to make syscalls to the hardware -&gt; this leads to issues</li> <li>Unlike VMs, which have dedicated Kernels, share the same Kernel on the host OS</li> <li>This can pose massive security risks as it means that containers could interact and affect one another.</li> <li>Sandboxing can resolve this<ul> <li>Sandboxing techniques include:</li> <li>Seccomp - Docker default profiles and custom profiles (K8s)</li> <li>Apparmor</li> <li>Both of the above work by whitelisting and blacklisting actions and calls:</li> <li>Whitelist - Block by default and allow particular calls / actions</li> <li>Blacklist - Allow by default and block particular calls/actions</li> <li>Both have varying use cases, there is no set case for what works best.</li> </ul> </li> </ul>"},{"location":"certifications/CKS/05_Minimize-Microservice-Vulnerabilities.html#59-gvisor","title":"5.9 - gVisor","text":"<ul> <li>The Linux Kernel allows applications to perform an untold number of syscalls to perform a variety of functions</li> <li>Whilst this can be great from a developer perspective, the same cannot be said for security perspective. The more opportunities for Kernel interaction there are, the more opportunities for attack.</li> <li>The core problem relates more to how each container in a multitenant environment would be interacting with the same underlying OS and Kernel -&gt; Need to improve isolation from container-container AND from container-Kernel</li> <li>gVisor aims to implement the isolation between the container and Kernel.</li> <li>When a program / container wants to make a syscall to the kernel, it first goes to gVisor</li> <li>gVisor as a sandbox tool contains a variety of tools:<ul> <li>Sentry - An independent application-level Kernel dedicated for containers; intercepting and responding to syscalls</li> <li>Sentry supports much less syscalls than the linux kernel as it's designed to support containers directly; limiting the opportunities for exploit.</li> <li>Gofer - A file proxy that implements the logic for containers to access the filesystem</li> </ul> </li> <li>gVisor can also facilitate and monitor network-based operations.</li> <li>Each container has its own gVisor Kernel acting between the container and the host kernel.</li> <li>The main con of gVisor is that its compatibility with applications can be limited, it can also result in slightly slower containers.</li> </ul>"},{"location":"certifications/CKS/05_Minimize-Microservice-Vulnerabilities.html#510-kata-containers","title":"5.10 - Kata Containers","text":"<ul> <li>Kata aims to set the isolation at its own VM / container</li> <li>Each container will have its own dedicated kernel running inside</li> <li>Removes the issue(s) when all container apps communicate with the same host kernel -&gt; if any issues occur, only the affected container will have issues</li> <li>The VMs created by Kata are lightweight and performance-focussed, therefore would not take long to spin up.</li> <li>The added isolation and VM would require additional resources</li> <li>Additionally, there are compatibility issues for virtualization</li> <li>Note: Wouldn't want to run Kata on cloud instances / not be able to, this would be nested virtualization and can lead to major performance issues.</li> </ul>"},{"location":"certifications/CKS/05_Minimize-Microservice-Vulnerabilities.html#511-runtime-classes","title":"5.11 - Runtime Classes","text":"<ul> <li>When running a container, the following steps from Docker CLI are applied</li> </ul> <ol> <li>Docker client converts command to a REST API</li> <li>REST API request passed to Docker Daemon</li> <li>If the image is not present in the local system, it is pulled from the Docker registry</li> <li>The call is made to containerd to start the container -&gt; containerd converts image to OCI-compliant bundle</li> <li>The OCI-compliant bundle is passed to the containerd-shim, triggering the runtime runC to start the container</li> <li> <p>RunC interacts with the Kernel's namespaces and CGroups on the Kernel to create the container</p> </li> <li> <p>RunC is referred to the default container runtime and adheres to OCI standards for format and runtime</p> </li> <li>With runc installed on a server, containers could be ran on their own via <code>runc run &lt;image&gt;</code></li> <li>Without docker's features e.g. image management, managing the container would be very tough</li> <li>runC is the default container runtime for many container systems</li> <li>Kata and gVisor use their own runtimes that are OCI-compatible</li> <li>The OCI-compatibility means that kata and runsc (gvisor) could be used with Docker without issue by adding a <code>--runtime &lt;runtime&gt;</code> to the <code>docker run</code> command.</li> </ol>"},{"location":"certifications/CKS/05_Minimize-Microservice-Vulnerabilities.html#512-using-runtimes-in-kubernetes","title":"5.12 - Using Runtimes in Kubernetes","text":"<ul> <li>Assuming gVisor is already installed on the nodes of a Kubernetes cluster, it can be used as the runtime for containers within easily.</li> <li>To do so, a runtimeclass Kubernetes object is required, which can be created using kubectl as usual. The handler should be a valid name associated with the given runtime / application.</li> </ul> <pre><code>## gvisor.yaml\n\napiVersion: node.k8s.io/v1beta1\nkind: RutnimeClass\nmetadata:\n  name: gvisor\nhandler: runsc\n</code></pre> <ul> <li>To specify the runtime to be used by the pod, add <code>runtimeClassName: &lt;runtime class name&gt;</code> to the pod spec</li> <li>To check if pods are running in this runtime, run <code>pgrep -a &lt;container name&gt;</code></li> <li>If no output is given, there is an isolation between the system and the container and it's running in the defined runtime as expected.</li> <li>To check the runtime: <code>pgrep -a &lt;runtime&gt;</code></li> </ul>"},{"location":"certifications/CKS/05_Minimize-Microservice-Vulnerabilities.html#513-one-way-ssl-vs-mutual-tls-mtls","title":"5.13 - One-Way SSL vs Mutual TLS (mTLS)","text":"<ul> <li>One-Way SSL:</li> <li>Receivers can only verify identities based on the information sent by clients e.g. emails, social media using usernames and passwords</li> <li>If there is no end-user to provide the information e.g. two services, what then?</li> <li>Mutual SSl is required:<ul> <li>Client and Server verify their identities</li> <li>When requesting data from the server, the client requests the servers public certificates</li> <li>When the server sends it back, it requests the clients certificate and the client verifies the server certificate with the CA it uses</li> <li>Once verified, the client sends its certificate with a symmetric key encrypted with the server's public key</li> <li>The server then validates the client certificate via the CA.</li> </ul> </li> <li>Once both certificates are verified, all communication can be encrypted using the symmetric key.</li> </ul>"},{"location":"certifications/CKS/05_Minimize-Microservice-Vulnerabilities.html#514-implement-pod-to-pod-encryption-via-mtls","title":"5.14 - Implement Pod-to-Pod Encryption via mTLS","text":"<ul> <li>By default, data transfer between pods is unencrypted -&gt; This is a MAJOR security risk</li> <li>Pods can be configured to use mTLS though.</li> <li>Similar to the previous example, but replace client and server with pod a and pod b respectively.</li> <li>This ensures that both pods verify each other's identities.</li> <li>How would this be managed across multiple pods and nodes?</li> <li>This is achievable by getting applications to encrypt communications by default, however this could then cause issues if there are differing encryption methods.</li> <li>Typically, third party applications are used to facilitate the mTLS e.g. Istio and Linkerd<ul> <li>These are Service Mesh tools and aren't confined to mTLS only, they are typically used to facilitate microservice architecture</li> </ul> </li> <li>Tools like this run alongside pods as sidecar containers</li> <li>When a pod sends a message to another, istio intercepts, encrypts and sends the message, where it is encrypted by the istio container running alongside the receiving pod.</li> <li>Istio supports varying types of encryption levels / modes:<ul> <li>Permissive / Opportunistic - Will accept unencrypted traffic where possible / deemed safe</li> <li>Enforced / Strict - No unencrypted traffic allowed at all</li> </ul> </li> </ul>"},{"location":"certifications/CKS/06_Supply-Chain-Security.html","title":"6.0 - Supply Chain Security","text":""},{"location":"certifications/CKS/06_Supply-Chain-Security.html#61-minimize-base-image-footprint","title":"6.1 - Minimize Base Image Footprint","text":"<ul> <li>Base vs Parent Image</li> <li>Parent image - any image used to build a custom image</li> <li>Base image - any image built from scratch</li> <li>Note: Parent vs Base image may be used interchangeably, not a major issue</li> <li>For these notes - base image = any image used to build a custom image</li> <li>Best Practices for image building:</li> <li>Images shouldn't be built that contain multiple applications e.g. databases, applications<ul> <li>Images should be modular i.e. 1 for a DB, etc. They should each solve their own problem and have their own independent set of dependencies</li> <li>Once deployed as containers, these images can be controlled individually for scaling</li> </ul> </li> <li>Persist State:<ul> <li>Data or state shouldn't be stored in containers as they are ephemeral in nature - one should be able to destroy and recreate a container without losing data</li> <li>Store on an external volume or via a caching service</li> </ul> </li> <li>Choosing a base image:<ul> <li>Should always consider base images that suit the technical need of the solution being developed</li> <li>Images can be viewed on Docker Hub</li> <li>Areas to note when searching for a base image:</li> <li>Is it from an official source?</li> <li>Is it up to date?</li> </ul> </li> <li>Slim/Minimal Images:<ul> <li>Minimizing the size of the image allows quicker pulling and building</li> <li>General steps that can be taken:</li> <li>Create slim/minimal images</li> <li>Find an official minimal image that exists</li> <li>Only install necessary packages e.g. remove shells/package managers/tools - anything which one can use to infiltrate a system</li> <li>Ensure images are suited to the environment that they are being used for e.g. for a development environment, include debug tools, for production, ensure as lean as possible</li> <li>Use multi-stage builds - ensures lean, production-ready images</li> </ul> </li> <li>Distroless Docker Images</li> <li>Contain only the application and docker runtime dependencies</li> <li>Provided by Google</li> <li>Minimizing image footprint leads to smaller area of attack for vulnerabilities -&gt; more secure image</li> </ul>"},{"location":"certifications/CKS/06_Supply-Chain-Security.html#62-image-security","title":"6.2 - Image Security","text":"<ul> <li>Docker images follow the naming convention where <code>image: &lt;image name&gt;</code></li> <li>Image name = image / repository referenced</li> <li>i.e. library/image name<ul> <li>Library = default account where docker official images are stored</li> <li>If referencing from a particular account - swap library with account name</li> </ul> </li> <li>Images typically pulled from docker registry at docker.io by default</li> <li>Private repositories can also be referenced</li> <li>Requires login via <code>docker login &lt;registry name&gt;</code></li> <li>It can then be referenced via the full path in the private registry</li> <li>To facilitate the authentication - create a secret of type docker-registry i.e.:</li> </ul> <p><code>kubectl create secret docker-registry &lt;name&gt; --docker-server=&lt;registry name&gt; --docker-username=&lt;username&gt; --docker-password=&lt;password&gt; --docker-email=&lt;email&gt;</code></p> <p>Then, in the pod spec, add:</p> <pre><code>imagePullSecrets:\n- Name: &lt;secret name&gt;\n</code></pre>"},{"location":"certifications/CKS/06_Supply-Chain-Security.html#63-whitelist-allowed-registries-image-policy-webhook","title":"6.3 - Whitelist Allowed Registries - Image Policy Webhook","text":"<ul> <li>By default, any image can be pulled from any registry (private or open) in Kubernetes</li> <li>This is a huge security risk, as if a image is pulled from a non-approved registry, it could contain multiple vulnerabilities that can be taken advantage of</li> <li>In general, need to ensure images are pulled from approved registries only i.e. sufficient governance</li> <li>This can be handled using admission controllers as discussed previously</li> <li>When a request comes in, the admission controller can check the registry</li> <li>We can make an admission webhook server and configure the webhook admission controller to validate the registry provided, along with any messages</li> <li>Alternatively:<ul> <li>Could deploy an OPA service - configure a validating webhook to connect to the opa service and check agains the rego policy</li> </ul> </li> <li>Alternatively again:<ul> <li>Could utilise a custom build imagepolicywebhook, which can communicate with an admission webhook server via an admission configuration file</li> </ul> </li> <li>ImagePolicyWebHook Admission Config File:</li> </ul> <pre><code>apiVersion: apiserver.config.k8s.io/v1\nkind: AdmissionConfiguration\nplugins:\n- name: ImagePolicyWebhook\n  configuration:\n    imagePolicy:\n      kubeConfigFile: /path/to/config/file\n      allowTTL: 50\n      denyTTL: 50\n      retryBackoff: 500\n      defaultAllow: true\n</code></pre> <ul> <li><code>defaultAllow: true</code> - if webhook not accessible, default behaviour allows pod to be created</li> <li>Setting to false denies by default unless the exceptions are explicitly defined elsewhere</li> <li>Access to the admission webhook server is defined in the kubeconfig file</li> </ul> <pre><code>## /path/to/kubeconfig-file\n\nclusters:\n- name: name-of-remote-imagepolicy-service\n  cluster:\n    certificate-authority: /path/to/ca.pem\n    server: https://images.example.com/policy\n\nusers:\n- name: name-of-api-server\n  user:\n    client-certificate: /path/to/cert.pem\n    client-key: /path/to/key.pem\n</code></pre> <ul> <li>Once setup, the admission controller can be added to the <code>--enable-admission-plugins</code> flag in the kube-apiserver <code>.service</code> or <code>yaml</code> file as appropriate.</li> <li>Additionally: pass <code>--admission-control-config-file=&lt;path to config file&gt;</code></li> <li>This is used to help enable the admission controller webhook and authenticate it appropriately</li> </ul>"},{"location":"certifications/CKS/06_Supply-Chain-Security.html#64-static-analysis-of-user-workloads-kubernetes-resources-docker-files-etc","title":"6.4 - Static Analysis of User Workloads - Kubernetes resources, Docker Files, etc","text":"<ul> <li>When submitting a resource creation request in kubernetes it goes through:</li> <li>Kubectl -&gt; authentication -&gt; authorisation -&gt; admission controllers -&gt; create pod</li> <li>What if we want to catch any security vulnerabilities before using kubectl? Static analysis can be used for this - resource files are reviewed against policies.</li> <li>Example tools - kubesec</li> <li>Helps analyse a given resource definition file and returns a report with an associated score based on each vulnerability along with rationale regarding the vulnerability</li> <li>Kubesec installation</li> <li>Installable via a binary.</li> <li>Run by <code>kubesec scan &lt;pod.yaml&gt;</code></li> <li>OR make a curl POST request:     <code>curl -sSX POST --data-binary @\"pod.yaml\" https://v2.kubesec.io/scan</code></li> <li>OR: <code>kubecsec http 8080 &amp;</code> - Runs a kubesec instance locally on the server</li> </ul>"},{"location":"certifications/CKS/06_Supply-Chain-Security.html#65-scan-images-for-known-vulnerabilities-trivy","title":"6.5 - Scan Images for Known Vulnerabilities (Trivy)","text":"<ul> <li>CVE = Common Vulnerabilities and Exposures</li> <li>User-submitted database for vulnerabilities, workarounds and why it's an issue</li> <li>What constitutes a CVE?</li> <li>Anything that can allow an attacker to bypass security checks and perform unwanted actions</li> <li>Anything that allows attackers to seriously affect application performance</li> <li>Each CVE gets a severity rating from 0-10 - helps to understand what vulnerabilities should be shown greater focus etc</li> <li>In general, a higher score = greater vulnerability</li> <li>Example - Download from http instead of https gives a score of around 7.3</li> <li>Kubernetes clusters will have various processes and packages running at any given point, the attack area can be minimized by actions such as deleting unnecessary packages as discussed previously.</li> <li>To understand the current state of the cluster in terms of vulnerabilities across processes, containers, and so on, one can utilise CVE Scanners</li> <li>Container scanners look for vulnerabilities in container / execution environment - applications in the container</li> <li>Once vulnerabilities are identified, the appropriate action(s) can be taken e.g. update versions, remove packages, etc</li> <li>In general - more packages = greater footprint = greater amount of vulnerabilities</li> </ul>"},{"location":"certifications/CKS/06_Supply-Chain-Security.html#example-trivy","title":"Example - Trivy","text":"<ul> <li>Provided by AquaSecurity as a simple CVE scanner for containers, artifacts, etc</li> <li>Can be integrated with CI/CD pipelines</li> <li>Can easily be installed as if installing a typical package</li> <li>To scan: <code>trivy image &lt;image name&gt;:&lt;tag&gt;</code></li> <li>Additional flags available e.g.:</li> <li><code>--severity=&lt;severity 1&gt;,&lt;severity 2&gt;</code></li> <li><code>--ignore-unfixed</code> (ignore any vulnerabilities that can't be fixed even if packages are updated)</li> <li>Trivy can be used to scan images in a tar format too e.g.:</li> <li><code>docker save &lt;image&gt; &gt; &lt;name&gt;.tar</code></li> <li><code>trivy image --input archive.tar</code></li> <li>Reducing vulnerabilities can be done by using minimal images e.g. alpine images like nginx-alpine</li> <li>Best practices:</li> <li>Continuously rescan images</li> <li>Use kubernetes admission controllers to scan images</li> <li>Use your own repository with pre-scanned images ready to go</li> <li>Integrate container scanning into CI/CD pipelines</li> </ul>"},{"location":"certifications/CKS/07_Monitoring-Logging-and-Runtime-Security.html","title":"7.0 - Monitoring, Logging, and Runtime Security","text":""},{"location":"certifications/CKS/07_Monitoring-Logging-and-Runtime-Security.html#71-perform-behavioural-analytics-of-syscall-processes","title":"7.1 - Perform Behavioural Analytics of Syscall Processes","text":"<ul> <li>Various ways of securing kubernetes clusters have been discussed so far:</li> <li>Securing kubernetes nodes</li> <li>Minimizing microservices vulnerabilities</li> <li>Sandboxing techniques</li> <li>MTLS Encryption</li> <li>Network Access Restriction</li> <li>No guarantee that utilising these 100% prevents the possibility of an attack, how can one prepare for the possibility? In general for vulnerabilities, the sooner that the possibility of a vulnerability has been exploited is noted, the better</li> <li>Actions that could be taken include:</li> <li>Alert notifications</li> <li>Rollbacks</li> <li>Set limits for transactions or resource usage</li> <li>Identifying breaches that have already occurred can be done using tools like Falco</li> <li>Syscalls can be monitored by tools like tracee</li> <li>Need to analyse syscalls like these in real time to monitor events occurring within the system and note any of suspicious nature<ul> <li>Example - Accessing a containers bash shell and going to the <code>/etc/shadow</code> section, deleting logs,</li> </ul> </li> </ul>"},{"location":"certifications/CKS/07_Monitoring-Logging-and-Runtime-Security.html#72-falco-overview-and-installation","title":"7.2 - Falco Overview and Installation","text":"<ul> <li>For functionality, Falco must monitor the syscalls coming from the applications in the user space into the linux kernel.</li> <li>This is done by a particular Kernel module - this is intrusive and forbidden by some Kubernetes service providers</li> <li>A workaround is available via the eBPF in a similar manner to the Aquasec Tracee tool.</li> <li>Syscalls are analysed by Sysdig libraries in the user space and filtered by the falco policy engine based on predefined rules to determine the nature of the event.</li> <li>Alert notifications are sent via various methods including email and slack at the users discretion.</li> <li>Steps are provided in the Falco getting started documentation for running it as a service on Linux</li> <li>Note: In the event the Kubernetes cluster is compromised, Falco will still be running.</li> <li>Alternatively, Falco can be installed as a daemonset via installing the helm charts.</li> <li>Falco pods should then be running on all nodes.</li> </ul>"},{"location":"certifications/CKS/07_Monitoring-Logging-and-Runtime-Security.html#73-use-falco-to-detect-threats","title":"7.3 - Use Falco to Detect Threats","text":"<ul> <li>Check that falco is running:</li> <li><code>systemctl status falco</code> (if running on host)</li> <li>Creating a pod as normal, in a separate terminal, one can ssh into the node and run: <code>journalctl -fu falco</code></li> <li>This allows inspection of the events generated / picked up by the falco service</li> <li>Note: the <code>fu</code> flag allows events to be automatically added as they appear</li> <li>In the original terminal, executing a shell in the pod generates an event picked up by falco.</li> <li>Details displayed include pod, namespace, container name, commands ran, etc.</li> <li>The same is applied for any activity ran.</li> <li>Falco implements several rules by default to detect events, such as creating a shell and reading particular files.</li> <li>Rules are defined in <code>rules.yaml</code> file.</li> <li>Elements included: rules, lists and macros</li> <li> <p>Rules:</p> <ul> <li>Defines all the conditions for which an event should be triggered</li> <li>Rule - Name of the rule</li> <li>Desc - What is the detailed explanation of the rule</li> <li>Condition - filtering expression applied against events matching the rule</li> <li>Output - output generated for any events matching the rule</li> <li>Priority - severity of the rule</li> </ul> </li> <li> <p>Custom rule example - shell opening in a contaienr anywhere not equal to root:</p> </li> </ul> <pre><code>- rule: detect shell inside a container\n  desc: alert if a shell such as bash is open inside the container\n  condition: container.id != host and proc.name = bash\n  output: bash shell opened (user=%user.name %container.id)\n  priority: WARNING\n</code></pre> <ul> <li>Note: Conditions are used via Sysdig filters e.g:</li> <li><code>container.id</code></li> <li><code>fd.name</code> (file descriptor)</li> <li><code>evt.type</code> (event type)</li> <li><code>user.name</code></li> <li><code>container.image.repository</code></li> <li>Outputs can utilise similar filters to the above.</li> <li>Priority can be any of the following, amongst others:</li> <li>EMERGENCY</li> <li>ALERT</li> <li>DEBUG</li> <li>NOTICE</li> <li>Note: For a set of similar commands e.g. opening any possible shell for the container, lists can be used:</li> </ul> <pre><code>- rule: detect shell inside a container\n  desc: alert if a shell such as bash is open inside the container\n  condition: container.id != host and proc.name = bash\n  output: bash shell opened (user=%user.name %container.id)\n  priority: WARNING\n\n  - list: linux_shells\n    items: [bash, zsh, ksh, sh, csh]\n</code></pre> <ul> <li>Macros can be used to shorten filters e.g.:</li> </ul> <pre><code>- macro: container\n  condition: container.id != host\n</code></pre>"},{"location":"certifications/CKS/07_Monitoring-Logging-and-Runtime-Security.html#74-falco-configuration-files","title":"7.4 - Falco Configuration Files","text":"<ul> <li>Main falco configuration file is located at /etc/falco/falco.yaml</li> <li>Vieweable via either <code>journalctl -fu falco</code> or <code>/usr/lib/systemd/system/falco.service</code></li> <li> <p>Contains all the configuration parameters associated with falco e.g. display format, output channel configuration etc.</p> </li> <li> <p>Common options:</p> </li> <li>How are rules loaded? Rules_file list</li> <li>Note: The order of the rules files is important, as this is the order that falco will check them -&gt; stick top priority rule files first.</li> <li>Logging of events - what format or verbosity is used.</li> <li>Minimum priority that should be logged determined by priority key (debug is the default)</li> <li>Output channels:<ul> <li><code>stdoutput</code> is set to true by default</li> <li>Can configure output to a particular file in a similar manner or a particular program</li> </ul> </li> </ul> <pre><code>## Rules file prioritisation\n\nrules_file:\n- /etc/falco/falco_rules.yaml\n- /etc/falco/falco_rules.local.yaml\n- /etc/falco/k8s_audit_rules.yaml\n- /etc/falco/rules.d\n\n## Logging parameters:\njson_output: false\nlog_stderr: true\nlog_syslog: true\nlog_level: info\n</code></pre> <pre><code>## Output channel example\nfile_output:\n  enabled: true\n  filename: /opt/falco/events.txt\n\nprogram_output:\n  enabled: true\n  program: \"jq '{text: .output}' | curl -d @- X POST https://hooks.slack.com/services/XXX\"\n</code></pre> <ul> <li>HTTP Endpoint Output Example:</li> </ul> <pre><code>http_output:\n  enabled: true\n  url: http://some.url/some/path\n</code></pre> <ul> <li>Note: For any changes made to this file, Falco must be restarted to take effect</li> <li>Rules:</li> <li>Default file: <code>/etc/falco/falco_rules.yaml</code></li> <li>Any changes made to this file will be overwritten when updating the Falco package. To avoid, add to <code>/etc/falco/falc_rules.yaml</code></li> <li>Example Config:</li> </ul> <pre><code>- rule: Terminal Shell in container\n  desc: A shell was used as the entrypoint/exec point into a container with an attached terminal\n  condition: &gt;\n    spawned_process and container\n    and shell_procs and proc.tty != 0\n    and container_entrypoint\n    and not user_expected_terminal_shell_in_container_conditions\n  output: &gt;\n    A shell was spawned in a container with an attached terminal (user=%user.name user_loginuid=%user.loginuid %container.info\n    shell=%proc.name parent=%proc.name cmdline=%proc.cmdline terminal=%proc.tty container_id=%container.id image=%container.image.repository)\n  priority: NOTICE\n</code></pre> <ul> <li>Hot reload can be used to avoid restarting the falco service and allow changes to take place:</li> <li>Find the process ID of Falco at <code>/var/run/falco.pid</code></li> <li>Run a kill -l command: <code>kill -1 $(cat /var/run/falco.pid)</code></li> </ul>"},{"location":"certifications/CKS/07_Monitoring-Logging-and-Runtime-Security.html#reference-links","title":"Reference Links","text":"<p>https://falco.org/docs/getting-started/installation/ https://github.com/falcosecurity/charts/tree/master/falco https://falco.org/docs/rules/supported-fields/ https://falco.org/docs/rules/default-macros/ https://falco.org/docs/configuration/</p>"},{"location":"certifications/CKS/07_Monitoring-Logging-and-Runtime-Security.html#75-mutable-vs-immutable-infrastructure","title":"7.5 - Mutable vs Immutable Infrastructure","text":"<ul> <li>Software upgrades can be done via manual methods or using configuration tools such as custom scripts or configuration managers like ansible</li> <li>In high-availability setups, could apply the same update approach to each server running the software - in-place updates</li> <li>Configuration remains the same, but the software has changed</li> <li>This leads to mutable infrastructure</li> <li>If the upgrade fails for a particular server due to dependency issues like network or files, a configuration drift can occur - each server behaves slightly differently to one another.</li> <li>To workaround, can just spin up new servers with the new updated software and delete the old servers upon successful updates</li> <li>This is the idea behind immutable infrastructure - Unchanged infrastructure</li> <li>Immutability is one of the primary thoughts on containers</li> <li>As they are made using images, any changes e.g. version updates should be applied to an image first, then that image is used to create new containers via rolling updates</li> <li>Note: containers can be changed during runtime e.g. copying files to and from containers - this is not in line with security best practices</li> </ul>"},{"location":"certifications/CKS/07_Monitoring-Logging-and-Runtime-Security.html#76-ensure-immutability-of-containers-at-runtime","title":"7.6 - Ensure Immutability of Containers at Runtime","text":"<ul> <li>Even though containers are designed to be immutable by default, there are ways to do in-place updates on them:</li> <li>Copying files to containers:<ul> <li><code>kubectl cp nginx.conf nginx:/etc/nginx</code></li> <li><code>Kubectl cp &lt;file name&gt; &lt;container name&gt;:&lt;target path&gt;</code></li> </ul> </li> <li> <p>Executing a shell into the container and making changes:</p> <ul> <li><code>Kubectl exec -ti nginx -- bash nginx:/etc/nginx</code></li> </ul> </li> <li> <p>To prevent this, one could add to the pod definition file security contexts in a similar manner to start with a readonly root file system e.g.:</p> </li> </ul> <pre><code>## nginx.yaml\n\napiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    run: nginx\n  name: nginx\nspec:\n  containers:\n  - image: nginx\n    name: nginx\n    securityContext:\n      readOnlyRootFileSystem: true\n</code></pre> <ul> <li>This is generally not advisable for applications that may need to write to different directories like storing cache data, and so on.</li> <li>This can be worked around by using volumes:</li> </ul> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    run: nginx\n  name: nginx\nspec:\n  containers:\n  - image: nginx\n    name: nginx\n    securityContext:\n      readOnlyRootFilesystem: true\n    volumeMounts:\n    - name: cache-volume\n      mountPath: /var/cache/nginx\n    - name: runtime-volume\n      mountPath: /var/run\n  volumes:\n  - name: cache-volume\n    emptyDir: {}\n  - name: runtime-volume\n    emptyDir: {}\n</code></pre> <ul> <li>Considering the same file above, in the event this is being ran with privileged set to true, the read-only option will be overwritten -&gt; even more proof that containers shouldn't run as root.</li> <li>In general:</li> <li>Avoid setting <code>readOnlyRootFileSystem</code> as false</li> <li>Avoid setting <code>privileged</code> to true and <code>runAsUser</code> to 0</li> <li>The above can be enforced via <code>PodSecurityPolicies</code> as discussed previously.</li> </ul>"},{"location":"certifications/CKS/07_Monitoring-Logging-and-Runtime-Security.html#77-use-audit-logs-to-monitor-access","title":"7.7 - Use Audit Logs to Monitor Access","text":"<ul> <li>Previously seen how falco can produce details for events such as namespace, pod, and commands used in Kubernetes, but how can these be audited?</li> <li>Kubernetes allows auditing by default via the kube-apiserver.</li> <li>When making a request to the cluster, all requests go to the api server</li> <li>When the request is made - it goes through \"RequestReceived\" stage - generates an event regardless of authentication and authorization</li> <li>Once authenticated and authorized, stage is ResponseStarted</li> <li>Once request completed - stage is ResponseComplete</li> <li>In the event of an error - stage is Panic</li> <li>Each of the above generate events, but this is not enabled by default, as a significant amount of unnecessary events would be recorded.</li> <li> <p>In general, want to monitor only the events we actually care about, such as deleting pods from a particular namespace.</p> </li> <li> <p>This can be done by creating a policy object configuration file in a similar manner to below:</p> </li> </ul> <pre><code>apiVersion: audit.k8s.io/v1\nkind: Policy\nomitStages: [\"RequestReceived\"]\nrules:\n- namespaces: [\"prod-namespaces\"]\n  verbs: [\"delete\"]\n  resources:\n  - groups: \" \"\n    resources: [\"pods\"]\n    resourceNames: [\"webapp-pod\"]\n  level: RequestResponse\n\n- level: Metadata\n  resources:\n  - groups: \" \"\n    resources: [\"secrets\"]\n</code></pre> <ul> <li>Note: omitStages is optional, define the stages you want to ignore in an array</li> <li>Rules: add each rule to be considered in a list, noting verbs, namespaces, resources where applicable, etc.</li> <li>For resources, can specify the groups that they belong to and the resources within these groups</li> <li>Additional refinement can be done via resourceNames</li> <li>Level determines logging verbosity - none, metadata, request, etc.</li> <li>Auditing is disabled by default in Kubernetes, need to configure an auditing backend for the kube-apiserver, two types are supported: log file backend or webhook service backend.</li> <li>The path to the audit-log file path must be added to the static pod definition <code>--audit-log-path=/var/path/to/file</code></li> <li>To point he apiserver to the policy to be referenced: <code>--audit-policy-file=/path/to/file</code></li> <li>Additional options include:</li> <li><code>--audit-log-maxage</code>: what is the longest a log will be kept for?</li> <li><code>--audit-log-maxbackup</code>: maximum number of log files to be retained on the host</li> <li><code>--audit-log-maxisize</code>: maximum file size for given log files</li> <li>To test, carry out the event(s) described in the policy files.</li> </ul>"},{"location":"certifications/CKS/Mocks/01_Mock-Exam-1.html","title":"8.1 - Mock Exam 1","text":""},{"location":"certifications/CKS/Mocks/01_Mock-Exam-1.html#q1","title":"Q1","text":"<p>A pod has been created in the omni namespace. However, there are a couple of issues with it. The pod has been created with more permissions than it needs. It allows read access in the directory <code>/usr/share/nginx/html/internal</code> causing an Internal Site to be accessed publicly. To check this, click on the button called Site (above the terminal) and add <code>/internal/</code> to the end of the URL. Use the below recommendations to fix this. Use the AppArmor profile created at <code>/etc/apparmor.d/frontend</code> to restrict the internal site.</p> <p>There are several service accounts created in the omni namespace. Apply the principle of least privilege and use the service account with the minimum privileges (excluding the default service account). Once the pod is recreated with the correct service account, delete the other unused service accounts in omni namespace (excluding the default service account).</p> <p>You can recreate the pod but do not create a new service accounts and do not use the default service account.</p>"},{"location":"certifications/CKS/Mocks/01_Mock-Exam-1.html#q2","title":"Q2","text":"<p>A pod has been created in the orion namespace. It uses secrets as environment variables.</p> <p>Extract the decoded secret for the <code>CONNECTOR_PASSWORD</code> and place it under <code>/root/CKS/secrets/CONNECTOR_PASSWORD</code>. You are not done, instead of using secrets as an environment variable, mount the secret as a read-only volume at path <code>/mnt/connector/password</code> that can be then used by the application inside.</p>"},{"location":"certifications/CKS/Mocks/01_Mock-Exam-1.html#q3","title":"Q3","text":"<p>A number of pods have been created in the delta namespace. Using the trivy tool, which has been installed on the controlplane, identify and delete pods except the ones with least number of <code>CRITICAL</code> level vulnerabilities.</p> <p>Note: Do not modify the objects in anyway other than deleting the ones that have critical vulnerabilities.</p>"},{"location":"certifications/CKS/Mocks/01_Mock-Exam-1.html#q4","title":"Q4","text":"<p>Create a new pod called audit-nginx in the default namespace using the nginx image.</p> <p>Secure the syscalls that this pod can use by using the audit.json seccomp profile in the pod's security context.</p> <p>The <code>audit.json</code> is provided at <code>/root/CKS</code> directory. Make sure to move it under the profiles directory inside the default seccomp directory before creating the pod</p>"},{"location":"certifications/CKS/Mocks/01_Mock-Exam-1.html#q5","title":"Q5","text":"<p>The CIS Benchmark report for the kube-apiserver is available at the tab called CIS Report 1. Inspect this report and fix the issues reported as <code>FAIL</code>.</p>"},{"location":"certifications/CKS/Mocks/01_Mock-Exam-1.html#q6","title":"Q6","text":"<p>There is something suspicious happening with one of the pods running an <code>httpd</code> image in this cluster. The Falco service shows frequent alerts that start with: File below a known binary directory opened for writing. Identify the rule causing this alert and update it as per the below requirements:</p> <ol> <li>Output should be displayed as: <code>CRITICAL File below a known binary directory opened for writing (user=user_name file_updated=file_name command=command_that_was_run)</code></li> <li>Alerts are logged to <code>/opt/security_incidents/alerts.log</code></li> </ol> <p>Do not update the default rules file directly. Rather use the <code>falco_rules.local.yaml</code> file to override.</p> <p>Note: Once the alert has been updated, you may have to wait for up to a minute for the alerts to be written to the new log location.</p>"},{"location":"certifications/CKS/Mocks/01_Mock-Exam-1.html#q7","title":"Q7","text":"<p>A pod called <code>busy-rx100</code> has been created in the production namespace. Secure the pod by recreating it using the runtimeClass called gvisor. You may delete and recreate the pod. Note: As long as the pod is recreated with the correct runtimeClass, the task will be marked correct. This lab environment does not support gvisor at the moment so if the pod is not in a running state, ignore it and move on to the next question.</p>"},{"location":"certifications/CKS/Mocks/01_Mock-Exam-1.html#q8","title":"Q8","text":"<p>We need to make sure that when pods are created in this cluster, they cannot use the latest image tag, irrespective of the repository being used.</p> <p>To achieve this, a simple Admission Webhook Server has been developed and deployed. A service called image-bouncer-webhook is exposed in the cluster internally. This Webhook server ensures that the developers of the team cannot use the latest image tag.</p> <p>Make use of the following specs to integrate it with the cluster using an ImagePolicyWebhook:</p> <ol> <li>Create a new admission configuration file at <code>/etc/admission-controllers/admission-configuration.yaml</code></li> <li> <p>The kubeconfig file with the credentials to connect to the webhook server is located at <code>/root/CKS/ImagePolicy/admission-kubeconfig.yaml</code>. Note: The directory /root/CKS/ImagePolicy/ has already been mounted on the kube-apiserver at path <code>/etc/admission-controllers</code> so use this path to store the admission configuration. </p> </li> <li> <p>Make sure that if the latest tag is used, the request must be rejected at all times.</p> </li> <li>Enable the Admission Controller.</li> </ol> <p>Finally, delete the existing pod in the magnum namespace that is in violation of the policy and recreate it, ensuring the same image but using the tag 1.27.</p>"},{"location":"certifications/CKS/Mocks/02_Mock-Exam-2.html","title":"8.2 - Mock Exam 2","text":""},{"location":"certifications/CKS/Mocks/02_Mock-Exam-2.html#q1","title":"Q1","text":"<p>A pod called redis-backend has been created in the prod-x12cs namespace. It has been exposed as a service of type ClusterIP. Using a network policy called <code>allow-redis-access</code>, lock down access to this pod only to the following:</p> <ol> <li>Any pod in the same namespace with the label <code>backend=prod-x12cs</code>.</li> <li>All pods in the <code>prod-yx13cs</code> namespace.</li> </ol> <p>All other incoming connections should be blocked. Use the existing labels when creating the network policy.</p>"},{"location":"certifications/CKS/Mocks/02_Mock-Exam-2.html#q2","title":"Q2","text":"<p>A few pods have been deployed in the <code>apps-xyz</code> namespace. There is a pod called <code>redis-backend</code> which serves as the backend for the apps <code>app1</code> and <code>app2</code>. The pod called <code>app3</code> on the other hand, does not need access to this <code>redis-backend</code> pod.</p> <p>Create a network policy called <code>allow-app1-app2</code> that will only allow incoming traffic from <code>app1</code> and <code>app2</code> to the <code>redis-pod</code>. Make sure that all the available labels are used correctly to target the correct pods.</p> <p>Do not make any other changes to these objects.</p>"},{"location":"certifications/CKS/Mocks/02_Mock-Exam-2.html#q3","title":"Q3","text":"<p>A pod has been created in the gamma namespace using a service account called <code>cluster-view</code>.</p> <p>This service account has been granted additional permissions as compared to the default service account and can view resources cluster-wide on this Kubernetes cluster.</p> <p>While these permissions are important for the application in this pod to work, the secret token is still mounted on this pod.</p> <p>Secure the pod in such a way that the secret token is no longer mounted on this pod. You may delete and recreate the pod.</p>"},{"location":"certifications/CKS/Mocks/02_Mock-Exam-2.html#q4","title":"Q4","text":"<p>A pod in the sahara namespace has generated alerts that a shell was opened inside the container.</p> <p>Change the format of the output so that it looks like below:</p> <p><code>ALERT timestamp of the event without nanoseconds,User ID,the container id,the container image repository</code></p> <p>Make sure to update the rule in such a way that the changes will persists across Falco updates.</p> <p>You can refer the falco documentation.</p>"},{"location":"certifications/CKS/Mocks/02_Mock-Exam-2.html#q5","title":"Q5","text":"<p>Martin is a developer who needs access to work on the <code>dev-a</code>, <code>dev-b</code> and <code>dev-z</code> namespaces.</p> <p>He should have the ability to carry out any operation on any pod in <code>dev-a</code> and <code>dev-b</code> namespaces. However, on the <code>dev-z</code> namespace, he should only have the permission to get and list the pods.</p> <p>The current set-up is too permissive and violates the above condition.</p> <p>Use the above requirement and secure martin's access in the cluster. You may re-create objects, however, make sure to use the same name as the ones in effect currently.</p>"},{"location":"certifications/CKS/Mocks/02_Mock-Exam-2.html#q6","title":"Q6","text":"<p>On the controlplane node, an unknown process is bound to the port 8088.</p> <p>Identify the process and prevent it from running again by stopping and disabling any associated services.</p> <p>Finally, remove the package that was responsible for starting this process.</p>"},{"location":"certifications/CKS/Mocks/02_Mock-Exam-2.html#q7","title":"Q7","text":"<p>A pod has been created in the omega namespace using the pod definition file located at <code>/root/CKS/omega-app.yaml</code>. However, there is something wrong with it and the pod is not in a running state.</p> <p>We have used a custom seccomp profile located at <code>/var/lib/kubelet/seccomp/custom-profile.json</code> to ensure that this pod can only make use of limited syscalls to the Linux Kernel of the host operating system.</p> <p>However, it appears the profile does not allow the read and write syscalls. Fix this by adding it to the profile and use it to start the pod.</p> <p>Q8: A pod definition file has been created at <code>/root/CKS/simple-pod.yaml</code>.</p> <p>Using the kubesec tool, generate a report for this pod definition file and fix the major issues so that the subsequent scan report no longer fails.</p> <p>Once done, generate the report again and save it to the file <code>/root/CKS/kubesec-report.txt</code></p>"},{"location":"certifications/CKS/Mocks/02_Mock-Exam-2.html#q9","title":"Q9","text":"<p>Create a new pod called secure-nginx-pod in the seth namespace. Use one of the images from the below which has no CRITICAL vulnerabilities.</p> <ol> <li>nginx</li> <li>nginx:1.19</li> <li>nginx:1.17</li> <li>nginx:1-alpine</li> <li>gcr.io/google-containers/nginx</li> <li>bitnami/nginx:latest</li> <li>httpd:2-alpine</li> </ol>"},{"location":"certifications/CKS/Mocks/03_Mock-Exam-3.html","title":"10.3 - Mock Exam 3","text":""},{"location":"certifications/CKS/Mocks/03_Mock-Exam-3.html#q1","title":"Q1","text":"<p>A kube-bench report is available at the Kube-bench assessment report tab. Fix the tests with FAIL status for 4 Worker Node Security Configuration . Make changes to the <code>/var/lib/kubelet/config.yaml</code></p> <p>After you have fixed the issues, you can update the published report in the Kube-bench assessment report tab by running <code>/root/publish_kubebench.sh</code> to validate results.</p>"},{"location":"certifications/CKS/Mocks/03_Mock-Exam-3.html#q2","title":"Q2","text":"<p>Enable auditing in this kubernetes cluster. Create a new policy file that will only log events based on the below specifications:</p> <ul> <li>Namespace: <code>prod</code></li> <li>Level: <code>metadata</code></li> <li>Operations: <code>delete</code></li> <li>Resources: <code>secrets</code></li> <li>Log Path: <code>/var/log/prod-secrets.log</code></li> <li>Audit file location: <code>/etc/kubernetes/prod-audit.yaml</code></li> <li>Maximum days to keep the logs: 30</li> </ul> <p>Once the policy is created it, enable and make sure that it works.</p>"},{"location":"certifications/CKS/Mocks/03_Mock-Exam-3.html#q3","title":"Q3","text":"<p>Enable PodSecurityPolicy in the Kubernetes API server.</p> <p>Create a PSP with below conditions:</p> <ol> <li>PSP name : <code>pod-psp</code></li> <li>Privilege to run as root on host: <code>false</code></li> <li>Allowed volumes to mount on pod: configMap,secret,emptyDir,hostPath</li> <li>seLinux, runAsUser, supplementalGroups, fsGroup: RunAsAny</li> </ol> <p>Fix the pod definition <code>/root/pod.yaml</code> based on this PSP and deploy the pod. Ensure that the pod is running after applying the above pod security policy.</p>"},{"location":"certifications/CKS/Mocks/03_Mock-Exam-3.html#q4","title":"Q4","text":"<p>We have a pod definition template <code>/root/kubesec-pod.yaml</code> on controlplane host. Scan this template using the kubesec tool and you will notice some failures.</p> <p>Fix the failures in this file and save the success report in <code>/root/kubesec_success_report.json</code>.</p> <p>Make sure that the final kubesec scan status is passed.</p>"},{"location":"certifications/CKS/Mocks/03_Mock-Exam-3.html#q5","title":"Q5","text":"<p>In the dev namespace create below resources:</p> <ul> <li>A role <code>dev-write</code> with access to <code>get</code>, <code>watch</code>, <code>list</code> and <code>create</code> pods in the same namespace.</li> <li>A Service account called <code>developer</code> and then bind <code>dev-write</code> role to it with a rolebinding called <code>dev-write-binding</code>.</li> <li>Finally, create a pod using the template <code>/root/dev-pod.yaml</code>. The pod Wshould run with the service account developer. Update <code>/root/dev-pod.yaml</code> as necessary</li> </ul>"},{"location":"certifications/CKS/Mocks/03_Mock-Exam-3.html#q6","title":"Q6","text":"<p>Try to create a pod using the template defined in <code>/root/beta-pod.yaml</code> in the namespace <code>beta</code>. This should result in a failure.</p> <p>Troubleshoot and fix the OPA validation issue while creating the pod.</p> <p>You can update <code>/root/beta-pod.yaml</code> as necessary.</p> <p>The Rego configuration map for OPA is in untrusted-registry under <code>opa</code> namespace.</p> <p>NOTE: In the end pod need not to be successfully running but make sure that it passed the OPA validation and gets created.</p>"},{"location":"certifications/CKS/Mocks/03_Mock-Exam-3.html#q7","title":"Q7","text":"<p>We want to deploy an ImagePolicyWebhook admission controller to secure the deployments in our cluster.</p> <p>Fix the error in <code>/etc/kubernetes/pki/admission_configuration.yaml</code> which will be used by ImagePolicyWebhook</p> <p>Make sure that the policy is set to implicit deny. If the webhook service is not reachable, the configuration file should automatically reject all the images.</p> <p>Enable the plugin on API server.</p> <p>The kubeconfig file for already created imagepolicywebhook resources is under <code>/etc/kubernetes/pki/admission_kube_config.yaml</code></p>"},{"location":"certifications/CKS/Mocks/03_Mock-Exam-3.html#q8","title":"Q8","text":"<p>Delete pods from alpha namespace which are not immutable.</p> <p>Note: Any pod which uses elevated privileges and can store state inside the container is considered to be non-immutable.</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/01_Fundamental-Cloud-Concepts/Fundamental%20Cloud%20Concepts.html","title":"1.0 - Fundamental Cloud Concepts","text":""},{"location":"certifications/aws-cloud-practitioner/course-notes/01_Fundamental-Cloud-Concepts/Fundamental%20Cloud%20Concepts.html#course-overview","title":"Course Overview","text":"<ul> <li>AWS Certs add value for technical and non-technical resources.</li> <li>Covers:</li> <li>Cloud Computing Concepts</li> <li>Organization of AWS Global Infrastructure</li> <li>Economics of Cloud Computing</li> <li>Tools and Services</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/01_Fundamental-Cloud-Concepts/Fundamental%20Cloud%20Concepts.html#understanding-cloud-computing","title":"Understanding Cloud Computing","text":""},{"location":"certifications/aws-cloud-practitioner/course-notes/01_Fundamental-Cloud-Concepts/Fundamental%20Cloud%20Concepts.html#setting-up-an-aws-account","title":"Setting up an AWS Account","text":"<ul> <li>aws.amazon.com \u2192 Create AWS Account</li> <li>Add root email address and account name</li> <li>Verify the email address</li> <li>Create root user password</li> <li>Add contact information and account information e.g. address and name</li> <li>Add payment details</li> <li>Confirm identity via SMS or call.</li> <li>Select support option (free will suffice)</li> <li>Go to AWS Management Console \u2192 Root User \u2192 Login</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/01_Fundamental-Cloud-Concepts/Fundamental%20Cloud%20Concepts.html#set-up-billing-alerts","title":"Set Up Billing Alerts","text":"<ul> <li>Account dropdown \u2192 Billing management</li> <li>Budgets \u2192 Create budget \u2192 Create from template (if desired)</li> <li>Fill out fields as appropriate.</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/01_Fundamental-Cloud-Concepts/Fundamental%20Cloud%20Concepts.html#traditional-data-centres","title":"Traditional Data Centres","text":"<ul> <li>Consider a company not on the cloud:</li> <li>They focus their application release in the US first, in US-based datacentres</li> <li>They want to launch in Asia and Europe and require funding to have infrastructure built there to support the needs</li> <li>Resources needed include:<ul> <li>Web servers</li> <li>File servers</li> <li>Database servers</li> </ul> </li> <li>Manually setting all this up for each region is long, arduous, and error-strewn.</li> <li>Further problems arise when demand scales in any of the pre-existing regions.</li> <li>Moving to cloud resolves all of these problems:</li> <li>No large upfront investment and planning required</li> <li>Use resources as required rather than full unpredictable forecasting</li> <li>New data centres and servers can be spun up / torn down as required</li> <li>Lower maintenance costs</li> <li>Security and compliance burden is alleviated from the organization.</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/01_Fundamental-Cloud-Concepts/Fundamental%20Cloud%20Concepts.html#benefits-of-cloud-computing","title":"Benefits of Cloud Computing","text":"<ul> <li>Trade capital expense for variable expense</li> <li>No need for initial investment to build out a datacentre</li> <li>Only pay for the resources used and how long they're used for</li> <li>Economies of scale</li> <li>AWS buys the datacentre resources on a large scale, allowing them to provision the resources on such a price - savings are passed to the customers</li> <li>No need to guess capacity - cloud computing can allow scaling on demand.</li> <li>Speed and agility increased - resources for testing and deployment can be spun up / torn down at will for minimal costs.</li> <li>Reduces time required to maintain infrastructure</li> <li>Reduced risk for the organisation around security and compliance</li> <li>Provides access to emerging technologies</li> <li>No more cost for maintaining data centres - more focus on the employees and other resources</li> <li> <p>Data can be easily switched to different regions.</p> </li> <li> <p>Elasticity: The ability to acquire resources as needed and release when no longer required.</p> </li> <li>Reliability: A solution's ability to provide functionality for its users when needed.</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/01_Fundamental-Cloud-Concepts/Fundamental%20Cloud%20Concepts.html#types-of-cloud-computing","title":"Types of Cloud Computing","text":"<p>Cloud Computing: The on-demand delivery of compute power, database storage, applications, and other IT resources through a cloud services platform via the internet with pay-as-you-go pricing.</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/01_Fundamental-Cloud-Concepts/Fundamental%20Cloud%20Concepts.html#cloud-computing-models","title":"Cloud Computing Models","text":"<ul> <li>Models vary based on the control users want to have over the resources</li> <li>Infrastructure as a Service (IaaS)</li> <li>Any servers users deploy to the cloud</li> <li>Can be configured to our desire and users are responsible for maintenance</li> <li>Platform as a Service (PaaS)</li> <li>A service preconfigured and available for users</li> <li>It can then be customised by users as appropriate e.g. Wordpress, AWS Elastic Beanstalk</li> <li>Software as a Service (SaaS)</li> <li>Any software users have access to and can use</li> <li>Users do not need to consider or maintain the infrastructure.</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/01_Fundamental-Cloud-Concepts/Fundamental%20Cloud%20Concepts.html#cloud-deployment-models","title":"Cloud Deployment Models","text":"<ul> <li>Public Cloud: Deployment to a provider like AWS</li> <li>On-Premises Private Cloud: Deployment to a cloud-like platform in a private datacenter (VMware is a common example)</li> <li>Hybrid: A combination of both e.g. cloud applications connected to a private data center.</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/01_Fundamental-Cloud-Concepts/Fundamental%20Cloud%20Concepts.html#cloud-computing-scenarios","title":"Cloud Computing Scenarios","text":""},{"location":"certifications/aws-cloud-practitioner/course-notes/01_Fundamental-Cloud-Concepts/Fundamental%20Cloud%20Concepts.html#scenario-1","title":"Scenario 1","text":"<ul> <li>Several production workloads in its datacentre</li> <li>VMware to manage infrastructure in their data centre</li> <li>Want to use AWS and integrate it with their data center for new workloads.</li> <li>What would they be following?</li> <li>Hybrid - Both public and private cloud used.</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/01_Fundamental-Cloud-Concepts/Fundamental%20Cloud%20Concepts.html#scenario-2","title":"Scenario 2","text":"<ul> <li>Company determining whether to fund a new line of business</li> <li>Team looking to monetize a new emerging technology</li> <li>New business line requires new infrastructure</li> <li>What benefit is most relevant?</li> <li>Increased agility and elasticity</li> <li>Pay-as-you-go</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/01_Fundamental-Cloud-Concepts/Fundamental%20Cloud%20Concepts.html#scenario-3","title":"Scenario 3","text":"<ul> <li>Insurance company</li> <li>Moving to cloud instead of colocating servers</li> <li>Want to have maximum control for security and compliance reasons</li> <li>What cloud computing model</li> <li>IaaS - Servers can be configured as required and maintained.</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/01_Fundamental-Cloud-Concepts/Fundamental%20Cloud%20Concepts.html#aws-global-infrastructure","title":"AWS Global Infrastructure","text":""},{"location":"certifications/aws-cloud-practitioner/course-notes/01_Fundamental-Cloud-Concepts/Fundamental%20Cloud%20Concepts.html#aws-regions-and-availability-zones","title":"AWS Regions and Availability Zones","text":"<ul> <li>AWS Regions</li> <li>Resource / Service deployment location as geographic regions</li> <li>Each geographic location has a cluster of data centres</li> <li>Availability Zones</li> <li>A zone of one or more data centres</li> <li>Multiple availability zones are included per AWS region</li> <li>All located within the geographic area of the AWS region (sub-regions effectively)</li> <li>Have redundant power, networking and connectivity for disaster recovery / high availability purposes.</li> <li>Example - US has 6 regions, each with at least 3 availability zones</li> <li>Availability: The extent to which an application is fulfilling its intended purpose. Applications that are highly-available are built in a manner where a single failure won't lessen its ability to be fully operational.</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/01_Fundamental-Cloud-Concepts/Fundamental%20Cloud%20Concepts.html#region-and-availability-zone-naming","title":"Region and Availability Zone Naming","text":"<ul> <li>Example: us-east-2a</li> <li>Format: <code>&lt;AREA&gt;-&lt;SUB-AREA&gt;-&lt;NUMBER&gt;&lt;Availability Zone Letter&gt;</code></li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/01_Fundamental-Cloud-Concepts/Fundamental%20Cloud%20Concepts.html#local-and-wavelength-zones","title":"Local and Wavelength Zones","text":""},{"location":"certifications/aws-cloud-practitioner/course-notes/01_Fundamental-Cloud-Concepts/Fundamental%20Cloud%20Concepts.html#local-zone","title":"Local Zone","text":"<ul> <li>Aim to place compute, storage, database, and other AWS services closer to end-users</li> <li>An extension of a particular AWS region</li> <li>Provides high-bandwidth, secure connections betwen local workloads and those running in AWS region.</li> <li>Allows seasmless connection to the full range of in-region services through the same APIs and toolsets.</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/01_Fundamental-Cloud-Concepts/Fundamental%20Cloud%20Concepts.html#wavelength-zones","title":"Wavelength Zones","text":"<ul> <li>AWS infrastructure deployments that embed AWS compute and storage services within communication service providers 5G networks.</li> <li>Allows application traffic from 5G devices to reach application servers without leaving the telecommunication networks.</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/01_Fundamental-Cloud-Concepts/Fundamental%20Cloud%20Concepts.html#aws-edge-locations","title":"AWS Edge Locations","text":""},{"location":"certifications/aws-cloud-practitioner/course-notes/01_Fundamental-Cloud-Concepts/Fundamental%20Cloud%20Concepts.html#points-of-prescence","title":"Points of Prescence:","text":"<ul> <li>Elements of AWS global infrastructure that exist outside of AWS regions.</li> <li>Located in or around populated areas - specific AWS services use them to deliver content to end users as quickly as possible.</li> <li>2 Types of infrastructure per point of prescence:</li> <li>Edge locations</li> <li>Regional edge caches</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/01_Fundamental-Cloud-Concepts/Fundamental%20Cloud%20Concepts.html#edge-locations","title":"Edge Locations","text":"<ul> <li>Used as nodes of a global content delivery network</li> <li>Primarily used by Amazon CloudFront and AWS Route 53</li> <li>Allows AWS to serve content from locations closest to users.</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/01_Fundamental-Cloud-Concepts/Fundamental%20Cloud%20Concepts.html#visualizing-aws-global-infrastructure","title":"Visualizing AWS Global Infrastructure","text":"<ul> <li>Global infrastructure</li> <li>Edge locations listed per region amongst other information</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/01_Fundamental-Cloud-Concepts/Fundamental%20Cloud%20Concepts.html#global-infrastructure-scenarios","title":"Global Infrastructure Scenarios","text":""},{"location":"certifications/aws-cloud-practitioner/course-notes/01_Fundamental-Cloud-Concepts/Fundamental%20Cloud%20Concepts.html#global-infrastructure-scenario-1","title":"Global Infrastructure Scenario 1","text":"<ul> <li>Looking to transfer to AWS with a few workloads</li> <li>Requirement to store backup data in multiple geographic areas</li> <li>What element of AWS Global Infrastructure will help best?</li> <li>AWS Regions - Regions are geographical areas e.g. could store data in one and run the applications in another.</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/01_Fundamental-Cloud-Concepts/Fundamental%20Cloud%20Concepts.html#global-infrastructure-scenario-2","title":"Global Infrastructure Scenario 2","text":"<ul> <li>Content served across the world</li> <li>Wanting to optimize performance to users worldwide</li> <li>Want to leverage a content delivery network</li> <li>What element is best suited to help this from AWS infrastructure?</li> <li>Edge Locations</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/01_Fundamental-Cloud-Concepts/Fundamental%20Cloud%20Concepts.html#global-infrastructure-scenario-3","title":"Global Infrastructure Scenario 3","text":"<ul> <li>Legacy applications transitioning to AWS</li> <li>99.5% uptime required</li> <li>Don't want issues at single datacentres to cause outages</li> <li>What element of the global infrastructure will help?</li> <li>Availability zones</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/01_Fundamental-Cloud-Concepts/Fundamental%20Cloud%20Concepts.html#understanding-cloud-economics","title":"Understanding Cloud Economics","text":""},{"location":"certifications/aws-cloud-practitioner/course-notes/01_Fundamental-Cloud-Concepts/Fundamental%20Cloud%20Concepts.html#economics-of-the-cloud","title":"Economics of the Cloud","text":"<ul> <li>Capitalized Expenditure (CapEx):</li> <li>Upfront costs or investments to attain a fixed asset</li> <li>Example for building a datacenter would be the building, servers, etc.</li> <li>Operating Expenditure (OpEx)</li> <li>Day-to-day business activities</li> <li>Example being maintenance costs</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/01_Fundamental-Cloud-Concepts/Fundamental%20Cloud%20Concepts.html#handling-demand-in-data-centre","title":"Handling Demand in Data Centre","text":"<ul> <li>As application usage grows, data centres need to grow with it in capacity</li> <li>When first launched, there would be a lot of unused capacity</li> <li>Eventually, demand would exceed capacity \u2192 data centre capacity needs to be increased</li> <li>In terms of CapEx vs OpEx</li> <li>CapEx starts out large due to initial acquiring</li> <li>OpEx maintained at generally consistent levels.</li> <li>Any time the capacity needs to be increased, CapEx goes up.</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/01_Fundamental-Cloud-Concepts/Fundamental%20Cloud%20Concepts.html#handling-demand-in-the-cloud","title":"Handling Demand in the Cloud","text":"<ul> <li>Companies in the cloud can ensure the capacity is always \"just enough\" to ensure the demand is met</li> <li>As the cloud resources are being leveraged, there is no CapEx costs, instead, the OpEx costs vary depending on the application usage adn the demand.</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/01_Fundamental-Cloud-Concepts/Fundamental%20Cloud%20Concepts.html#financial-implications","title":"Financial Implications","text":"Own Data Centre Cloud Infrastructure Large CapEx No upfront investment Potential for under used capacity or unmet demand Pay as you go for infrastructure (OpEx) Increasing capacity takes time and additional investment (CapEx) Capacity scales to meet user demand and can be immediately provisioned Monthly costs will map to predicted infrastructure needs Costs mirror usage levels - use more, pay more, use less, pay less etc."},{"location":"certifications/aws-cloud-practitioner/course-notes/01_Fundamental-Cloud-Concepts/Fundamental%20Cloud%20Concepts.html#organizing-and-optimizing-aws-costs","title":"Organizing and Optimizing AWS Costs","text":"<ul> <li>AWS Cost Explorer</li> <li>User Interface for AWS Cost Analysis</li> <li>Provides breakdowns per:<ul> <li>Service</li> <li>Cost tag</li> </ul> </li> <li>Provides 3-monthly forecasting.</li> <li>Provides recommendations for cost optimizations.</li> <li>Data accessible via API</li> <li>AWS Budgets</li> <li>Takes data from AWS Cost explorer to plan and track usage across services.</li> <li>Tracks cost per service, service usage, reserved instance utilisation and coverage, and savings plans utilisation and coverage.</li> <li>AWS Cost Planning Tools:</li> <li>AWS Pricing Calculator<ul> <li>Allows in-depth analysis of cost for multiple AWS services for cloud-based workloads.</li> </ul> </li> <li>AWS Migration Hub:<ul> <li>Provides recommendations and a business case for transitioning workloads to the cloud.</li> </ul> </li> <li>Deprecated Tools:</li> <li>AWS TCO Calculator - Enabled estimated savings for using cloud infrastructure to be determined</li> <li>AWS Simple Monthly Calculator</li> <li>AWS Resource Tags:</li> <li>Metadata assigned to specific AWS resources</li> <li>Key/Value</li> <li>Common usage includes department, environment, or project.</li> <li>Cost allocation can report can include costs grouped by active tags.</li> <li>AWS Organizations:</li> <li>Allows organisations to manage multiple accounts under a single master account</li> <li>Offers consolidated billing for all accounts.</li> <li>Facilitates centralized logging and security standard implementation.</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/01_Fundamental-Cloud-Concepts/Fundamental%20Cloud%20Concepts.html#building-a-business-case-for-the-cloud","title":"Building a Business Case for the Cloud","text":"<ul> <li>Steps to build a business case:</li> <li>Analyze the current workloads</li> <li>Forecast the infrastructure needs</li> <li>Create a Total Cost of Ownership (TCO) for both options.</li> <li>Tools available:</li> <li>AWS Migration Hub - Gathers information from multiple services and tools in AWS to forecast required infrastructure.</li> <li>Migration Evaluator - Similar to the migration hub but provides a more in-depth analysis.</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/01_Fundamental-Cloud-Concepts/Fundamental%20Cloud%20Concepts.html#aws-pricing-calculator","title":"AWS Pricing Calculator","text":"<ul> <li>Used to estimate future workloads</li> <li>Accessible at calculator.aws</li> <li>Create Estimate \u2192 Fill out desired fields i.e.:</li> <li>Location or Service Type</li> <li>Configure Services:<ul> <li>Quick Estimate or Advanced Estimate</li> <li>Service Details e.g. for EC2 check the resources</li> <li>Expected utilisation</li> <li>Pricing Strategies</li> <li>Any sub-services e.g. Elastic Block Storage (EBS) for EC2 Instances</li> </ul> </li> <li>Upfront, Monthly and Total 12 month costs are provided upon completion.</li> <li>Any additional services can then be added to the estimate as required e.g. Amazon RDS for PostgreSQL</li> <li>The estimate can then be exported or shared.</li> <li>Additionally estimates can be grouped within the estimate e.g. group based on \"application\" or \"function\".</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/01_Fundamental-Cloud-Concepts/Fundamental%20Cloud%20Concepts.html#reviewing-costs-costs-explorer","title":"Reviewing Costs - Costs Explorer","text":"<ul> <li>AWS Accounts \u2192 Profile Dropdown \u2192 Billing Dashboard \u2192 Cost Explorer \u2192 Launch Cost Explorer</li> <li>Provides initial overviews such as:</li> <li>Current monthly cost</li> <li>Forecast month-end costs</li> <li>Daily cost grouping<ul> <li>Can be filtered per AWS Service, region, resource, etc.</li> </ul> </li> <li>From LHS pane \u2192 Reports</li> <li>Includes reports such as:<ul> <li>Monthly costs per account</li> <li>Daily costs</li> </ul> </li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/01_Fundamental-Cloud-Concepts/Fundamental%20Cloud%20Concepts.html#applying-cloud-economics","title":"Applying Cloud Economics","text":""},{"location":"certifications/aws-cloud-practitioner/course-notes/01_Fundamental-Cloud-Concepts/Fundamental%20Cloud%20Concepts.html#cloud-economics-scenario-1","title":"Cloud Economics Scenario 1","text":"<ul> <li>Multiple departments within AWS</li> <li>FInance requesting clean separation of AWS costs within departments</li> <li>All resources are within a single AWS Account</li> <li>What approach would meet this need for future costs with minimal effort?</li> <li>Resource tags</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/01_Fundamental-Cloud-Concepts/Fundamental%20Cloud%20Concepts.html#cloud-economics-scenario-2","title":"Cloud Economics Scenario 2","text":"<ul> <li>Company considering transition to the cloud</li> <li>2 physical data centres</li> <li>Stakeholders wanting financial insight</li> <li>Which approach to make a business case?</li> <li>Use the migration hub or migration evaluator</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/01_Fundamental-Cloud-Concepts/Fundamental%20Cloud%20Concepts.html#cloud-economics-scenario-3","title":"Cloud Economics Scenario 3","text":"<ul> <li>Web developer</li> <li>Looking to move site to cloud.</li> <li>Financial estimates needed.</li> <li>What approach?</li> <li>Use the pricing calculator.</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/01_Fundamental-Cloud-Concepts/Fundamental%20Cloud%20Concepts.html#supporting-aws-infrastructure","title":"Supporting AWS Infrastructure","text":""},{"location":"certifications/aws-cloud-practitioner/course-notes/01_Fundamental-Cloud-Concepts/Fundamental%20Cloud%20Concepts.html#support-resources","title":"Support Resources","text":"<ul> <li>AWS support \u2192 Allows support tickets to be submitted</li> <li>Includes Personal Health Dashboard. and Trusted Advisor</li> <li>AWS Support:</li> <li>Enables support from AWS resources for workloads running in the cloud</li> <li>Provided in different tiers based on need and scope</li> <li>Includes tools to provide automated answers and recommendations</li> <li>AWS Personal Health Dashboard</li> <li>Provides alerts and remediation guidance when events occur in AWS that may impact you e.g. regional outages</li> <li>AWS Trusted Advisor</li> <li>An automated tool to check AWS usage against best practices</li> <li>Accessible from the AWS console</li> <li>Provides multiple checks based on the support plan tier, in addition to core checks</li> <li>Checks include:<ul> <li>Cost optimization</li> <li>Performance</li> <li>Security</li> <li>Fault Tolerance</li> <li>Service Limits</li> </ul> </li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/01_Fundamental-Cloud-Concepts/Fundamental%20Cloud%20Concepts.html#aws-support-plan-tiers","title":"AWS Support Plan Tiers","text":"<ul> <li>Support plan tiers are based on:</li> <li>Communication method</li> <li>Response Time</li> <li>Cost</li> <li>Type of guidance offered</li> <li>Basic Support</li> <li>Provided for all customers</li> <li>Access to trusted advisor (7 core checks)</li> <li>Documentation, forums, and 24x7 customer service access</li> <li>AWS Personal health dashboard</li> <li>Free</li> <li>Developer Support</li> <li>Everything in basic</li> <li>Includes access to support engineers via email during business hours</li> <li>1 Primary contact</li> <li>$29/month</li> <li>Business support</li> <li>All of developer support</li> <li>Full set of trusted advisor checks</li> <li>24x7 phone, email and chat access to support engineers</li> <li>unlimited contacts for support requests</li> <li>Third-party software support provided</li> <li>$100/month (based on AWS usage)</li> <li>Enterprise Support:</li> <li>All features of business support</li> <li>Includes technical account manager (TAM)</li> <li>Includes concierge support team</li> <li>$15,000/month</li> <li>Support Response Times dependent upon the nature of the request:</li> <li>General Guidance</li> <li>System Impaired</li> <li>Production System Impaired</li> <li>Production System Down</li> <li>Business-Critical System Down</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/01_Fundamental-Cloud-Concepts/Fundamental%20Cloud%20Concepts.html#aws-support-tools","title":"AWS Support Tools","text":"<ul> <li>AWS Console \u2192 Health View and Trusted Advisor are automatically included as widgets</li> <li>Also accessible from search bar</li> <li>Trusted Advisor:</li> <li>Shows summary and recommendation categories</li> <li>Recommendations only provided based on checks available.</li> <li>Details on recommended actions provided, with remediation steps.</li> <li>Will show checks which aren't included for reference.</li> <li>Checks can be downloaded.</li> <li>AWS Health Dashboard</li> <li>Service health dashboard</li> <li>Issue and events logs</li> <li>Service history.</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/01_Fundamental-Cloud-Concepts/Fundamental%20Cloud%20Concepts.html#when-you-need-help","title":"When You Need Help","text":"<ul> <li>Resources available for help:</li> <li>AWS Quickstart - Provides steps for standard platform deployments</li> <li>AWS Partner Network Consulting Partners - 3rd Party Consultants that are Partners with AWS</li> <li>AWS Professional Services</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/01_Fundamental-Cloud-Concepts/Fundamental%20Cloud%20Concepts.html#infrastructure-support-scenarios","title":"Infrastructure Support Scenarios","text":""},{"location":"certifications/aws-cloud-practitioner/course-notes/01_Fundamental-Cloud-Concepts/Fundamental%20Cloud%20Concepts.html#infrastructure-support-scenario-1","title":"Infrastructure Support Scenario 1","text":"<ul> <li>Moving multiple workloads into AWS</li> <li>One workload is mission-critical</li> <li>24/7 support needed</li> <li>What support level? - Business Support</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/01_Fundamental-Cloud-Concepts/Fundamental%20Cloud%20Concepts.html#infrastructure-support-scenario-2","title":"Infrastructure Support Scenario 2","text":"<ul> <li>Evaluating AWS for future or workloads</li> <li>Workloads supports multiple offices globally</li> <li>Email text or call to support needed.</li> <li>Response within 15 mins needed.</li> <li>What support plan? Enterprise</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/01_Fundamental-Cloud-Concepts/Fundamental%20Cloud%20Concepts.html#infrastructure-support-scenario-3","title":"Infrastructure Support Scenario 3","text":"<ul> <li>Account for a personal project</li> <li>No Technical Guidance needed</li> <li>Want access to Trusted Advisor etc</li> <li>Basic Support plan needed.</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html","title":"2.0 - Understanding AWS Core Services","text":""},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#interacting-with-aws","title":"Interacting with AWS","text":""},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#methods-of-interacting-with-aws","title":"Methods of Interacting with AWS","text":"<ul> <li>AWS Console - Browser-based web app for interacting with most or all of the services.</li> <li>Typically used for testing out AWS services.</li> <li>AWS CLI - Command line access for administering AWS resources</li> <li>AWS SDK - Programmatic access - allows you to interact with AWS services via code.</li> <li>Supported by a multitude of programming languages e.g. NodeJS, JavaScript, Python, GO.</li> <li>Repeating tasks typically automated via the CLI and or SDK.</li> <li>SDK allows automation of AWS tasks within custom applications.</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#using-the-aws-console","title":"Using the AWS Console","text":"<ul> <li>User sign in available</li> <li>Root user == Admin - can do anything</li> <li>User information shown e.g. account, organization, quotas, security.</li> <li>Region and support selectors available.</li> <li>Services dropdown shows most of available services along with search bar.</li> <li>Certain regions will be shown to not be region-specific.</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#using-the-aws-cli","title":"Using the AWS CLI","text":"<ul> <li>AWS Console \u2192 Security Credentials \u2192 Access Keys \u2192 Create New Access Keys</li> <li>Advised to create access keys for IAM users only, not root.</li> <li>Copy access key and secret access key (one-time viewing only!)</li> <li>Installation:</li> <li>Install per platform requirements</li> <li>Verify with <code>aws --version</code></li> <li>Add access keys: <code>aws configure --profile &lt;profile name&gt;</code><ul> <li>Add access key, secret access key, default region and output format</li> </ul> </li> <li>Verify with sample command e.g. <code>aws s3 ls --profile &lt;profile name&gt;</code></li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#interacting-with-aws-scenarios","title":"Interacting with AWS Scenarios","text":""},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#interacting-with-aws-scenario-1","title":"Interacting with AWS Scenario 1","text":"<ul> <li>Several production workloads</li> <li>New web application that manages digital assets</li> <li>Automatically need to create a user account in AWS Cognito on signup</li> <li>What interaction method? SDK - tailored to the application SDK.</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#interacting-with-aws-scenario-2","title":"Interacting with AWS Scenario 2","text":"<ul> <li>Company considering AWS</li> <li>Want to use AWS Relational Database Service and test a single database</li> <li>What interaction method? Console</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#interacting-with-aws-scenario-3","title":"Interacting with AWS Scenario 3","text":"<ul> <li>Startup company with web and mobile app</li> <li>Set of repeatable tasks for generating reports.</li> <li>AWS CLI recommended.</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#compute-services","title":"Compute Services","text":"<ul> <li>Any service that allows leveraging of cloud-based virtual machines.</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#ec2-overview","title":"EC2 Overview","text":"<ul> <li>A web service that provides resizable compute capacity in the cloud.</li> <li>Use cases include:</li> <li>Web application hosting</li> <li>Batch processing of data</li> <li>Web services endpoint</li> <li>Desktop in the cloud</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#ec2-instance-types","title":"EC2 Instance Types","text":"<ul> <li>Determines the processor, memory and storage type</li> <li>Cannot be changed without downtime</li> <li>Provided in the following categories:</li> <li>General-purpose</li> <li>Compute, memory and storage optimized</li> <li>Accelerated computing (special / high performance)</li> <li>Pricing based on instance type and category.</li> <li>Certain instance types have unique capabilities.</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#root-device-type","title":"Root Device Type","text":"<ul> <li>2 Primary Types:</li> <li>Instance store - Ephemeral storage physicall attached to the host the virtual server is running on.</li> <li>Elastic Block Store (EBS) - Persistent storage that exists seperately from the host the virtual server is running on.<ul> <li>Generally more commonly used than instance store.</li> </ul> </li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#amazon-machine-image-ami","title":"Amazon Machine Image (AMI)","text":"<ul> <li>A template for an EC2 instance, including configuration, OS and data</li> <li>AWS by default provides many AMIs</li> <li>AMIs can be shared across AWS accounts.</li> <li>Custom AMIs can be created based on configuration.</li> <li>Commercial AMIs are available in the AWS marketplace.</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#ec2-purchase-types","title":"EC2 Purchase Types","text":"<ul> <li>Options available:</li> <li>On-demand</li> <li>Reserved<ul> <li>Provides discounts over on-demand when can commit to a specific period of time.</li> <li>Provides a capacity reservation for the specific instance type.</li> <li>Instance types:</li> <li>Standard - highest discount and works for steady workloads<ul> <li>Cost Models:</li> <li>All upfront (highest savings but biggest upfront investment)</li> <li>Partial upfront with reduced monthly cost.</li> <li>No upfront but still providing a reduced monthly cost compared to standard.</li> </ul> </li> <li>Convertible - Allows conversion of some attributes, works for steady workloads</li> <li>Scheduled - Works for a particular time window you reserve, typically used for predictable workloads.</li> </ul> </li> <li>Savings Plan<ul> <li>Similar to reserved instances</li> <li>Supports compute with EC2, Fargate and Lambda</li> <li>Unlike reserved, it doesn't reserve capacity</li> <li>Provides up to 72% savings</li> <li>1-3 Year terms available.</li> </ul> </li> <li>Spot<ul> <li>Allows leveraging excess EC2 capacity</li> <li>Provides up to 90% discount over on demand pricing</li> <li>Market price for all instance types per availability zone - the spot price</li> <li>When requesting instances, if bid is higher than spot price - instance will launch</li> <li>If spot price grows to exceed bid, the instance terminates within 2 minutes.</li> </ul> </li> <li>Dedicated<ul> <li>Dedicated physical server in the datacentre.</li> <li>May be required for server software licensing or compliance requirements.</li> </ul> </li> <li>If instance is consistent and always needed - standard or convertible reserved.</li> <li>If batch processing and the process can start and stop without any adverse impact - use spot instances.</li> <li>If an inconsistent need for instances that cannot be stopped without affecting the job - on-demand.</li> <li>For specific per-server licensing or compliance requirements - use dedicated hosts.</li> <li>If using Lambda or Fargate alongside EC2 - choose a savings plan.</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#launching-an-ec2-instance","title":"Launching an EC2 Instance","text":"<ul> <li>AWS Console \u2192 EC2 \u2192 Launch instance</li> <li>Select AMI \u2192 Custom / Marketplace / Community / Free</li> <li>Select Instance Type</li> <li>Configure instance details e,g,:</li> <li>Network</li> <li>Subnet</li> <li>Placement group</li> <li>IAM assigning</li> <li>User data - commands to run when starting</li> <li>Configure storage</li> <li>Configure network security group</li> <li>For general purpose testing, allow only \"my IP\" to access e.g. for SSH</li> <li>Summary &amp; Launch</li> <li>Note: a key pair should be selected or created as appropriate.</li> <li>Once launch, find public dns and access via browser to validate.</li> <li>To terminate - select instance, then under actions \u2192 instance state \u2192 terminate</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#elastic-beanstalk-overview","title":"Elastic Beanstalk Overview","text":"<ul> <li>Automates deployment and scaling of EC2 Workloads (PaaS rather than IaaS)</li> <li>Supports only a specific set of technologies</li> <li>Leverages existing AWS services</li> <li>Only pay for other services leveraged.</li> <li>Handles provisioning, load balancing, scaling and monitoring.</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#supported-application-platforms","title":"Supported Application Platforms","text":"<ul> <li>Includes:</li> <li>Docker</li> <li>JAVA</li> <li>NodeJS</li> <li>Python</li> <li>GO</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#features","title":"Features","text":"<ul> <li>Monitoring</li> <li>Deployment</li> <li>Scaling</li> <li>EC2 Customizations</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#use-cases","title":"Use Cases","text":"<ul> <li>Deploy an application with minimal knowledge of other services</li> <li>Reduce overall maintenance needed for application</li> <li>Few customizations needed.</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#launching-an-app-on-elastic-beanstalk","title":"Launching an App on Elastic Beanstalk","text":"<ul> <li>Tutorials provided depending on the applications to be deployed, along with sample applications for deployment.</li> <li>AWS Console \u2192 Elastic Beanstalk \u2192 Get Started</li> <li>Provide details:</li> <li>Application Name</li> <li>Application Platform</li> <li>Application code \u2192 Sample application or upload code</li> <li>Verify successful deployment by using the URL provided.</li> <li>Functions provided incluide:</li> <li>Health</li> <li>Logs</li> <li>Monitoring</li> <li>Alarms</li> <li>Events</li> <li>To terminate, Actions \u2192 Terminate</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#aws-lambda-overview","title":"AWS Lambda Overview","text":"<ul> <li>Allows running of code without provisioning or managing servers.</li> <li>Only pay for the compute time consumed.</li> <li>Allows running of code for any type of app or backend service with minimal administration.</li> <li>Available memory configurable between 128-3008Mb</li> <li>Integrates with many AWS services like S3 and Dynamo DB</li> <li>Enable event-driven workflows</li> <li>Primary service for serverless architectures</li> <li>Advantages include:</li> <li>Reduced maintenance requirements</li> <li>Enables tolerance without additional work</li> <li>Scales based ond emand</li> <li>Pricing based on usage only.</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#compute-services-scenarios","title":"Compute Services Scenarios","text":""},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#compute-services-scenario-1","title":"Compute Services Scenario 1","text":"<ul> <li>Moving multiple workloads into AWS</li> <li>One workload to be leveraged for at least 5 years</li> <li>Organization is looking for most cost-effective solution</li> <li>Answer:</li> <li>All upfront reserved - 3 years</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#compute-services-scenario-2","title":"Compute Services Scenario 2","text":"<ul> <li>PHP app to be deployed to virtual server</li> <li>Minimal experience managing EC2 Instances</li> <li>Need to scale on demand</li> <li>Answer:</li> <li>AWS Elastic Beanstalk</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#compute-services-scenario-3","title":"Compute Services Scenario 3","text":"<ul> <li>Data processing workloads</li> <li>Workloads happen daily and can start or stop without issue</li> <li>Workload to be leveraged for at least 1 year</li> <li>Answer:</li> <li>Spot Instances as workload can be started/stopped without issue.</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#content-and-network-delivery-services","title":"Content and Network Delivery Services","text":""},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#amazon-vpc-and-direct-connect","title":"Amazon VPC and Direct Connect","text":""},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#vpc","title":"VPC","text":"<ul> <li>VPC = Virtual Private Cloud</li> <li>Isolated section of AWS Cloud where resources can be launched into a user-defined virtual network</li> <li>Enables virtual networks in AWS</li> <li>Supports IPv4 and IPv6</li> <li>Allows configuration of:<ul> <li>IP address range</li> <li>subnet</li> <li>Route tables</li> <li>network gateways</li> </ul> </li> <li>Supports private and public subnets</li> <li>Can utilize NAT for private subnets</li> <li>Enables connection to data center</li> <li>Can connect to other VPCs</li> <li>Supports private connections to many AWS services</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#direct-connect","title":"Direct Connect","text":"<ul> <li>A cloud service solution that makes it easy to establish a dedicated network connection from your data center to AWS.</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#amazon-route-53","title":"Amazon Route 53","text":"<ul> <li>Domain Name Service (DNS)</li> <li>Global AWS service (not regional)</li> <li>Highly available \u2192 Minimal downtime</li> <li>Enables global resource routing</li> <li>DNS:</li> <li>Translates readily memorized domain names to the numerical IP addresses needed for locating and identifying computer services and devices with the underlying network protocols.</li> <li>DNS Changes are not instantaneous - may take around 2 hours.</li> <li>Example:</li> <li>Consider an application hosted in us-east-1 and eu-west-1</li> <li>If the us-east-1 goes down, Route 53 could be configured to automatically route the requests made to that server to the other region.</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#elastic-load-balancing","title":"Elastic Load Balancing","text":"<ul> <li>Elasticity - The ability for infrastructure supporting an application to scale up and down based on demand.</li> <li>Elastic load balancing:</li> <li>Distributes traffic across multiple targets</li> <li>Integrates with EC2, ECS and Lambda</li> <li>Supports one or more availability zones in a region.</li> <li>3 Types of load balancer available:<ul> <li>Application (ALB)</li> <li>Network (NLB)</li> <li>Classic (ELB)</li> </ul> </li> <li>Scaling on EC2:</li> <li>Vertical scaling - scaling up with a larger instance type and additional resources.<ul> <li>Would require downtime for changes to be made</li> </ul> </li> <li>Horizontal scaling - add additional instances to handle application demand.</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#amazon-cloudfront-and-api-gateway","title":"Amazon CloudFront and API Gateway","text":"<ul> <li>CloudFront:</li> <li>Content Delivery Network</li> <li>Enables users to get content from closest server to them</li> <li>Supports static and dynamic content</li> <li>Utilizes AWS Edge locations</li> <li>Includes advanced security features such as:<ul> <li>AWS Shield for DDoS</li> </ul> </li> <li>Edge locations are scattered worldwide.</li> <li>API Gateway:</li> <li>A fully managed API management service</li> <li>Directly integrates with many AWS sservices</li> <li>Allows monitoring and metrics on API calls</li> <li>Supports VPC and on-prem private applications</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#aws-global-accelerator","title":"AWS Global Accelerator","text":"<ul> <li>Networking service that sends user traffic through Amazon's global network infrastructure.</li> <li>Utilizes IP addresses that route to edge locations</li> <li>When request reaches edge locations, traffic is routed through AWS network</li> <li>Routes requests to many resources including</li> <li>NLB</li> <li>ALB</li> <li>EC2 Instances</li> <li>Elastic IP Addresses</li> <li>Can lead to varying performance improvements:</li> <li>Distance between user and initial endpoint is minimized via edge location usage</li> <li>Traffic is optimized by using AWS networks instead of public internet</li> <li>Results in improvement of first-byte latency, etc</li> <li>Provides superior fault tolerance by not relying on DNS resolution.</li> <li>Use Cases:</li> <li>Non-HTTP Protocol e.g. UDP, MQTT, VOIP</li> <li>Static IP required.</li> <li>Instant Failover required.</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#aws-global-accelerator-scenarios","title":"AWS Global Accelerator Scenarios","text":""},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#aws-global-accelerator-scenario-1","title":"AWS Global Accelerator Scenario 1","text":"<ul> <li>2 Corporate Data centres</li> <li>Want their data centres to work alongside AWS for specific workloads</li> <li>Wanting persistent connection to AWS</li> <li>What Service?</li> <li>Answer:  AWS Direct Connect</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#aws-global-accelerator-scenario-2","title":"AWS Global Accelerator Scenario 2","text":"<ul> <li>Company serves content through their site to users around the globe</li> <li>Looking to optimize performance to users around the world.</li> <li>Want to leverage a CDN,</li> <li>What service?</li> <li>Answer: Amazon CloudFront</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#aws-global-accelerator-scenario-3","title":"AWS Global Accelerator Scenario 3","text":"<ul> <li>Internal application on EC2 Server</li> <li>Downtime currently as demand &gt; capacity</li> <li>Bigger server or more servers?</li> <li>What scaling approach and services?</li> <li>Answer: Horizontal Scaling and Elastic Load Balancing</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#file-storage-services","title":"File Storage Services","text":""},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#amazon-s3-overview","title":"Amazon S3 Overview","text":"<ul> <li>Files stored as objects in \"buckets\"</li> <li>Provides different storage classes per use case</li> <li>Data can be stored across multiple availability zones</li> <li>Enables URL access for files</li> <li>Offers configurable rules for data lifecycle</li> <li>Can serve as a static website host</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#non-archival-storage-class","title":"Non-Archival Storage Class","text":"<ul> <li>S3 Standard - default, for frequently accessed data</li> <li>S3 Intelligent Tiering - Moves data to the correct storage class based on usage</li> <li>S3 Standard-IA - For infrequently accessed data with standard resilience</li> <li>S3 One-Zone-IA - For infrequently accessed data only stored in 1 availability zone.</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#s3-intelligent-tiering","title":"S3 Intelligent Tiering","text":"<ul> <li>Automatically moves files based on access</li> <li>Moves between frequent and infrequent access (like hot/cold blobs in Azure)</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#s3-lifecycle-policies","title":"S3 Lifecycle Policies","text":"<ul> <li>Objects in a bucket can transition or expire based on criteria</li> <li>Transitions enable objects to move to another storage class based on time</li> <li>Expiration can delete objects based on age</li> <li>Policies can also factor in versions of a specific object in the bucket.</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#s3-transfer-acceleration","title":"S3 Transfer Acceleration","text":"<ul> <li>Enabled per bucket</li> <li>Allows for optimized uploading of data using Edge locations</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#hosting-a-website-on-amazon-s3","title":"Hosting a Website on Amazon S3","text":"<ul> <li>AWS Console \u2192 S3</li> <li>Create Bucket</li> <li>Supply details:<ul> <li>Name (must be unique across accounts)</li> <li>Region</li> <li>Versioning</li> <li>Tags</li> <li>Permissions</li> <li>Enable public access (if acting as a public website)</li> </ul> </li> <li>Select bucket and upload desired files e.g. \"index.html and logo\"</li> <li>Set the. permissions for users as required.</li> <li>Configure the file storage classes as required e.g. storage, intelligent-tiering.</li> <li>Encryption - Amazon S3 Master Key ensures encryption at-rest.</li> <li>When viewing the files, each should come with an object URL</li> <li>Configure the permissions as required e.g. \"everyone read access for the object\"</li> <li>Under properties \u2192 Static website hosting \u2192 \"use this bucket to host a website\"</li> <li>Select index file and error document where required.</li> <li>Redirect rules are optional.</li> <li>Once enabled, URL endpoint provided - permissions must be set to allow access</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#glacier-and-glacier-deep-architecture","title":"Glacier and Glacier Deep Architecture","text":"<ul> <li>S3 Glacier</li> <li>Designed for archiving of data in S3 as a separate storage class</li> <li>Offers configurable retrieval times - quick / less quick</li> <li>Files can be sent directly or through lifecycle rules in S3</li> <li>Classes:<ul> <li>S3 Glacier</li> <li>Designed for archival data</li> <li>90-Day minimum storage duration</li> <li>Minutes or hours retrieval time</li> <li>Retrieval fee per GB retrieved</li> <li>Over 5x less expensive than S3 Standard storage class</li> <li>S3 Glacier Deep Archive</li> <li>Archival data</li> <li>180-day minimum storage duration</li> <li>Data retrieval in hours</li> <li>Pay a retrieval fee per GB</li> <li>Over 23x less expensive than S3 Standard storage class.</li> </ul> </li> <li>Typically, the management console is used to quickly set up S3 glacier, the data can then be uploaded and retrieved programatically</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#elastic-block-store-ebs","title":"Elastic Block Store (EBS)","text":"<ul> <li>EBS = Persistent block storage for use in EC2 storage</li> <li>Can scale to support petabytes of data per workload</li> <li>Enables redundancy within an availability zone</li> <li>Allows users to take snapshots of its data</li> <li>Offers encryption of its volumes</li> <li>Provides multiple volume types including:<ul> <li>General SSD - Cost-effective for general workloads</li> <li>Provisioned IOPS SSD - High performance for low latency applications</li> <li>Cold HDD - Less frequently accessed data</li> <li>Throughput optimized HDD - Frequently accessed data</li> </ul> </li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#elastic-file-system","title":"Elastic File System","text":"<ul> <li>Elastic File System</li> <li>Fully managed NFS File System</li> <li>Designed for Linux Workloads</li> <li>Scales up to petabyte scale</li> <li>Stores data across multiple AZs</li> <li>Storage classes:</li> <li>Standard</li> <li>Infrequent access</li> <li>Provides configurable lifecycle data rules</li> <li>For windows workloads consider Amazon FSx</li> <li>Fully managed windows file system</li> <li>Includes native windows features like Windows NTFS, Active Directory Integration, and SMB Support</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#data-transfer-with-aws-snowball","title":"Data Transfer with AWS Snowball","text":"<ul> <li>Service to phyically migrate petabyte scale data to AWS</li> <li>AWS snowmobille - \"\" for exobyte scale data</li> <li>Snowball:</li> <li>Designed for large scale data transfer</li> <li>Supports petabyte-scale transfer</li> <li>Physical device delivered by AWS</li> <li>Snowball can be connected to network to upload data</li> <li>Device then returned by local carrier</li> <li>AWS recives device and loads data into S3</li> <li>Snowmobile</li> <li>Designed for large scale data transfer</li> <li>Supports exabyte-scale transfer</li> <li>Shipping container delivered to location</li> <li>AWS sets up connection to network</li> <li>Data loaded onto snowmobile</li> <li>AWS loads data into S3 when container received by AWS.</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#file-storage-scenarios","title":"File Storage Scenarios","text":""},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#file-storage-scenario-1","title":"File Storage Scenario 1","text":"<ul> <li>Site offering daily developer tutorials</li> <li>S3 stores assets needed per tutorials</li> <li>Assets very popular within the week of launch</li> <li>Not popular a week after</li> <li>What's the most cost-effective solution whilst maintaining durability?</li> <li>S3 Lifecycle rules with S3 Standard IA storage class</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#file-storage-scenario-2","title":"File Storage Scenario 2","text":"<ul> <li>Social media company</li> <li>2Pb of user-generated content to be migrated</li> <li>Is there a faster method than uploading over the internet?</li> <li>AWS Snowball</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#file-storage-scenario-3","title":"File Storage Scenario 3","text":"<ul> <li>Messaging app</li> <li>Looking for shared file system between 8 different Linux EC2 Instances</li> <li>File system to support 1Pb of data</li> <li>What approach?</li> <li>Utilise elastic file system (EFS) - needs to definitely be for linux workloads and maximum of Petabyte data.</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#database-services-and-utilities","title":"Database Services and Utilities","text":"<ul> <li>As with other services on AWS, you can utilise IaaS, PaaS or SaaS for database services and utilities. IaaS can be leveraged by EC2 Instances, however, PaaS and SaaS options are also available.</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#amazon-relational-database-system-rds","title":"Amazon Relational Database System (RDS)","text":"<ul> <li>PaaS for databases</li> <li>Fully managed service for relational database</li> <li>Handles provisioning, patching, backup and recovery of the database</li> <li>Supports deployments across multiple AZs</li> <li>Some platforms support read replicas</li> <li>Launches into a VPC</li> <li>Provides both general-purpose SSD and Provisioned IOPS SSD options for storage.</li> <li>Database options include:</li> <li>MySQL</li> <li>PostgreSQL</li> <li>MariaDB</li> <li>Oracle Database</li> <li>SQL Server</li> <li>Amazon Aurora<ul> <li>A MySQL and PostgreSQL-compatible relational database built for the cloud.</li> <li>Aims to optimize performance, simplicity and costs.</li> </ul> </li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#amazon-database-migration-service-dms","title":"Amazon Database Migration Service (DMS)","text":"<ul> <li>Allows moving of data into AWS from existing databases.</li> <li>Supports both one-time and continuous migration.</li> <li>Supports many popular database types.</li> <li>Only pay for compute leveraged by process.</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#amazon-dynamodb-overview","title":"Amazon DynamoDB Overview","text":"<ul> <li>Fully managed NoSQL database service - SaaS</li> <li>Provides key-value and document database functionality</li> <li>Offers extremely low latency at any scale</li> <li>Supports automated scaling based on configuration</li> <li>Offers in-memory cache with the DynamoDB Accelerator - DAXX</li> <li>Use cases:</li> <li>Scale without excessive maintenance</li> <li>Serverless applications</li> <li>Implementations where low latency is key</li> <li>Data models without blob storage</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#amazon-elasticache-and-redshift","title":"Amazon Elasticache and Redshift","text":""},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#elasticache","title":"Elasticache","text":"<ul> <li>Fully-managed in-memory data stores</li> <li>Supports Memcached and Redis</li> <li>Low latency in response times</li> <li>Enables scaling and replicas to meet application demand</li> <li>Common use cases:</li> <li>Database layer caching</li> <li>Session storage</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#redshift","title":"Redshift","text":"<ul> <li>Scalable data warehouse service</li> <li>Supports petabyte scale warehousing of data</li> <li>Leverages high performance disks and columnar storage</li> <li>Offers full encryption of contents</li> <li>Isolation via a VPC</li> <li>Querying of exabytes-worth of data can be done by AWS Redshift Spectrum</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#database-services-scenarios","title":"Database Services Scenarios","text":""},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#database-services-scenario-1","title":"Database Services Scenario 1","text":"<ul> <li>Financial services</li> <li>Data warehouse getting transitioned up to AWS for analysis</li> <li>Data warehouse needs to support up to 2PB</li> <li>What approach?</li> <li>Utilise AWS Redshift</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#database-services-scenario-2","title":"Database Services Scenario 2","text":"<ul> <li>MySQL DB to be launched for new web application</li> <li>Need direct access to the virtual server running it</li> <li>Answer: Use an EC2 instance</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#database-services-scenario-3","title":"Database Services Scenario 3","text":"<ul> <li>Store realtime analytics</li> <li>Low latency and high scaling</li> <li>Minimize maintenance needs:</li> <li>DynamoDB</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#app-integration-services","title":"App Integration Services","text":""},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#aws-messaging-services","title":"AWS Messaging Services","text":""},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#simple-notification-service-sns","title":"Simple Notification Service (SNS)","text":"<ul> <li>Fully managed pub/sub messaging service</li> <li>Enables creation of decoupled applications</li> <li>Organised according to topics</li> <li>Integrates with multiple AWS services</li> <li>Provides end-user notifications via any of SMS, email, etc.</li> <li>Example use case: User Signup</li> <li>User signup is set up on a SNS Topic</li> <li>Lambda function, SQS Queue and email notifications could all be triggered based on the topic event.</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#amazon-simple-queue-service-sqs","title":"Amazon Simple Queue Service (SQS)","text":"<ul> <li>Fully managed message queue service</li> <li>enables building decoupled and fault-tolerant applications</li> <li>Up to 256kb data payload</li> <li>Messages storeable up to 14 days</li> <li>2 Queue Types:</li> <li>Standard</li> <li>First-in-first out (FIFO)</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#sample-architecture","title":"Sample Architecture","text":"<ul> <li>SNS and SQS could be leveraged in many ways together, in a similar manner to below:</li> </ul> <ul> <li>If the Analytics service fails - no problem! the data will remain in the queue and then get processed once the services are back up and running.</li> <li>SQS Leads to fault tolerance.</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#aws-step-functions","title":"AWS Step Functions","text":"<ul> <li>Enables orchestration of workflows through a fully managed service</li> <li>Supports serverless architectures</li> <li>Supports complex workflows including error handling</li> <li>Charged per state transition along with other AWS services leveraged</li> <li>Workflow defined using Amazon States Language (example below)</li> </ul> <ul> <li>Step functions can integrate with many AWS services including compute, database, and messaging services.</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#app-integration-services-scenarios","title":"App Integration Services Scenarios","text":""},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#app-integration-services-scenario-1","title":"App Integration Services Scenario 1","text":"<ul> <li>Non-profit org</li> <li>Database server going down prevented signup</li> <li>Expected downtime in future</li> <li>AWS Service needed to prevent lost signups</li> <li>Recommended service: Simple Queue Services (SQS)</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#app-integration-services-scenario-2","title":"App Integration Services Scenario 2","text":"<ul> <li>Onboarding steps for new customers</li> <li>Steps include integrations with CRM, emails, analytics</li> <li>Recommended service: Step Functions</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#app-integration-services-scenario-3","title":"App Integration Services Scenario 3","text":"<ul> <li>Ecommerce building a custom platform</li> <li>New functionality getting added</li> <li>Aspects of the platform should listen for events like orders and refunds</li> <li>Don't know all elements that would need to respond to the events.</li> <li>Recommended Service: Simple Messaging Service (SNS)</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#management-and-governance-services","title":"Management and Governance Services","text":""},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#aws-cloudtrail","title":"AWS CloudTrail","text":"<ul> <li>Allows logging and continuous monitoring of account activity for actions across AWS Infrastructure.</li> <li>Provides event history of AWS account activity.</li> <li>Audit trails logged in S3 buckets or CloudWatch logs</li> <li>Logs events per region</li> <li>Meets many compliance requirements for infrastructure auditing.</li> <li>Should be enabled by default for every AWS account as part of best practices.</li> <li>Can be consolidated via AWS Organizations.</li> <li>Use cases:</li> <li>Compliance requirements.</li> <li>Forensic analysis for incidents</li> <li>Operational analysis.</li> <li>Troubleshooting</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#aws-cloudwatch-and-aws-config","title":"AWS CloudWatch and AWS Config","text":""},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#cloudwatch","title":"Cloudwatch","text":"<ul> <li>Provides metrics, logs and alarms for infrastructure.</li> <li>Monitoring and management service.</li> <li>Collects logs, metrics and events from most AWS services.</li> <li>Enables alarms based on metrics.</li> <li>Provides visualization capabilities for metrics.</li> <li>Allows for custom dashboards for metrics.</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#aws-config","title":"AWS Config","text":"<ul> <li>Continually evaluates infrastructure against a set of rules.</li> <li>Provides configuration history for infrastructure</li> <li>Works against rules that are customisable or can create custom validations</li> <li>Includes conformance packs for compliance standards</li> <li>Can work with AWS Organizations for both cross-region and cross-account setups</li> <li>Provides remediation steps for any checks not passed.</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#aws-systems-manager","title":"AWS Systems Manager","text":"<ul> <li>Provides a unified user interface to view operational data from multiple AWS services.</li> <li>Provides multiple tools to make it easier to manage AWS Infrastructure</li> <li>Enables automation tasks for common maintenance actions.</li> <li>Provides a secure way of accessing servers with only AWS credentials.</li> <li>Stores commonly used parameters securely for operational use.</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#aws-cloudformation","title":"AWS CloudFormation","text":"<ul> <li>Managed service for provisioning infrastructure based on templates e.g. provision 3 EC2 instances for dev environment, 1 for prod.</li> <li>Only pay for the resources provisioned.</li> <li>Templates written in YAML or JSON \u2192 Leverages infrastructure as code.</li> <li>Manages dependencies between resources.</li> <li>Provides drift detection to find changes in your infrastructure (like Terraform state)</li> <li>Example template:</li> </ul> <pre><code>Description: Creates an S3 Bucket\nResources:\n  SampleS3Bucket:\n    Type: AWS::S3::Bucket\n    Properties:\n      BucketName: &lt;bucket name&gt;\n</code></pre>"},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#aws-opsworks","title":"AWS OpsWorks","text":"<ul> <li>Configuration management service</li> <li>Provides managed instances of Chef and Puppet</li> <li>Configuration defined as code for servers.</li> <li>Chef and Puppet manage the lifecycle of configuration changes with servers</li> <li>Works in a hybrid cloud architecture for both cloud and on-prem infrastructure.</li> <li>Comprised of 3 mains ervices:</li> <li>AWS OpsWork for Chef Automate</li> <li>AWS OpsWork for Puppet Enterprise</li> <li>AWS OpsWork Stacks - Can define application in \"layers\" to then be managed by Chef/Puppet as appropriate.</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#aws-organizations-and-control-tower","title":"AWS Organizations and Control Tower","text":""},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#aws-organizations","title":"AWS Organizations","text":"<ul> <li>Allows organisations the ability to manage multiple accounts from a master account</li> <li>Provides organizations with the ability to leverage consolidated billing for all accounts.</li> <li>Allows centralized logging and security standard enforcement.</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#aws-control-tower","title":"AWS Control Tower","text":"<ul> <li>Centralises users across all AWS accounts</li> <li>Provides a way to create new AWS accounts based on templates.</li> <li>Integrates guardrails for accounts.</li> <li>Includes a dashboard to gain operational insight from a single view.</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#management-and-governance-scenarios","title":"Management and Governance Scenarios","text":""},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#management-and-governance-scenario-1","title":"Management and Governance Scenario 1","text":"<ul> <li>Financial services.</li> <li>Discovered a security setting was disabled on a server</li> <li>Wants events like this to be monitored</li> <li>What service?</li> <li>AWS Config</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#management-and-governance-scenario-2","title":"Management and Governance Scenario 2","text":"<ul> <li>SaaS company</li> <li>New application including several components.</li> <li>Wanting to minimise manual work required when creating infrastructure.</li> <li>What service? CloudFormation</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/02_Understanding-Core-AWS-Services/Understanding%20AWS%20Core%20Services.html#management-and-governance-scenario-3","title":"Management and Governance Scenario 3","text":"<ul> <li>Cloud Server needed to support manufacturing process was deleted</li> <li>Want to follow up with the person</li> <li>What service: CloudTrail</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/03_Introduction-to-Security-and-Architecture/Introduction%20to%20Security%20and%20architecture%20on%20AWS.html","title":"3.0 - Introduction to Security and architecture on AWS","text":""},{"location":"certifications/aws-cloud-practitioner/course-notes/03_Introduction-to-Security-and-Architecture/Introduction%20to%20Security%20and%20architecture%20on%20AWS.html#aws-architecture-core-concepts","title":"AWS Architecture Core Concepts","text":""},{"location":"certifications/aws-cloud-practitioner/course-notes/03_Introduction-to-Security-and-Architecture/Introduction%20to%20Security%20and%20architecture%20on%20AWS.html#security-and-architecture-overview","title":"Security and Architecture overview","text":"<ul> <li>All AWS users must remain in line with AWS's Acceptable Use Policy - determines acceptable / unacceptable usage.</li> <li>Unacceptable usage includes:</li> <li>Sending unsolicited mass emails</li> <li>Hosting or distributing harmful content</li> <li>Details penetration testing for specific services.</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/03_Introduction-to-Security-and-Architecture/Introduction%20to%20Security%20and%20architecture%20on%20AWS.html#least-privilege-access","title":"Least Privilege Access","text":"<ul> <li>When granting access to a service, grant only the minimum privileges required to complete the task</li> <li>Also suggests not to use root accounts for daily account activities.</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/03_Introduction-to-Security-and-Architecture/Introduction%20to%20Security%20and%20architecture%20on%20AWS.html#shared-responsibility-model","title":"Shared Responsibility Model","text":"<ul> <li>When working in the cloud, security and compliance is a shared responsibility between AWS and the customer.</li> <li>AWS Responsibility - Security of the cloud</li> <li>Access and training for Amazon employees</li> <li>Global data centre and underlying data network maintenance</li> <li>Hardware for global infrastructure maintenance</li> <li>Configuration management for infrastructure</li> <li>Patching cloud infrastructure and services</li> <li>Customer Responsibility - Security for what they do in the cloud</li> <li>Individual access to cloud resources and training</li> <li>Data security and encryption (in transit and at rest)</li> <li>Operating system, network and firewall configuration (if using IaaS)</li> <li>Code deployed onto cloud infrastructure.</li> <li>Patching guest OS and custom applications</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/03_Introduction-to-Security-and-Architecture/Introduction%20to%20Security%20and%20architecture%20on%20AWS.html#aws-well-architected-framework","title":"AWS Well-Architected Framework","text":"<ul> <li>A collection of best practices across 5 key areas to create systems that add business value on AWS</li> <li>Key Pillars:</li> <li>Operational Excellence - Running and monitoring systems for business value</li> <li>Security - Running and monitoring systems for business value</li> <li>Reliability - Enabling infrastructure to recover from disruptions</li> <li>Performance efficiency - Efficient resource usage</li> <li>Cost optimizations - Minimize costs for desired value</li> <li>Available at https://aws.amazon.com/architecture/well-architected</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/03_Introduction-to-Security-and-Architecture/Introduction%20to%20Security%20and%20architecture%20on%20AWS.html#high-availability-and-fault-tolerance","title":"High-Availability and Fault Tolerance","text":"<ul> <li>Under the reliability pillar of the well-architected framework.</li> <li>2 main categories of reliability:</li> <li>Fault-Tolerance - Being able to support the failure of components within architecture</li> <li>High-Availability - Keeping entire solution running as expected in the event of disruption</li> <li>Building solutions on AWS, consider:</li> <li>Most managed AWS services offer high availability out of the box</li> <li>When building solutions on EC2, fault tolerance needs to be architected.</li> <li>Multiple availability zones should be leveraged</li> <li>Some services can enable fault tolerance in custom applications e.g.:<ul> <li>Simple Queue Service (SQS)</li> <li>Route 53</li> </ul> </li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/03_Introduction-to-Security-and-Architecture/Introduction%20to%20Security%20and%20architecture%20on%20AWS.html#compliance","title":"Compliance","text":"<ul> <li>Standards to note include:</li> <li>PCI-DSS: For processing credit cards</li> <li>HIPAA - Healthcare data</li> <li>SOC 1-3 - Third-party reviews of operational processes</li> <li>FedRAMP - US Government Data handling</li> <li>ISO - Personal Identifier Info</li> <li>Services to maintain compliance include:</li> <li>AWS Config - Provides conformance packs for standards</li> <li>AWS Artifact - Provides self-service access to reports</li> <li>Amazon GuardDuty - Provides intelligent threat detection</li> <li>AWS Config \u2192 Conformance Packs \u2192 Deploy Conformance Pack</li> <li>Use template as appropriate - many standards have templates pre-done</li> <li>AWS Artifact \u2192 Find desired compliance standard for review.</li> <li>May need to sign NDA with AWS to allow access</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/03_Introduction-to-Security-and-Architecture/Introduction%20to%20Security%20and%20architecture%20on%20AWS.html#scenarios","title":"Scenarios","text":""},{"location":"certifications/aws-cloud-practitioner/course-notes/03_Introduction-to-Security-and-Architecture/Introduction%20to%20Security%20and%20architecture%20on%20AWS.html#scenario-1","title":"Scenario 1","text":"<ul> <li>Application processing credit cards</li> <li>Processing done directly not through a service</li> <li>PCI DSS compliance report needed for AWS</li> <li>Where to get this info? AWS Artifact to get the reports.</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/03_Introduction-to-Security-and-Architecture/Introduction%20to%20Security%20and%20architecture%20on%20AWS.html#scenario-2","title":"Scenario 2","text":"<ul> <li>Personal information stored in their system</li> <li>What's the responsibility for security</li> <li>What's their responsibility? Review the Shared responsibility model - Shared with AWS - the company is responsible for the security and compliance of anything they do on AWS with services and resources.</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/03_Introduction-to-Security-and-Architecture/Introduction%20to%20Security%20and%20architecture%20on%20AWS.html#scenario-3","title":"Scenario 3","text":"<ul> <li>New tool for digital asset management</li> <li>How best to leverage the capabilities? Review the well-architected framework</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/03_Introduction-to-Security-and-Architecture/Introduction%20to%20Security%20and%20architecture%20on%20AWS.html#aws-identities-and-user-management","title":"AWS Identities and User Management","text":""},{"location":"certifications/aws-cloud-practitioner/course-notes/03_Introduction-to-Security-and-Architecture/Introduction%20to%20Security%20and%20architecture%20on%20AWS.html#introduction-to-aws-iam","title":"Introduction to AWS IAM","text":"<ul> <li>Identity and Access Management (IAM)</li> <li>Controls access to AWS resources</li> <li>Free to use service</li> <li>Covers both authentication and authorization</li> <li>Supports identity federation through SAML providers including Active Directory</li> <li>IAM Identity Types:</li> <li>Users - Account for single individual to access resources</li> <li>Groups - Allows you to manage permissions for a group of IAM users</li> <li>Roles - Enables a user or AWS service to assume permissions for a task</li> <li>Policies in AWS IAM</li> <li>JSON document that defines permissions for an AWS IAM Identity</li> <li>Defines both AWS services that the identity can access and what actions can be done on that service</li> <li>Can be customer-managed or managed by AWS.</li> </ul> <ul> <li>Best practices:</li> <li>Set up multi-factor authentication \u2192 provides additional security</li> <li>Least privilege access \u2192 only grant the minimal permissions required for their current tasks</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/03_Introduction-to-Security-and-Architecture/Introduction%20to%20Security%20and%20architecture%20on%20AWS.html#creating-and-managing-iam-users","title":"Creating and Managing IAM Users","text":"<ul> <li>AWS Console \u2192 IAM \u2192 Users \u2192 Add User</li> <li>Provide relevant details:</li> <li>Name</li> <li>Access type (programmatic or management console access)</li> <li>Console password</li> <li>Permissions</li> <li>Template policies (AWS-managed) exist for many individual resource types</li> <li>Tags</li> <li>Groups \u2192 Create group</li> <li>Provide details:<ul> <li>Group name</li> <li>Policies</li> </ul> </li> <li>Create group</li> <li>Users \u2192 User \u2192 Groups Tab \u2192 Add user to group</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/03_Introduction-to-Security-and-Architecture/Introduction%20to%20Security%20and%20architecture%20on%20AWS.html#enabling-multi-factor-authentication","title":"Enabling Multi-Factor Authentication","text":"<ul> <li>MFA for root user can only be managed for root user</li> <li>Account \u2192 IAM \u2192 Security \u2192 Enable MFA \u2192 Virtual MFA (Recommended) and setup accordingly</li> <li>Repeat for any IAM users.</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/03_Introduction-to-Security-and-Architecture/Introduction%20to%20Security%20and%20architecture%20on%20AWS.html#amazon-cognito","title":"Amazon Cognito","text":"<ul> <li>A managed service that facilitates user directory service for custom applications.</li> <li>Like IAM but for custom applications</li> <li>Provides UI components for many platforms</li> <li>Provides security capabilities to control account access</li> <li>Enables controlled access to AWS resources</li> <li>Leverages external providers such as:</li> <li>Google</li> <li>Amazon</li> <li>Facebook</li> <li>MS Active Directory</li> <li>SAML 2.0</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/03_Introduction-to-Security-and-Architecture/Introduction%20to%20Security%20and%20architecture%20on%20AWS.html#iam-scenarios","title":"IAM Scenarios","text":""},{"location":"certifications/aws-cloud-practitioner/course-notes/03_Introduction-to-Security-and-Architecture/Introduction%20to%20Security%20and%20architecture%20on%20AWS.html#iam-scenario-1","title":"IAM Scenario 1","text":"<ul> <li>Team needing access (same level to cloud systems)</li> <li>Wants to do it quicker than managing each individual user</li> <li>What approach? Utilise group permissions</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/03_Introduction-to-Security-and-Architecture/Introduction%20to%20Security%20and%20architecture%20on%20AWS.html#iam-scenario-2","title":"IAM Scenario 2","text":"<ul> <li>EC2 Servers need access to data within S3 Buckets</li> <li>Created a user in IAM for servers and uploaded keys to the server.</li> <li>Is this best practice? If not what should be done?</li> <li>Use an IAM Role within EC2 - Mitigates a lot of security risks \u2192 Gives the services the permissions required.</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/03_Introduction-to-Security-and-Architecture/Introduction%20to%20Security%20and%20architecture%20on%20AWS.html#iam-scenario-3","title":"IAM Scenario 3","text":"<ul> <li>Transitioning to cloud</li> <li>Concerns about securing access to AWS resources with a password</li> <li>Wanting additional security, how?</li> <li>Use Multi Factor Authentication</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/03_Introduction-to-Security-and-Architecture/Introduction%20to%20Security%20and%20architecture%20on%20AWS.html#data-architecture-on-aws","title":"Data Architecture on AWS","text":""},{"location":"certifications/aws-cloud-practitioner/course-notes/03_Introduction-to-Security-and-Architecture/Introduction%20to%20Security%20and%20architecture%20on%20AWS.html#integrating-on-premise-data","title":"Integrating On-Premise Data","text":"<ul> <li>Solutions available:</li> <li>AWS Storage Gateway - A Hybrid Cloud Storage Service</li> <li>AWS DataSync - Automated data transfer service</li> <li>Storage Gateway</li> <li>Integrates cloud storage to local network</li> <li>Deployed as a VM or a specific hardware appliance</li> <li>Integrates with S3 and EBS</li> <li>Supports 3 gateway types:<ul> <li>Tape Gateway - Enables tape backup processes to store data in the cloud on virtual tapes</li> <li>Volume Gateway - Provides cloud-based iSCSI volumes to local applications</li> <li>File Gateway - Stores files in Amazon S3 whilst providing cached low-latency local access</li> </ul> </li> <li>DataSync:</li> <li>Agent deployed as a VM on your network</li> <li>Integrates with S3, EFS and FSX for Windows File Server on AWS</li> <li>Offers greatly improved transfer speed via custom protocols</li> <li>Charged per GB Transferred</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/03_Introduction-to-Security-and-Architecture/Introduction%20to%20Security%20and%20architecture%20on%20AWS.html#processing-data","title":"Processing Data","text":"<ul> <li>Services:</li> <li>AWS Glue - Managed Extract, Transform and Load Service (ETL)</li> <li>Amazon EMR - Big Data cloud processing using popular tools</li> <li>AWS Data Pipeline - Data workflow orchestration service across AWS services</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/03_Introduction-to-Security-and-Architecture/Introduction%20to%20Security%20and%20architecture%20on%20AWS.html#aws-glue","title":"AWS Glue","text":"<ul> <li>Fully managed ETL service on aws</li> <li>Supports data in Amazon RDS, DynamoDB, Redshift and S3</li> <li>Supports serverless model of execution</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/03_Introduction-to-Security-and-Architecture/Introduction%20to%20Security%20and%20architecture%20on%20AWS.html#amazon-emr","title":"Amazon EMR","text":"<ul> <li>Enables big data processing on Amazon EC2 and S3</li> <li>Supports popular open source frameworks and tools</li> <li>Operates in a clustered environment without additional configuration</li> <li>Supports many different big data use cases</li> <li>Supported EMR Framewkorks:</li> <li>Apache:<ul> <li>Spark</li> <li>Hive</li> <li>HBase</li> <li>Flink</li> <li>Hudi</li> </ul> </li> <li>Presto</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/03_Introduction-to-Security-and-Architecture/Introduction%20to%20Security%20and%20architecture%20on%20AWS.html#aws-data-pipeline","title":"AWS Data Pipeline","text":"<ul> <li>Managed ETL service on AWS</li> <li>Manages data workflow through AWS services</li> <li>Supports S3, EMR, Redshift, DynamoDB and RDS</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/03_Introduction-to-Security-and-Architecture/Introduction%20to%20Security%20and%20architecture%20on%20AWS.html#analysing-data","title":"Analysing Data","text":"<ul> <li>Services:</li> <li>Amazon Athena - Service that enables querying of data stored in Amazon S3</li> <li>Amazon Quicksight - Business intelligence service enabling data dashboards</li> <li>Amazon CloudSearch - Managed search service for custom applications</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/03_Introduction-to-Security-and-Architecture/Introduction%20to%20Security%20and%20architecture%20on%20AWS.html#athena","title":"Athena","text":"<ul> <li>Fully managed serverless service</li> <li>Enables querying of large-scale data stored within Amazon S3</li> <li>Queries written using standard SQL</li> <li>Charges based on data scanned per query (amount, time taken, etc)</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/03_Introduction-to-Security-and-Architecture/Introduction%20to%20Security%20and%20architecture%20on%20AWS.html#quicksight","title":"Quicksight","text":"<ul> <li>Fully managed business intelligence service</li> <li>Enables dynamic data dashboard based on data stored in AWS</li> <li>Charged on per user and per-session pricing model</li> <li>Multiple versions provided based on needs</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/03_Introduction-to-Security-and-Architecture/Introduction%20to%20Security%20and%20architecture%20on%20AWS.html#cloudsearch","title":"CloudSearch","text":"<ul> <li>Fully managed search service on AWSSupports scaling of search infrastructure to meet demand</li> <li>Charged per hour and instance type of search infrastructure</li> <li>Enables developers to integrate search into custom applications.</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/03_Introduction-to-Security-and-Architecture/Introduction%20to%20Security%20and%20architecture%20on%20AWS.html#integrating-ai-and-machine-learning","title":"Integrating AI and Machine Learning","text":"<ul> <li>Many services available - only focusing on the following:</li> <li>Amazon Rekognition - Computer vision service powered by machine Learning</li> <li>Amazon Translate - Text translation service powered by machine learning</li> <li>Amazon Transcribe - Speech to Text solution using machine learning</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/03_Introduction-to-Security-and-Architecture/Introduction%20to%20Security%20and%20architecture%20on%20AWS.html#rekognition","title":"Rekognition","text":"<ul> <li>Fully managed image and video recognition deep learning service</li> <li>Identifies objects in image</li> <li>Identifies objects and actions in videos</li> <li>Can detect people using facial analysis</li> <li>Supports custom labels for business objects</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/03_Introduction-to-Security-and-Architecture/Introduction%20to%20Security%20and%20architecture%20on%20AWS.html#translate","title":"Translate","text":"<ul> <li>Fully managed service for translation of text</li> <li>54 (at least) languages supported</li> <li>Can perform language identification</li> <li>Can work in batch and real-time</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/03_Introduction-to-Security-and-Architecture/Introduction%20to%20Security%20and%20architecture%20on%20AWS.html#transcribe","title":"Transcribe","text":"<ul> <li>Speech to text translation - fully managed</li> <li>Recorded speech converted into text in custom applications</li> <li>Includes specific sub service for medical use</li> <li>Supports 31 different languages and works in batch and real-time.</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/03_Introduction-to-Security-and-Architecture/Introduction%20to%20Security%20and%20architecture%20on%20AWS.html#data-architecture-scenarios","title":"Data Architecture Scenarios","text":""},{"location":"certifications/aws-cloud-practitioner/course-notes/03_Introduction-to-Security-and-Architecture/Introduction%20to%20Security%20and%20architecture%20on%20AWS.html#data-architecture-scenario-1","title":"Data Architecture Scenario 1","text":"<ul> <li>Financial company</li> <li>Large scale data set needs to be processed</li> <li>Not wanting to manage servers, just the processing</li> <li>Recommended Service: AWS Glue</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/03_Introduction-to-Security-and-Architecture/Introduction%20to%20Security%20and%20architecture%20on%20AWS.html#data-architecture-scenario-2","title":"Data Architecture Scenario 2","text":"<ul> <li>Biotech company</li> <li>Wanting to identify an approach for controlled lab access</li> <li>Wanting to use AI to determine access level via facial recognition</li> <li>Recommended Service: AWS Rekognition</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/03_Introduction-to-Security-and-Architecture/Introduction%20to%20Security%20and%20architecture%20on%20AWS.html#data-architecture-scenario-3","title":"Data Architecture Scenario 3","text":"<ul> <li>Custom services around machine learning</li> <li>Wanting visualisation of sales data</li> <li>Data currently stored in Redshift</li> <li>Recommended Service: AWS Quicksight - better for non-technical use case</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/03_Introduction-to-Security-and-Architecture/Introduction%20to%20Security%20and%20architecture%20on%20AWS.html#disaster-recovery-on-aws","title":"Disaster Recovery on AWS","text":"<ul> <li>Disaster Recovery = Any critical situation that has a negative impact on a business continuity or finances.</li> <li>Examples include hardware/software failure, network outage, data centre destruction, human error, etc.</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/03_Introduction-to-Security-and-Architecture/Introduction%20to%20Security%20and%20architecture%20on%20AWS.html#disaster-recovery-architecture","title":"Disaster Recovery Architecture","text":"<ul> <li>Architectures vary in cost, complexity, and time to restore:</li> <li>Backup and Restore<ul> <li>All production data backed up into Amazon S3</li> <li>Data can be stored in either standard or archival storage classes</li> <li>EBS data can be stored as snapshots in S3</li> <li>In the event of DR -  a process is started to launch a new environment</li> <li>Longest recovery time out of the 4 methods, but the cheapest.</li> </ul> </li> <li>Pilot Light<ul> <li>Key infrastructure components are kept running in the cloud</li> <li>Designed to reduce recovery time compared to backup and restore - core pieces of system are kept up to date</li> <li>Incurs cost of infrastructure conitnually running in the cloud</li> <li>AMIs are prepared for additional systems to be launched quickly</li> </ul> </li> <li>Warm Standby<ul> <li>Scaled down version of the full environment running in the cloud</li> <li>Critical systems can be running on less capable instance types</li> <li>Instance types and other systems can be ramped up for disaster recovery event</li> <li>Cost incurred for continuous running of infrastructure</li> </ul> </li> <li>Multi Site<ul> <li>Full environment running in the cloud at all times e.g. 1 in us-west 1 and us-west 2</li> <li>Uses instance types needed for production and recovery</li> <li>Provides near seamless recovery process</li> <li>Incurs most cost over the other approaches</li> </ul> </li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/03_Introduction-to-Security-and-Architecture/Introduction%20to%20Security%20and%20architecture%20on%20AWS.html#selecting-a-disaster-recovery-architecture","title":"Selecting a Disaster Recovery Architecture","text":"<ul> <li>Need to consider:</li> <li>Recovery Time Objective (RTO) - How long for the system to get back up and running to ideal business state</li> <li>Recovery Point Objective (RPO) - How much data loss (in terms of time) occurs during a DR scenario.</li> <li>Generally RTO and RPO decrease with increased cost</li> <li>Pilot Light and Warm standby can have variable RTO and RPOs depending on the resources and services leveraged.</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/03_Introduction-to-Security-and-Architecture/Introduction%20to%20Security%20and%20architecture%20on%20AWS.html#disaster-recovery-scenarios","title":"Disaster Recovery Scenarios","text":""},{"location":"certifications/aws-cloud-practitioner/course-notes/03_Introduction-to-Security-and-Architecture/Introduction%20to%20Security%20and%20architecture%20on%20AWS.html#disaster-recovery-scenario-1","title":"Disaster Recovery Scenario 1","text":"<ul> <li>Several production workloads</li> <li>Disaster recovery approach needed</li> <li>Seamless transition needed</li> <li>Recommended approach: Multi Site</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/03_Introduction-to-Security-and-Architecture/Introduction%20to%20Security%20and%20architecture%20on%20AWS.html#disaster-recovery-scenario-2","title":"Disaster Recovery Scenario 2","text":"<ul> <li>Startup</li> <li>No DR in place</li> <li>Minimizing costs more important than RTO</li> <li>Recommended approach: Backup and Restore</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/03_Introduction-to-Security-and-Architecture/Introduction%20to%20Security%20and%20architecture%20on%20AWS.html#disaster-recovery-scenario-3","title":"Disaster Recovery Scenario 3","text":"<ul> <li>Disaster Recovery Approach</li> <li>Few key servers up and running in the event of an event</li> <li>Servers have smaller instance types than production</li> <li>Recommended approach: Pilot light</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/03_Introduction-to-Security-and-Architecture/Introduction%20to%20Security%20and%20architecture%20on%20AWS.html#architecting-applications-on-amazon-ec2","title":"Architecting Applications on Amazon EC2","text":""},{"location":"certifications/aws-cloud-practitioner/course-notes/03_Introduction-to-Security-and-Architecture/Introduction%20to%20Security%20and%20architecture%20on%20AWS.html#scaling-ec2-infrastructure","title":"Scaling EC2 Infrastructure","text":"<ul> <li>Scaling methods available:</li> <li>Vertical scaling \u2192 make servers bigger and use more resources</li> <li>Horizontal scaling \u2192 scaling out and adding more servers of the same type</li> <li>Services linked to EC2 to facilitate this:</li> <li>Auto Scaling Group<ul> <li>Set of EC2 Instances with rules for scaling management</li> <li>Launch template defining the instance configuration for the group</li> <li>Defines the minimum, maximum and desired number of instances</li> <li>Health Checks performed on each instance</li> <li>Exists within 1 or more availability zones in a single region</li> <li>Works with on-demand and spot instances</li> </ul> </li> <li>Elastic Load Balancer</li> <li>AWS Secrets Manager</li> <li>Secure way to integrate credentials, API keys, tokens and other secret content.</li> <li>Integrates natively with RDS, DocumentDB, and Redshift</li> <li>Can auto-rotate credentials with integrated services</li> <li>Enables fine-grained access control to secrets</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/03_Introduction-to-Security-and-Architecture/Introduction%20to%20Security%20and%20architecture%20on%20AWS.html#controlling-access-to-ec2-instances","title":"Controlling Access to EC2 Instances","text":"<ul> <li>Security in Amazon VPC</li> <li>Security groups - enables firewall-like controls for resources within VPCs</li> <li>Network ACLs - controls inbound and outbound traffic with subnets for VPCs</li> <li>AWS VPN - Secure access to an entire VPC using an encrypted tunnel</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/03_Introduction-to-Security-and-Architecture/Introduction%20to%20Security%20and%20architecture%20on%20AWS.html#security-groups","title":"Security Groups","text":"<ul> <li>Serve as firewall for your EC2 instances</li> <li>Control inbound and outbound traffic</li> <li>Works at the instance level</li> <li>EC2 Instances can belong to multiple security groups</li> <li>VPCs have default security groups</li> <li>All outbound traffic allowed by default</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/03_Introduction-to-Security-and-Architecture/Introduction%20to%20Security%20and%20architecture%20on%20AWS.html#network-acls","title":"Network ACLs","text":"<ul> <li>Works at a subnet level with a VPC</li> <li>Enables you to allow and deny traffic</li> <li>Each VPC has a default ACL that allows all inbound and outbound traffic</li> <li>Custom ACLs deny all traffic by default</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/03_Introduction-to-Security-and-Architecture/Introduction%20to%20Security%20and%20architecture%20on%20AWS.html#aws-vpn","title":"AWS VPN","text":"<ul> <li>Creates an encrypted tunnel into your VPC</li> <li>Can be used to connect data centre or even individual client machines</li> <li>Supported via:</li> <li>site to site VPN - e.g. AWS and on-prem data centre interaction</li> <li>Client VPN</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/03_Introduction-to-Security-and-Architecture/Introduction%20to%20Security%20and%20architecture%20on%20AWS.html#protecting-infrastructure-from-attacks","title":"Protecting Infrastructure from Attacks","text":"<ul> <li>Recommended services:</li> <li>AWS Shield - Managed DDoS protection service</li> <li>Amazon Macie</li> <li>Amazon Inspector</li> <li>DDoS = Distributed Denial of Service</li> <li>An attack where a server or group of servers are flooded with more traffic than they can handle in a coordinated effort to bring the system down.</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/03_Introduction-to-Security-and-Architecture/Introduction%20to%20Security%20and%20architecture%20on%20AWS.html#aws-shield","title":"AWS Shield","text":"<ul> <li>Provides protection against DDoS attacks for apps on AWS</li> <li>Enables ongoing threat detection and mitigation</li> <li>2 Different service levels:</li> <li>Standard</li> <li>Advanced</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/03_Introduction-to-Security-and-Architecture/Introduction%20to%20Security%20and%20architecture%20on%20AWS.html#amazon-macie","title":"Amazon Macie","text":"<ul> <li>Utilizes machine learning to analyze data stored in S3</li> <li>Can detect personal information and intellectual property in S3</li> <li>Provides dashboards that show how the data is being stored and accessed</li> <li>Enables alerts if it detects anything unusual about data access</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/03_Introduction-to-Security-and-Architecture/Introduction%20to%20Security%20and%20architecture%20on%20AWS.html#amazon-inspector","title":"Amazon Inspector","text":"<ul> <li>Enables scanning of EC2 instances for security vulnerabilities</li> <li>Charged by instance per assessment run</li> <li>Rule packages:</li> <li>Network reachability assessment</li> <li>Host assessment - tests for host configuration vulnerabilities</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/03_Introduction-to-Security-and-Architecture/Introduction%20to%20Security%20and%20architecture%20on%20AWS.html#deploying-predefined-solutions","title":"Deploying Predefined Solutions","text":"<ul> <li>Multiple solutions available:</li> <li>Service Catalog - Managed catalog of IT services for an organization</li> <li>AWS Marketplace - Catalog of software to run on AWS via 3rd parties</li> <li>Service catalog</li> <li>Targeted to serve as an organisational service catalog for the cloud</li> <li>Includes anything from single server image to multi-tier custom applications</li> <li>Enables organisations to leverage services that meet compliance</li> <li>Supports a lifecycle for services released in the catalog</li> <li>AWS Marketplace</li> <li>Curated catalog of third party solutions for customers to run on AWS</li> <li>Provides anything from AMIs, CloudFormation Stacks and SaaS based solutions</li> <li>Enables different pricing options to overcome licensing in the cloud</li> <li>Charges noted in bill</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/03_Introduction-to-Security-and-Architecture/Introduction%20to%20Security%20and%20architecture%20on%20AWS.html#developer-tools","title":"Developer Tools","text":"<ul> <li>AWS CodeCommit - Basically GitHub</li> <li>Managed source control service</li> <li>Uses git for repositories</li> <li>Control access via IAM policies</li> <li>Serves as an alternative to Github and Bitbucket</li> <li>AWS CodeBuild - CI</li> <li>Fully managed build and continuous integration service</li> <li>No need to maintain infrastructure</li> <li>Charged per minute used for build operations</li> <li>AWS CodeDeploy - CD</li> <li>Managed deployment service for deploying custom applications</li> <li>Deploys to amazon EC2, Fargate, Lambda and On-Prem servers</li> <li>Provides a dashboard for deployments in the AWS console</li> <li>AWS CodePipeline \u2192 Full pipeline scenario</li> <li>Fully managed continuous delivery service</li> <li>Automates build, test and deployment via previous services</li> <li>Integrates with github</li> <li>AWS CodeStar \u2192 Bootstrap pipeline for application</li> <li>Workflow tool to automate the use of other developer services</li> <li>Creates a complete continuous delivery toolchain for a custom application</li> <li>Provides custom dashboards and configurations in the AWS console</li> <li>Only charged for the other services leveraged</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/03_Introduction-to-Security-and-Architecture/Introduction%20to%20Security%20and%20architecture%20on%20AWS.html#application-architecture-scenarios","title":"Application Architecture Scenarios","text":""},{"location":"certifications/aws-cloud-practitioner/course-notes/03_Introduction-to-Security-and-Architecture/Introduction%20to%20Security%20and%20architecture%20on%20AWS.html#application-architecture-scenario-1","title":"Application Architecture Scenario 1","text":"<ul> <li>Financial service recently transitioned to AWS</li> <li>Want to create compliant IT services that other depts can use</li> <li>Recommended: Service Catalog</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/03_Introduction-to-Security-and-Architecture/Introduction%20to%20Security%20and%20architecture%20on%20AWS.html#application-architecture-scenario-2","title":"Application Architecture Scenario 2","text":"<ul> <li>Leverages AWS for multiple production workloads</li> <li>Recently experienced downtime due to a app failing on EC2</li> <li>Wanting to minimise downtime</li> <li>Recommended: Scale out / use an auto scaling group and load balancer</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/03_Introduction-to-Security-and-Architecture/Introduction%20to%20Security%20and%20architecture%20on%20AWS.html#application-architecture-scenario-3","title":"Application Architecture Scenario 3","text":"<ul> <li>Company deals with sensitive information</li> <li>Put policies for data stored in S3</li> <li>Concerns about the policies being changed accidentally and breaches going unoticed</li> <li>Recommended: Amazon Macie \u2192 classifies data and monitors only the recommended data.</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/03_Introduction-to-Security-and-Architecture/Introduction%20to%20Security%20and%20architecture%20on%20AWS.html#exam-prep","title":"Exam Prep","text":"<ul> <li>Multiple choice or multiple answer questions</li> <li>Score 100-1000</li> <li>Passing score = 700</li> <li>Areas of focus:</li> <li>Fundamental cloud concepts - 26%</li> <li>Technology - 33% (AWS Global infrastructure, services, etc.)</li> <li>Security and Compliance - 25% (shared responsibility, compliance reports etc)</li> <li>Billing and Pricing - 15% (TCO Calculator, etc and the value provided)</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/03_Introduction-to-Security-and-Architecture/Introduction%20to%20Security%20and%20architecture%20on%20AWS.html#registering","title":"Registering","text":"<ul> <li>Cloud practitioner cert page \u2192 portal</li> <li>Create account if required (if have a partner account, sign in accordingly)</li> <li>Select the exam required</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/03_Introduction-to-Security-and-Architecture/Introduction%20to%20Security%20and%20architecture%20on%20AWS.html#recommended-studying-steps","title":"Recommended Studying Steps","text":"<ul> <li>Cloud Concepts:</li> <li>How do cloud platforms differ from traditional data centres</li> <li>How does AWS organise its infrastructure globally</li> <li>CapEx vs OpEx</li> <li>How does scalability differ in the cloud from normal data centres</li> <li>Security:</li> <li>Shared responsibility model</li> <li>Review highlighted best practices for AWS account and resources</li> <li>Securing traffic within VPC</li> <li>Least Privilege</li> <li>IAM and Identity Types</li> <li>Billing and Pricing</li> <li>Tools to understand AWS costs</li> <li>Understand cost effective ways for core services</li> <li>Cost vs data centres</li> <li>How can organisations manage and review costs</li> <li>Support plan levels available</li> <li>Technology</li> <li>Each of the services in the services lists (Critical)</li> <li>Implement basic solutions</li> <li>Review architectural principles for fault tolerance and HA</li> <li>Analyze scalability approaches</li> <li>Think about the intent of each question</li> <li>What's required for the answer? MCQ, MAQ?</li> <li>Can always come back to questions if required.</li> <li>Guess if don't know - better than nothing</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/03_Introduction-to-Security-and-Architecture/Introduction%20to%20Security%20and%20architecture%20on%20AWS.html#important-next-steps","title":"Important Next Steps","text":""},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html","title":"4.1 - Exam Prep 1","text":"<p>Tags: Done</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#questions-1-10","title":"Questions 1-10","text":""},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#question-1","title":"Question 1","text":"<p>A - Operational Excellence</p> <p>Rationale: Technically all, but by adopting IAC, infrastructure deployments are made more consistent and reliable.</p> <p>Additional Notes:</p> <ul> <li>Fault tolerance is not a pillar</li> <li>Security and Cost Optimization aren't fully related to IAC</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#question-2","title":"Question 2","text":"<p>B - Reserved 3-year no upfront</p> <p>Additional Notes:</p> <ul> <li>Dedicated host = too expensive</li> <li>On-demand instance does not provide enough for the desired workload</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#question-3","title":"Question 3","text":"<p>C - Basic</p> <p>Rationale: By elimination, all other plans would include and cost more.</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#question-4","title":"Question 4","text":"<p>B - Custom Amazon Machine Image (AMI)</p> <p>Rationale: EC2 instances are created based on AMIs, for custom configs a custom AMI needs to be created.</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#question-5","title":"Question 5","text":"<p>C - Elastic Load Balancing</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#question-6","title":"Question 6","text":"<p>B - Agility</p> <p>Rationale: Within a very quick amount of time the team have been able to spin up the required resources for experimentation.</p> <p>Additional Notes:</p> <ul> <li>Elasticity relates more to scaling</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#question-7","title":"Question 7","text":"<p>B - Operational Excellence</p> <p>C - Security</p> <p>D - Cost-Optimization</p> <p>Rationale: HA and Fault Tolerance are not pillars well-architected solutions (though they should be adopted if possible!)</p> <p>*Got wrong initially</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#question-8","title":"Question 8","text":"<p>B - Region</p> <p>Rationale: Regions have multiple AZs, AZs have edge locations, etc.</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#question-9","title":"Question 9","text":"<p>C - Security Groups</p> <p>Rationale: Security Groups can have inbound/outbound rules set to limit traffic.</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#question-10","title":"Question 10","text":"<p>C - AWS Direct Connect</p> <p>Additional Notes:</p> <ul> <li>VPN is over public internet.</li> </ul> <p>*Got wrong initially</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#questions-11-20","title":"Questions 11-20","text":""},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#question-11","title":"Question 11","text":"<p>A - S3</p> <p>Rationale: Other services do not offer the varying levels of access.</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#question-12","title":"Question 12","text":"<p>C - AWS Virtual Private Network (VPN)</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#question-13","title":"Question 13","text":"<p>A - CloudFormation</p> <p>Rationale:</p> <ul> <li>Resource Manager is for managing resources</li> <li>CodeCommit and CodeDeploy are for CI/CD operations</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#question-14","title":"Question 14","text":"<p>A - Duration (based on memory allocated)\\</p> <p>D - Number of requests</p> <p>Rationale: If it's a resource intensive Lambda AND frequently used then those require greater consideration.</p> <p>No charges applied based on instances</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#question-15","title":"Question 15","text":"<p>D - Loose Coupling</p> <p>Rationale: Avoid tight coupling, HA and Least Privilege Access don't relate to the statement</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#question-16","title":"Question 16","text":"<p>D - AWS Control Tower</p> <p>Rationale: Cost explorer is unrelated, as is IAM</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#question-17","title":"Question 17","text":"<p>D - AWS Organisations</p> <p>Rationale: Only one that makes sense.</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#question-18","title":"Question 18","text":"<p>B - Amazon RDS</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#question-19","title":"Question 19","text":"<p>B - AWS CodeDeploy</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#question-20","title":"Question 20","text":"<p>C - Amazon DynamoDB</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#questions-21-30","title":"Questions 21-30","text":""},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#question-21","title":"Question 21","text":"<p>B - Custom Software Development</p> <p>Rationale: TCO highlights areas of cost differences for on-prem vs cloud - question is asking \"what is effectively gonna be the same\"</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#question-22","title":"Question 22","text":"<ul> <li>User Access Management</li> <li>Encryption of data at rest and in transit</li> </ul> <p>Rationale: Edge location is managed by AWS, as is datacenter connectivity</p> <p>*Got wrong initially</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#question-23","title":"Question 23","text":"<ul> <li>Backup and Restore</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#question-24","title":"Question 24","text":"<p>C - Reducing unused capacity</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#question-25","title":"Question 25","text":"<p>D - AWS Macie</p> <p>Rationale: Cloudformation is infra-related, GuardDuty and ACLs are network focussed.</p> <p>*Got wrong initially</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#question-26","title":"Question 26","text":"<p>C - AWS Pricing Calculator</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#question-27","title":"Question 27","text":"<p>B - AWS Route53</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#question-28","title":"Question 28","text":"<p>B - Security</p> <p>Rationale: Incident Management = Security</p> <p>*Got wrong initially</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#question-29","title":"Question 29","text":"<p>C - Grant the user permissions for only the items needed by that user to perform a task</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#question-30","title":"Question 30","text":"<p>D - Amazon S3</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#questions-31-40","title":"Questions 31-40","text":""},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#question-31","title":"Question 31","text":"<p>A - AWS Trusted Advisor</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#question-32","title":"Question 32","text":"<p>C - AWS Well-Architected Framework</p> <p>Rationale: Only one that makes sense for \"before\" deploying anything</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#question-33","title":"Question 33","text":"<p>A - Business</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#question-34","title":"Question 34","text":"<p>C - AWS Web Application Firewall</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#question-35","title":"Question 35","text":"<p>B - Go Global in Minutes</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#question-36","title":"Question 36","text":"<p>D - Patching the OS on EC2 Instances</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#question-37","title":"Question 37","text":"<p>B - Access keys per IAM user</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#question-38","title":"Question 38","text":"<p>D - AWS CloudTrail</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#question-39","title":"Question 39","text":"<p>B - Create an IAM group, assign permissions to the group, and add IAM users to the group</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#question-40","title":"Question 40","text":"<p>B - AWS CloudHSM</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#questions-41-50","title":"Questions 41-50","text":""},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#question-41","title":"Question 41","text":"<p>A - AWS Storage Gateway</p> <p>Rationale: The only one that allows for local storage</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#question-42","title":"Question 42","text":"<p>A - Amazon Elasticache</p> <p>Rationale: The online one that makes sense.</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#question-43","title":"Question 43","text":"<p>C - Design for Failure</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#question-44","title":"Question 44","text":"<p>A - Amazon Machine Image (AMI)</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#question-45","title":"Question 45","text":"<p>C - Increase speed and agility</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#question-46","title":"Question 46","text":"<p>D - Availability Zone</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#question-47","title":"Question 47","text":"<p>C - Enable Multi-Factor Authentication</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#question-48","title":"Question 48","text":"<p>A - Spot Instance</p> <p>Rationale: Flexible start/stop time &amp; can stop and restart when needed</p> <p>*Got wrong initially</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#question-49","title":"Question 49","text":"<p>A - Sustainability</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#question-50","title":"Question 50","text":"<p>D - High-Availability</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#questions-51-60","title":"Questions 51-60","text":""},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#question-51","title":"Question 51","text":"<p>C - Utilize MySQL on Amazon EC2</p> <p>Rationale: In RDS you have no access to the root OS.</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#question-52","title":"Question 52","text":"<p>B - Dedicated Host</p> <p>Rationale: Ensures a per-server license can be maintained.</p> <p>*Got wrong initially</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#question-53","title":"Question 53","text":"<p>D - AWS Cost Explorer</p> <p>Rationale: C only allows for current usage, D allows for future estimations etc.</p> <p>*Got wrong initially</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#question-54","title":"Question 54","text":"<p>A - AWS Organisations</p> <p>Rationale: Cloudwatch is auditing, Cloudformation is IAC, Direct Connect is network-based.</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#question-55","title":"Question 55","text":"<p>D - Developer</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#question-56","title":"Question 56","text":"<p>C - Amazon Elastic File System (EFS)</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#question-57","title":"Question 57","text":"<p>A - AWS Professional Services</p> <p>Rationale: D is resources external to AWS, A is the only internal one.</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#question-58","title":"Question 58","text":"<p>D - Amazon Redshift</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#question-59","title":"Question 59","text":"<p>C - Configuration Management</p> <p>Rationale: Customer data is handled by customer, Data center physical security and edge location management is handled by AWS.</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#question-60","title":"Question 60","text":"<p>D - Reduced total cost of ownership (TCO)</p> <p>Rationale: Any increased costs goes against the question, we're not ELIMINATING outright the Opex costs.</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#questions-61-65","title":"Questions 61-65","text":""},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#question-61","title":"Question 61","text":"<p>B - AWS Config</p> <p>Rationale: All other services in the questions don't provide the services required.</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#question-62","title":"Question 62","text":"<p>C - Reliability</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#question-63","title":"Question 63","text":"<p>C - AWS Snowball</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#question-64","title":"Question 64","text":"<p>D - Stop guessing capacity</p> <p>Rationale: The only one that makes sense</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#question65","title":"Question65","text":"<p>D - Hybrid Cloud</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%201.html#initial-evaluation","title":"Initial Evaluation","text":"<ul> <li>57/65 = 87%</li> <li>Passing grade</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html","title":"4.2 - Exam Prep 2","text":""},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#questions1-10","title":"Questions1-10","text":""},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#question-1","title":"Question 1","text":"<p>B. AWS Lambda</p> <p>C - Amazon SQS</p> <p>E - Amazon DynamoDB</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#question-2","title":"Question 2","text":"<p>A - No upfront capitalized expenditures (capex)</p> <p>Rationale: You pay for what you use</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#question-3","title":"Question 3","text":"<p>C - On-demand instance</p> <p>Rational - better for timeframe, reserved would be for \"few years\"</p> <p>*Got wrong initially</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#question-4","title":"Question 4","text":"<p>C - Reserved instance - 1 year - All upfront</p> <p>Rationale: Max cost saving</p> <p>*Got wrong initially</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#question-5","title":"Question 5","text":"<p>A - AWS OpsWorks</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#question-6","title":"Question 6","text":"<p>A - Spot Instance</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#question-7","title":"Question 7","text":"<p>D - AWS Route53 (local and AWS DNS)</p> <p>E - AWS Storage Gateway</p> <p>*got wrong initially</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#question-8","title":"Question 8","text":"<p>A - Both Opex and TCO will decreast</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#question-9","title":"Question 9","text":"<p>D - AWS Cloudformation</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#question-10","title":"Question 10","text":"<p>A - Cost Optimization</p> <p>C - Operational Excellence</p> <p>E - Performance Efficiency</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#questions11-20","title":"Questions11-20","text":""},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#question-11","title":"Question 11","text":"<p>A - Cost Optimization</p> <p>B - Fault Tolerance</p> <p>C - Performance</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#question-12","title":"Question 12","text":"<p>C - Edge location</p> <p>Rationale: Remember that only Cloudfront utilises Edge locations</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#question-13","title":"Question 13","text":"<p>D - Setup AWS VPN for the VPC</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#question-14","title":"Question 14","text":"<p>D - Utilize Amazon Aurora on Amazon RDS</p> <p>Rationale: Aurora is MySQL-compatible, the others can be eliminated by MySQL compatibility.</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#question-15","title":"Question 15","text":"<p>B - Amazon Well-Architected Framework</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#question-16","title":"Question 16","text":"<p>A - Reduced TCO</p> <p>B - Lower variable opex</p> <p>E - No need for upfront capex</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#question-17","title":"Question 17","text":"<p>D - Amazon S3 Glacier</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#question-18","title":"Question 18","text":"<p>D - Consolidated Billing</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#question-19","title":"Question 19","text":"<p>D - Multi-site</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#question-20","title":"Question 20","text":"<p>D - AWS CodePipeline</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#questions21-30","title":"Questions21-30","text":""},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#question-21","title":"Question 21","text":"<p>C - Amazon CloudWatch</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#question-22","title":"Question 22","text":"<p>A - Cost Optimization</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#question-23","title":"Question 23","text":"<p>C - Security Groups</p> <p>D - Network Access Control Lists (ACLs)</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#question-24","title":"Question 24","text":"<p>D - Amazon Cloudfront</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#question-25","title":"Question 25","text":"<p>A - AWS Trusted Advisor</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#question-26","title":"Question 26","text":"<p>B - AWS Cost Explorer</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#question-27","title":"Question 27","text":"<p>B - Variable operational expenditures (opex) tied to usage</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#question-28","title":"Question 28","text":"<p>C - AWS Key Management Service (KMS)</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#question-29","title":"Question 29","text":"<p>A - AWS Marketplace</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#question-30","title":"Question 30","text":"<p>B - Amazon Route53</p> <p>*Got wrong initally</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#questions31-40","title":"Questions31-40","text":""},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#question-31","title":"Question 31","text":"<p>B - Performance Efficiency</p> <p>*Got wrong initially</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#question-32","title":"Question 32","text":"<p>B - Cost and Usage Reports (CUR)</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#question-33","title":"Question 33","text":"<p>A - Providing least-privilege access to AWS resources</p> <p>C - Client-side data encryption</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#question-34","title":"Question 34","text":"<p>A - Managing network connectivity for data centers</p> <p>C - Training data center employees</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#question-35","title":"Question 35","text":"<p>C - Create a new IAM user that you will use instead of the root user</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#question-36","title":"Question 36","text":"<p>D - Elastic Load Balancing</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#question-37","title":"Question 37","text":"<p>D - AWS Artifact</p> <p>*got wrong initially</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#question-38","title":"Question 38","text":"<p>A - benefit from massive economies of scale</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#question-39","title":"Question 39","text":"<p>C - AWS Database Migration Service (DMS)</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#question-40","title":"Question 40","text":"<p>C - Security</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#questions41-50","title":"Questions41-50","text":""},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#question-41","title":"Question 41","text":"<p>C - Loose coupling</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#question-42","title":"Question 42","text":"<p>D - AWS QuickStarts</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#question-43","title":"Question 43","text":"<p>B - Region</p> <p>*got wrong initially</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#question-44","title":"Question 44","text":"<p>B - AWS Partner Network Consulting Partners</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#question-45","title":"Question 45","text":"<p>A - AWS CloudTrail</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#question-46","title":"Question 46","text":"<p>A - Static Website Hosting</p> <p>D - Multiple storage classes to support different access needs</p> <p>E - Lifecycle rules to transition objects between storage classes</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#question-47","title":"Question 47","text":"<p>B - AWS Identity and Access Management (IAM)</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#question-48","title":"Question 48","text":"<p>B - Patch Management</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#question-49","title":"Question 49","text":"<p>C - Setup a password policy</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#question-50","title":"Question 50","text":"<p>C - Amazon Elasticache</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#questions51-60","title":"Questions51-60","text":""},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#question-51","title":"Question 51","text":"<p>B - Cost of Servers</p> <p>C - Cost of data storage</p> <p>E - Utility costs for data center</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#question-52","title":"Question 52","text":"<p>B - AWS Config</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#question-53","title":"Question 53","text":"<p>B - Amazon DynamoDB</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#question-54","title":"Question 54","text":"<p>D - Amazon Redshift</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#question-55","title":"Question 55","text":"<p>C - AWS Shield</p> <p>*Got wrong initially</p> <p>Rationale:</p> <ul> <li>Config does resource configuration</li> <li>Guardduty covers network traffic</li> <li>Macie covers data protection</li> </ul>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#question-56","title":"Question 56","text":"<p>A - Amazon Elastic Block Store</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#question-57","title":"Question 57","text":"<p>D - AWS Organisations</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#question-58","title":"Question 58","text":"<p>A - Enterprise</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#question-59","title":"Question 59","text":"<p>C - AWS Organisations</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#question-60","title":"Question 60","text":"<p>C - Dedicated Host</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#questions61-65","title":"Questions61-65","text":""},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#question-61","title":"Question 61","text":"<p>B - Elasticity</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#question-62","title":"Question 62","text":"<p>A - Amazon Virtual Private Cloud (VPC)</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#question-63","title":"Question 63","text":"<p>D - High Availability</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#question-64","title":"Question 64","text":"<p>C - Automatically patches database engine based on configuration settings</p> <p>D - Can enable high-availability by leveraging multiple availability zones</p> <p>E - Provides the ability to implement read replicas for most database engines</p> <p>*Got wrong initially</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#question-65","title":"Question 65","text":"<p>A - Business</p>"},{"location":"certifications/aws-cloud-practitioner/course-notes/04_Exam_Preparation/Exam%20Prep%202.html#initial-evaluation","title":"Initial Evaluation","text":"<p>56/65 = 86% = Pass</p>"},{"location":"certifications/green-software-for-practitioners/01_Introduction.html","title":"1.0 - Introduction","text":""},{"location":"certifications/green-software-for-practitioners/01_Introduction.html#11-course-introduction","title":"1.1 - Course Introduction","text":""},{"location":"certifications/green-software-for-practitioners/01_Introduction.html#111-what-is-green-software","title":"1.1.1 - What is Green Software?","text":"<ul> <li>An emerging discipline linking climate science, electricity markets, as well as the design of software, hardware, and data centers.</li> <li>This course aims to provide guidance on building and running green software apps.</li> <li> <p>This is achieved by providing guidelines for \"what it means\" to be a green software practictioner.</p> </li> <li> <p>Green software is carbon-efficient software.</p> </li> <li>Only 3 activities reduce the carbon emissions of software:</li> <li>Energy efficiency</li> <li>Carbon awareness</li> <li>Hardware efficiency.</li> </ul> <p></p>"},{"location":"certifications/green-software-for-practitioners/01_Introduction.html#112-who-should-read-this","title":"1.1.2 - Who Should Read This?","text":"<ul> <li>Anyone building, deploying or managing software should have awareness of the green software principles.</li> <li>By doing so, one can make decisons that have a meaningful impact on their applications' carbon pollution levels.</li> </ul>"},{"location":"certifications/green-software-for-practitioners/01_Introduction.html#113-history","title":"1.1.3 - History","text":"<ul> <li>The original eight principles were defined in 2019, with some merged together, and an addition focusing on climate commitments</li> </ul>"},{"location":"certifications/green-software-for-practitioners/01_Introduction.html#114-how-to-be-a-green-software-practitioner","title":"1.1.4 - How to be a Green Software Practitioner","text":"<ul> <li> <p>Any green software practitioner should have awareness of:</p> </li> <li> <p>Carbon Efficiency - Emit the least amount of carbon possible</p> </li> <li>Energy Efficiency - Use the least amount of energy possible</li> <li>Carbon Awareness - Do more when electricity is cleaner, and less when it is dirtier.</li> <li>Hardware Efficiency - Use the least amount of embodied carbon possible</li> <li>Measurement - What cannot be measured, cannot be improved</li> <li>Climate Commitments - Understand the exact mechanism of carbon reduction</li> </ul>"},{"location":"certifications/green-software-for-practitioners/01_Introduction.html#115-principles-patterns-and-practices","title":"1.1.5 - Principles, Patterns, and Practices","text":"<ul> <li>A green software patterm is a specific example of how to apply one or more of the green software principles in the real world.</li> <li>Whilst principles describe the theory, patterns are the practical advice used for implementation; they are vendor-neutral.</li> <li>Practices are patterns applied to a specific vendor's product, and inform practitioners how to use the product in a more sustainable manner.</li> </ul>"},{"location":"certifications/green-software-for-practitioners/01_Introduction.html#12-glossary","title":"1.2 - Glossary","text":""},{"location":"certifications/green-software-for-practitioners/01_Introduction.html#acronyms","title":"Acronyms","text":"Acronym Term Definition CFE Carbon-free energy This term is usually used to talk about the percentage of renewable energy used as a proportion of the total energy used. CH4 Methane A common gaseous hydrocarbon that has a warming effect 25 times that of CO2. CO2 Carbon dioxide One of the most common greenhouse gases. CO2eq / CO2-eq / CO2e Carbon dioxide equivalent Carbon is used as a common form of measurement for all greenhouse gases. This unit of measurement indicates the potential impact of non-CO2 gases on global warming in carbon terms. COP Conference of the Parties An annual event involving all parties in the United Nations Framework Convention on Climate Change. gCO2eq/kWh grams of carbon per kilowatt hour The standard unit of carbon intensity is gCO2eq/kWh, or grams of carbon per kilowatt hour. GHGs Greenhouse gases Greenhouse gases are a group of gases that trap heat from solar radiation in the Earth's atmosphere. These gases act as a blanket, increasing the temperature on the surface of the Earth. GWP Global warming potential The potential impact of greenhouse gases on global warming. Measured in terms of CO2e. IPCC Intergovernmental Panel on Climate Change The objective of the IPCC is to provide governments at all levels with scientific information that they can use to develop climate policies. J joules Energy is measured in joules (J). kWh kilowatt hours Energy consumption is measured in kilowatt hours (kWh). MMTCDE Million metric tonnes of carbon dioxide equivalent Measurement term for CO2eq. NDC Nationally Determined Contribution The means by which members of the Paris Climate Agreement are expected to update their progress. PCA Paris Climate Agreement An international treaty agreed in 2015 by 196 parties and the UN to reduce the Earth's temperature increase. PPA Power Purchase Agreement A contract you sign with a power plant to purchase RECs. PUE Power usage effectiveness The metric used to measure data center energy efficiency. REC Renewable Energy Credit Renewable energy credits (also known as renewable energy certificates) represent the energy generated by renewable energy sources. SBTi Science Based Targets initiative A body that defines and promotes best practice in science-based target setting. For example, creating the standards for net zero. SCI Software Carbon Intensity A standard which gives an actionable approach to software designers, developers and operations to measure the carbon impacts of their systems. SF6 Sulfur hexafluoride A man-made gas used as an electrical insulator that has a warming effect 23,500 times that of CO2. UNFCCC United Nations Framework Convention on Climate A group created to achieve the stabilization of greenhouse gas concentrations in the atmosphere at a level that would prevent dangerous interference with the climate system. VCM Voluntary Carbon Market A decentralized market where private actors voluntarily buy and sell carbon credits that represent certified removals or reductions of greenhouse gases (GHGs) in the atmosphere. VCS Verified Carbon Standard A standard for certifying carbon emissions reductions. WMO World Meteorological Organization A specialized agency of the United Nations whose mandate covers weather, climate and water resources."},{"location":"certifications/green-software-for-practitioners/01_Introduction.html#useful-terms","title":"Useful Terms","text":"<ul> <li> <p>Carbon Intensity - Measures the amount of greenhouse gases emitted per unit of electricity produced.</p> </li> <li> <p>Demand Shaping - The strategy of moving workloads to regions or times when the carbon intensity is less</p> </li> <li> <p>Greenhouse Gas protocol - The most widely used and internationally recognized greenhouse gas accounting standard.</p> </li> <li> <p>Value chain emissions - These are scope 3 emissions according to the GHG protocol, and the most significant source of emissions. They encompass the full range of activities needed to create a product or service, from conception to distribution.</p> </li> <li> <p>Energy proportionality - Measures the relationship between power consumed by a computer and the rate at which useful work is done (its utilization).</p> </li> <li> <p>Static power draw - This refers to how much electricity is drawn when a device is in an idle state.</p> </li> <li> <p>Embodied carbon (also known as \"embedded carbon\") - The amount of carbon pollution emitted during the creation and disposal of a device.</p> </li> </ul>"},{"location":"certifications/green-software-for-practitioners/01_Introduction.html#14-additional-information","title":"1.4 - Additional Information","text":""},{"location":"certifications/green-software-for-practitioners/01_Introduction.html#141-the-green-software-foundation","title":"1.4.1 - The Green Software Foundation","text":"<ul> <li>The Linux Foundation provides a neutral, trusted hub for developers to code, manage, and scale open technology projects.</li> <li>Founded in 2000, The Linux Foundation is supported by more than 1,000 members and is the world's leading home for collaboration on open source software, open standards, open data and open hardware.</li> <li> <p>The Linux Foundation's methodology focuses on leveraging best practices and addressing the needs of contributors, users and solution providers to create sustainable models for open collaboration.</p> </li> <li> <p>The Linux Foundation hosts Linux, the world's largest and most pervasive open source software project in history.</p> </li> <li>It is also home to Linux creator Linus Torvalds and lead maintainer Greg Kroah-Hartman.</li> <li> <p>The success of Linux has catalyzed growth in the open source community, demonstrating the commercial efficacy of open source and inspiring countless new projects across all industries and levels of the technology stack.</p> </li> <li> <p>As a result, the Linux Foundation today hosts far more than Linux; it is the umbrella for many critical open source projects that power corporations today, spanning virtually all industry sectors.</p> </li> <li>Some of the technologies we focus on include big data and analytics, networking, embedded systems and IoT, web tools, cloud computing, edge computing, automotive, security, blockchain, and many more.</li> </ul>"},{"location":"certifications/green-software-for-practitioners/02_Carbon-Efficiency.html","title":"2.0 - Carbon Efficiency","text":""},{"location":"certifications/green-software-for-practitioners/02_Carbon-Efficiency.html#21-introduction","title":"2.1 - Introduction","text":"<ul> <li>Understanding the environmental impact of greenhouse gases is key to understanding software's carbon footprint.</li> <li>One should consider:</li> <li>The kinds of greenhouse gases that exist</li> <li>How the gases in the environment are emitted and measured.</li> <li>Who is attempting to control and reduce these emissions; and how.</li> <li>The GHG Protocol.</li> </ul>"},{"location":"certifications/green-software-for-practitioners/02_Carbon-Efficiency.html#22-carbon-efficiency","title":"2.2 - Carbon Efficiency","text":""},{"location":"certifications/green-software-for-practitioners/02_Carbon-Efficiency.html#global-warming-vs-climate-change","title":"Global Warming vs Climate Change","text":"<ul> <li>Global Warming: The long-term heating of earth's climate system, observed since the pre-industrial period.</li> <li>Effect occuring due to human activities, mainly fossil fuel burning.</li> <li>Climate Change: Long-term shifts in temperatures and weather patterns. Some shifts are natural, but human activities are often the driver.</li> </ul>"},{"location":"certifications/green-software-for-practitioners/02_Carbon-Efficiency.html#climate-vs-weather","title":"Climate vs Weather","text":"<ul> <li>Weather: The conditions of the atmosphere over a short period of time</li> <li>Climate: The conditions of the atmosphere over long periods of time</li> <li>Any changes to long-term conditions result in short-term changes.</li> <li>Example: If the average atmosphere temperature increases, the average temperature of the weather will also increase.</li> <li>Examples of measurable changes to weather conditions due to climate change include:</li> <li>Changes to water cycles and rainfall</li> <li>Icecaps melting</li> <li>Land, air and ocean heating</li> <li> <p>Ocean currents, acidity and salinity changes.</p> </li> <li> <p>A lot of the changes can lead to flooding in coastal and rural areas; the latter via increased rainfall. Drought, wildfires and more frequent extreme conditions may also occur.</p> </li> </ul>"},{"location":"certifications/green-software-for-practitioners/02_Carbon-Efficiency.html#greenhouse-gases-and-the-greenhouse-effect","title":"Greenhouse Gases and the Greenhouse Effect","text":"<ul> <li>Greenhouse gases: A group of gases that trap heat from solar radition in the Earth's atmosphere.</li> <li>The gases act as a blanket, increasing earth's surface temperature.</li> <li>This is a natural phenomenon that is being accelerated due to carbon emissions, the acceleration is now at a afaster rate than animals and plants can adapt to.</li> <li>The grenhouse gases and effect are essential for life on earth, and typically comes from natural sources such as animals and geological activity.</li> <li>The effect allows earth to have a higher temperature than it would via solar radiation alone, but it must be managed appropriately.</li> </ul>"},{"location":"certifications/green-software-for-practitioners/02_Carbon-Efficiency.html#carbon-and-co2eq","title":"Carbon and CO2eq","text":"<ul> <li>Carbon acts as a broad term to cover the impact of all types of emissions and activities relating to global warming.</li> <li>CO2eq/CO2-eq/CO2e = Carbon equivalence - the term used to measure impact.</li> <li>Example: 1 ton of methane = 84 tons of CO2 over 20 years, it is therefore 84 tons CO2eq.</li> </ul>"},{"location":"certifications/green-software-for-practitioners/02_Carbon-Efficiency.html#monitoring-climate-change","title":"Monitoring Climate Change","text":"<ul> <li>To try and mitigate the effects of climate change, some agreements and events have been arranged, a summary follows:</li> </ul>"},{"location":"certifications/green-software-for-practitioners/02_Carbon-Efficiency.html#the-paris-climate-agreement","title":"The Paris CLimate Agreement","text":"<ul> <li>An international treaty agreed in 2015 by 196 parties and the UN to reduce earth's temperature increase.</li> <li>It aims to keep the rise between 1.5 - 2 degrees celsius.</li> <li>The agreement is reviewed every 5 years and mbobilizes finance to developing nations to mitigate climate change impact for them, as well as prepare them for extreme situations.</li> <li>Each party updates their progress via a Nationally Determined Contribution (NDC).</li> </ul>"},{"location":"certifications/green-software-for-practitioners/02_Carbon-Efficiency.html#united-nations-framework-convention-on-climate-unfccc","title":"United Nations Framework Convention on CLimate (UNFCCC)","text":"<ul> <li>A group created to achieve the stabilization of greenhouse gas concentrations.</li> </ul>"},{"location":"certifications/green-software-for-practitioners/02_Carbon-Efficiency.html#conference-of-parties-cop","title":"Conference of Parties (COP)","text":"<ul> <li>An annual event involving all parties in the UNFCCC.</li> <li>Each party's progress on tackling global warming as part of the Paris Climate agreement is reviewed and assessed.</li> <li>Further actions and strategies for reducing emissions and supporting low-carbon options are discussed and agreed.</li> </ul>"},{"location":"certifications/green-software-for-practitioners/02_Carbon-Efficiency.html#intergovernmental-panel-on-climate-change-ipcc","title":"Intergovernmental Panel on Climate Change (IPCC)","text":"<ul> <li>Created by the UN to provide governments at all levels with with scientific information to help develop climate policies.</li> <li>Provies reports into international climate change negotiations.</li> <li>An organization of governments that are members of the UN or the World Meteorogical Organization (WMO).</li> </ul>"},{"location":"certifications/green-software-for-practitioners/02_Carbon-Efficiency.html#summary","title":"Summary","text":"<ul> <li>Greenhouse gases are a group of gases contributing to global warming. Carbon is often used as a broad term to refer to the impact of all types of emissions and activities on global warming. CO2eq is a measurement term used to measure this impact.</li> <li>The international community, in groups such as the UNFCCC, has come together to limit the impact of global warming by reducing emissions, aiming for a 'preferable' lower limit of 1.5\u00b0C.</li> <li>This was agreed through the UN IPCC in 2015 in the Paris Climate Agreement and is monitored at the regular COP event.</li> <li>Everything we do emits carbon into the atmosphere, and our goal is to emit the least amount of carbon possible.</li> <li>This constitutes the first principle of green software: carbon efficiency, emitting the least amount of carbon possible per unit of work.</li> </ul>"},{"location":"certifications/green-software-for-practitioners/03_Energy-Efficiency.html","title":"3.0 - Energy Efficiency","text":""},{"location":"certifications/green-software-for-practitioners/03_Energy-Efficiency.html#31-introduction","title":"3.1 - Introduction","text":"<ul> <li> <p>The energy efficiency principle is simple: Use the least amount of energy possible</p> </li> <li> <p>Energy = Ability to do work</p> </li> <li>Multiple forms of energy exist and can each be converted to another in some form e.g. chemical to electrical conversion happens during coal burning.</li> <li> <p>In the example above, electricity is the secondary energy converted from another type; this allows measurement of electricity usage.</p> </li> <li> <p>All software consumes electricity, the best way to reduce consumption and subseqeuent carbon emissions is to make applications more efficient.</p> </li> <li> <p>Additionally, green software practitioners take responsibility for the energy consumed by their products, and design them to consume as little as possible by default.</p> </li> <li> <p>At every step of the process, the waste between steps should be minimised and efficiency should be maximised.</p> </li> <li> <p>The final area of consideration is the user, how can, when they're running the application, minimise their unnecessary emissions.</p> </li> <li>This is managed by how the application is designed, such as batching jobs together.</li> </ul>"},{"location":"certifications/green-software-for-practitioners/03_Energy-Efficiency.html#32-energy-efficiency","title":"3.2 - Energy Efficiency","text":""},{"location":"certifications/green-software-for-practitioners/03_Energy-Efficiency.html#fossil-fuels-and-high-carbon-sources-of-energy","title":"Fossil Fuels and High-Carbon Sources of Energy","text":"<ul> <li>Most electricity is produced via fossil fuel burning.</li> <li>The fuels are made from decomposing plants and animals, found within the earth's crust, and comprise of carbon and hyrdrogen.</li> <li> <p>Typical examples: coal, oil, natural gas.</p> </li> <li> <p>As most electricity comes from fossil fuels, the most significant carbon emission cause, one can immediately see how tightly linked electricity usage is to fossil fuel emissions.</p> </li> <li>To be carbon efficient, one therefore needs to be energy efficient as energy is a proxy for carbon.</li> </ul>"},{"location":"certifications/green-software-for-practitioners/03_Energy-Efficiency.html#low-carbon-sources-of-energy","title":"Low-Carbon Sources of Energy","text":"<ul> <li>Clean energy comes from renewable, zero-emission sources that don't pollute the atmosphere when used.</li> <li>They also save enrgy through energy efficient practices.</li> <li>There are often overlaps between clean, green, and renewable energy sources, key differences include:</li> <li>Clean Energy - Doesn't produce carbon emissions e.g. nuclear</li> <li>Green energy - relies on sources from nature</li> <li>Renewable Energy - sources don't expire e.g. solar, wind</li> </ul>"},{"location":"certifications/green-software-for-practitioners/03_Energy-Efficiency.html#energy-measurement","title":"Energy Measurement","text":"<ul> <li>Energy is measured in Joules (J); the SI unit of energy.</li> <li>Power is measured in watts (W), 1W = 1 Joule per second</li> <li>Kilowatt (kW) = 1000 Joules per second</li> <li>kilowatt-hour (kWh) = measure of energy corresponding to 1 kilowatt sustained for 1 hour.</li> </ul>"},{"location":"certifications/green-software-for-practitioners/03_Energy-Efficiency.html#improving-energy-efficiency","title":"Improving Energy Efficiency","text":""},{"location":"certifications/green-software-for-practitioners/03_Energy-Efficiency.html#power-usage-effectiveness","title":"Power Usage Effectiveness","text":"<ul> <li>Data centers use Power Usage Effectiveness (PUE) to measure data center energy efficiency.</li> <li>This defines how much energy the infrastructure uses compared to cooling and other supporting overheads.</li> <li>Examples:</li> <li>1.0 - Computing is using nearly all the enrgy</li> <li>2.0 - An additional watt of IT power is needed to cool and distribute power to the infrastructure for every watt it uses.</li> <li>Effectively, PUE = a multiplier to an application's energy consumption.</li> <li>Example: if an application uses 10kWh and the data center's PUE is 1.5, the actual consumption is 15kWh; 5kWh goes towards operational overhead of the data center and 10kWh is used to run the application.</li> </ul>"},{"location":"certifications/green-software-for-practitioners/03_Energy-Efficiency.html#energy-proportionality","title":"Energy Proportionality","text":"<ul> <li>Measures the relationship between power consumed by a computer and the rate at which useful work is done (its utilization).</li> <li>Utilization = how much of a computer's resources are used, usually expressed as a percentage.</li> <li>Power usage and utilization isn't proportional or linear in relationship, plateuing at high power usage.</li> <li>The more a computer is utilized, the more efficient it becomes at converting electricity to computation operations.</li> <li>To improve hardware efficiency, workloads should be run on as few servers as possible; this would maximise utilization rate and therefore energy efficiency.</li> </ul>"},{"location":"certifications/green-software-for-practitioners/03_Energy-Efficiency.html#static-power-draw","title":"Static Power Draw","text":"<ul> <li>How much electricity is drawn when in an idle state.</li> <li>Typically varies by configuration and hardware components, but all parts have it.</li> <li>It's the reason that lots of devices have power-saving modes; where the screen and disk are put to sleep, or the CPU's frequency is changed.</li> <li> <p>These modes save electricity, but typically have adverse effects on performance e.g. slow restarts.</p> </li> <li> <p>Servers aren't typically configured for aggresive or minimal power saving.</p> </li> <li>Typical use cases for servers demand total capacity as quickly as possible, as the server needs to respond to rapidly changing demands.</li> <li>This leads to many servers in idle modes during low demand periods; which incurs carbon costs from both embedded carbon and inefficient utilization.</li> </ul>"},{"location":"certifications/green-software-for-practitioners/03_Energy-Efficiency.html#summary","title":"Summary","text":"<ul> <li>Electricity is a proxy for carbon, so building an application that is energy efficient is equivalent to building an application that is carbon efficient.</li> <li>Green software takes responsibility for its electricity consumption and is designed to consume as little as possible.</li> <li>Quantifying the energy consumption of an application is a step in the right direction to start thinking about how an application can operate more efficiently. However, understanding your application's energy consumption is not the only story.</li> <li>The hardware your software is running on uses some of the electricity for operational overhead. This is called power usage efficiency (PUE) in the cloud space.</li> <li>The concept of energy proportionality adds another layer of complexity since hardware becomes more efficient at turning electricity into useful operations the more it's used.</li> <li>Understanding this gives green software practitioners a better insight into how their application behaves with respect to energy consumption in the real world.</li> </ul>"},{"location":"certifications/green-software-for-practitioners/04_Carbon-Awareness.html","title":"4.0 - Carbon Awareness","text":""},{"location":"certifications/green-software-for-practitioners/04_Carbon-Awareness.html#41-introduction","title":"4.1 - Introduction","text":"<ul> <li> <p>The Carbon Awareness Principle: Do more when the electricity is cleaner, and less when it is dirtier.</p> </li> <li> <p>Electricity is production comes with a lot of variability per method e.g.:</p> </li> <li>Locations - where is it produced?</li> <li>Time - when is it produced?</li> <li> <p>How - what source is used to produce it?</p> </li> <li> <p>Emission rates typically vary depending on the source e.g. clean, renewable sources like solar offer little carbon emissions in comparison to fossil fuels.</p> </li> <li>Carbon awareness is the idea of doing more when more energy comes from low-carbon sources, and doing less when more energy comes from high carbon sources.</li> </ul>"},{"location":"certifications/green-software-for-practitioners/04_Carbon-Awareness.html#42-carbon-awareness","title":"4.2 - Carbon Awareness","text":""},{"location":"certifications/green-software-for-practitioners/04_Carbon-Awareness.html#key-concepts","title":"Key Concepts","text":""},{"location":"certifications/green-software-for-practitioners/04_Carbon-Awareness.html#carbon-intensity","title":"Carbon Intensity","text":"<ul> <li>How much Carbon (CO2e) is emitted per kWh of electricity consumed. Measured in gCO2eq/kWh (grams of carbon per kilowatt hour).</li> <li>Carbon intensity is a mix of all the current power sources in the power grid, both the low and high-carbon using resources will be considered in the measurement.</li> </ul>"},{"location":"certifications/green-software-for-practitioners/04_Carbon-Awareness.html#variability-of-carbon-intensity","title":"Variability of Carbon Intensity","text":"<ul> <li>This varies by location, depending on the clean energy sources available.</li> <li>Additional variability follows due to changes in when clean energy could be available / unavailable due to weather conditions e.g. on a windy day carbon intensity will decrease / not be as intense.</li> </ul>"},{"location":"certifications/green-software-for-practitioners/04_Carbon-Awareness.html#dispatchability-curtailment","title":"Dispatchability &amp; Curtailment","text":"<ul> <li>Electricity demand varies throughout the day.</li> <li>If a utility doesn't provide enough electricity to meet demand, a brownout occurs (a dip in the power line's voltage level)</li> <li>If a utility produces more electricity than is required,a  blackout occurs / is put in place to stop infrastructure burning out.</li> <li>The balance between the demand and supply is handled by the utility provider.</li> <li>For fossil fuels, it's easier to control the amount of power produced; this is dispatchability.</li> <li>For renewable power, we cannot easily control how much is produced e.g. we cannot control whether it's a sunny day.</li> <li>If a power source produces more electricity needed, that electricity will be thrown away, this is curtailment.</li> </ul>"},{"location":"certifications/green-software-for-practitioners/04_Carbon-Awareness.html#marginal-carbon-intensity","title":"Marginal Carbon Intensity","text":"<ul> <li>In the event of suddenly needing to access more power than expected, the energy supplied would come from a marginal power plant.</li> <li>Marginal Power Plants are dispatchable; meaning these plants are often powered by fossil fuels.</li> <li>Marginal Carbon Intensity - The carbon intensity of the power plant employed to meet the new extra demand.</li> <li>Fossil-fueled power plants rarely scale down to zero. Typically they have a minimum functioning threshold.</li> <li>Others don't scale at all, and are considered a consistent, always-on baseload.</li> <li>Because of this, there may be the need to curtail (throw away) renewable energy while still consuming fossil fuel power plant energy.<ul> <li>In these scenarios, the marginal carbon intensity will be 0 gCO2eq/kWh - any new demand will match the renewable energy we are curtailing.</li> </ul> </li> </ul>"},{"location":"certifications/green-software-for-practitioners/04_Carbon-Awareness.html#energy-markets","title":"Energy Markets","text":"<ul> <li>The exact market model varies around the world but broadly follows the same model.</li> <li> <p>When the demand for electricity goes down, utilities need to reduce the supply to balance supply and demand. They can do this in one of two ways:</p> </li> <li> <p>1. Buy less energy from fossil fuel plants.</p> </li> <li> <p>Energy from fossil fuel plants is usually the most expensive so this is the preferred method. This directly translates to burning fewer fossil fuels.</p> </li> <li> <p>2. Buy less energy from renewable sources.</p> </li> <li>Renewable sources are the cheapest, so they prefer not to do this.</li> <li> <p>If a renewable source doesn't manage to sell all of its electricity, it has to throw the rest away.</p> </li> <li> <p>Reducing the amount of electricity consumed in your applications can help decrease the energy's carbon intensity seeing as the first thing to be scaled back are fossil fuels.</p> </li> <li> <p>When the demand for electricity goes up, utilities need to increase the supply to balance supply and demand. They can do this in one of two ways:</p> </li> <li> <p>1. Buy more energy from renewable sources that are currently being curtailed.</p> </li> <li>If you are curtailing, it means you have excess energy you could dispatch.</li> <li>Renewable energy is already the cheapest, so curtailed renewable energy will be the cheapest dispatchable energy source.</li> <li> <p>Renewable plants will then sell the energy they would have had to curtail.</p> </li> <li> <p>1. Buy more energy from fossil fuel plants.</p> </li> <li>Fossil fuels are inherently dispatchable; they can quickly increase energy production by burning more.</li> <li> <p>However, coal costs money, so this is the least preferred solution.</p> </li> <li> <p>Energy markets are some of the most complex markets in the world so the above explanation is a simplification.</p> </li> <li>But what's important to understand is that our goal is to increase investment into lower carbon energy sources, like renewables, and decrease investment into higher carbon sources, like coal.</li> <li>The best way to ensure money flows in the right direction is to make sure you use electricity with the least carbon intensity.</li> </ul>"},{"location":"certifications/green-software-for-practitioners/04_Carbon-Awareness.html#carbon-awareness-tips","title":"Carbon Awareness Tips","text":"<ul> <li>In general, it's advised to use electricity when the carbon intensity is low, as this ensures investment flows towards low-carbon emitting plants and away from high-carbon emitting plants.</li> <li>Carbon intensity is lower when more energy comes from lower-carbon sources and higher when it comes from higher-carbon sources.</li> </ul>"},{"location":"certifications/green-software-for-practitioners/04_Carbon-Awareness.html#demand-shifting","title":"Demand Shifting","text":"<ul> <li>Being carbon aware means responding to shifts in carbon intensity by increasing or decreasing your demand.</li> <li>If your work allows you to be flexible with when and where you run workloads, you can shift accordingly - consuming electricity when the carbon intensity is lower and pausing production when it is higher.</li> <li> <p>For example, training a Machine Learning model at a different time or region with much lower carbon intensity.</p> </li> <li> <p>Studies show these actions can result in 45% to 99% carbon reductions depending on the number of renewables powering the grid.</p> </li> <li>Demand shifting can be further broken down into spatial and temporal shifting.</li> </ul>"},{"location":"certifications/green-software-for-practitioners/04_Carbon-Awareness.html#spatial-shifting","title":"Spatial Shifting","text":"<ul> <li>Spatial shifting means moving your computation to another physical location where the current carbon intensity is lower.</li> <li>It might be a region that naturally has lower carbon sources of energy.</li> <li>For example, moving to different hemispheres depending on the season for more sunlight hours.</li> </ul>"},{"location":"certifications/green-software-for-practitioners/04_Carbon-Awareness.html#temporal-shifting","title":"Temporal Shifting","text":"<ul> <li>If you can't shift your computation spatially to another region, another option you have is to shift to another time. Perhaps later in the day or night when it's sunnier or windier and, therefore, the carbon intensity is lower.</li> <li>This is called temporal demand shifting. We can predict future carbon intensity reasonably well through advances in weather forecasting.</li> <li>Some of the biggest technology companies have recognized the importance of carbon awareness and are using advanced modeling techniques to implement demand shifting.</li> <li>Google Carbon Aware Data Centers - Google launched a project to make some of the cloud workloads carbon aware.</li> <li>They created models to predict tomorrow's carbon intensity and workload</li> <li>They then shaped large-scale workloads so more would happen when and where the carbon intensity is lowest, but in such a way that they could still handle the expected load.</li> <li>Microsoft Carbon Aware Windows - Microsoft announced a project to make Windows 11 more sustainable. Initially, this means running Windows updates when the carbon intensity is lower.</li> </ul>"},{"location":"certifications/green-software-for-practitioners/04_Carbon-Awareness.html#demand-shaping","title":"Demand Shaping","text":"<ul> <li>Demand shifting is the strategy of moving computation to regions or times when the carbon intensity is lowest. Demand shaping is a similar strategy. - However, instead of moving demand to a different region or time, we shape our computation to match the existing supply.</li> <li>If carbon intensity is low, increase the demand; do more in your applications.</li> <li>If carbon intensity is high, decrease demand; do less in your applications.</li> <li>Demand shaping for carbon-aware applications is all about the supply of carbon.</li> <li>When the carbon cost of running your application becomes high, shape the demand to match the supply of carbon.</li> <li>This can happen automatically, or the user can make a choice.</li> <li>Eco mode is an example of demand shifting. Eco modes are found in everyday appliances like cars or washing machines.</li> <li>When activated, some amount of performance is sacrificed in order to consume fewer resources (gas or electricity).</li> <li> <p>Because there is this trade-off with performance, eco modes are always presented to a user as a choice.</p> </li> <li> <p>Software applications can also have eco modes that can - either automatically or with user consent - make decisions to reduce carbon emissions.</p> </li> <li>One example of this is video conferencing software that adjusts streaming quality automatically.</li> <li> <p>Rather than streaming at the highest quality possible at all times, it reduces the video quality to prioritize audio when the bandwidth is low.</p> </li> <li> <p>Another example is TCP/IP. The transfer speed increases in response to how much data is broadcast over the wire.</p> </li> <li>A third example is progressive enhancement with the web. The web experience improves depending on the resources and bandwidth available on the end user's device.</li> <li>Demand shaping is related to a broader concept in sustainability, which is to reduce consumption.</li> <li>We can achieve a lot by becoming more efficient with resources, but we also need to consume less at some point.</li> <li>As Green Software practitioners, we would consider canceling a process when the carbon intensity is high instead of demand shifting - reducing the demands of our application and the expectations of our end users.</li> </ul>"},{"location":"certifications/green-software-for-practitioners/04_Carbon-Awareness.html#summary","title":"Summary","text":"<ul> <li>Carbon awareness means understanding that the energy you consume does not always have the same impact in terms of carbon intensity.</li> <li>Carbon intensity varies depending on the time and place it is consumed.</li> <li>The nature of fossil fuels and renewable energy sources means that consuming energy when carbon intensity is low increases the demand for renewable energy sources and increases the percentage of renewable energy in the supply.</li> <li>Demand shifting means moving your energy consumption to different locations or times of days where the carbon intensity is lower.</li> <li>Demand shaping means adapting your energy consumption around carbon intensity variability in order to consume more in periods of low intensity and less in periods of high intensity.</li> </ul>"},{"location":"certifications/green-software-for-practitioners/05_Hardware-Efficiency.html","title":"5.0 - Hardware Efficiency","text":""},{"location":"certifications/green-software-for-practitioners/05_Hardware-Efficiency.html#51-introduction","title":"5.1 - Introduction","text":"<p>Hardware efficiency principle: Use the least amount of embodied carbon possible</p>"},{"location":"certifications/green-software-for-practitioners/05_Hardware-Efficiency.html#key-concepts","title":"Key Concepts","text":""},{"location":"certifications/green-software-for-practitioners/05_Hardware-Efficiency.html#embodied-carbon","title":"Embodied Carbon","text":"<ul> <li>The device you are using to read this on produced carbon when it was manufactured and, once it reaches the end of life, disposing of it may release more.</li> <li> <p>Embodied carbon (also referred to as \"embedded carbon\") is the amount of carbon pollution emitted during the creation and disposal of a device.</p> </li> <li> <p>When calculating the total carbon pollution for computers running software, both the carbon pollution associated with running the computer as well as the embodied carbon of the computer must be accounted for.</p> </li> <li> <p>Embodied carbon varies significantly between end-user devices. For some devices, the carbon emitted during manufacturing is much higher than that emitted during usage, as illustrated by a study from University of Zurich.</p> </li> <li> <p>As a result, the embodied carbon cost can sometimes be much higher than the carbon cost of the electricity powering it.</p> </li> <li> <p>By thinking in terms of embodied carbon, any device, even one not consuming electricity, is responsible for the release of carbon over its lifetime.</p> </li> </ul>"},{"location":"certifications/green-software-for-practitioners/05_Hardware-Efficiency.html#amortization","title":"Amortization","text":"<ul> <li>A way to account for embodied carbon is to amortize the carbon over the expected life span of a device.</li> <li>For example, suppose it took 4000kg CO2eq to build a server, and we expect it to last four years.</li> <li>Amortization means that we can say the server emits 1000kg CO2eq/year.</li> </ul>"},{"location":"certifications/green-software-for-practitioners/05_Hardware-Efficiency.html#52-hardware-efficiency","title":"5.2 - Hardware Efficiency","text":""},{"location":"certifications/green-software-for-practitioners/05_Hardware-Efficiency.html#improving-hardware-efficiency","title":"Improving Hardware Efficiency","text":"<ul> <li>If we take into account the embodied carbon, it is clear that by the time we come to buy a computer, it's already emitted a good deal of carbon. Computers also have a limited lifespan, which means they eventually are unable to handle modern workloads and need to be replaced.</li> <li>In these terms, hardware is a proxy for carbon, and since our goal is to be carbon efficient, we must also be hardware efficient.</li> <li>There are two main approaches to hardware efficiency:</li> <li>For end-user devices, it's extending the lifespan of the hardware.</li> <li>For cloud computing, it's increasing the utilization of the device.</li> </ul>"},{"location":"certifications/green-software-for-practitioners/05_Hardware-Efficiency.html#extending-the-lifespan-of-hardware","title":"Extending the Lifespan of Hardware","text":"<ul> <li>In the example we saw previously, if we can add just one more year to the lifespan of our server, then the amortized carbon drops from 1000kg CO2eq/year to 800kg CO2eq/year.</li> <li>Hardware is retired when it breaks down or struggles to handle modern workloads.</li> <li>Of course, hardware will always break down eventually but, as developers, we can use software to build applications that run on older hardware and extend their lifetime.</li> </ul>"},{"location":"certifications/green-software-for-practitioners/05_Hardware-Efficiency.html#increasing-device-utilization","title":"Increasing Device Utilization","text":"<ul> <li>In the cloud space, hardware efficiency most often translates to an increase in the utilization of servers. It's better to use one server at 100% utilization than 5 servers at 20% utilization because of the cost of embodied carbon.</li> <li>In the same way that owning one car and using it every day of the week is much better than owning five and using a different one each day of the week, it is much more efficient to use servers at their full capacity rather than employing several at below capacity.</li> <li> <p>Although emissions are the same, the embodied carbon that is used is much lower.</p> </li> <li> <p>The most common reason for having under-utilized servers is so that peak capacity is accounted for. Running servers at 20% means that you know you will be able to handle peaks in demand without impacting performance.</p> </li> <li>However, in the meantime, all that spare capacity just sitting there idle represents wasted embodied carbon.</li> <li> <p>Being hardware efficient means making sure that every hardware device is being utilized as much as possible for as long as possible.</p> </li> <li> <p>This is one of the main advantages of the public cloud; you know that when you do need to scale up, the space will be there to take up the slack.</p> </li> <li> <p>With multiple organizations making use of the public cloud, spare capacity can always be made available to whoever needs it, so that no servers are sitting idle.</p> </li> <li> <p>It's important to note that simply moving operations to the public cloud will not automatically reduce your emissions.</p> </li> <li>It simply gives you the space to be able to re-architect your software so that a reduction is possible.</li> </ul>"},{"location":"certifications/green-software-for-practitioners/05_Hardware-Efficiency.html#summary","title":"Summary","text":"<ul> <li>Embodied carbon is the amount of carbon pollution emitted during the creation and disposal of a device.</li> <li>When calculating your total carbon pollution, you must consider both that which is emitted when running the computer as well as the embodied carbon associated with its creation and disposal.</li> <li>Extending the lifetime of a device has the effect of amortizing the carbon emitted so that its CO2eq/year is reduced.</li> <li>Cloud computing is more energy efficient then an on-premise server as it can apply demand shifting as well as demand shaping.</li> </ul>"},{"location":"certifications/green-software-for-practitioners/06_Measurement.html","title":"6.0 - Measurement","text":""},{"location":"certifications/green-software-for-practitioners/06_Measurement.html#61-introduction","title":"6.1 - Introduction","text":"<ul> <li> <p>The measurement principle: what you cannot measure, you cannot improve</p> </li> <li> <p>The Greenhouse Gas (GHG) protocol is the most commonly-used method for organizations to measure their total carbon emissions.</p> </li> <li> <p>Understanding GHG scopes and how to measure your software against industry standards will help you see to what extent you are applying Green Software principles and how far you have to go to improve.</p> </li> <li> <p>To complement the GHG protocol, you can also use the Software Carbon Intensity (SCI) specification.</p> </li> <li> <p>While the GHG is a more generic measurement suitable for all types of organizations, the SCI is specifically for measuring a rate of software emissions and designed to incentivize the elimination of those emissions.</p> </li> <li> <p>The GHG is a protocol to measure total emissions, the SCI is a tool to enable the elimination of software driven emissions.</p> </li> </ul>"},{"location":"certifications/green-software-for-practitioners/06_Measurement.html#62-measurement","title":"6.2 - Measurement","text":""},{"location":"certifications/green-software-for-practitioners/06_Measurement.html#the-ghg-protocol","title":"The GHG Protocol","text":"<ul> <li>The Greenhouse Gas protocol is the most widely used and internationally recognized greenhouse gas accounting standard. 92% of Fortune 500 companies use the GHG protocol when calculating and disclosing their carbon emissions.</li> <li>The GHG protocol divides emissions into three scopes:</li> <li>Scope 1: Direct emissions from operations owned or controlled by the reporting organization, such as on-site fuel combustion or fleet vehicles.</li> <li>Scope 2: Indirect emissions related to emission generation of purchased energy, such as heat and electricity.</li> <li> <p>Scope 3: Other indirect emissions from all the other activities you are engaged in.</p> <ul> <li>Including all emissions from an organization's supply chain; business travel for employees, and the electricity customers may consume when using your product.</li> </ul> </li> <li> <p>Scope 3, sometimes referred to as value chain emissions, is the most significant source of emissions and the most complex to calculate for many organizations.</p> </li> <li>These encompass the full range of activities needed to create a product or service, from conception to distribution.</li> <li>In the case of a laptop, for example, every raw material used in its production emits carbon when being extracted and processed.</li> <li> <p>Value chain emissions also include emissions from the use of the laptop, meaning the emissions from the energy used to power the laptop after it has been sold to a customer.</p> </li> <li> <p>Through this approach, it's possible to sum up all the GHG emissions from every organization and person in the world and reach a global total.</p> </li> </ul>"},{"location":"certifications/green-software-for-practitioners/06_Measurement.html#what-scope-does-my-application-fall-into","title":"What Scope Does my Application Fall Into?","text":"<ul> <li>We have already seen how the GHG protocol asks us to bucket software emissions according to scopes 1-3. But how does this work when it comes to software?</li> <li>Most organizations have many applications running with different architectures and in different environments.</li> <li>As such, the scope your emissions fall into, both in terms of energy generated as well as embodied carbon, depending on your specific scenario.</li> <li>For cloud applications running on servers that you own, the energy usage of your software falls into scope 2, and the embodied carbon of all your servers falls into scope 3.</li> <li>For cloud applications running on a public cloud, both the energy usage of your application and the embodied carbon fall into scope 3.</li> <li>In scenarios where you are running a hybrid private/public cloud application, part of its emissions will fall into scope 2 and part will fall into scope 3.</li> <li>Similarly, for your customer-facing front-end application, energy usage falls into your organization's scope 3, since your customer will purchase the energy to power their device.</li> <li>For software, regardless of whether it's run on infrastructure you own, rent, or consumers own, there are three parameters to consider for bucketing emissions:</li> <li>How much energy it consumes</li> <li>How clean or dirty that electricity is</li> <li>How much hardware it needs to function</li> </ul>"},{"location":"certifications/green-software-for-practitioners/06_Measurement.html#calculating-a-total-for-software-carbon-emissions","title":"Calculating a Total for Software Carbon Emissions","text":"<ul> <li>To calculate a total for software carbon emissions, you need access to detailed data regarding the energy consumption, carbon intensity, and hardware that your software is running on.</li> <li>This is challenging data to gather, even in the case of an organization's own closed-source software products where they can track its usage with telemetry or logs.</li> <li>Open-source software maintainers don't have the same visibility into how and where their software is used, how much energy is consumed, and on what hardware.</li> <li>Open-source projects typically have multiple contributors from multiple organizations. As a result, it's unclear who should be responsible for calculating the emissions as well as who is accountable for eliminating them.</li> <li>When you also consider that open-source software makes up 90% of a typical enterprise stack, it is clear that there is going to be a large amount of carbon emissions that are not accounted for.</li> </ul>"},{"location":"certifications/green-software-for-practitioners/06_Measurement.html#do-totals-tell-the-whole-story","title":"Do Totals Tell the Whole Story?","text":"<ul> <li>A total is only one metric that describes the state of something. To make the right decisions, you need to look at many different metrics.</li> <li>Imagine a scenario where you are the leader of an organization and charged with reducing the emissions of your software. You measure the emissions in Q1 and come out with a total of 34 tonnes.</li> <li> <p>After making some investments into projects that eliminate emissions, you find that by Q2 the emissions have increased to 45 tonnes. Does this mean your efforts failed?</p> </li> <li> <p>Not necessarily. We know that a total by itself doesn't tell the whole story and must look at other metrics to find out if an emissions-reduction project has been successful. - For example, if you measured the carbon intensity as well as the carbon total, you might come out with a different perspective.</p> </li> <li>In the same project, if the carbon intensity was 3.3g CO2eq/user in Q1, and 2.9g CO2eq/user in Q2, you might consider the project a success and continue to invest further.</li> <li>While the total informed you that your organization's carbon emissions had increased overall, the intensity gave a more complete perspective that would help you to make a more informed decision on how to proceed.</li> </ul>"},{"location":"certifications/green-software-for-practitioners/06_Measurement.html#software-carbon-intensity-specification","title":"Software Carbon Intensity Specification","text":"<ul> <li>The Software Carbon Intensity (SCI) specification is a methodology developed by the Standards Working Group in the Green Software Foundation, designed to score a software application along a dimension of sustainability and to encourage action towards eliminating emissions.</li> <li>It's not a replacement for the GHG protocol, but an additional metric that helps software teams understand how their software behaves in terms of carbon emissions so they can make more informed decisions.</li> <li>While the GHG protocol calculates the total emissions, the SCI is about calculating the rate of emissions.</li> <li> <p>In automotive terms, the SCI is more like a miles per gallon measurement and the GHG protocol is more like the total carbon footprint of a car manufacturer and all their cars they produce every year.</p> </li> <li> <p>Instead of bucketing the carbon emissions of software into scopes 1-3, it buckets them into operational emissions (carbon emissions from the running of software) and embodied emissions (carbon emissions from the physical resources required to run the software).</p> </li> <li> <p>It's also an intensity rather than a total, which is more inclusive of open-source software.</p> </li> <li> <p>An important thing to note is that it is not possible to reduce your SCI score by purchasing offsets in the form of neutralizations, compensations, or by offsetting electricity in the form of renewable energy credits.</p> </li> <li> <p>This means that an organization that makes no efforts towards reducing their emissions but simply spends money on carbon credits cannot achieve a good SCI score.</p> </li> <li> <p>Offsets are an essential component of any climate strategy; however, offsets are not eliminations and therefore are not included in the SCI metric.</p> </li> <li> <p>If you make your application more energy efficient, hardware efficient, or carbon aware, your SCI score will decrease.</p> </li> <li>The only way to reduce your SCI score is to invest time or resources into one of those three principles.</li> <li>As such, adopting the SCI as a metric for your software application along with the GHG protocol, will drive investment into one of the three pillars of green software.</li> </ul>"},{"location":"certifications/green-software-for-practitioners/06_Measurement.html#the-sci-equation","title":"The SCI Equation","text":"<ul> <li>The SCI is a method of scoring any software application, not limited to just cloud or not just end-user applications, but all types of applications in between.</li> <li> <p>It provides a common language to describe how software behaves with respect to carbon emissions and how a proposed change might eliminate some of them.</p> </li> <li> <p>The equation to calculate an SCI score is elegantly simple. This simplicity means it can be applied in a number of different scenarios.</p> </li> </ul> <pre><code>SCI = ((E * I) + M) per R\n</code></pre> <ul> <li>E = Energy consumed by a software system</li> <li>I = Location-based marginal carbon emissions</li> <li>M = Embodied emissions of a software system.</li> <li> <p>R = Functional unit (e.g. carbon per additional user, API-call, ML job, etc)</p> </li> <li> <p>This summarizes to:</p> </li> </ul> <pre><code>SCI = C per R (Carbon per R)\n</code></pre> <ul> <li>R is the core characteristic of the SCI and turns it into an intensity rather than a total. This is what we call a functional unit.</li> </ul>"},{"location":"certifications/green-software-for-practitioners/06_Measurement.html#calculating-the-sci-score","title":"Calculating the SCI Score","text":""},{"location":"certifications/green-software-for-practitioners/06_Measurement.html#decide-what-to-include","title":"Decide what to Include","text":"<ul> <li> <p>What software components to include or exclude in the SCI score means defining the boundaries of your software; where it starts and where it ends.</p> </li> <li> <p>For every software component you include, you will need to measure its impact. For every major component you exclude, you need to explain why.</p> </li> <li> <p>The SCI specification doesn't currently make any demands regarding what to include and what not to include. However, you must include all supporting infrastructure and systems that significantly contribute to the software operation.</p> </li> <li> <p>Your SCI score might decrease because you tightened your software boundary and excluded more software components.</p> </li> <li>Conversely, your SCI score might increase because you are including software components you previously excluded. Therefore, when you report your SCI score, especially any improvements in the score, it's essential to disclose your software boundary.</li> </ul>"},{"location":"certifications/green-software-for-practitioners/06_Measurement.html#choose-the-functional-unit","title":"Choose the Functional Unit","text":"<ul> <li>As we have seen, the SCI is a rate rather than a total and measures the intensity of emissions according to the chosen functional unit.</li> <li> <p>The specification currently doesn't prescribe the functional unit and you are free to pick whichever best describes how your application scales.</p> <ul> <li>For example, if your application scales by the number of users, then pick users as your functional unit.</li> </ul> </li> <li> <p>Future iterations of the SCI might prescribe specific functional units for different types of applications to aid with comparability.</p> </li> <li>For instance, we might ask streaming applications to choose minutes as their functional unit in order to standardize measurement across all streaming applications.</li> </ul>"},{"location":"certifications/green-software-for-practitioners/06_Measurement.html#decide-how-to-measure-your-emissions","title":"Decide how to Measure Your Emissions","text":"<ul> <li>Now you have a list of the software components you want to measure and the functional unit you will use to measure them.</li> <li> <p>The next step is to decide how you will quantify the emissions of each software component.</p> </li> <li> <p>There are two methods of quantification; measurement and calculation.</p> </li> <li>Measurement is using counters of some form.<ul> <li>For example, measuring the energy consumption of your software component by using a hardware device in the wall socket.</li> <li>Or using counters on hardware that directly measure energy consumption.</li> <li>If you can directly count your units, you should use the measurement approach.</li> </ul> </li> <li>Calculation involves indirect counting, often using a model of some form.<ul> <li>For instance, if you cannot directly measure your application's energy consumption but instead have a model that estimates the energy consumption based on the CPU utilization, this is considered calculation rather than measurement.</li> </ul> </li> <li> <p>These resources can help you to decide on measurement and calculation methods:</p> </li> <li> <p>Look at the Software Carbon Intensity Data project. This project is responsible for providing advice on how to quantify different software components' emissions.</p> </li> </ul>"},{"location":"certifications/green-software-for-practitioners/06_Measurement.html#quantify","title":"Quantify","text":"<ul> <li>Now you are ready to execute. Using the methodology described in the previous steps, start to quantify the SCI score for each software component in your boundary.</li> <li> <p>Your total SCI score of your software application is the combined score of all the different components.</p> </li> <li> <p>You may calculate multiple SCI scores for the same application. The SCI score is helpful information to understand how your application behaves with respect to carbon emissions in different scenarios. For example, a streaming application might choose carbon per minute as a metric.</p> </li> <li>It might also calculate the carbon per user per day. The carbon per $ revenue metric might give another helpful dimension.</li> </ul>"},{"location":"certifications/green-software-for-practitioners/06_Measurement.html#summary","title":"Summary","text":"<ul> <li>The GHG protocol is a metric for measuring an organization's total carbon emissions and is used by organizations all over the world.</li> <li>The GHG protocol puts carbon emissions into three scopes. Scope 3, also known as value chain emissions, refers to the emissions from organizations that supply others in a chain. In this way, one organization's scope 1 and 2 will sum up into another organization's scope 3.</li> <li>Calculating software-driven emissions using the GHG protocol is possible but can be difficult for open-source software.</li> <li>The SCI is a metric designed specifically to calculate software emissions and is a rate rather than a total.</li> <li>The functional unit of measurement is not prescribed in the SCI and you should choose something that reflects your application.</li> </ul>"},{"location":"certifications/green-software-for-practitioners/07_Climate-Commitments.html","title":"7.0 - Climate Commitments","text":""},{"location":"certifications/green-software-for-practitioners/07_Climate-Commitments.html#71-introduction","title":"7.1 - Introduction","text":"<ul> <li>Climate Commitments Principle: Understand the Exact Mechanism of Carbon Reduction</li> </ul>"},{"location":"certifications/green-software-for-practitioners/07_Climate-Commitments.html#72-climate-commitments","title":"7.2 - Climate Commitments","text":""},{"location":"certifications/green-software-for-practitioners/07_Climate-Commitments.html#carbon-reduction-methodologies","title":"Carbon Reduction Methodologies","text":"<ul> <li>There are many ways to reduce emissions but it's important to understand the exact mechanism of the reduction when thinking about reduction targets.</li> </ul>"},{"location":"certifications/green-software-for-practitioners/07_Climate-Commitments.html#abatement-carbon-elimination","title":"Abatement / Carbon Elimination","text":"<ul> <li>The Science Based Targets Initiative refers to a mechanism called abatement, which means eliminating sources of CO2 emissions associated with a company's operations and value chain so that they do not enter the atmosphere.</li> <li> <p>The value chain describes the full range of activities needed to create a product or service, from conception to distribution. This includes increasing energy efficiency to eliminate some of the emissions associated with energy generation.</p> </li> <li> <p>Abatement is not enough on its own as there will always be some emissions that can't be eliminated due to technological or economic constraints, but it must form the core of every organization's strategy as it is an area where almost every company can improve on.</p> </li> <li> <p>To balance those residual emissions, we need to look at other mechanisms such as offsets, compensations or neutralizations.</p> </li> </ul>"},{"location":"certifications/green-software-for-practitioners/07_Climate-Commitments.html#offsets","title":"Offsets","text":"<ul> <li>Offsets are direct investments in emission-reduction projects through the purchase of carbon credits on the voluntary carbon market (VCM).</li> <li>The VCM is a decentralized market where private actors voluntarily buy and sell carbon credits that represent certified removals or reductions of GHGs from the atmosphere.</li> <li> <p>To offset emissions, you need to purchase the equivalent volume of carbon credits to compensate for those emitted, where 1 carbon credit corresponds to 1 tonne of CO2 absorbed or reduced.</p> </li> <li> <p>Various positive benefits can stem from these projects, from ecosystem protection to empowering local communities.</p> </li> <li>However, to ensure these programs are implemented correctly and have the desired effect on the environment and the aim to reach world net zero, there are global standards that they must meet such as Verified Carbon Standard (VCS) and Gold Standard (GS).</li> </ul>"},{"location":"certifications/green-software-for-practitioners/07_Climate-Commitments.html#sci-and-offsets","title":"SCI and Offsets","text":"<ul> <li>There are some limitations to carbon offsets and that is why they are not considered in an organization's SCI score.</li> <li>For example, imagine two applications, both running on a cloud platform that is 100% carbon offset and matched 100% by renewable energy.</li> <li>Application A has invested significant time and resources into making sure it is using resources efficiently, whereas application B uses resources very inefficiently.</li> <li> <p>For the SCI to be a helpful metric, application A needs to score better than application B.</p> </li> <li> <p>If the SCI considered offsets, both applications would score 0. This wouldn't tell us anything about how efficiently they are using resources.</p> </li> <li> <p>Although application B is emitting more carbon molecules into the atmosphere, since its score is 0 and the lowest score is 0, why would it make further investments into improving its carbon efficiency?</p> </li> <li> <p>Organizations need to have plans for how to both eliminate as well as neutralize emissions and the SCI helps them to drive the elimination of emissions due to software.</p> </li> <li>This makes the SCI an essential component of any net-zero strategy.</li> </ul>"},{"location":"certifications/green-software-for-practitioners/07_Climate-Commitments.html#compensating-carbon-avoidance","title":"Compensating / Carbon Avoidance","text":"<ul> <li>Compensations are actions that companies take to help society avoid or reduce emissions outside of their value chain.</li> <li>This is essentially investing in other organizations' abatement projects.</li> <li>This includes actions such as:</li> <li>Conservation - Credits are created based on carbon not released through protecting old trees.</li> <li>Community Projects - These projects help communities worldwide, mainly undeveloped ones, by introducing sustainable living methods.</li> <li>Waste to energy - These projects capture methane/landfill gas in smaller villages, human or agriculture waste, and convert it into electricity.</li> </ul>"},{"location":"certifications/green-software-for-practitioners/07_Climate-Commitments.html#neutralizing-carbon-removal","title":"Neutralizing / Carbon Removal","text":"<ul> <li>Neutralizations are actions that companies take to remove carbon from the atmosphere within or beyond their value chain.</li> <li>Neutralizations refer to the removal and permanent storage of atmospheric carbon to counterbalance the effect of releasing CO2 into the atmosphere. This includes actions such as:<ul> <li>Enhancing natural carbon sinks that remove CO2 from the atmosphere. For example, forest restoration, since photosynthesis removes CO2 naturally.</li> <li>Forest expansion comes with challenges as it's essential not to impact the dynamics of farmland and food supply elsewhere.</li> <li>Modern farming methods can also prolong the time carbon remains stored in soil.</li> <li>Direct air capture is the process of capturing CO2 from the air and storing it permanently, either underground or in long-lived products like concrete.</li> </ul> </li> <li>The effectiveness of these methods is typically measured based on whether they can deliver carbon removal at the scale and speed needed.</li> <li>When it comes to carbon removal projects, durability is a critical consideration.</li> <li>The durability of a project describes how long the carbon dioxide will be kept from the atmosphere.</li> <li> <p>Short-term durability is up to 100 years, medium-term is 100 to 1,000 years, and long-term is more than 1,000 years.</p> </li> <li> <p>Solutions that rely on Earth's natural carbon cycle have short-term durability measured in decades.</p> </li> <li>For example, forestry projects have a durability of 40 to 100 years.</li> <li>Engineered solutions such as direct air capture often have long-term durability measured in millennia. For example, direct air capture has a durability of 10,000 years.</li> <li>Long-term projects are typically orders of magnitude more expensive than short-term projects.</li> <li>Once emitted, carbon remains in the atmosphere for 5,000 years. To be considered net zero, carbon that has been emitted needs to be permanently removed.</li> <li>A short-term carbon removal project will only remove carbon for 100 years, after which it's back in the atmosphere warming up our planet.</li> <li>This is one of the reasons why abatement is preferred to neutralization. Never releasing carbon is far better than releasing carbon and then trying to keep it out of the atmosphere for 5,000 years.</li> </ul>"},{"location":"certifications/green-software-for-practitioners/07_Climate-Commitments.html#climate-commitments","title":"Climate Commitments","text":"<ul> <li>There are many different climate reduction strategies that an organization can commit to, from carbon neutral to net zero.</li> <li>Understanding the different meanings and implications of each one can help you decide on the right strategy for your organization.</li> </ul>"},{"location":"certifications/green-software-for-practitioners/07_Climate-Commitments.html#carbon-neutral","title":"Carbon Neutral","text":"<ul> <li>To achieve carbon neutrality, an organization must measure its emissions, then match the total to its emissions offsets through carbon reduction projects.</li> <li> <p>This can include carbon removal projects (neutralizations) and carbon avoidance projects (compensations).</p> </li> <li> <p>Carbon neutrality is defined by an internationally recognized standard: PAS 2060. Although this does recommend an organization sets abatement targets, it doesn't demand they reduce their emissions.</p> </li> <li> <p>So to be considered carbon neutral, an organization can just measure and offset without investing resources in eliminating their carbon emissions.</p> </li> <li> <p>To be carbon neutral, you must cover direct emissions (scope 1 and 2). The general expectation is that organizations measure and offset their scopes 1 and 2, and business travel from scope 3. However, there is no specific requirement to include that.</p> </li> <li> <p>Carbon neutral is a significant first step for any organization since it encourages measurement. However, there are not enough carbon offsets in the world to offset the emissions of all the organizations.</p> </li> <li>Therefore, any strategy that doesn't include abatement will not scale or help the world achieve the 1.5 degree target set by the Paris Climate Agreement. This is where net zero comes into play.</li> </ul>"},{"location":"certifications/green-software-for-practitioners/07_Climate-Commitments.html#net-zero","title":"Net Zero","text":"<ul> <li>Net zero means reducing emissions according to the latest climate science and balancing remaining residual emissions through carbon removals (neutralizations).</li> <li> <p>Net zero, by definition, requires emissions reductions in line with a 1.5\u00b0C pathway. All businesses must do this to achieve net-zero global emissions by 2050.</p> </li> <li> <p>The critical differentiator between net zero and carbon neutral is net zero's focus on abatement rather than neutralizations and compensations.</p> </li> <li> <p>A net-zero target aims to eliminate emissions and only to use offsetting for the residual emissions that you cannot eliminate</p> </li> <li> <p>The standard for net zero is being developed by the Science Based Targets initiative (SBTi). They calculate that there is a 66% probability of limiting global warming to 1.5\u00b0C if we reach a level of abatement of about 90% of all GHG emissions by mid-century.</p> </li> <li> <p>So, to meet a net-zero target, an organization needs to eliminate 90% of its emissions by 2050. The remaining emissions can only be offset using neutralizations and permanent carbon removals.</p> </li> <li> <p>A net-zero strategy would mean that the actual amount of carbon in the atmosphere remains constant.</p> </li> <li> <p>Also, to be a net-zero target, you must cover direct and indirect, i.e. supply chain emissions (scopes 1,2 and 3).</p> </li> <li>Therefore, your entire value chain needs to be included in the scope of your net-zero target.</li> <li> <p>This is significant since scope 3 often represents the majority of emissions.</p> </li> <li> <p>SCI as part of a Net-Zero strategy</p> </li> <li>The SCI is a metric specifically designed to drive the elimination of emissions. The only way to reduce your score is to invest time and resources into actions that eliminate emissions.</li> <li>The only activities the SCI recognizes as elimination actions are making your application more energy-efficient, more hardware efficient, or consuming lower-carbon energy sources.<ul> <li>Offsets are an essential component of any climate strategy; however, offsets are not eliminations and therefore are not included in the SCI metric.</li> </ul> </li> <li>Any net-zero strategy needs to have plans for how to both eliminate as well as neutralize emissions.</li> <li>The SCI helps organizations drive the elimination of emissions due to software. This makes the SCI an essential component of any net-zero strategy.</li> </ul>"},{"location":"certifications/green-software-for-practitioners/07_Climate-Commitments.html#100-renewable","title":"100% Renewable","text":"<ul> <li> <p>When organizations set a target of 100% renewable power, they might distinguish between being matched by vs. powered by renewables.</p> </li> <li> <p>\"Powered by\", means you are directly powered by a renewable power source, say a hydro dam.</p> </li> <li> <p>In that scenario, the electrons flowing into your device can only come from that source, so you can confidently say that you are 100% powered by renewables.</p> </li> <li> <p>For most people, we live on an interconnected grid, with many producers pumping electricity in and many consumers taking electricity out.</p> </li> <li>This means the electrons coming into your device are a mixture of all the electrons going into the grid. For example, suppose the grid only has 5% of wind supply.</li> <li> <p>You are getting 5% of wind-generated electrons and 95% fossil fuel-generated electrons.</p> </li> <li> <p>You can't track individual electrons. Once the electrons from a wind farm are on a grid, they all mix with the electrons from a fossil fuel plant.</p> </li> <li>So there is no way for a consumer to insist the electrons that it uses only come from renewable sources.</li> </ul>"},{"location":"certifications/green-software-for-practitioners/07_Climate-Commitments.html#renewable-energy-certificates-rec","title":"Renewable Energy Certificates (REC)","text":"<ul> <li> <p>To solve this problem, a renewable plant sells two things. The first is its electricity, which it sells into a grid. The second is a REC, a Renewable Energy Certificate. 1 REC equals 1kWh of energy.</p> </li> <li> <p>If you want to be 100% matched by renewable energy and are on the grid, the solution is to buy enough RECs to cover the amount of electricity you consume.</p> </li> <li> <p>For instance, if you consume 100 kWh of electricity every day, then to be 100% matched by renewables, you buy 100 RECs.</p> </li> <li> <p>When organizations set 100% renewable targets purchasing RECs on the market is the solution they often employ to meet their commitments.</p> </li> </ul>"},{"location":"certifications/green-software-for-practitioners/07_Climate-Commitments.html#ppas","title":"PPAs","text":"<ul> <li>You might also hear the term PPA used alongside RECs. A PPA is a Power Purchase Agreement, which is another way to purchase RECs.</li> <li> <p>If you estimate you need 500MWh of electricity per year for a particular data center, you might sign a PPA to purchase 500MWh per year from a renewable plant. You would then get all the RECs associated with this power plant.</p> </li> <li> <p>PPAs are typically very long-term contracts. A renewable plant can find financing with one of these agreements since it already has had a buyer for its electricity for many years.</p> </li> <li> <p>PPAs encourage something called additionality. Purchasing a PPA drives the creation of new renewable plants.</p> </li> <li>PPAs are a solution that gets us towards a future where everyone has access to 100% renewable energy.</li> </ul>"},{"location":"certifications/green-software-for-practitioners/07_Climate-Commitments.html#247-hourly-matching","title":"24/7 Hourly Matching","text":"<ul> <li>When it comes to 100% renewable claims, the critical question is, what is the granularity of matching? Do you sum up and net off yearly, monthly, weekly, daily, or hourly?</li> <li>That question is essential because to truly transition to renewable energy, we need 100% of the power to come from low-carbon energy sources like renewables 100% of the time.</li> <li> <p>This fine granular matching is often called 24/7 hourly matching.</p> </li> <li> <p>24/7 hourly matching is one of the many strategies we need to employ to help accelerate the transition to a 100% renewable-powered grid. For example, Google and Microsoft have both committed to 24/7 hourly matching by 2030.</p> </li> </ul>"},{"location":"certifications/green-software-for-practitioners/07_Climate-Commitments.html#daily-vs-hourly-matching","title":"Daily vs Hourly Matching","text":"<ul> <li> <p>Imagine an organization has a demand curve like this, each blue square represents 1kWh:</p> </li> <li> <p>They have purchased RECs from a wind farm that generated electricity with a curve, so each green square represents 1 REC.</p> </li> <li>Matching by day means the organization consumed 18 kWh and bought 18 RECs.</li> <li> <p>As a result, they netted off to zero. So they can say they are 100% matched by renewable energy daily.</p> </li> <li> <p>However, if we looked at it in hourly buckets (each square here is 2 hrs in length), then it seems a bit different:</p> </li> <li> <p>The total amount of energy consumed is still 18kWh. However, there are only a few hours in the day where we are 100% matched by renewable energy for that hour.</p> </li> <li> <p>So for some hours, we have way more renewable energy than we need. Conversely, we have way less renewable energy than we require for most hours.</p> </li> <li> <p>In the above example, they are 100% matched by renewable energy on an hourly basis for only 6 hrs of the day.</p> </li> </ul>"},{"location":"certifications/green-software-for-practitioners/07_Climate-Commitments.html#carbon-free-energy","title":"Carbon-Free Energy","text":"<ul> <li> <p>The number we use to describe how successful we are at 24/7 hourly matching is the carbon-free energy percentage.</p> </li> <li> <p>Carbon-free energy is defined as the average percentage of carbon-free energy consumed in a particular location on an hourly basis.</p> </li> <li> <p>So for the previous example, if measured using daily matching, we are 100% matched with renewable energy.</p> </li> <li>However, we are only 33.1% matched if measured using hourly matching. The CFE percentage is, therefore, 33.1%</li> </ul>"},{"location":"certifications/green-software-for-practitioners/07_Climate-Commitments.html#carbon-awareness-as-part-of-a-247-hourly-matching-strategy","title":"Carbon Awareness as Part of a 24/7 Hourly Matching Strategy","text":"<ul> <li>Carbon aware computing involves responding to electrical carbon intensity signals and changing the behavior of software, so it emits less carbon.</li> <li> <p>Carbon awareness also helps an organization meet their 24/7 hourly matching target and increase its CFE percentage.</p> </li> <li> <p>One example of a behavior change is shifting compute to a time when more renewable energy is available.</p> </li> <li>For example, delaying the start of a training run of a machine learning model, or even delaying charging of a laptop, to when the carbon intensity of electricity is lower, and the supply of renewable energy is higher.</li> </ul>"},{"location":"certifications/green-software-for-practitioners/07_Climate-Commitments.html#summary","title":"Summary","text":"<ul> <li> <p>There are a number of methodologies commonly applied to help in the overall fight against climate change. These fall into the general categories of carbon elimination (also known as 'abatement'), carbon avoidance (a.k.a. 'compensating'), or carbon removal (a.k.a. 'neutralizing').</p> </li> <li> <p>Abatement includes increasing energy efficiency to eliminate some of the emissions associated with energy generation. Abatement is the most effective way to fight climate change although complete carbon elimination is not possible.</p> </li> <li> <p>Compensating includes the adoption of renewable energy sources, sustainable living practices, recycling, planting trees etc.</p> </li> <li> <p>Neutralizations refer to the removal and permanent storage of atmospheric carbon to counterbalance the effect of releasing CO2 into the atmosphere. Neutralizations tend to remove the carbon from the atmosphere in the short and medium-term.</p> </li> <li> <p>An organization can call itself Carbon Neutral when its total emissions are matched by the total of its emissions offsets through carbon reduction projects</p> </li> <li> <p>Net zero aims to eliminate emissions and only offset the residual emissions that you cannot eliminate to reach the 1.5\u00b0C target set by the Paris Climate Agreement.</p> </li> <li> <p>The SCI is carefully designed so that eliminating emissions, through energy efficiency, hardware efficiency and carbon awareness is the only way to reduce the score. Together with a separate neutralization strategy, it can form the basis of a net-zero strategy for an organization.</p> </li> <li> <p>When organizations set a target of 100% renewable power, they can either be \"matched by\" vs. \"powered by\" renewables, where \"powered by\" means the electrons flowing into your device can only come from renewable sources. This can be achieved by purchasing RECs as part of a PPA.</p> </li> <li> <p>24/7 hourly matching is one of the many strategies we need to employ to help accelerate the transition to a 100% renewable-powered grid.</p> </li> </ul>"},{"location":"certifications/terraform-associate/01_Deploying-Infrastructure.html","title":"1.0 - Deploying Infrastructure","text":""},{"location":"certifications/terraform-associate/01_Deploying-Infrastructure.html#11-ec2-instance-creation","title":"1.1 - EC2 Instance Creation","text":"<ul> <li>Terraform supports a significant number of providers, this becomes an important consideration for organisations when working with terraform.</li> <li>In the case of AWS, can make use of any of the following:</li> <li>Static credentials</li> <li>Environment variableS</li> <li>Shared credentials files</li> <li>EC2 Roles</li> <li>For this demo, using static credentials:</li> <li>AWS Console -&gt; IAM -&gt; Create User -&gt; Allow programmatic access -&gt; create user -&gt; access and secret keys shown</li> <li>Add following to first_ec2.tf file:</li> </ul> <pre><code>\nprovider \"aws\" {\n    region = \"eu-west-2\"\n}\n\nresource \"aws_instance\" \"myec2\" {\n    ami = \"ami-&lt;ami id&gt;\"\n    instance_type = \"t2.micro\n}\n</code></pre> <ul> <li> <p>The above code first configures the provider, aws, providing the region location for the vm deployment, along with the access and secret keys</p> </li> <li> <p>The code then looks to create an aws resource, in this case an ec2 instance of a particular ami and instance type:</p> </li> <li>Instance type specifies parameters such as memory and cpu</li> <li>Note: ami and instance_type are the only two required parameters for a aws_instance resource, there are optionals available</li> <li>To initialize: <code>terraform init</code></li> <li>Terraform initialises the current directory, downloading additional plugins required; which vary for each provider.</li> <li>To validate the syntax used in config files: <code>terraform validate</code></li> <li>To format: <code>terraform fmt</code></li> <li>To plan the execution of the configuration: <code>terraform plan</code></li> <li>To apply: <code>terraform apply</code></li> <li>To destroy: <code>terraform destroy</code></li> </ul>"},{"location":"certifications/terraform-associate/01_Deploying-Infrastructure.html#12-providers-and-resources","title":"1.2 - Providers and Resources","text":""},{"location":"certifications/terraform-associate/01_Deploying-Infrastructure.html#introduction","title":"Introduction","text":"<ul> <li>Terraform is capable of supporting multiple providers</li> <li>The provider details the infrastructure is to be launched on must be specified in the configuration files</li> <li>In some/most cases, authentication tokens will also be required - these should never be included in source code, referenced instead via environment variables or another appropriate method.</li> <li>When the command <code>terraform init</code> is ran, terraform downloads plugins associated with the provider to the <code>.terraform</code> directory in the project root.</li> <li>Each provider has different resources that can be created, with each resource type specific to the particular provider e.g. for AWS:</li> <li><code>aws_instance</code> - Virtual Machine/Instance</li> <li><code>aws_alp</code> - Application load balancer</li> <li><code>iam_user</code> - Identification Application Managment User</li> <li>The above resources couldn't be provisioned by e.g. Azure with this syntax, would have to use the provided syntax for Azure</li> <li>Consider the following example:</li> </ul> <pre><code>\nprovider \"digitalocean\" {\n    token = \"&lt;TOKEN&gt;\"\n}\n\nresource \"digitalocean_droplet\" \"droplet\" {\n    image = \"image id\"\n    name = \"web-1\"\n    region = \"nyc1\"\n    size = \"&lt;size-identifier&gt;\"\n}\n\n</code></pre> <ul> <li>In terms of syntax, the general format is followed, defining a provider and configuring it, followed by a resource which is configured as desired.</li> <li>Since this is for a different provider (DigitalOcean), the syntax for parameters such as the image or region names</li> <li>In terms of authentication, only one token is required for provider configuration unlike AWS.</li> </ul>"},{"location":"certifications/terraform-associate/01_Deploying-Infrastructure.html#provider-maintainers","title":"Provider Maintainers","text":"<ul> <li>There are 3 main types of provider tiers in Terraform:</li> <li>Official - Owned and maintained by HashiCorp</li> <li>Partner - Owned and maintained by a direct partner of HashiCorp</li> <li>Community - Owned and Maintained by Individual Contributors</li> </ul>"},{"location":"certifications/terraform-associate/01_Deploying-Infrastructure.html#required-providers","title":"Required Providers","text":"<ul> <li>For providers not directly maintained by HashiCorp, a <code>required_providers</code> block is required, taking DigitalOcean as an example, the configuration before would become:</li> </ul> <pre><code>terraform {\n  required_providers {\n    digitalocean = {\n      source = \"digitalocean/digitalocean\n    }\n  }\n}\n\nprovider \"digitalocean\" {\n    token = \"&lt;TOKEN&gt;\"\n}\n</code></pre> <ul> <li>Details on how to reference each provider is typically provided in the provider's documentation.</li> </ul>"},{"location":"certifications/terraform-associate/01_Deploying-Infrastructure.html#13-destroying-infrastructure","title":"1.3 - Destroying Infrastructure","text":"<ul> <li>Obviously we don't want to keep infrastructure running forever and racking up charges</li> <li>To bring down infrastructure, can run the command <code>terraform destroy</code></li> <li> <p>If you wish to destroy a specific target, add the -target flag in the format:     <code>Terraform destroy -target &lt;resource_type&gt;.&lt;resource_name&gt;</code>     e.g. <code>terraform destroy -target aws_instance.ec2</code></p> </li> <li> <p>Alternatively could comment out the resource you don't wish to be applied or destroyed, though this is not recommended in general practice</p> </li> </ul> <p>Note: to automatically destroy: <code>terraform destroy --auto-approve</code></p>"},{"location":"certifications/terraform-associate/01_Deploying-Infrastructure.html#14-terraform-state-files","title":"1.4 - Terraform State Files","text":"<ul> <li>Terraform keeps track of the infrastructure being created in a state file</li> <li>Allows terraform to map real-world resources to existing configurations</li> <li>Includes resource details including instance ID, IP addresses, tags,  etc.</li> <li>Therefore, when <code>terraform destroy</code> is ran against a a particular target, all the details relating to that resource will be removed from the state file</li> <li>State file is always named <code>terraform.tfstate</code></li> <li>If changes made outside the terraform configuration files, when terraform apply is ran again, the tfstate file will be checked to make sure that the real world configuration is set up correctly, if not it will attempt to apply the appropriate changes to return/update to the desired state.</li> <li>This may not work OR it may require recreation of resources (if the affected resource(s) have any dependencies)</li> <li>To destroy a specific resource: <code>terraform destroy -target &lt;resource_type&gt;.&lt;local_name&gt;</code></li> </ul>"},{"location":"certifications/terraform-associate/01_Deploying-Infrastructure.html#15-desired-and-current-states","title":"1.5 - Desired and Current States","text":""},{"location":"certifications/terraform-associate/01_Deploying-Infrastructure.html#151-desired-state","title":"1.5.1 - Desired State","text":"<ul> <li>The state defined within a configuration resource block e.g. in an <code>aws_instance</code> resource, if <code>instance_type = t2.micro</code>, that is reflective of the desired state - you desire an aws_instance of size <code>t2.micro</code> to be created.</li> </ul>"},{"location":"certifications/terraform-associate/01_Deploying-Infrastructure.html#152-current-state","title":"1.5.2 - Current State","text":"<ul> <li>State defined within the <code>terraform.tfstate</code> file.</li> <li>Holds configuration data regarding the resources created by Terraform.</li> <li>Used as the comparison point to determine any changes that need to be applied during <code>terraform apply</code> command execution.</li> </ul>"},{"location":"certifications/terraform-associate/01_Deploying-Infrastructure.html#153-updating-current-state","title":"1.5.3 - Updating Current State","text":"<ul> <li>The current state can be updated by running <code>terraform refresh</code></li> <li>The current state is always refreshed during a <code>plan</code> or <code>apply</code> command execution</li> <li>Terraform will always apply changes to ensure that the current state matches that of the desired state.</li> </ul>"},{"location":"certifications/terraform-associate/01_Deploying-Infrastructure.html#16-challenges-of-current-state-and-computed-values","title":"1.6 - Challenges of Current State and Computed Values","text":"<ul> <li>If not running Terraform in a system with a readily-available editor, one can run <code>terraform show</code> to view the state file contents.</li> <li>Note: Suppose a change is made outside the desired state that isn't defined as code e.g adding a security group, Terraform will not act to update the current state since it's not in the desired state.</li> <li>One would have to use <code>terraform import</code> to achieve this.</li> </ul>"},{"location":"certifications/terraform-associate/01_Deploying-Infrastructure.html#17-provider-versioning","title":"1.7 - Provider Versioning","text":"<ul> <li>Terraform providers exist to provide a link between terraform and the service provider, allowing terraform to provision infrastructure to the appropriate provider.</li> <li>Plugins for the providers are released separately from Terraform and are updated in their own time.</li> <li>In terraform init, the latest provider plugin will automatically be downloaded if the version isn't specified.</li> <li>In production, it's recommended to specify the version that you know the code works for e.g. AWS version 2.7.</li> <li>To specify the provider version, in the provider block of the configuration file, add <code>version = \"version number\"</code> as an exact number or express it as a criteria:</li> </ul> Version Argument Description &gt;=x.y Greater than or equal to version value &lt;=x.y Less than or equal to version value ~&gt;x.y Any version in the subrange of value x i.e. any version of 2.y &gt;=a.b,&lt;=c.d Any version between a.b and c.d <ul> <li>Note: Certain provider plugins aren't compatible with particular versions of Terraform - it will specify and suggest alternate versions if this occurs.</li> <li>If wanting to make an upgrade / change: <code>terraform init -upgrade</code></li> </ul>"},{"location":"certifications/terraform-associate/01_Deploying-Infrastructure.html#18-provider-versioning","title":"1.8 - Provider Versioning","text":"<ul> <li>Providers are split into 2 categories:</li> <li>Hashicorp-Distributed: Automatically downloaded during Terraform init</li> <li> <p>Third-party</p> </li> <li> <p>The need for third party providers arises whenever an official provider doesn't support a particular functionality, or when organizations have developed their own platform to run terraform on.</p> </li> <li> <p>Hashicorp distributed providers are listed under \"Major Cloud Providers\" under the HashiCorp website; third party providers are under the \"Community\" tab.</p> </li> <li> <p>When attempting to initialise with a third party provider, it's likely that an error will occur.</p> </li> <li>As mentioned, terraform init cannot automatically install the plugins for third party providers, they must be installed manually.</li> <li>The installation can be achieved by placing the plugins into the system's user plugins directory (OS-dependent).</li> </ul> OS Directory Windows <code>%APPDATA%\\terraform.d\\plugins</code> Other (e.g. Linux, MAC) <code>~/.terraform.d/plugins</code>"},{"location":"certifications/terraform-associate/02_Working-With-Configurations.html","title":"2.0 - Working with Configurations","text":""},{"location":"certifications/terraform-associate/02_Working-With-Configurations.html#21-attributes-and-output-values","title":"2.1 - Attributes and Output Values","text":"<ul> <li>Terraform can output the values of certain attributes of a resource.</li> <li>Output attributes can be used for both user reference, as well as input variables for other resources to be created.</li> <li>Example: When an elastic IP address is created, it should automatically be added to the security group for whitelisting.</li> <li>For an output, if you set a value as <code>&lt;resource_type&gt;.&lt;resource_id&gt;</code>, this will output all the attributes associated with that resource.</li> <li>For a particular output, append the attribute name to the above reference i.e. <code>&lt;resource_type&gt;.&lt;resource_id&gt;.&lt;attribute_name&gt;</code>.</li> <li> <p>Example: <code>aws_eip.lb.public_ip</code></p> </li> <li> <p>Note: A list of attributes that can be output is generally listed with each resource in the Terraform documentation.</p> </li> <li> <p>In general:</p> </li> <li>Attribute: Reference of a value associated with a particular property of a resource</li> <li> <p>Outputs: Used to output the value of a particular attribute.</p> </li> <li> <p>Example output:</p> </li> </ul> <pre><code>output \"public_ip\" {\n    value = aws_eip.lb.public_ip\n    description = \"&lt;insert description&gt;\"\n}\n</code></pre> <ul> <li>For all the attribute outputs available, omit the attribute name from the output value i.e.:</li> </ul> <pre><code>output \"public_ip\" {\n    value = aws_eip.lb\n    description = \"&lt;insert description&gt;\"\n}\n</code></pre> <ul> <li>Note: Outputs can also be used by other projects to reference specific values e.g. if project B needs to refer to the IP address output by project A - this is done by referring to the terraform state outputs of the source project.</li> </ul>"},{"location":"certifications/terraform-associate/02_Working-With-Configurations.html#22-referencing-cross-account-resource-attributes","title":"2.2 - Referencing Cross-Account Resource Attributes","text":""},{"location":"certifications/terraform-associate/02_Working-With-Configurations.html#introduction","title":"Introduction","text":"<ul> <li>As suggested previously, when creating resources, one should be able to use attributes and outputs to allow automatic configuration</li> <li>Examples follow:</li> <li>Creating an Elastic IP and assigning it to an AWS EC2 Instance.</li> <li>Creating an Elastic IP and assigning it to a Security Group for whitelisting</li> </ul>"},{"location":"certifications/terraform-associate/02_Working-With-Configurations.html#221-eip-association-to-ec2-instance","title":"2.2.1 - EIP Association to EC2 Instance","text":"<ul> <li>Associating an EIP with the EC2 instance, one requires an <code>aws_eip_association</code> resource, which will specify the following values based on other attributes:</li> <li><code>instance_id = aws_instance.instance.id</code></li> <li><code>allocation_id = aws_eip_eip.id</code></li> </ul>"},{"location":"certifications/terraform-associate/02_Working-With-Configurations.html#222-eip-association-with-security-group","title":"2.2.2 - EIP Association with Security Group","text":"<ul> <li>Defining the security group should follow a format similar to:</li> </ul> <pre><code>resource \"aws_security_group\" \"allow_tls\" {\n    name = \"allow_tls\"\n    description = \"Allow TLS inbound traffic\"\n    vpc_id = aws_vpc.main.id\n    ingress {\n        description = \"TLS from VPC\"\n        from_port = 443\n        to_port = 443\n        8\n        protocol = \"tcp\"\n        cidr_blocks = [aws_eip.myeip.public_ip]\n    }\n    egress {\n        from_port = 0\n        to_port = 0\n        protocol = \"-1\"\n        cidr_blocks = [\"0.0.0.0/0\"]\n    }\n    tags = {\n        Name = \"allow_tls\"\n    }\n}\n</code></pre>"},{"location":"certifications/terraform-associate/02_Working-With-Configurations.html#23-terraform-variables","title":"2.3 - Terraform Variables","text":"<ul> <li>When working with Terraform. there's a significant chance there will be multiple static values in the project e.g. ami, ports, commands.</li> <li>Changing these values can become tedious to alter if used multiple times.</li> <li>To avoid this, it's advised to utilise variables.</li> <li>Typically variables are supplied from a file <code>variables.tf</code> in a centralised location, stored in the format:</li> </ul> <pre><code>variable \"&lt;variable id&gt;\" {\n    default = \"&lt;value default&gt;\"\n    type = &lt;type&gt;\n    description = \"&lt;variable description&gt;\"\n}\n</code></pre> <ul> <li>To reference a variable in a configuration, add <code>var.&lt;variable_id&gt;</code> where appropriate.</li> <li> <p>Depending on the type of the variable, to adhere to data types, you may need to include it within parentheses e.g. [] for lists, or {} for maps.</p> </li> <li> <p>Once done, when <code>terraform apply</code> is ran, the variables stored in <code>variables.tf</code> are referenced.</p> </li> <li>This massively simplifies things in production for variable values subject to change, such as IP addresses.</li> </ul>"},{"location":"certifications/terraform-associate/02_Working-With-Configurations.html#24-variable-assignment","title":"2.4 - Variable Assignment","text":"<ul> <li>Variables can be assigned via 4 main methods:</li> <li>Environment Variables:<ul> <li>A fallback method in case the others do not work.</li> <li>Terraform will search its own local environment to find an environment variable to apply.</li> <li>Terraform environment variables are set by any environment variables prefixed with <code>TF_VAR_</code>.</li> <li>Example: <code>export TF_VAR_&lt;variable name&gt; &lt;variable value&gt;</code></li> </ul> </li> <li>Command-line flags:<ul> <li>Not recommended unless only altering 1 variable,</li> <li>Used by appending a flag during <code>plan</code> or <code>apply</code> commands e.g.:</li> <li><code>terraform plan -var=\"variable_id=value\"</code></li> <li>Usually used when wanting to quickly test the effects of a new variable.</li> </ul> </li> <li>From a file:<ul> <li>Recommended over command line flags</li> <li>In a new file <code>terraform.tfvars</code>, one can specify each variable's value in the form <code>variable_id = &lt;value&gt;</code></li> <li>To reference this file, append <code>-var-file=\"filename.tfvars\"</code> when running <code>plan</code> or <code>apply</code> commands.</li> <li>This will overrule any defaults set in <code>variables.tf</code></li> </ul> </li> <li> <p>Variable Defaults:</p> <ul> <li>Store variable values in <code>variables.tf</code> and specify default values, referencing via <code>var.variable_id</code>.</li> </ul> </li> <li> <p>If no additional variable values are specified, the value will be assumed to be the default value (specified in <code>variables.tf</code>)</p> </li> <li>If no default is specified, the value will need to be entered during command execution.</li> <li>Any variables entered at CLI level will take precedence over defaults and environment variables.</li> </ul>"},{"location":"certifications/terraform-associate/02_Working-With-Configurations.html#25-data-types","title":"2.5 - Data Types","text":""},{"location":"certifications/terraform-associate/02_Working-With-Configurations.html#251-example","title":"2.5.1 - Example","text":"<ul> <li>Consider a company where every employee has a particular identification number, if that employee wanted to create a form of infrastructure, it should be done with that number only.</li> <li> <p>So in <code>variables.tf</code>, the variable <code>instance_name</code> should be of type number.</p> </li> <li> <p>Suppose that in the <code>terraform.tfvars</code> the value for instance_name is set to a value that isn't of that data type/the data type also isn't specified, eg. john-123; what will happen?</p> </li> <li>This value will not be accepted and the plan will fail.</li> </ul>"},{"location":"certifications/terraform-associate/02_Working-With-Configurations.html#252-data-types-overview","title":"2.5.2 - Data Types Overview","text":"<ul> <li>To specify a variable's data type, simply add the type in the variable within <code>variables.tf</code> in the form <code>type = type</code>.</li> <li>Key types used include:</li> <li>String: A set of unicode characters representing text e.g. <code>\"hello\"</code></li> <li>List: A sequential list of values identified by position within the list, position starting with 0 e.g. <code>[\"London\", \"Paris\", \"Helsinki\"]</code></li> <li>Map: A group of values categorized by labels e.g. <code>{name = \"Joe\", age = 23}</code><ul> <li>This can contain multiple data types if desired.</li> </ul> </li> <li> <p>Number: Numerical values</p> </li> <li> <p>By defining data types in <code>variables.tf</code>, users can use this as a reference point when defining variables in the <code>.tfvars</code> file.</p> </li> <li>In some cases, if a variable type isn't specified, errors will arise as the program will assume a different type is expected.</li> </ul>"},{"location":"certifications/terraform-associate/02_Working-With-Configurations.html#26-fetching-data-from-maps-and-lists-in-variables","title":"2.6 - Fetching Data from Maps and Lists in Variables","text":"<ul> <li>When working with lists in terraform, sometimes you wish to reference a particular value from that list, rather than include all the values.</li> <li>When referencing items from a map, follow the format: <code>var.map_id[\"map_key\"]</code></li> <li>When referencing items from a list: <code>var.list_id[list_position]</code></li> <li>List positions ALWAYS start from [0].</li> </ul>"},{"location":"certifications/terraform-associate/02_Working-With-Configurations.html#27-count-and-count-index","title":"2.7 - Count and Count Index","text":"<ul> <li>The count parameter on resources can simplify configurations and allow easier scalability of configurations.</li> <li>Commonly, if wanting to create a small number of the same resource, e.g. 2 identical instances, one could define them as separate instances, however this is not sustainable.</li> <li>The Terraform function <code>count</code> can be used to save code space by adding <code>count = value</code></li> <li>In resource blocks where count is set, an additional count object is available in expressions, so each instance's configurations can still be modified.</li> <li>This object has just one attribute: <code>count.index</code>, which starts with 0 for the first instance and continues like a list index.</li> <li> <p>This is commonly used for altering properties such as the name.</p> </li> <li> <p>When wanting to utilize the count index, append <code>.count.index</code> to the chosen property.</p> </li> <li> <p>Example application to create 3 virtual machines:</p> </li> <li><code>machine_instance.0</code> -&gt; count = 1</li> <li><code>machine_instance.1</code> -&gt; count = 2</li> <li> <p><code>machine_instance_2</code> -&gt; count = 3</p> </li> <li> <p>The above isn't a common practice, usually resources are configured for different environments like staging, development, etc.</p> </li> <li>Count can still be utilized for this, but it'll reference positions in a list instead.</li> <li>This can be done in a similar manner to <code>var.&lt;variable&gt;[count_index]</code> - which will look iteratively through the list and apply each desired entry.</li> </ul>"},{"location":"certifications/terraform-associate/02_Working-With-Configurations.html#28-conditional-expressions","title":"2.8 - Conditional Expressions","text":"<ul> <li>Expressions that use booleans to select one of two values, true or false.</li> <li> <p>Defined in Terraform in a similar manner to <code>condition ? true_val : false_val</code></p> </li> <li> <p>Example: Suppose there are a set of resources that should only be created if a particular variable is set e.g. <code>use_dev_env = true</code></p> </li> <li> <p>For each resource, add an attribute property followed by the condition:     <code>attribute = var.&lt;variable name&gt; == true ? &lt;true value&gt; : false value</code></p> </li> <li> <p>In the other dependent variable:     <code>attribute = var.&lt;variable name&gt; == false ? &lt;true_value&gt; : &lt;false_value&gt;</code></p> </li> <li> <p>The boolean variable is defined with a default value in <code>terraform.tfvars</code> and <code>variables.tf</code></p> </li> </ul>"},{"location":"certifications/terraform-associate/02_Working-With-Configurations.html#29-local-values","title":"2.9 - Local Values","text":"<ul> <li> <p>A local value assigns a a name to an expression, allowing it to be used multiple times within a module without repeating it.</p> </li> <li> <p>To define local tags, add <code>locals {}</code> and then the desired values in the required format.</p> </li> <li>Locals can be categorized and referenced accordingly e.g. <code>development = {}</code>, <code>production = {}</code></li> <li>To reference a category, add <code>local.&lt;category_name&gt;.&lt;category value</code></li> <li>An example follows:</li> </ul> <pre><code>locals {\n    common_tags {\n        Owner = \"DevOps Team\"\n        service = \"backend\n    }\n}\n</code></pre> <ul> <li>A common example for using locals is for non-sensitive defaults and conditionals, like resource name prefixes:</li> <li> <p><code>name_prefix = var.name !=\"\" ? var.name : local.name</code></p> </li> <li> <p>The above example defines a naming convention. If <code>var.name</code> is blank, then the prefix defined in <code>local.name</code> is used.</p> </li> <li> <p>Locals can be helpful to avoid repeating the same values or expressions multiple times.</p> </li> <li> <p>They should be used in moderation, as they can make a configuration hard to read by future users of the files, they should only be used in situations where a single value or result is used in many places and said value is likely to be changed.</p> </li> </ul>"},{"location":"certifications/terraform-associate/02_Working-With-Configurations.html#210-functions","title":"2.10 - Functions","text":"<ul> <li>Terraform has many built-in functions that can be used to transform and combine values.</li> <li>The general syntax for a function is the function name followed by arguments separated by commas.</li> <li>User-defined functions aren't supported, but built-in function categories are:</li> <li>Numeric</li> <li>String</li> <li>Collection</li> <li>Encoding</li> <li>Filesystem</li> <li>Date and time</li> <li>Hash and Crypto</li> <li>IP Network</li> <li> <p>Type connection</p> </li> <li> <p>Further details for each is provided in the Terraform documentation.</p> </li> <li> <p>A popular function is <code>lookup</code>, which can be used to look up the value of a single element from a map given its key. If the key doesn't exist, a default value will be used.</p> </li> <li> <p>Example: <code>lookup(map, key, default)</code></p> </li> <li> <p>Another function is <code>element</code>, which retrieves a single element from a list, example usage: <code>element(list, index)</code> - <code>count.index</code> is often used here.</p> </li> <li> <p>File\" <code>file(\"/path/to/file\")</code> reads the contents of the file defined in quotation marks, commonly used for ssh keys, etc.</p> </li> <li> <p><code>formatdate</code> &amp; <code>timestamp</code> are often used in conjunction to format the value returned by <code>timestamp</code> into a more readable manner</p> </li> </ul>"},{"location":"certifications/terraform-associate/02_Working-With-Configurations.html#211-data-sources","title":"2.11 - Data Sources","text":"<ul> <li>Data sources allow data to be fetched or computed for use elsewhere within Terraform configuration.</li> <li>As an example, if an AWS EC2 Instance was to be configured, the desired AMI will differ depending on the region.</li> <li>Rather than manually hardocde the AMI, a data source can be used to filter the appropriate AMIs for a given region.</li> <li> <p>Data source code is defined under a <code>data</code> block and reads from a specific data source, exporting it to the data block identifier.</p> </li> <li> <p>Example:</p> </li> </ul> <pre><code>data \"aws_ami\" \"app_ami\" {\n    most_recent = true\n    owners = [\"amazon]\n    filter {\n        name = \"name\"\n        values = [\"amazn2-ami-hvm*\"]\n    }\n}\n</code></pre> <ul> <li>Now when <code>terraform plan</code> is applied, the data source block will automatically search for the latest iteration for the Amazon Linux 2 AMI for the chosen region. This can be altered for different owners, ami values, etc.</li> </ul>"},{"location":"certifications/terraform-associate/02_Working-With-Configurations.html#212-terraform-debugging","title":"2.12 - Terraform Debugging","text":"<ul> <li>Terraform tracks all changes in a series of logs, which can be enabled by setting the environment variable <code>TF_LOG</code>.</li> <li>Accepted values are:<ul> <li>TRACE</li> <li>DEBUG</li> <li>INFO</li> <li>WARN</li> <li>ERROR</li> </ul> </li> <li>Set <code>TF_LOG</code> via <code>export TF_LOG=&lt;value&gt;</code></li> <li> <p>To save logs, set <code>TF_LOG_PATH=/path/to/log/file</code></p> </li> <li> <p>Now when all commands are ran, the logs are pushed to the path set in <code>TF_LOG_PATH</code></p> </li> <li><code>TRACE</code> is the most extensive overview and the default setting for <code>TF_LOG</code>, the logs increase in verbosity in the order of the list above.</li> </ul>"},{"location":"certifications/terraform-associate/02_Working-With-Configurations.html#213-terraform-format","title":"2.13 - Terraform Format","text":"<ul> <li> <p>When working with Terraform, readability is very important. Good practice for example is to ensure that all equals signs are aligned.</p> </li> <li> <p>To format in this manner, run <code>terraform fmt</code>.</p> </li> <li>This will automatically apply any indentations and alignments needed to make the configuration valid.</li> </ul>"},{"location":"certifications/terraform-associate/02_Working-With-Configurations.html#214-validate-config-files","title":"2.14 - Validate Config Files","text":"<ul> <li> <p>Prior to running <code>terraform plan</code>, it's important to ensure configuration files are syntactically correct.</p> </li> <li> <p>Otherwise, when <code>plan</code> and <code>apply</code> are ran, errors may occur which can derail things.</p> </li> <li> <p>To validate, run <code>terraform validate</code></p> </li> <li> <p>This checks the syntax for the configuration files ensuring there are no incorrect attributes, all variables are declared, etc.</p> </li> </ul>"},{"location":"certifications/terraform-associate/02_Working-With-Configurations.html#415-load-order-and-semantics","title":"4.15 - Load Order and Semantics","text":"<ul> <li>Generally, Terraform will load all the configuration files within the specific directory in alphabetical order, so long as the files end in <code>.tf</code>.</li> <li>In general practice, code should be split into multiple files. For example, a file for providers, a file for all networking resources, etc.</li> <li>This allows for easier management of infrastructure.</li> <li>Note: When adding 2 of the same resource, you must give different IDs after defining the resource type.</li> </ul>"},{"location":"certifications/terraform-associate/02_Working-With-Configurations.html#216-dynamic-blocks","title":"2.16 - Dynamic Blocks","text":"<ul> <li>Often there are repeatable nested blocks of resource code that need to be defined.</li> <li>If not managed carefully, this could lead to long stretches of code that are difficult to manage. Commonly, this occurs with resources that have multiple entries e.g.:</li> <li>Security Groups</li> <li>Ingress Rules</li> <li> <p>Egress Rules</p> </li> <li> <p>To work with this, one can use a dynamic block, indicated by the usage of <code>dynamic</code> prior to the resource identifier.</p> </li> <li> <p>Dynamic blocks allow you to iteratively add content defined in a separate variable list or map.</p> </li> <li> <p>Ingress Property Example:</p> </li> </ul> <pre><code>\nresource \"aws_security_group\" \"dynamicsg\" {\n    name = \"dynamic-sg\"\n    description = \"Ingress for Vault\"\n\n    dynamic \"ingress\" {\n        for_each = var.sg_ports\n        iterator = port\n        content {\n            from_port = port.value\n            to_port = port.value\n            protocol = \"tcp\"\n            cidr_blocks = [\"0.0.0.0/0\"]\n        }\n    }\n\n    dynamic \"egress\" {\n        for_each = var.sg_ports\n        content {\n            from_port = egress.value\n            to_port = egress.value\n            protocol = \"tcp\"\n            cidr_blocks = [\"0.0.0.0/0\"]\n        }\n    }\n}\n\n</code></pre> <ul> <li> <p>This dynamic block will iteratively fill out the contents of the ingress and egress blocks with the content defined in a separate variable as a list. The egress block is filled out in a similar manner.</p> </li> <li> <p>Iterators are optional arguments which set the name of a temporary variable that represents the current element of a more complex value. It's commonly seen in conjunction with the <code>for_each</code> operator i.e. <code>for_each = var.sg_ports</code> - where <code>sg_ports</code> is a list to be iterated over.</p> </li> <li> <p>If omitted, the name of the variable defaults to the dynamic block's label (similar to the egress example above).</p> </li> </ul>"},{"location":"certifications/terraform-associate/02_Working-With-Configurations.html#217-tainting-resources","title":"2.17 - Tainting Resources","text":"<ul> <li>Consider a scenario where a new resource has been created, but users have made a lot of manual changes to it in terms of both infrastructure and within the server.</li> <li>To deal with this, one can either import the changes to Terraform or delete and recreate the resource to update the configuration.</li> <li>The command <code>terraform taint</code> manually marks a resource as \"tainted\", forcing the resource to be destroyed and recreated during the next <code>terraform apply</code> execution.</li> <li> <p>To taint, use the command similar to: <code>terraform taint &lt;resource type&gt;.&lt;resource_id&gt;</code></p> </li> <li> <p>Newer approach (from version ~0.15 onwards) is to utilize the <code>-replace</code> flag i.e. <code>terraform apply -replace=\"&lt;resource type&gt;.&lt;resource id&gt;\"</code>, this cuts out the middle-man step.</p> </li> </ul>"},{"location":"certifications/terraform-associate/02_Working-With-Configurations.html#218-splat-expressions","title":"2.18 - Splat Expressions","text":"<ul> <li>An expression that produces a list of all the attributes, denoted by *.</li> <li>Essentially denotes \"anything\" or \"all\".</li> <li>Example:</li> </ul> <pre><code>provider \"aws\" {\n    region = \"eu-west-2\"\n}\n\nresource \"aws_iam_user\" \"lb\" {\n    name  \"iamuser.${count.index}\"\n    count = 3\n    path = \"/system/\"\n}\n\noutput \"arns\" {\n    value = aws_iam_user.lb[*].arn\n}\n</code></pre> <ul> <li>The above aims to create 3 IAM users in AWS.</li> <li>The value <code>aws_iam_user.lb[*].arn</code> will look for each of the 3 arns associated with the IAM user.</li> <li>The resultant output will therefore be:</li> </ul> <pre><code>arns = [\n\"arn:aws:iam::746085785702:user/system/iamuser.0\",\n\"arn:aws:iam::746085785702:user/system/iamuser.1\",\n\"arn:aws:iam::746085785702:user/system/iamuser.2\"\n]\n</code></pre> <ul> <li>This could also be applied to any other listable properties.</li> <li>Officially: A splat expression provides a more concise way to express a common operation that could otherwise be performed by a <code>for</code> operation</li> </ul>"},{"location":"certifications/terraform-associate/02_Working-With-Configurations.html#219-terraform-graph","title":"2.19 - Terraform Graph","text":"<ul> <li>A command used to generate a visual representation of a configuration or execution plan.</li> <li>Expressed in the DOT format, which can be converted to an image format.</li> <li>Usage: <code>terraform graph &gt; filename.dot</code></li> <li>For conversion, one can utilize a graph visualization package like Graphviz.</li> </ul>"},{"location":"certifications/terraform-associate/02_Working-With-Configurations.html#220-saving-terraform-plan-to-a-file","title":"2.20 - Saving Terraform Plan to a File","text":"<ul> <li>When generating a Terraform plan, it can be beneficial to save it to a particular path.</li> <li>By doing so, this plan can be viewed later or can be used toa pply changes specified only by that plan.</li> <li>To save a plan: <code>terraform plan -out /path/to/file</code></li> <li>To apply a saved plan: <code>terraform apply /path/to/file</code></li> </ul>"},{"location":"certifications/terraform-associate/02_Working-With-Configurations.html#221-terraform-output","title":"2.21 - Terraform Output","text":"<ul> <li>Used to extract the value of an output variable from the state file.</li> <li>Configured by <code>terraform output &lt;state file attribute&gt;</code></li> <li>Advised to use for verification and debugging purposes. Particular outputs can be manually added in the .tf configs for ease e.g. Load Balancer DNS.</li> </ul>"},{"location":"certifications/terraform-associate/02_Working-With-Configurations.html#222-terraform-settings","title":"2.22 - Terraform Settings","text":"<ul> <li>The <code>terraform {}</code> block is used to configure the behavior of Terraform itself when acting upon the configuration defined.</li> <li>Common settings include:</li> <li><code>required_version</code> - string criteria to determine the minimum / acceptable versions of Terraform that can be used with the configuration</li> <li> <p><code>required_providers {}</code> - Specifies all providers required by the current module, mapping each to a specific source and assigning version constraints.</p> </li> <li> <p>Example:</p> </li> </ul> <pre><code>terraform {\n  required_Version = \"&gt; &lt;major&gt;.&lt;minor&gt;.&lt;patch&gt;\"\n\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"4.52.0\"\n    }\n  }\n}\n\n&lt;AWS Provider configuration&gt;\n</code></pre>"},{"location":"certifications/terraform-associate/02_Working-With-Configurations.html#223-challenges-with-large-infrastructure","title":"2.23 - Challenges with Large Infrastructure","text":"<ul> <li>When dealing with significantly large configurations, API limits for a provider may be incurred.</li> <li>This occurs as when Terraform plan runs, the state is refreshed for each resource defined - this can take significantly long for large amounts of infrastructure.</li> <li> <p>To work around this, it's advised to break resources up into separate configuration files, this can be on per-resource type, per function, per component, etc.</p> </li> <li> <p>If still facing issues post-breaking configuration down, one can stop Terraform from querying the current state by adding the <code>-refresh=false</code> flag.</p> </li> <li>Alternatively, Terraform commands can be used to target specific resources e.g. <code>-target=&lt;resource type&gt;.&lt;resource name&gt;</code> or <code>-target=&lt;resource type&gt;</code> for all instances of a particular resource.</li> <li>This is NOT a recommended approach for production!</li> </ul>"},{"location":"certifications/terraform-associate/02_Working-With-Configurations.html#224-zipmap-function","title":"2.24 - ZipMap Function","text":"<ul> <li>Creates a map from a list of keys and values e.g.:</li> </ul> <pre><code>zipmap([\"a\", \"b\", \"c\"], [\"1\", \"2\", \"3\"])\n\n|\nv\n\n{\n    \"a\" = \"1\",\n    \"b\" = \"2\",\n    \"c\" = \"3\"\n}\n</code></pre>"},{"location":"certifications/terraform-associate/02_Working-With-Configurations.html#225-comments-in-terraform","title":"2.25 - Comments in Terraform","text":"<ul> <li>Terraform supports multiple ways of writing comments:</li> <li>Single-line: <code>#</code> or <code>//</code></li> <li>Multi-line: <code>/*</code> and <code>*/</code></li> </ul>"},{"location":"certifications/terraform-associate/02_Working-With-Configurations.html#226-resource-behavior-and-meta-arguments","title":"2.26 - Resource Behavior and Meta Arguments","text":""},{"location":"certifications/terraform-associate/02_Working-With-Configurations.html#227-meta-arguments-lifecycle","title":"2.27 - Meta Arguments: Lifecycle","text":""},{"location":"certifications/terraform-associate/02_Working-With-Configurations.html#228-challenges-with-count","title":"2.28 - Challenges with Count","text":""},{"location":"certifications/terraform-associate/02_Working-With-Configurations.html#229-set-data-type","title":"2.29 - Set Data Type","text":""},{"location":"certifications/terraform-associate/02_Working-With-Configurations.html#230-for_each","title":"2.30 - For_Each","text":""},{"location":"certifications/terraform-associate/03_Provisioners.html","title":"3.0 - Provisioners","text":""},{"location":"certifications/terraform-associate/03_Provisioners.html#31-introduction-to-provisioners","title":"3.1 - Introduction to Provisioners","text":"<ul> <li>Provisioners are used to execute scripts or commands locally or on a remote instance during resource creation. A classic example is installing NGINX on a web-server.</li> </ul> <pre><code>provisioner \"remote-exec\" {\n    inline = [\n        \"sudo amazon-linux-extras install -y nginx1.12\",\n        \"sudo systenmctl start nginx\"\n    ]\n\n    connection {\n        type = \"ssh\"\n        user = \"ec2-user\"\n        private_key = file(\"~/path/to/key\")\n        host = self.public_ip\n    }\n}\n</code></pre> <ul> <li>When defining the <code>remote-exec</code> provisioner, one must define the inline commands to be run as well as the method of connection (with associated parameters).</li> </ul>"},{"location":"certifications/terraform-associate/03_Provisioners.html#32-provisioner-types","title":"3.2 - Provisioner Types","text":"<ul> <li>There are 2 main types of provisioner:</li> <li>Local-Exec:<ul> <li>Allows invocation of local executables after the resource is created</li> <li>Commands defined run on the machine where terraform's being run on e.g. \"runner VM\", local machine, etc.</li> </ul> </li> <li>Remote-Exec:<ul> <li>Commands executed directly on the remote machine / resource.</li> <li>Connection made typically via SSH, defined in Terraform code.</li> </ul> </li> </ul>"},{"location":"certifications/terraform-associate/03_Provisioners.html#33-remote-exec-implementation","title":"3.3 - Remote-Exec Implementation","text":"<ul> <li>For remote-exec to run, the resource must be created first i.e. the provisioner must be added within a resource block.</li> <li>Requires 2 properties to be defined:</li> <li>Inline: The commands to be ran</li> <li>Connection: Connection parameters for the desired method.</li> </ul> <pre><code>resource \"aws_instance\" \"myec2\" {\n  ami           = \"ami-0a13d44dccf1f5cf6\"\n  instance_type = \"t2.micro\"\n  key_name      = \"remote-exec-keypair\"\n\n  #configure provisioner with inline commands\n  provisioner \"remote-exec\" {\n    inline = [\n      \"sudo amazon-linux-extras install -y nginx1.12\", #install nginx\n      \"sudo systemctl start nginx\"\n    ]\n\n    connection {\n      #connection method\n      type = \"ssh\"\n      user = \"ec2-user\"\n      #private key for authentication\n      private_key = file(\"./remote-exec-keypair.pem\")\n      host        = self.public_ip\n      }\n  }\n}\n</code></pre> <ul> <li>In the <code>inline</code> block, any commands to be run in the machine are added, separated by a comma - these must be syntactically correct.</li> <li>Under connection, specify the parameters required for the desired connection method e.g. for SSH, one needs the user, private key, and host IP.</li> </ul>"},{"location":"certifications/terraform-associate/03_Provisioners.html#34-local-exec-implementation","title":"3.4 - Local-Exec Implementation","text":"<ul> <li>Allows commands to be run on the local machine immediately after resource creation.</li> <li>Typically, this is in the form of triggering Ansible playbooks.</li> <li>Define the commands required in a similar manner to that of remote-exec, but remove any notion of connections.</li> </ul>"},{"location":"certifications/terraform-associate/04_Modules-and-Workspaces.html","title":"4.0 - Modules and Workspaces","text":""},{"location":"certifications/terraform-associate/04_Modules-and-Workspaces.html#41-dry-principle","title":"4.1 - DRY Principle","text":"<ul> <li>Focused on the aspect of software engineering - Don't Repeat Yourself</li> <li>This aims to reduce the repetition of software patterns, such as defining functions for specific tasks where possible.</li> <li> <p>This can also be applied to Terraform, one can define resources as modules to be called repeatably.</p> </li> <li> <p>Modules are typically defined in the following manner:</p> </li> </ul> <pre><code>module \"&lt;module name&gt;\" {\n    source = \"/path/to/module\"\n\n    input_var = &lt;value&gt;\n    ....\n    ....\n}\n</code></pre> <ul> <li>When Terraform is ran, it will check the defined source path to determine the configuration details.</li> </ul>"},{"location":"certifications/terraform-associate/04_Modules-and-Workspaces.html#42-module-implementation-ec2-instance","title":"4.2 - Module Implementation: EC2 Instance","text":"<ul> <li> <p>Adopting the following Architecture:</p> </li> <li> <p>Root</p> </li> <li>Modules</li> <li> <p>Projects</p> <ul> <li>A</li> <li>B</li> </ul> </li> <li> <p>A sample EC2 Module can be created by adding the following to a file under <code>modules</code></p> </li> </ul> <pre><code>resource \"aws_instance\" \"myec2\" {\n  ami           = \"ami-0a13d44dccf1f5cf6\"\n  instance_type = \"t2.micro\"\n  key_name      = \"remote-exec-keypair\"\n  #configure provisioner with inline commands\n  provisioner \"remote-exec\" {\n    inline = [\n      \"sudo amazon-linux-extras install -y nginx1.12\", #install nginx\n      \"sudo systemctl start nginx\"\n    ]\n    connection {\n      #connection method\n      type = \"ssh\"\n      user = \"ec2-user\"\n      #private key for authentication\n      private_key = file(\"./remote-exec-keypair.pem\")\n      host        = self.public_ip\n    }\n  }\n}\n</code></pre> <ul> <li>This can then be referenced by any terraform files in the projects folder, which will pull all the required provider plugins alongside it.</li> <li>Using modules makes things significantly easier for management and readability, as all configuration is stored and therefore managed in 1 place.</li> <li>Similarly, any users unfamiliar to Terraform will not be overwhelmed when using the module, as they would only need to reference the module and add any required input variables.</li> </ul>"},{"location":"certifications/terraform-associate/04_Modules-and-Workspaces.html#43-variables-and-terraform-modules","title":"4.3 - Variables and Terraform Modules","text":"<ul> <li>A common challenge with infrastructure management is the need to build environments, such as dev, staging, and production, with generally similar setups, but with slightly different variables.</li> <li>Module variables cannot be overridden, if you wish to change the values of a property, you can create a variables.tf file to reference in the folder</li> <li>Now, if a value is hardcoded in the source module, the variable value can be overwrote by any configuration referencing said module.</li> </ul> <pre><code>module \"&lt;module name&gt;\" {\n    source = \"/path/to/module\"\n\n    input_var = &lt;value&gt;\n    ....\n    ....\n}\n</code></pre>"},{"location":"certifications/terraform-associate/04_Modules-and-Workspaces.html#44-terraform-registry","title":"4.4 - Terraform Registry","text":"<ul> <li>A repository of modules written by the Terraform community.</li> <li>Modules are typically developed by third parties, however some community-submitted providers are also available.</li> <li>Hashicorp review modules for verification, and verified modules are actively maintained to remain up to date with Terraform and the respective providers.</li> <li>In the Terraform registry, modules are typically supported with:</li> <li>Usage examples</li> <li>Documentation and Variable Usage Guidance</li> <li>Using a module from Terraform Registry is similar to a standard module stored locally, just reference the source appropriately and specify any other parameters required.</li> <li><code>terraform init</code> will then reference this configuration and pull down the required files.</li> </ul>"},{"location":"certifications/terraform-associate/04_Modules-and-Workspaces.html#45-terraform-workspace","title":"4.5 - Terraform Workspace","text":"<ul> <li>Workspace = Groupings of applications.</li> <li>Terraform allows the use of multiple workspaces. Each workspace can use a set of environment variables.</li> <li> <p>This is often useful for running multiple environments on the same machine.</p> </li> <li> <p>To utilize workspaces:</p> </li> </ul> Command Definition <code>terraform workspace list</code> List the existing workspaces  Current workspace denoted by * <code>terraform workspace select &lt;workspace_name&gt;</code> Switch to a pre-existing workspace <code>terraform workspace new &lt;workspace_name&gt;</code> Create a new workspace (automatically switches) <code>terraform workspace delete &lt;workspace_name&gt;</code> Delete a particular workspace  Use <code>-force</code> to force deletion if not an empty workspace. <code>terraform workspace show</code> Show current workspace details"},{"location":"certifications/terraform-associate/04_Modules-and-Workspaces.html#46-implementing-terraform-workspace","title":"4.6 - Implementing Terraform Workspace","text":"<ul> <li>Considering a scenario for a set of workspaces, with a variable that's subject to change from workspace-to-workspace.</li> <li>Rather than having separate variables files, which would be hard to track and maintain, one can use a standalone variables file in a similar manner to the following:</li> </ul> <pre><code>variable \"instance_type\" {\n    type = \"map\"\n    default = {\n        workspace1 = value1\n        workspace2 = value2\n        workspace3 = value3\n    }\n}\n</code></pre> <ul> <li> <p>Using variables like this is achieved by: <code>lookup(var.&lt;var name&gt;, terraform.workspace)</code></p> </li> <li> <p>Each workspace's state file is stored in a new folder, <code>terraform.tfstate.d</code>. The default workspace's state file remains in the standard location.</p> </li> </ul>"},{"location":"certifications/terraform-associate/05_Remote-State-Management.html","title":"5.0 - Remote State Management","text":""},{"location":"certifications/terraform-associate/05_Remote-State-Management.html#51-integrating-with-git-for-team-management","title":"5.1 - Integrating with Git for Team Management","text":"<ul> <li>For personal projects and testing, local terraform state will suffice.</li> <li>When working on client projects and in a team, it's likelier and easier to use a remote state; storing it in a backend within the cloud.</li> <li>Source code should always be stored in a central Git repository (Without exposing any credentials!).</li> </ul>"},{"location":"certifications/terraform-associate/05_Remote-State-Management.html#52-security-challenges","title":"5.2 - Security Challenges","text":"<ul> <li>For security purposes, sensitive information such as usernames and passwords should not be stored in an online repository.</li> <li>For sensitive information, one could store them in separate files outside the repository to be referenced by the <code>file</code> function, however this still commits the values to state; so not that secure.</li> </ul>"},{"location":"certifications/terraform-associate/05_Remote-State-Management.html#53-remote-state-management","title":"5.3 - Remote State Management","text":"<ul> <li> <p>A feature of Terraform that allows you to store tfstate files in a central repository that isn't easily accessible like Git.</p> </li> <li> <p>Typically, many cloud providers offer solutions for this, such as using an S3 bucket in AWS.</p> </li> <li>Generally, Terraform supports 2 types of remote backends:</li> <li>Standard: Allows state storage and locking</li> <li>Enhanced: All features of standard backends, with the addition of remote management.</li> </ul>"},{"location":"certifications/terraform-associate/05_Remote-State-Management.html#54-implementing-s3-backend","title":"5.4 - Implementing S3 Backend","text":"<ul> <li>Terraform has multiple backends supported for remote storage.</li> <li>For AWS, the standard practice is to use an S3 bucket.</li> <li> <p>Unless a remote backend is specified, the TFstate file will be stored locally by default.</p> </li> <li> <p>Remote backends can be implemented in a similar format to:</p> </li> </ul> <pre><code>terraform {\n    backend \"s3\" {\n        bucket = \"bucket name\"\n        key    = \"tfstate_filename.tfstate\"\n        region = \"&lt;region&gt;\"\n    }\n}\n\n</code></pre> <ul> <li>Note: The bucket has to be created manually in AWS prior to remote state creation.</li> </ul>"},{"location":"certifications/terraform-associate/05_Remote-State-Management.html#55-state-file-locking","title":"5.5 - State File Locking","text":"<ul> <li>During any operation that affects the state file, such as an <code>apply</code> command, the state file is locked by Terraform.</li> <li>This is hugely important as if someone edited the state file during application, the state will become corrupted.</li> <li>State file locking automatically occurs when working with Terraform in the CLI and state file if stored locally.</li> <li>For remote backends, other methods are required for state locking.</li> </ul>"},{"location":"certifications/terraform-associate/05_Remote-State-Management.html#56-integrating-dynamodb-with-s3-for-state-locking","title":"5.6 - Integrating DynamoDB with S3 for State Locking","text":"<ul> <li>To implement state locking on a TFState file stored in an S3 bucket, one can utilize a DynamoDB table.</li> <li>This DB, like the S3 bucket for remote stage storage, must be created manually.</li> <li>Once created, it can be referenced in Terraform in  a similar manner to the following:</li> </ul> <pre><code>resource \"aws_dynamodb_table\" \"terraform_state_lock\" {\n    name = \"terraform-lock\"\n    read_capacity = 5\n    write_capacity = 5\n    hash_key = \"LockID\"\n    attribute {\n    name = \"LockID\"\n    type = \"S\"\n    }\n}\n</code></pre>"},{"location":"certifications/terraform-associate/05_Remote-State-Management.html#57-terraform-state-management","title":"5.7 - Terraform State Management","text":"<ul> <li>It's good practice to use the <code>terraform state</code> command to make any state file modifications, rather than editing the state directly.</li> <li>State commands:</li> </ul> State Command Description <code>list</code> List resources in the state <code>mv</code> Move an item in the state <code>pull</code> Pull current state and output to a standard format <code>push</code> Update a remote state from a local state file <code>replace-provider</code> Replace a provider in the state <code>rm</code> Remove instances from the state <code>show</code> Show a resource in the state <ul> <li>The <code>mv</code> command is primarily used when you want to rename an existing resource without destroying and recreating it.</li> <li>The command will output a backup copy of the state prior to saving any changes.</li> <li>The mv command syntax generally follows:</li> <li><code>terraform state mv [options] /path/to/src</code></li> <li> <p><code>terraform state mv &lt;name1&gt; &lt;name2&gt;</code></p> </li> <li> <p>The <code>pull</code> command is used to manually download and output the state from a remote state.</p> </li> <li> <p>This is useful for reading values out of the state file and pairing it with other commands.</p> </li> <li> <p>The <code>push</code> command, though rarely used, can manually upload a local state file to a remote state.</p> </li> <li> <p>The <code>rm</code> command can be used to remove items from the state - this will not destroy the items, just stops Terraform from managing them.</p> </li> <li> <p><code>show</code> shows all the attributes for all the resources. For a particular resource, use <code>terraform state show  &lt;resource type&gt;.&lt;resource id&gt;</code></p> </li> </ul>"},{"location":"certifications/terraform-associate/05_Remote-State-Management.html#58-importing-existing-resources-with-terraform-import","title":"5.8 - Importing Existing Resources with Terraform Import","text":"<ul> <li>If a resource has been created via manual methods, but you wish to manage it via Terraform, it must be imported.</li> <li>To import a resource, a  corresponding resource block with particular parameters must be defined.</li> <li> <p>This resource block is defined as normal, but must contain values specific to the resource e.g. name must be correct.</p> </li> <li> <p>To import, run <code>terraform import &lt;resource type&gt;.&lt;resource_identifier&gt;</code></p> </li> <li> <p>This will look up the resource based on the provided configuration and save the mapping to the Terraform state.</p> </li> <li> <p>This can be verified by <code>terraform state</code> commands.</p> </li> <li> <p>Once verified, any changes to the resource will now have to be made via Terraform.</p> </li> </ul>"},{"location":"certifications/terraform-associate/06_Security.html","title":"6.0 - Security","text":""},{"location":"certifications/terraform-associate/06_Security.html#61-handling-access-and-secret-keys","title":"6.1 - Handling Access and Secret Keys","text":"<ul> <li>Any credentials should NEVER be stored in a <code>.tf</code> file or associated project.</li> <li>They should always be stored as secrets or environment variables.</li> <li>For AWS, this could be achieved by running <code>aws configure</code> upon downloading the AWS CLI.</li> <li>Similar operations available for Azure and GCP e.g. <code>azure login</code></li> </ul>"},{"location":"certifications/terraform-associate/06_Security.html#62-provider-use-case-multiple-regions","title":"6.2 - Provider Use-Case: Multiple Regions","text":"<ul> <li>Deploying to multiple regions can often be a challenge, as it may require multiple sets of authentication keys and regions.</li> <li>Using AWS as an example, if the <code>region</code> parameter is removed from <code>providers.tf</code>, then users will be asked to enter it and runtime.</li> <li> <p>To support multi-region deployment, provider aliases can be utilised. This is essentially adding an ID to each instance of a provider.</p> </li> <li> <p>Example usage:</p> </li> </ul> <pre><code>provider \"aws\" {\n    region = \"us-west-1\"\n}\n\nprovider \"aws\" {\n    alias = \"aws02\"\n    region = \"ap-south-1\"\n    profile = \"account02\"\n}\n</code></pre> <ul> <li>If one was to deploy resources to the region of ap-south-1 rather than the default us-west-1, one can specify the provider to be used via the alias.</li> <li>Usage example: <code>provider = &lt;provider_type&gt;.&lt;provider_alias&gt;</code></li> <li>Example from above: <code>provider = aws.aws02</code></li> </ul>"},{"location":"certifications/terraform-associate/06_Security.html#63-handling-multiple-aws-profiles-with-providers","title":"6.3 - Handling Multiple AWS Profiles with Providers","text":"<ul> <li>When considering resource deployment to multiple accounts, in particular with AWS, one must consider the credentials file at <code>~/.aws/credentials</code></li> <li>This file can store credentials for multiple accounts, in a format similar to:</li> </ul> <pre><code>[account01]\naws_access_key_id = ACCESS_KEY\naws_secret_access_key = SECRET_KEY\n\n[account02]\naws_access_key_id = ACCESS_KEY\naws_secret_access_key = SECRET_KEY\n</code></pre> <ul> <li>To utilise the credentials from a specific account, in the desired provider, add <code>profile = \"&lt;account name&gt;\"</code></li> </ul>"},{"location":"certifications/terraform-associate/06_Security.html#64-terraform-and-assume-role-with-aws-sts","title":"6.4 - Terraform and Assume-Role with AWS STS","text":"<ul> <li>When working with multiple accounts and credentials, it's advised to make use of the assume-role functionality of the AWS Security Token Service (STS).</li> <li>This is a web service that enables the request of temporary, limited-privilege credentials for AWS Identity and Access Management (IAM) users or for users that you authenticate (federated users)</li> <li>By doing so, one can keep a single set of usernames and passwords, along with authentication keys on an identity account. Each account underneath can then be accessed using assume role functionality.</li> <li>Assume-Role can be allowed for IAM roles via a policy similar to the following:</li> </ul> <pre><code>{\n    \"Version\": \"YYYY-MM-DD\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"sts:AssumeRole\",\n            \"Resource\": \"arn:aws:iam:&lt;ID&gt;:role/&lt;rolename&gt;|\n        }\n    ]\n}\n</code></pre> <ul> <li> <p>To provision resources via an account with assume-role privileges:     <code>aws sts assume-role --role-arn &lt;arn url&gt; --role-session-name &lt;session_name&gt;</code></p> </li> <li> <p>Then, one needs to add particular values to the provider.tf file to specify the role being used. Within the provider block add:</p> </li> </ul> <pre><code>assume_role {\n    role_arn = \"&lt;role arn&gt;\"\n    session_name = \"&lt;session_name&gt;\"\n}\n</code></pre> <ul> <li>Role ARNs are resource-specific, session names can be chosen to the user's desire.</li> </ul>"},{"location":"certifications/terraform-associate/06_Security.html#65-the-sensitive-parameter","title":"6.5 - The Sensitive Parameter","text":"<ul> <li>When managing large sets of infrastructure in Terraform, it's highly likely it will involve the use of sensitive information such as credentials.</li> <li>This should never be output directly in plaintext for security reasons.</li> <li>To avoid, use the <code>sensitive</code> parameter, refer to the following example:</li> </ul> <pre><code>output \"db_password\" {\n    value = aws_db_instance.db.password\n    description = \"Password for Database Login\"\n    sensitive = true\n}\n</code></pre> <ul> <li>Setting the sensitive parameter to true prevents it being output in the CLI, but it will still be stored in the state, so it's better to utilise a secrets manager for this.</li> </ul>"},{"location":"certifications/terraform-associate/07_Terraform-Cloud-and-Enterprise.html","title":"7.0 - Terraform Cloud and Enterprise","text":""},{"location":"certifications/terraform-associate/07_Terraform-Cloud-and-Enterprise.html#71-terraform-cloud-overview","title":"7.1 - Terraform Cloud Overview","text":"<ul> <li>Terraform Cloud manages Terraform runs in a consistent and reliable environment. It provides various features such as:</li> <li>Access controls</li> <li>Private registry for module sharing</li> <li> <p>Policy controls</p> </li> <li> <p>Terraform Cloud projects are stored in workspace repositories</p> </li> <li>Within these workspace repositories, information detailing the project can be found alongside additional info regarding Terraform runs, such as:</li> <li>Plan details</li> <li>Monthly cost estimates</li> <li> <p><code>terraform apply</code> details.</p> </li> <li> <p>In some cases, policy checks may be present, this is essentially to verify any tags associated with resources.</p> </li> <li> <p>Users are allowed to comment on runs to keep track of progress and provide updates when necessary.</p> </li> <li> <p>Environment variables can be set within Terraform Cloud, and the TFstate file can be viewed.</p> </li> <li> <p>Terraform cloud can also be linked to Github repositories for projects, so when any changes are made, they are automatically applied to the workspace repository.</p> </li> </ul>"},{"location":"certifications/terraform-associate/07_Terraform-Cloud-and-Enterprise.html#72-creating-infrastructure-with-terraform-cloud","title":"7.2 - Creating Infrastructure with Terraform Cloud","text":"<ul> <li>Pricing for Terraform Cloud depends on the user's requirements.</li> <li>For teams and governance, more features would be required compared to a personal user.</li> <li>TO create an account, review the following link</li> <li>When getting started, you must first create an organization and a workspace.</li> <li>Then link a version control tool e.g. Github.</li> <li> <p>Providers may need to be added - achievable via <code>Settings -&gt; VCS Providers</code></p> </li> <li> <p>For Github, the following needs to be added:</p> </li> <li>An optional display name for the VCS provider.</li> <li>Client ID</li> <li> <p>Client Secret</p> </li> <li> <p>For the latter two, setup on Github is required:</p> </li> <li>In a repository of choice: <code>settings -&gt; developer settings -&gt; oauth applications</code></li> <li>Register the oauth application, detailing parameters such as:<ul> <li>Homepage URL (refer to Terraform Documentation)</li> <li>Template Callback URL</li> </ul> </li> <li>The above step will present the client ID and secret to be added in Terraform Cloud's VCS Provider setup.</li> <li> <p>Add the Client parameters to the Terraform Cloud setup generates the Callback URL -&gt; add this to the Github application.</p> </li> <li> <p>To create infrastructure with Terraform Cloud, add/commit any sets of files to the Github repository, then create a Workspace in Terraform Cloud.</p> </li> <li> <p>When creating the workspace, connect the chosen VCS provider, and select the desired repository.</p> </li> <li> <p>Once configuration is complete, Terraform-related variables and environment variables must be configured, this can include AWS access keys, defaults, etc.</p> </li> <li>Queue a plan - Initiating a <code>terraform plan</code> invocation using the code in the linked repository</li> <li> <p>If the plan is successful, the <code>terraform apply</code> command can be invoked, or a comment can be added alongside in the event of failure.</p> </li> <li> <p>Terraform state file is stored on Terraform Cloud by default in this scenario for plan, apply and destroy operations.</p> </li> <li>For production environments or plans, cost estimation for running projects and applying configurations can be obtained.</li> </ul>"},{"location":"certifications/terraform-associate/07_Terraform-Cloud-and-Enterprise.html#73-sentinel","title":"7.3 - Sentinel","text":"<ul> <li>An embedded policy-as-code framework integrated with the products provided by Hashicorp.</li> <li>Allows fine-grained, logic-based policy decisions, which can be extended to use info from external sources.</li> <li>A paid feature of Terraform.</li> <li> <p>Carries out policy checks during <code>plan</code> and <code>apply</code> invocations.</p> </li> <li> <p>As an example:</p> </li> <li>A policy may be put in place for EC2 instances e.g. \"forbid creation if no tags are set\"</li> <li> <p>This policy would be attached to a policy set, which would then be applied to a workspace.</p> </li> <li> <p>To create a policy set:</p> </li> <li>Settings -&gt; Policy Set -&gt; Connect a Policy Set</li> <li>Configure VCS Connection as Required</li> <li> <p>Configure Settings for policy and what workspace(s) to apply the policy to.</p> </li> <li> <p>To create the policy:</p> </li> <li>Settings -&gt; Policies -&gt; Create Policy</li> <li>Add policy where required.</li> <li>Set enforcement mode.         1. Hard-Mandatory: Cannot Override         1. Soft-Mandatory: Can be Overrode         1. Advisory: For logging purposes</li> <li>Add policy code (see Terraform Documentation)</li> <li> <p>Associate the policy with a policy set</p> </li> <li> <p>Now when a plan is queued, the policies will be checked to see if the <code>apply</code> can be ran, displaying the results as logs in the UI.</p> </li> <li> <p>Example Policy:</p> </li> </ul> <pre><code>import \"tfplan\"\n\n\nmain = rule {\n    all tfplan.resources.aws_instance as _, instances {\n        all instances as _, r {\n            (length(r.applied.tags) else 0) &gt; 0\n        }\n    }\n}\n</code></pre>"},{"location":"certifications/terraform-associate/07_Terraform-Cloud-and-Enterprise.html#74-remote-backend","title":"7.4 - Remote Backend","text":"<ul> <li>The remote backend stores Terraform state files and may be used to run operations in the Terraform Cloud.</li> <li>TF Cloud may also be used with local operations, in which case, only the state is stored in the remote backend.</li> </ul>"},{"location":"certifications/terraform-associate/07_Terraform-Cloud-and-Enterprise.html#741-remote-operations","title":"7.4.1 - Remote Operations","text":"<ul> <li>When using full remote operations, commands like <code>terraform plan</code> can be executed in Terraform Cloud's runtime environment, with log output streamed to the local terminal.</li> <li> <p>To configure the backend, the following must be applied to the Terraform configuration files</p> </li> <li> <p>In the file containing the resource(s), add a block containing <code>backend \"remote\" {}</code></p> </li> <li>In <code>backend.hcl</code> add the following:</li> <li><code>workspaces { name = \"repository_name\" }</code></li> <li><code>hostname = \"app.terraform.io\"</code></li> <li> <p><code>organization = \"organization_name\"</code></p> </li> <li> <p>Once setup, when Terraform plan or apply is ran, it will run the Terraform Cloud UI. The logs can then be viewed directly via this method.</p> </li> <li>Additionally, cost estimations and Sentinel Policies will be checked if enabled.</li> <li>If resources are configured locally but remote operations are desired, a workspace with a VCS connection cannot be used.</li> </ul>"},{"location":"certifications/terraform-associate/07_Terraform-Cloud-and-Enterprise.html#75-implementing-remote-backend","title":"7.5 - Implementing Remote Backend","text":"<ul> <li>Steps:</li> <li>Create workspace without VCS Connection</li> <li>Configure <code>backend.hcl</code> - detailing workspace, hostname, and organization info</li> <li>Configure resource configuration files with Terraform block containing <code>backend \"remote\" { .... }</code></li> <li> <p>Initialize the config with the backend file via <code>terraform init -backend-config=backend.hcl</code></p> </li> <li> <p>For authenticaiton with a remote backend, a token is required.</p> </li> <li>Run <code>terraform login</code> to generate the token - the credentials are stored to a particular path upon successful execution.</li> <li> <p>The API token can be found on the Terraform Cloud UI, which is then copied into the user input requested.</p> </li> <li> <p>Step 4 can then be re-ran if there were any issues - ensuring any required environment variables are set in the TF Cloud Workspace.</p> </li> </ul>"},{"location":"certifications/terraform-associate/08_Exam-Preparation.html","title":"8.0 - Exam Preparation","text":""},{"location":"certifications/terraform-associate/08_Exam-Preparation.html#81-important-pointers","title":"8.1 - Important Pointers","text":""},{"location":"certifications/terraform-associate/08_Exam-Preparation.html#811-overview","title":"8.1.1 - Overview","text":"Exam Property Description Type Multiple Choice Format Online Proctored Duration 1 Hour Total Number of Questions 57 Price $70.50 Language English Expiration Time 2 Years"},{"location":"certifications/terraform-associate/08_Exam-Preparation.html#812-question-types","title":"8.1.2 - Question Types","text":"<ul> <li>True/False: Based on statement provided.</li> <li>Multiple-Choice: Select correct answer(s) from options.</li> <li>Text-Match: Given a text box or code extract, what should be added?</li> </ul>"},{"location":"certifications/terraform-associate/08_Exam-Preparation.html#813-example-questions","title":"8.1.3 - Example Questions","text":"What is the Name of the file that stores state information?  terraform.tfstate  When referencing a file, for terraform 0.11, it would follow a format similar to ${file(\"path/to/file\")}, how does this translate to terraform v0.12?  file(\"path/to/file\")  What does the command `terraform init` do?  This would likely be a multiple-choice question, choose all applicable."},{"location":"certifications/terraform-associate/08_Exam-Preparation.html#814-important-pointers","title":"8.1.4 - Important Pointers","text":""},{"location":"certifications/terraform-associate/08_Exam-Preparation.html#providers","title":"Providers","text":"<ul> <li>Responsivle for understandign API interactions adn exposing resources</li> <li>Most providers correspond to one cloud or on-prem platform, offering resource types corresponding to each of the platform's features.</li> <li>Provider(s) used are specified by the <code>provider {}</code> block.</li> <li>One can automatically upgrade to the latest version of the designated provider by using <code>terraform init -upgrade</code></li> </ul>"},{"location":"certifications/terraform-associate/08_Exam-Preparation.html#provider-architecture","title":"Provider Architecture","text":"<ul> <li>Providers act as the link between cloud providers and Terraform Configurations.</li> <li>They handle the underlying API interactions and authentication between host and provider.</li> <li>Multiple instances of the same provider can be used via the <code>alias</code> property.</li> </ul>"},{"location":"certifications/terraform-associate/08_Exam-Preparation.html#terraform-init","title":"Terraform Init","text":"<ul> <li>Run <code>terraform init</code> to initialize the working directory where the Terraform config files are stored.</li> <li>Configuration is searched for module blocks and the required source code is retrieved from the specified sources</li> <li>Any providers specified have to be initialised via Terraform before use.</li> <li>No additional files are created with <code>terraform init</code>.</li> </ul>"},{"location":"certifications/terraform-associate/08_Exam-Preparation.html#terraform-plan","title":"Terraform Plan","text":"<ul> <li><code>terraform plan</code> creates an execution of the infrastructure described in the configuration files.</li> <li>Unless told otherwise, will perform a refresh of the terraform state to determine the exact changes required to meet the desired state.</li> <li>Typically used to check the changes match expectations without applying changes.</li> </ul>"},{"location":"certifications/terraform-associate/08_Exam-Preparation.html#terraform-apply","title":"Terraform Apply","text":"<ul> <li><code>terraform apply</code> applies the changes specified to reach the desired state of the configuration</li> <li>Changes detailed in <code>terraform.tfstate</code> file</li> </ul>"},{"location":"certifications/terraform-associate/08_Exam-Preparation.html#terraform-refresh","title":"Terraform Refresh","text":"<ul> <li>Used to refresh the state file of the configuration to account for any changes made not via Terraform</li> <li>Only updates the state file, any configuration files are unchanged</li> <li><code>terraform refresh</code></li> </ul>"},{"location":"certifications/terraform-associate/08_Exam-Preparation.html#terraform-destroy","title":"Terraform Destroy","text":"<ul> <li><code>terraform destroy</code> will tear down all or specific parts of the infrastructure defined in the configuration.</li> <li>Not the only way of destruction, one could simply remove the configuration of the resource(s)</li> </ul>"},{"location":"certifications/terraform-associate/08_Exam-Preparation.html#terraform-format","title":"Terraform Format","text":"<ul> <li><code>terraform fmt</code> rewrites terraform configuration files in a canonical format and style.</li> <li>Recommended for use in projects to maintain readability</li> </ul>"},{"location":"certifications/terraform-associate/08_Exam-Preparation.html#terraform-validate","title":"Terraform Validate","text":"<ul> <li><code>terraform validate</code> validates the configuration files in a directory</li> <li>Checks whether a configuration is syntactically valid or not.</li> <li>Useful for general verification of reusable modules, as well as attribute names and value types.</li> <li>Generally runs automatically as a test step or post-save check for a reusable module in a CI system prior to execution.</li> <li>Cannot be run until <code>terraform init</code> has been executed.</li> </ul>"},{"location":"certifications/terraform-associate/08_Exam-Preparation.html#terraform-provisioners","title":"Terraform Provisioners","text":"<ul> <li>Used to specify model-specific actions on a local or remote machine to carry out configuration / setup tasks.</li> <li>Should only be used as a last resort, typically one would use Packer in conjunction with Terraform.</li> <li>Provisioners are added within resource blocks.</li> <li>For remote-exec, inline commands and connection method must be specified in a similar manner to the following:</li> </ul> <pre><code>provisioner \"remote-exec\" {\n    inline = [\n        \"sudo amazon-linux-extras install -y nginx1.12\", #install nginx\n        \"sudo systemctl start nginx\" ## start nginx\n    ]\n    connection {\n        #connection method\n        type = \"ssh\"\n        user = \"ec2-user\"\n        #private key for authentication\n        private_key = file(\"./remote-exec-keypair.pem\")\n        host = self.public_ip\n    }\n}\n</code></pre> <ul> <li>For local-exec, similar to the above, but without the <code>connection {}</code> block.</li> </ul>"},{"location":"certifications/terraform-associate/08_Exam-Preparation.html#debugging-terraform","title":"Debugging Terraform","text":"<ul> <li>Terraform has logs that can be enabled by setting the <code>TF_LOG</code> environment variable to one of the following, depending on desired verbosity:</li> <li>TRACE</li> <li>DEBUG</li> <li>INFO</li> <li>WARN</li> <li>ERROR</li> <li>To persist log output, set <code>TF_LOG_PATH=\"/path/to/file\"</code></li> <li>Logs should be saved as a <code>.log</code> format.</li> </ul>"},{"location":"certifications/terraform-associate/08_Exam-Preparation.html#terraform-import","title":"Terraform Import","text":"<ul> <li><code>terraform import</code> allows one to import existing infrastructure and bring it under Terraform's management</li> <li>Current implementation can only import resources into the state, it doesn't generate the configuration, which must be written manually.</li> <li>Prior to command execution, write a resource configuration block for the resource, which the live resource will be mapped to.</li> <li>Usage Example: <code>terraform import aws_instance.myec2 &lt;instance ID&gt;</code></li> </ul>"},{"location":"certifications/terraform-associate/08_Exam-Preparation.html#local-values","title":"Local Values","text":"<ul> <li>Assigns a name to an expression, allowing easy reuse within a module without having to manually type it each time</li> <li>Locals can refer to other locals, but as standard practice, reference cycles aren't allowed i.e. a local cannot refer to itself or a variable that refers backt to it.</li> <li>It's advised to group together logically-related local values to a single block, particularly if the values are dependent upon one another.</li> </ul>"},{"location":"certifications/terraform-associate/08_Exam-Preparation.html#data-types","title":"Data Types","text":"<ul> <li>There are 4 maind ata types in Terraform:</li> <li>String - Unicode characters representing text</li> <li>List - Sequential list of values identified by their position, first value being position \"0\"</li> <li>Map - A group of values identified by named labels/keys e.g. age=52</li> <li>Number - Numerical values</li> </ul>"},{"location":"certifications/terraform-associate/08_Exam-Preparation.html#terraform-modules","title":"Terraform Modules","text":"<ul> <li>Resources can be centralised and called out from TF modules when required.</li> </ul>"},{"location":"certifications/terraform-associate/08_Exam-Preparation.html#terraform-modules-root-and-child","title":"Terraform Modules - Root and Child","text":"<ul> <li>Each terraform configuration has at least one module, the root module, which consists of the resources in <code>.tf</code> files in the main directory.</li> <li>Modules can call other modules, which allos the inclusion of the child module's resources into the configuration in a concise manner.</li> <li>A module that includes a module block in the following manner is a child module.</li> </ul> <pre><code>module \"ec2\" {\n    source = \"../../modules/ec2\"\n}\n</code></pre>"},{"location":"certifications/terraform-associate/08_Exam-Preparation.html#modules-accessing-output-values","title":"Modules - Accessing Output Values","text":"<ul> <li>Resources defined in a module are encapsulated, the calling module cannot access their attributes directly</li> <li>The child module can declare output values to selectively export certain values to be accessed by the calling module in a similar manner to</li> </ul> <pre><code>output \"mys3bucket\" {\n    value = aws_s3_bucket.mys3.bucket_domain_name\n}\n</code></pre>"},{"location":"certifications/terraform-associate/08_Exam-Preparation.html#suppressing-values-in-cli-output","title":"Suppressing Values in CLI Output","text":"<ul> <li>An output can be marked as sensitive information using the <code>sensitive</code> argument in a similar manner to:</li> </ul> <pre><code>output \"db_password\" {\n    value = aws_db_instance.db_password\n    Description = \"The password for logging into the database\"\n    Sensitive = true\n}\n</code></pre> <ul> <li>Setting an output value in the root module as sensitive prevents Terraform from showing its value when outputs are generated during <code>terraform apply</code>.</li> <li>Sensitive outputs are still recorded in the state as fully accessible values, anyone who can access the state file can therefore view the sensitive information.</li> </ul>"},{"location":"certifications/terraform-associate/08_Exam-Preparation.html#module-versions","title":"Module Versions","text":"<ul> <li>It's recommended to constrain the acceptable version numbers for each external module to avoid unexpected changes.</li> <li>Version constraints are supported for modules installed from a module registry e.g. Terraform Registry in a similar manner to the following:</li> </ul> <pre><code>module \"ec2_cluster\" {\n    source = \"terraform-aws-modules/ec2-instance/aws\"\n    version = \"~&gt; 2.0\"\n    name = \"my-cluster\"\n    instance_count = 1\n    ami = \"ami-0a13d44dccf1f5cf6\"\n    instance_type = \"t2.micro\"\n    subnet_id = \"subnet-5fbf1013\"\n    tags = {\n        Terraform = \"true\"\n        Environment = \"dev\"\n    }\n}\n</code></pre>"},{"location":"certifications/terraform-associate/08_Exam-Preparation.html#terraform-registry","title":"Terraform Registry","text":"<ul> <li>A registry directly integrated into Terraform, containing modules submitted by the Terraform community, including third-party providers.</li> <li>General syntax for referencing a registry module: <code>&lt;NAMESPACE&gt;/&lt;NAME&gt;/&lt;PROVIDER&gt;</code></li> </ul>"},{"location":"certifications/terraform-associate/08_Exam-Preparation.html#private-registry-for-module-sources","title":"Private Registry for Module Sources","text":"<ul> <li>To use a module from a private registry, such as those provided by Terraform cloud, use the general syntax <code>&lt;HOSTNAME&gt;&lt;NAMESPACE&gt;/&lt;NAME&gt;/&lt;PROVIDER&gt;</code></li> <li>When fetching a private registry module, a version must be specified.</li> </ul>"},{"location":"certifications/terraform-associate/08_Exam-Preparation.html#functions","title":"Functions","text":"<ul> <li>Terraform includes a number of built-in functions to transform and combine values.</li> <li>No user-defined functions are supported.</li> <li>Examples of built-in functions include:</li> <li>Min</li> <li>Max</li> <li>Element</li> <li>Lookup</li> </ul>"},{"location":"certifications/terraform-associate/08_Exam-Preparation.html#count-and-count-index","title":"Count and Count Index","text":"<ul> <li>The count parameter on resources can simplify configurations and allow scaling resources by incremental values.</li> <li>In resource blolocks where the <code>count</code> value is set, an additional count object <code>count.index</code> is available in expressions to be applied to other parameters associated with the resource.</li> <li>Example:</li> </ul> <pre><code>Resource 1 Configuration (Instance 1)\nresource \"aws_instance\" \"instance-1\" {\n    key_name = \"machine_instance.${count.index}\"\n    ami = \"ami-0a13d44dccf1f5cf6\"\n    instance_type = \"t2.micro\"\n    count = 3\n}\n</code></pre> <ul> <li>Alternatively:</li> </ul> <pre><code>variable \"elb_names\" {\n    type = list\n    default = [\n        \"dev-loadbalancer\",\n        \"stage-loadbalanacer\",\n        \"prod-loadbalancer\"\n    ]\n}\n\nresource \"aws_iam_user\" \"lb\" {\n    name = var.elb_names[count.index]\n    count = 3\n    path = \"/system/\"\n}\n</code></pre>"},{"location":"certifications/terraform-associate/08_Exam-Preparation.html#use-case-find-the-issue","title":"Use Case: Find the Issue","text":"<ul> <li>Some exam questions require one to find an issue with a sample piece of configuration to ensure it is aligned to best practice. An example follows:</li> </ul> <pre><code>terraform {\n    backend \"s3\" {\n        bucket = \"mybucket\"\n        key = \"path/to/key\"\n        region = \"us-east-1\"\n        access_key = 1234\n        secret_key = 1234567890\n    }\n}\n</code></pre> <ul> <li>For the above, tje ossie jere os tjat <code>access_key</code> and <code>secret_key</code> are not required as <code>key</code> is already specified.</li> </ul>"},{"location":"certifications/terraform-associate/08_Exam-Preparation.html#terraform-lock","title":"Terraform Lock","text":"<ul> <li>If supported by the backend, Terraform will lock the state file for all operations that affect it.</li> <li>There is a force-unlock command to manually unlock the state file in the event of any issues:  <code>terraform force-unlock &lt;lock id&gt;</code></li> </ul>"},{"location":"certifications/terraform-associate/08_Exam-Preparation.html#use-case-resources-deleted-out-of-terraform","title":"Use Case: Resources Deleted Out of Terraform","text":"<ul> <li>Scenario-based questions are also a possibility.</li> <li>\"You have created an EC2 instance. Someone has modified it manually, what happens when terraform plan is ran?\"</li> <li>Possible scenarios here would be:</li> <li>If the instance type is changed, the resource is terminated and to be recreated.</li> <li>Terraform will attempt to rever the instance type to that of the desired type</li> <li>Terraform will look to create the resource once again.</li> </ul>"},{"location":"certifications/terraform-associate/08_Exam-Preparation.html#resource-block","title":"Resource Block","text":"<ul> <li>Resource blocks describe one or more infrastructure objects, such as virtual networks, compute instances, etc.</li> <li>Each resource block declares a resource of a given type e.g. <code>aws_instance</code> along with a local identifiacation name e.g <code>web</code>.</li> </ul>"},{"location":"certifications/terraform-associate/08_Exam-Preparation.html#sentinel","title":"Sentinel","text":"<ul> <li>An embedded policy-as-code framework integrated with the Hashicorp Enterprise products.</li> <li>Use cases include:</li> <li>Verification for tags on resources</li> <li>Verification of encryption methods.</li> <li>In general for Terraform Enterprise, the workload follows plan -&gt; sentinel checks -&gt; apply</li> </ul>"},{"location":"certifications/terraform-associate/08_Exam-Preparation.html#sensitive-data-in-state-file","title":"Sensitive Data in State File","text":"<ul> <li>When managing any  sensitive data in Terraform, it's good practice to treat the state file as a whole, as sensitive data.</li> <li>Terraform Cloud always encrypts the state at rest and protects it with TLS in transit.</li> <li>Terraform Cloud is capable of tracking the identity of the user requesting the state file and logging any changes made to it.</li> <li>If using a backend like S3, encryption at rest is supported when encryption is active.</li> </ul>"},{"location":"certifications/terraform-associate/08_Exam-Preparation.html#dealing-with-credentials-in-config","title":"Dealing with Credentials in Config","text":"<ul> <li>Hardcoding credentials is never good practice as it poses significant security risks.</li> <li>Credentials can be stored outside of the terraform configuration.</li> <li>Storing credentials as environment variables is generally considered as best practice as they aren't committed.</li> <li>One could also use secret managers or external tools like Hashicorp Vault.</li> </ul>"},{"location":"certifications/terraform-associate/08_Exam-Preparation.html#remote-backend-terraform-cloud","title":"Remote Backend - Terraform Cloud","text":"<ul> <li>Stores Terraform state and may be used to run operations in Terraform Cloud</li> <li>When using for remote operations tasks, tasks like <code>terraform apply</code> can be executed in Terraform Cloud's runtime environment; output logs are then viewable on the local machine.</li> </ul>"},{"location":"certifications/terraform-associate/08_Exam-Preparation.html#miscellaneous","title":"Miscellaneous","text":"<ul> <li>Terraform doesn't require GO as a prerequisite.</li> <li>Terraform works well with Linux, Windows and MAC</li> <li>Windows Server isn't required for usage.</li> </ul>"},{"location":"certifications/terraform-associate/08_Exam-Preparation.html#terraform-graph","title":"Terraform Graph","text":"<ul> <li>A command used to generate a visual representation of either a configuration or execution format.</li> <li>The output format of Terraform graph is in the DOT format, which can be converted to SVG, PNG, etc for visutalization.</li> </ul>"},{"location":"certifications/terraform-associate/08_Exam-Preparation.html#splat-expressions","title":"Splat Expressions","text":"<ul> <li>Allows quick access to a list of all attributes</li> <li>Will output all possible values within the configuration where applicable.</li> <li>Referencing modules:</li> </ul> <pre><code>resource \"aws_instance\" \"example\" {\n    ami = \"ami-abc123\"\n    instance_type = \"t2.micro\"\n\n    ebs_block_device {\n        device_name = \"sda2\"\n        volume_size = 16\n    }\n    ebs_block_device {\n        device_name = \"sda3\"\n        volume_size = 20\n    }\n}\n</code></pre> <ul> <li>The arguments on the <code>ebs_block_device</code> nested blocks can be accessed via a splat expression.</li> <li>Example usage to obtain all device name values: <code>aws_instance.example.ebs_block_device[*].device_name</code></li> <li>The nested blocks in this particular resource type don't have any exported attributes, but if <code>ebs_block device</code> were to have a documented <code>id</code> attribute, then a list of them could be accessed via <code>aws_instance.example.ebs_block_device[*].id</code></li> </ul>"},{"location":"certifications/terraform-associate/08_Exam-Preparation.html#terraform-technologies","title":"Terraform Technologies","text":"<ul> <li>In any given terraform resource block, there are 4 main term types used:</li> <li>Resource Type: Specifies what resource is being defined in the resource block</li> <li>Local Identification name for the resource</li> <li>Attributes e.g. <code>AMI</code>, <code>ACCESS_KEY</code>, etc.</li> <li>Attribute values.</li> </ul>"},{"location":"certifications/terraform-associate/08_Exam-Preparation.html#provider-configuration","title":"Provider Configuration","text":"<ul> <li>A block for provider configuration isn't mandatory for all terraform configurations.</li> <li>When no resources are to be created, no provider needs to be specified.</li> </ul>"},{"location":"certifications/terraform-associate/08_Exam-Preparation.html#terraform-unlock","title":"Terraform Unlock","text":"<ul> <li>If supported by the backend, terraform will lock the state file for all the operations that could write to the state file</li> <li>Not all backends have locking unctionality</li> <li>Terraform has a <code>force-unlock</code> command if unlocking failed</li> <li>Example usage: <code>terraform force-unlock &lt;lock id&gt;</code></li> </ul>"},{"location":"certifications/terraform-associate/08_Exam-Preparation.html#miscellaneous-pointers-01","title":"Miscellaneous Pointers - 01","text":"<ul> <li>Primary benefits of Infrastructure as Code tools:</li> <li>Automation</li> <li>Versioning</li> <li>Reusability</li> <li>There are various tools available for IaC:</li> <li>Terraform</li> <li>AWS CloudFormation</li> <li>Azure Resource Formation</li> <li>Google Cloud Deployment Manager</li> </ul>"},{"location":"certifications/terraform-associate/08_Exam-Preparation.html#miscellaneous-pointers-02","title":"Miscellaneous Pointers - 02","text":"<ul> <li>Sentinel is a proactive service</li> <li>Terraform doesn't modify the infrastructure, only the state file</li> <li>Slice function is not a function to be used with strings, unlike join and split.</li> <li>It is not necessary to include the module version when pulling code from Terraform registry.</li> </ul>"},{"location":"certifications/terraform-associate/08_Exam-Preparation.html#miscellaneous-pointers-03","title":"Miscellaneous Pointers - 03","text":"<ul> <li>Overuse of dynamic blocks can make configurations unreadable</li> <li>Terraform apply can change, destroy and provision resources, but not import configuration directly.</li> </ul>"},{"location":"certifications/terraform-associate/08_Exam-Preparation.html#terraform-enterprise-and-cloud","title":"Terraform Enterprise and Cloud","text":"<ul> <li>Terraform Enterprise allows several additional features not included in cloud, such as:</li> <li>Single-Sign On (SSO)</li> <li>Auditing</li> <li>Private Data Center</li> <li>Clustering</li> <li>It should also be noted that Team and Governance features are not available for free on Terraform Cloud.</li> </ul>"},{"location":"certifications/terraform-associate/08_Exam-Preparation.html#variables-with-undefined-values","title":"Variables with Undefined Values","text":"<ul> <li>If a variable is included with an undefined value, an error will not immediately occur.</li> <li>Terraform will ask for the user to supply a value to be associated with the undefined variable.</li> </ul>"},{"location":"certifications/terraform-associate/08_Exam-Preparation.html#environment-variables","title":"Environment Variables","text":"<ul> <li>Can be used to set variables</li> <li>Environment variables must be fof the format <code>TF_VAR_&lt;variable name&gt;</code> e.g.</li> <li><code>export TF_VAR_region=us-west-1</code></li> <li><code>export TF_VAR_alist='[1,2,3]'</code></li> </ul>"},{"location":"certifications/terraform-associate/08_Exam-Preparation.html#structural-data-types","title":"Structural Data Types","text":"<ul> <li>A structural type allows multiple values of several distinct types to be grouped together as a single value.</li> <li>List contains multiple values of the same type while object can contain multiple values of different types:</li> </ul> Structural Type Description Object A collection of named attributes that each have their own type e.g.  <code>object({&lt;Attribute Name&gt; = &lt;Type&gt;, ... })</code> Tuple <code>tuple ([&lt;TYPE&gt;, ... ])</code>"},{"location":"certifications/terraform-associate/08_Exam-Preparation.html#backend-configuration","title":"Backend Configuration","text":"<ul> <li>Backends are configured directly in Terraform files via the <code>terraform {}</code> block.</li> <li>Once configured, any remote backends must be initialized.</li> <li>Example Usage - S3 Backend</li> </ul> <pre><code>terraform {\n    backend \"s3\" {\n        bucket = \"demoremotebackend\"\n        key = \"remotedemo.tfstate\"\n        region = \"eu-west-2\"\n        access_key = \"&lt;ACCESS KEY&gt;\"\n        secret_key = \"&lt;SECRET KEY&gt;\"\n        #dynamodb_table = \"s3-state-lock\"\n    }\n}\n</code></pre>"},{"location":"certifications/terraform-associate/08_Exam-Preparation.html#backend-configuration-types","title":"Backend Configuration Types","text":"<ul> <li>First-Time Configuration:</li> <li>When Cconfiguring a backend for the first time (moving from no defined backend to an explictly configured one), Terraform will give an option to migrate the currently-existing state to the new backend.</li> <li>Allows adoption of backends without losing data in the state file.</li> <li>Partial Configuration</li> <li>Not every required argument needs to be specified for a backend configuration</li> <li>It may be better to omit certain arguments to avoid storing secrets within the main configuration</li> <li> <p>With a partial configuration, the remaining arguments must be provided as part of the initialization process.</p> </li> <li> <p>Partial Configuration Example:</p> </li> </ul> <pre><code>terraform {\n    backend \"consul\" {}\n}\n</code></pre> <ul> <li>The arguments are then supplied via the <code>terraform init</code> command i.e.  <code>terraform init -backend-config=&lt;parameter 1&gt;=&lt;value 1&gt; -backend-config=&lt;parameter 2&gt;=&lt;value 2&gt; ...</code></li> </ul>"},{"location":"certifications/terraform-associate/08_Exam-Preparation.html#terraform-taint","title":"Terraform Taint","text":"<ul> <li>Manually marks a Terraform-managed resource as tainted, forcing it to be destroyed and recreated on the next execution of <code>apply</code>.</li> <li> <p>Once tainted, the next <code>plan</code> will show the resource tainted will be destroyed and recreated.</p> </li> <li> <p>Can be used to taint resources within a module</p> </li> <li>Usage: <code>terraform taint [options] &lt;resource type&gt;.&lt;resource id&gt;</code></li> <li>For multiple submodules, apply: <code>module.foo.module.bar.&lt;resource_type&gt;.&lt;resource_id&gt;</code></li> </ul>"},{"location":"certifications/terraform-associate/08_Exam-Preparation.html#local-provisioner","title":"Local Provisioner","text":"<ul> <li>The local-exec provisioner invokes commands to be ran on your local machine after a resource is created.</li> <li>Defined in a similar manner to:</li> </ul> <pre><code>resource \"aws_instance\" \"myec2\" {\n    ami = \"ami-0a13d44dccf1f5cf6\"\n    instance_type = \"t2.micro\"\n    provisioner \"local-exec\" {\n        command = \"echo ${aws_instance.myec2.private_ip} &gt;&gt; privateips.txt\"\n    }\n}\n</code></pre>"},{"location":"certifications/terraform-associate/08_Exam-Preparation.html#remote-exec-provisioner","title":"Remote-Exec Provisioner","text":"<ul> <li>Invokes commands on a remote resource after it's created.</li> <li>Supports multiple connection types including SSH and WinRM</li> <li>Example Usage:</li> </ul> <pre><code>resource \"resource type\" \"local ID\" {\n    .....\n    provisioner \"remote-exec\" {\n        inline = [\n            \"sudo amazon-linux-extras install -y nginx1.12\", #install nginx\n            \"sudo systemctl start nginx\" ## start nginx\n        ]\n        connection {\n            #connection method\n            type = \"ssh\"\n            user = \"ec2-user\"\n            #private key for authentication\n            private_key = file(\"./remote-exec-keypair.pem\")\n            host = self.public_ip\n        }\n    }\n}\n</code></pre>"},{"location":"certifications/terraform-associate/08_Exam-Preparation.html#provisioner-failure-behaviour","title":"Provisioner Failure Behaviour","text":"<ul> <li>By default, provisioners that fail will cause the <code>apply</code> command to fail.</li> <li>This behaviour can be altered by changing the <code>on_failure</code> setting to 1 of 2 values:</li> <li>Continue: Ignore the error and continue with the resource creation</li> <li>Fail: Raise an error and stop the apply (default behaviour).</li> </ul>"},{"location":"certifications/terraform-associate/08_Exam-Preparation.html#provisioner-types","title":"Provisioner Types","text":"<ul> <li>Two types of provisioners:</li> <li>Creation-Time<ul> <li>Only run during creation, not during updating or any other lifecycle</li> <li>If a creation-time provisioner fails, the resource is marked as tainted</li> </ul> </li> <li>Destroy-Time<ul> <li>Ran before the resource is destroyed.</li> </ul> </li> </ul>"},{"location":"certifications/terraform-associate/08_Exam-Preparation.html#input-variables","title":"Input Variables","text":"<ul> <li>The value associated with a variable can be assigned via multiple methods</li> <li>Value associated can be defined either by the CLI or TFVars file.</li> <li>To load a custom tfvars file: <code>terraform apply -var-file=\"filename.tfvars\"</code></li> </ul>"},{"location":"certifications/terraform-associate/08_Exam-Preparation.html#variable-definition-precedence","title":"Variable Definition Precedence","text":"<ul> <li>Terraform loads variables in the following order, with the importance increasing as the list progresses:</li> <li>Environment Variables</li> <li>Terraform.tfvars</li> <li>Terraform.tfvars.json</li> <li>Any <code>*.auto.tfvars</code> or <code>*.auto.tfvars.json</code> files processed alphabetically</li> <li>Any <code>-var</code> or <code>-var-file</code> CLI arguments provided.</li> <li>If a variable is defined multiple times with different values, the last definition will be the applied value.</li> </ul>"},{"location":"certifications/terraform-associate/08_Exam-Preparation.html#local-backend","title":"Local Backend","text":"<ul> <li>Stores the state on the local file system</li> <li>Locks the state using system APIs</li> <li>Performs operations locally</li> <li>The default backend used by Terraform</li> <li>Path should be specified where appropriate</li> </ul>"},{"location":"certifications/terraform-associate/08_Exam-Preparation.html#required-providers","title":"Required Providers","text":"<ul> <li>Each Terraform module must declare which providers it needs so Terraform can install and use them</li> <li>Provider requirements are declared in a <code>required_providers</code> block in a similar manner to:</li> </ul> <pre><code>terraform {\n    required_providers {\n        mycloud = {\n            source = \"mycorp/mycloud\"\n            version = \"~&gt; 1.0\"\n        }\n    }\n}\n</code></pre>"},{"location":"certifications/terraform-associate/08_Exam-Preparation.html#required-version","title":"Required Version","text":"<ul> <li>The <code>required_version</code> setting accepts a version constraint string, specifying the version of Terraform to be used with your configuration.</li> <li>If the running of Terraform doesn't match the specified constraints, Terraform will produce an error and exit without applying any more changes.</li> </ul>"},{"location":"certifications/terraform-associate/08_Exam-Preparation.html#version-arguments","title":"Version Arguments","text":"Version Argument Description &gt;= x.y Greater than or equal to x.y &lt; Less than or equal to x.y ~&gt;x.y Any version in the range x.y &gt;=X.Y,&lt;=A.B Any version in the range X.Y - A.B"},{"location":"certifications/terraform-associate/08_Exam-Preparation.html#fetching-values-from-a-map","title":"Fetching Values from a Map","text":"<ul> <li>Reference a value from a map variable via <code>var.&lt;variable name&gt;[\"&lt;key&gt;\"]</code></li> </ul>"},{"location":"certifications/terraform-associate/08_Exam-Preparation.html#terraform-and-git","title":"Terraform and GIT","text":"<ul> <li>In practice, careful consideration should be taken when committing Terraform code.</li> <li>The <code>.gitignore</code> should be configured to ignore files which may contain sensitive information such as:</li> <li><code>terraform.tfstate</code></li> <li><code>*.tfvars</code></li> <li>Arbirtrary git repositories can be used by prefixing the address with a special <code>git::</code> prefix</li> <li>Any valid git URL can be specified to select one of the protocols supported by GIT.</li> <li>By default, Terraform will clone and use the default branch (HEAD) in the selected repository</li> <li>This can be overwritten by adding the ref argument e.g.:</li> </ul> <pre><code>module \"vpc\" {\n    source = \"git:://https://example.com/repo.git?ref=v1.0\"\n}\n</code></pre> <ul> <li>The value of the <code>ref</code> argument can be any reference that would be accepted by the <code>git checkout</code> command, including branch and tag names.</li> </ul>"},{"location":"certifications/terraform-associate/08_Exam-Preparation.html#dependency-types-implicit","title":"Dependency Types - Implicit","text":"<ul> <li>With implicity dependencies, terraform automatically finds references of the object and creates an implicit ordering requirement between the two resources.</li> <li>A common example is creating an Elastic IP address to be associated with an EC2 instance's Public IP, which can be specified as <code>aws_eip.my-eip.private_ip</code></li> <li>This defines an implicit dependency that will inform Terraform to create the EIP first before creating the EC2 instance.</li> </ul>"},{"location":"certifications/terraform-associate/08_Exam-Preparation.html#dependency-types-explicit","title":"Dependency Types - Explicit","text":"<ul> <li>Explicitly specifying a dependency is only required when a resource relies on another's behaviour but doesn't access any of that resource's data.</li> <li>One can add <code>depends_on = [&lt;resource_type.&lt;resource_id&gt;]</code> to specify an explicit dependency</li> </ul>"},{"location":"certifications/terraform-associate/08_Exam-Preparation.html#state-command","title":"State Command","text":"<ul> <li>List resources in the state: <code>terraform state list</code></li> <li>Move or rename items within the state: <code>terraform state mv</code></li> <li>Manually download and output the state from state file: <code>terraform state pull</code></li> <li>Remove items from the state file: <code>terraform state rm</code></li> <li>Show the attributes of a resource in the state file: <code>terraform state show</code></li> </ul>"},{"location":"certifications/terraform-associate/08_Exam-Preparation.html#data-source-code","title":"Data Source Code","text":"<ul> <li>Data sources allow data to be fetched or computed for use elswhere within the configuration.</li> <li>Reads from a specific data source and exports the results under a particular value.</li> <li>Common example is AWS AMIs as these vary from region to region.</li> </ul>"},{"location":"certifications/terraform-associate/08_Exam-Preparation.html#terraform-plan-destroy","title":"Terraform Plan Destroy","text":"<ul> <li>The behaviour of <code>terraform destroy</code> can be previewed by <code>terraform plan -destroy</code></li> </ul>"},{"location":"certifications/terraform-associate/08_Exam-Preparation.html#terraform-module-sources","title":"Terraform Module Sources","text":"<ul> <li>The module installer supports installation from a number of different source types, such as local paths, terraform registry, etc.</li> <li>Local path references allow for factoring out portions of configuration within a single source repository.</li> <li>A local path must begin with either a <code>./</code> or <code>../</code> to indicate it's a local path.</li> </ul>"},{"location":"certifications/terraform-associate/08_Exam-Preparation.html#larger-infrastructure","title":"Larger Infrastructure","text":"<ul> <li>Cloud providers have certain amounts of rate limiting, meaning only a certain number of resources can be reqiested over a period of time.</li> <li>It's recommended that larger configurations are broken into multiple smaller sets that can be independently applied.</li> <li>Alternatively, can use the <code>-refresh=false</code> and target flags, though this is not recommended.</li> </ul>"},{"location":"certifications/terraform-associate/08_Exam-Preparation.html#miscellaneous-pointers","title":"Miscellaneous Pointers","text":"<ul> <li>Lookup retrieves the value of a single element from a map <code>lookup(map, key, default)</code></li> <li>Various commands refresh the state implicitly, such as <code>plan</code>, <code>apply</code>, and <code>destroy</code>, <code>init</code> and <code>import</code> do not.</li> <li>Array data type is not supported by Terraform.</li> <li>Various variable definition files can be loaded such as:</li> <li><code>terraform.tfvars</code></li> <li><code>terraform.tfvars.json</code></li> <li>Any files with <code>.auto.tfvars.json</code></li> <li>Both implicit and explicit dependencies are stored in <code>terraform.tfstate</code></li> <li><code>terraform init -upgrade</code> automatically will upgrade all previously installed plugins to their newest versions.</li> <li>The terraform console provides an interactive console for evaluating expressions.</li> <li>The declaration of variables differs between terraform 0.11 and 0.12:</li> <li>0.11: <code>\"${var.varname}\"</code></li> <li>0.12: <code>var.varname</code></li> </ul>"},{"location":"certifications/vault-associate/01_Introduction.html","title":"1.0 - Introduction","text":""},{"location":"certifications/vault-associate/01_Introduction.html#11-introduction-to-the-course-and-certification","title":"1.1 - Introduction to the Course and Certification","text":""},{"location":"certifications/vault-associate/01_Introduction.html#notes","title":"Notes","text":""},{"location":"certifications/vault-associate/01_Introduction.html#introduction","title":"Introduction","text":"<ul> <li>Organisations can have security breaches due to any of, amongst other use cases:</li> <li>Database credentials or access/secret keys being stored in notepad</li> <li>Secrets handwritten on sticky notes</li> <li>This is obviously a huge potential security risk, as if secrets like this are compromised, servers, applications, and private data could all be exposed to attack.</li> <li>HashiCorp Vault aims to mitigate these risks by:</li> <li>Manage secrets and protect sensitive data</li> <li>Provide identity-based access management</li> <li>Generate dynamic secrets e.g. database credentials, AWS secrets, etc.</li> <li>All the while remaining an open-source tool.</li> </ul>"},{"location":"certifications/vault-associate/01_Introduction.html#dynamic-secrets-high-level-view","title":"Dynamic Secrets - High-Level View","text":"<ul> <li>Consider a developer wanting to authenticate to a database for testing in a system where Vault has been implemented.</li> </ul> <ul> <li>Credential request is sent to Vault</li> <li>Vault immediately generates the credentials for the user</li> <li>This is hugely beneficial as it doesn't depend on database admins generating the credentials.</li> <li>Additionally, Vault will monitor the timeframe of these credentials - if the Time-To-Live (TTL) for the credentials passes, Vault will automatically revoke these credentials - the user will then need to generate new credentials.</li> </ul>"},{"location":"certifications/vault-associate/01_Introduction.html#course-disclaimers","title":"Course Disclaimers","text":"<ul> <li>Course structure \u2260 HashiCorp Exam structure - all relevant content is included.</li> <li>Exam:</li> <li>1-hour multiple choice assessment that is online proctored</li> <li>Cost $70.50</li> <li>Expires after 2 years</li> <li>Course is aimed to start from scratch with Vault - the notes should work towards exam readiness.</li> <li>Course Notes and Code can be found here: https://github.com/zealvora/hashicorp-certified-vault-associate</li> </ul>"},{"location":"certifications/vault-associate/02_Getting-Started-with-Vault.html","title":"2.0 - Getting Started with Vault","text":""},{"location":"certifications/vault-associate/02_Getting-Started-with-Vault.html#201-overview-of-hashicorp-vault","title":"2.01 - Overview of HashiCorp Vault","text":""},{"location":"certifications/vault-associate/02_Getting-Started-with-Vault.html#notes","title":"Notes","text":""},{"location":"certifications/vault-associate/02_Getting-Started-with-Vault.html#getting-started","title":"Getting Started","text":"<ul> <li>Vault allows enhanced management and storage of secrets such as tokens, passwords, tokens, certificates, etc.</li> <li>Secret management in general is one of the most common and high-priority challenges faced by organisations - this generally includes storage, access management, and rotation of secrets.</li> <li>Secrets include anything from database passwords, AWS access/keys, API tokens, encryption keys, etc.</li> </ul>"},{"location":"certifications/vault-associate/02_Getting-Started-with-Vault.html#dynamic-secrets-and-vault-console","title":"Dynamic Secrets and Vault Console","text":"<ul> <li>As discussed in the previous section, Vault aims to offer Dynamic Secrets i.e. users request the credentials from Vault, the credentials generated are then rotated based on the policies set.</li> <li>Vault was previously CLI-only, but is now packaged with a UI / Console to help with management.</li> <li>This console can be easily used to generate new credentials for many different secret types.</li> <li>This can be viewed via the Vault CLI, the following command provides credential details including password, username, and lease time:</li> </ul> <pre><code>vault read database/creds/readonly\n</code></pre> <ul> <li>Other functionalities include encryption.</li> <li>In general, once installed, Vault can remove the manual aspect of many secret management tasks, allowing engineers and admins to spend more time on the work they're required for.</li> </ul>"},{"location":"certifications/vault-associate/02_Getting-Started-with-Vault.html#202-installation-of-hashicorp-vault","title":"2.02 - Installation of HashiCorp Vault","text":"<ul> <li>Vault is available on all major operating systems and can also be installed on platforms such as Kubernetes clusters.</li> </ul>"},{"location":"certifications/vault-associate/02_Getting-Started-with-Vault.html#windows","title":"Windows","text":"<ul> <li>Download and extract binary file from https://www.vaultproject.io/downloads</li> <li>Or use package managers such as Chocolatey:</li> </ul> <pre><code>choco install vault\n</code></pre> <ul> <li>Verify the installation:</li> </ul> <pre><code>vault\n</code></pre> <ul> <li>Ensure Vault's location is added to the PATH env variable.</li> </ul>"},{"location":"certifications/vault-associate/02_Getting-Started-with-Vault.html#linux","title":"Linux","text":"<ul> <li>https://learn.hashicorp.com/tutorials/vault/getting-started-install?in=vault/getting-started</li> </ul>"},{"location":"certifications/vault-associate/02_Getting-Started-with-Vault.html#ubuntu","title":"Ubuntu","text":"<ul> <li>Add the HashiCorp GPG key</li> </ul> <pre><code>curl -fsSL https://apt.releases.hashicorp.com/gpg | sudo apt-key add -\n</code></pre> <ul> <li>Add the official HashiCorp Linux repository - replace arch=&lt;&gt; as appropriate (confirm via <code>arch</code></li> </ul> <pre><code>sudo apt-add-repository \"deb [arch=amd64] https://apt.releases.hashicorp.com $(lsb_release -cs) main\"\n</code></pre> <ul> <li>Update and Install</li> </ul> <pre><code>sudo apt-get update &amp;&amp; sudo apt-get install vault\n</code></pre> <ul> <li>Again, ensure it is in $PATH if installing manually</li> </ul> <pre><code>echo $PATH\nmv &lt;vault binary&gt; $PATH\n</code></pre>"},{"location":"certifications/vault-associate/02_Getting-Started-with-Vault.html#macos","title":"MacOS","text":"<p>https://learn.hashicorp.com/tutorials/vault/getting-started-install?in=vault/getting-started</p>"},{"location":"certifications/vault-associate/02_Getting-Started-with-Vault.html#203-initializing-vault-with-dev-server-mode","title":"2.03 - Initializing Vault with Dev Server Mode","text":"<ul> <li>Vault has two modes for operation:</li> <li>Dev<ul> <li>Typically used for local development, testing, and practice</li> <li>Very insecure</li> <li>All data is stored in-memory \u21d2 Any data stored will be lost upon restart</li> </ul> </li> <li>Prod</li> <li>Vault can be started as a server in \"dev\" mode via the following command:</li> </ul> <pre><code>vault server -dev\n</code></pre> <ul> <li>Resultant output provides details regarding the vault server.</li> </ul> <pre><code>Vault server configuration:\n\n             Api Address: http://127.0.0.1:8200\n                     Cgo: disabled\n         Cluster Address: https://127.0.0.1:8201\n              Go Version: go1.17.5\n              Listener 1: tcp (addr: \"127.0.0.1:8200\", cluster address: \"127.0.0.1:8201\", max_request_duration: \"1m30s\", max_request_size: \"33554432\", tls: \"disabled\")\n               Log Level: info\n                   Mlock: supported: false, enabled: false\n</code></pre> <ul> <li>By default, the UI is available at the address listed against <code>Api Address</code></li> </ul> <ul> <li>To obtain the token for login - refer to the end of the output of the command used to start Vault in dev server mode, extract the root token and login to the UI:</li> </ul> <pre><code>The unseal key and root token are displayed below in case you want to\nseal/unseal the Vault or re-authenticate.\n\nUnseal Key: &lt;Key&gt;\nRoot Token: &lt;Token&gt;\n\nDevelopment mode should NOT be used in production installations!\n</code></pre> <ul> <li>Once in the UI, menu options available will include:</li> <li>Secrets - Secrets stored via various storage methods (engines)</li> <li>Access -</li> <li>Policies</li> <li>Tools</li> <li>To verify the status of the Vault Server, use the <code>vault status</code> command, appending the server address via the \u2014address option.</li> </ul> <pre><code>vault status --address=$VAULT_ADDR\n</code></pre> <ul> <li>Where $VAULT_ADDR is the address of the Vault server provided at Vault initialisation.</li> <li>To stop the server, simply use <code>CTRL+C</code> on the terminal where Vault is running. Note that upon restart, the root token will be different.</li> </ul>"},{"location":"certifications/vault-associate/02_Getting-Started-with-Vault.html#204-creating-a-secret","title":"2.04 - Creating a Secret","text":"<ul> <li>One of Vault's key features is to read and write arbitrary secrets securely.</li> <li>It does so utilising Secrets engines - these are components responsible for the storage, generation, or encryption of data.</li> <li>Secrets can be stored based on a specific secret engine - each engine offers particular features.</li> </ul>"},{"location":"certifications/vault-associate/02_Getting-Started-with-Vault.html#secret-example","title":"Secret Example","text":"<ul> <li>When starting Vault in dev server mode, two secret engines exist as part of the standard setup:</li> <li>Cubbyhole</li> <li>Key-Value</li> </ul>"},{"location":"certifications/vault-associate/02_Getting-Started-with-Vault.html#gui-secret-generation-keyvalue","title":"GUI Secret Generation - KeyValue","text":"<ul> <li>To create a secret, navigate to the secret engine of choice and select <code>create secret</code></li> </ul> <ul> <li>For keyvalue add the following:</li> <li>path - secret name</li> <li>secret metadata - maximum number of versions</li> <li>version data - add password associated with secret in key-value form</li> <li>Once saved, the secret is logged and the password can be checked by selecting the eye icon</li> <li>This secret can be edited at any point - changes are logged as versions which can be switched between at any point</li> <li>Delete and destroy operations are available for each version, or all versions can be destroyed</li> <li>In this case, destroy is permanent deletion, delete offers the chance to recover the secret.</li> </ul>"},{"location":"certifications/vault-associate/02_Getting-Started-with-Vault.html#cli-creation","title":"CLI Creation","text":"<ul> <li>Options for secret generation in the KV engine via the CLI can be viewed via, this includes CRUD operations, amongst others:</li> </ul> <pre><code>vault kv -h\n</code></pre> <ul> <li>To create in the kv engine:</li> </ul> <pre><code>vault kv put /path/to/secret key=value\n</code></pre> <ul> <li>Secret creation  can then be verified in the HC Vault UI</li> <li>New versions of secrets can be created via the CLI:</li> </ul> <pre><code>vault kv put secret/secret-name key=new-value\n</code></pre> <ul> <li>To read a secret:</li> </ul> <pre><code>vault kv get secret/secret-name\n</code></pre> <ul> <li>To read a particular version,  append <code>-version=&lt;version number&gt;</code></li> <li>Example:</li> </ul> <pre><code>vault kv get -versions=&lt;version number&gt; secret/secret-name\n</code></pre> <ul> <li>For deletion:</li> </ul> <pre><code>vault kv delete -versions=&lt;number&gt; path/to/secret\n</code></pre> <ul> <li>For undeletion:</li> </ul> <pre><code>vault kv undelete -versions=&lt;number&gt; path/to/secret\n</code></pre> <ul> <li>For full version destruction:</li> </ul> <pre><code>vault kv destroy -versions=&lt;number&gt; path/to/secret\n</code></pre> <ul> <li>For full secret destruction - covering all versions and data relating to the secrets:</li> </ul> <pre><code>vault kv metadata delete path/to/secret\n</code></pre> <ul> <li>Note: <code>-h</code> option provides MANY options for use case examples</li> </ul>"},{"location":"certifications/vault-associate/02_Getting-Started-with-Vault.html#205-overview-of-secrets-engine","title":"2.05 - Overview of Secrets Engine","text":""},{"location":"certifications/vault-associate/02_Getting-Started-with-Vault.html#secret-engine-overview","title":"Secret Engine Overview","text":"<ul> <li>Secrets engines are components that store, generate or encrypt data.</li> <li>Secrets can be stored based on specific secret engines, each offer particular features.</li> </ul>"},{"location":"certifications/vault-associate/02_Getting-Started-with-Vault.html#secret-engine-types","title":"Secret Engine Types","text":"<ul> <li>Similar to Terraform Providers, multiple secret engine types are available, each providing particular features for specific use cases. Examples include:</li> <li>AWS</li> <li>Active Directory</li> <li>Key/Value</li> <li>SSH</li> <li>Azure</li> </ul>"},{"location":"certifications/vault-associate/02_Getting-Started-with-Vault.html#secret-engine-paths","title":"Secret Engine Paths","text":"<ul> <li>Secret engines are enabled at a given path. Once enabled, the secrets are stored at that particular path.</li> <li>You can control where a secret is installed via the following command:</li> </ul> <pre><code>vault kv put &lt;secret engine path&gt;/&lt;secret name&gt; mykey=myvalue\n</code></pre>"},{"location":"certifications/vault-associate/02_Getting-Started-with-Vault.html#secret-engine-lifecycle","title":"Secret Engine Lifecycle","text":"<ul> <li>In general, engines can be:</li> <li>Enabled</li> <li>Disabled</li> <li>Tuned</li> <li> <p>Moved</p> Option Description Enable Enables a secrets engine at a particular path. By default, they will be enabled at their \"type\" e.g. \"aws\" enables at \"aws/\" Disable Disables an existing secrets engine - this by default will revoke all secrets associated with the engine Move Moves the path for an existing secrets engine, </li> </ul>"},{"location":"certifications/vault-associate/02_Getting-Started-with-Vault.html#example-keyvalue-secret-engine","title":"Example - Key/Value Secret Engine","text":"<ul> <li>The KV secrets engine stores arbitrary secrets within the configured physical storage for Vault</li> <li>Key names must always be strings</li> <li>Provides various functionalities e.g. versioning.</li> <li> <p>Further information regarding secrets engines is available at https://www.vaultproject.io/docs/secrets</p> <p>Note: There are two versions of the kv secrets engine, version 2 is the latest.</p> <ul> <li>Engines can be enabled via the CLI or the UI.</li> </ul> </li> </ul>"},{"location":"certifications/vault-associate/02_Getting-Started-with-Vault.html#enabling-a-secrets-engine-ui","title":"Enabling a Secrets Engine - UI","text":"<ul> <li>From the home page, select <code>Enable new engine</code></li> <li> <p>Select the desired engine from the list provided, in this case, KV.</p> <p></p> </li> <li> <p>Configure the <code>Path</code> AND the Maximum number of versions per key to keep, then enable the engine.</p> </li> <li>The secret engine is now available for usage and can have secrets be created within.</li> </ul>"},{"location":"certifications/vault-associate/02_Getting-Started-with-Vault.html#enabling-a-secrets-engine-cli","title":"Enabling a Secrets Engine - CLI","text":"<ul> <li> <p>To enable a secrets engine, run:</p> <p><code>powershell vault secrets enable -path=demopath -version=2 kv</code></p> <p></p> </li> </ul>"},{"location":"certifications/vault-associate/02_Getting-Started-with-Vault.html#disabling-a-secret-engine","title":"Disabling a Secret Engine","text":"<ul> <li> <p>To disable a secret engine via the CLI, run:</p> <p><code>powershell vault secrets disable &lt;pathname&gt;/</code></p> <p></p> </li> </ul>"},{"location":"certifications/vault-associate/02_Getting-Started-with-Vault.html#206-overview-of-dynamic-secrets","title":"2.06 - Overview of Dynamic Secrets","text":""},{"location":"certifications/vault-associate/02_Getting-Started-with-Vault.html#dynamic-secrets-overview","title":"Dynamic Secrets Overview","text":"<ul> <li>In the KV secret engine, data had to be stored manually.</li> <li>By contrast, certain engines work based on dynamic secrets - these do not exist until a request comes in to generate them.</li> <li>A typical use case is when requesting access to a database. If integrated, users can send a request to HashiCorp Vault to generate a username and password dynamically to the user for testing purposes.</li> <li>These secrets can be set to expire for enhanced security measures.</li> <li>Another use case is using the AWS secrets engine to generate temporary AWS credentials e.g. access and secret keys.</li> </ul>"},{"location":"certifications/vault-associate/02_Getting-Started-with-Vault.html#example-aws-secret-engine","title":"Example - AWS Secret Engine","text":"<ul> <li>Assuming a user has created a role via the AWS secret engine - if a developer navigates to the role under the AWS secret engine and selects \"generate\" - a set of dynamic AWS credentials are created for the user and is added to \"your\" AWS account under IAM.</li> </ul> <ul> <li>As part of the generated credentials it is worth noting:</li> <li>Authorisation keys (access and secret)</li> <li>Lease ID - Unique Identifier for the Leased Credentials</li> <li>Renewable - Can a user make multiple requests to this engine for credentials?</li> <li>Lease Duration - How long do the credentials last?</li> <li>Vault will automatically revoke the dynamic credentials upon the completion of the lease duration - alternatively it can be revoked manually by the UI under Access \u2192 Leases.</li> </ul>"},{"location":"certifications/vault-associate/02_Getting-Started-with-Vault.html#207-generating-aws-credentials-dynamically","title":"2.07 - Generating AWS Credentials Dynamically","text":""},{"location":"certifications/vault-associate/02_Getting-Started-with-Vault.html#example-generating-aws-credentials-dynamically-from-scratch","title":"Example - Generating AWS Credentials Dynamically from Scratch","text":"<ul> <li>From the home screen in the UI, select \"Create New Engine\" and select \"AWS\" from it.</li> <li>Leave the path and other options as default for now and select \"Enable Engine\".</li> <li>Options will now be available for use - roles and configurations.</li> <li>Roles - What access level does the credentials requested for generation allow?</li> <li>Select \"Create Role\" and assign a role name as required.</li> <li>In terms of policy, you can select any policy listed under IAM \u2192 Access Management \u2192 Policies in the AWS Console, or write your own.</li> </ul> <ul> <li>Opening the policy, copy the policy (JSON Format) into Vault and modify it accordingly.</li> </ul> <ul> <li>Once created, Vault users can use that role to generate a set of dynamic credentials, which will have the permissions assigned based on the policy defined, in this case, full access to all EC2 instances.</li> <li>To allow this, one needs to create a user to act as a service account for Vault.</li> <li>Add user titled e.g. vaultuser</li> <li>Set access type to \"AWS Management Console Access\" and create the user</li> <li>Note the password and attach \"AdministratorAccess\" policy</li> <li>Generate Access and Secret Access Keys for the vaultuser account</li> <li>In Vault, under Configuration \u2192 Configure AWS, add the Access and Secret Keys associated with the Vault user.<ul> <li>Configure the region and IAM endpoint as required.</li> </ul> </li> <li>Now any developer wanting access credentials with the associated role simply has to select \"Generate\" on the role and will be provided with temporary dynamic Access and Secret keys for AWS.</li> <li> <p>Vault will use the user account to create the temporary user with the assigned policy.</p> <p></p> </li> </ul>"},{"location":"certifications/vault-associate/02_Getting-Started-with-Vault.html#208-managing-leases","title":"2.08 - Managing Leases","text":""},{"location":"certifications/vault-associate/02_Getting-Started-with-Vault.html#overview-of-lease","title":"Overview of Lease","text":"<ul> <li>With every dynamic secret and service type authentication token, Vault will create a lease.</li> <li>Leases are metadata containing information regarding how the credentials are used e.g. time to live (TTL), renewability, etc.</li> <li>Once the lease expires, Vault can automatically revoke the data such that the secret can no longer be used.</li> <li>Dynamic credentials can be manually revoked regardless.</li> <li>To view leases, in Vault UI navigate to Access \u2192 Leases</li> <li>From here, you can navigate to the lease associated with the role or secret engine that you wish to view or revoke.</li> <li>The lease can easily be revoked by selecting \"revoke lease\"</li> <li>Lease TTL can be configured with two options:</li> <li>Default TTL (seconds)</li> <li>Maximum TTL (seconds)</li> </ul> <p>Note: When creating a role and noting a policy, you can also copy the policy ARN rather than the json policy.</p> <ul> <li>Prior to a Vault Lease Expiring you are able to request renewal of the lease either via the UI (selecting \"renew lease\" when viewing the lease\") or running the following command:</li> </ul> <pre><code>vault lease renew -increment=&lt;time in seconds&gt; path/to/lease\n</code></pre> <ul> <li> <p>This will extend the lease duration by the increment defined.</p> </li> <li> <p>Note: The default and maximum TTLs will vary depending on the organization, but they can be customised to suit and renewed via the methods above.</p> </li> <li> <p>If a user requests a renewal longer than the maximum TTL, the request will be denied.</p> </li> </ul>"},{"location":"certifications/vault-associate/02_Getting-Started-with-Vault.html#209-path-based-revocation","title":"2.09 - Path-Based Revocation","text":"<ul> <li>When revoking leases in HashiCorp Vault, there are typically two main methods:</li> </ul> Command Description <code>vault lease revoke &lt;lease id&gt;</code> Revoke a lease of particular ID <code>vault lease revoke -prefix aws/</code> Revoke all AWS access keys - substitute AWS with the prefix associated with the desired secret engine <ul> <li>Lease IDs are structured in a way such that their prefix is always the path where the secret was requested from - this allows trees of secrets to be revoked by prefixes.</li> </ul>"},{"location":"certifications/vault-associate/02_Getting-Started-with-Vault.html#210-transit-secret-engine","title":"2.10 - Transit Secret Engine","text":""},{"location":"certifications/vault-associate/02_Getting-Started-with-Vault.html#overview","title":"Overview","text":"<ul> <li>Many applications require proper encryption/decryption functionalities</li> <li>Building the custom logic to handle this can add excessive workloads to application developers, particularly if they lack expertise in this area.</li> <li>Vault has a functionality to support this in its transit secrets engine - which handles cryptographic functions on data-in-transit.</li> <li>Vault doesn't store the data sent to the secrets engine, so it can also be viewed as encryption as a service.</li> <li>Therefore, instead of developing and managing cryptographic-related operations, application developers can get Vault to cover this.</li> </ul> <ul> <li>Based on the above, one can see:</li> <li>Apps can send data to vault to be encrypted prior to storage in databases.</li> <li>The app can then retrieve the encrypted data from the database and request decryption via Vault</li> <li>Once the transit secrets engine is enabled, you are required to create an encryption key to facilitate cryptographic operations.</li> </ul> <ul> <li>The primary use cases will be encrypt and decrypt.</li> <li>Selecting encrypt, you can easily enter any text to be encrypt and be provided ciphertext</li> </ul> <ul> <li>This data will not be stored by the Vault - to decrypt, provide the ciphertext provisioned during encryption.</li> <li>To do this via the CLI:</li> <li><code>vault write transit/encrypt/&lt;key-name&gt; plaintext=&lt;base64 / encoded text&gt;</code></li> <li>The ciphertext is then provided and must be stored safely outside the vault - otherwise the data cannot be decrypted</li> <li>To decrypt: <code>vault write transit/decrypt/&lt;key-name&gt; ciphertext=&lt;cipher key&gt;</code></li> </ul>"},{"location":"certifications/vault-associate/02_Getting-Started-with-Vault.html#dealing-with-larger-data-blobs","title":"Dealing with larger Data Blobs","text":"<ul> <li>When data is encrypted, the encryption key to encrypt plaintext is referred to as a data key.</li> <li>This data key needs to be protected so that the encrypted data cannot be decrypted easily by an unauthorized party.</li> <li>When the data is large, naturally we wouldn't want to send it over traditional network means for encryption/decryption operations as this would increase latency.</li> <li>The transit engine allows generation of a data key that ca be used locally for encryption and decryption operations.</li> </ul> <ul> <li>The data key can now be used for encryption operations, the ciphertext can then be used for decryption operations with vault, submitting the ciphertext to obtain the plaintext decryption key.</li> <li>Best practices:</li> <li>Whenever generating a data key in plaintext - the response contains the plaintext of the data key as well as its ciphertext</li> <li>Use the plaintext to encrypt the large data and store the ciphertext in the desired location e.g. key/value secrets engine.</li> <li>When the blob requires decryption, request Vault to decrypt the ciphertext of the data key - allowing you to get the plaintext back for local decryption.</li> </ul>"},{"location":"certifications/vault-associate/02_Getting-Started-with-Vault.html#important-features","title":"Important Features","text":""},{"location":"certifications/vault-associate/02_Getting-Started-with-Vault.html#key-rotation","title":"Key Rotation","text":"<ul> <li>It's not recommended to encrypt all data with the same encryption keys for obvious reasons.</li> <li>Transit Engine allows a rotation of the encryption key.</li> <li>Vault maintains the versioned keyring and the vault operator can decide the minimum version allowed for decryption operations e.g. only the latest could be allowed.</li> <li>When creating an encryption key, the format will always be <code>vault:v&lt;number&gt;:&lt;ciphertext&gt;</code> - the number following the v denotes the version.</li> <li>To rotate the encryption key - simply select \"rotate encryption key\" - for future encryption options you will then be allowed to select which version you wish to encrypt the data with.</li> </ul>"},{"location":"certifications/vault-associate/02_Getting-Started-with-Vault.html#minimum-decrypt-version","title":"Minimum Decrypt Version","text":"<ul> <li>As multiple versions of encryption keys appear, this leads to an increased likelihood of working keys being obtained for decryption in the event of an attack.</li> <li>Using the min_decryption_version setting we can plan on what data can get decrypted.</li> <li>As a result of this, found ciphertext to obsolete data cannot be decrypted, but in an emergency, the min_decryption_version can be moved back to allow for legitimate decryption.</li> <li>You can configure the minimum decryption and encryption versions by editing the encryption key.</li> </ul>"},{"location":"certifications/vault-associate/02_Getting-Started-with-Vault.html#rewrapping-data","title":"Rewrapping Data","text":"<ul> <li>When a key is rotated, Vault can allow encrypted data to be rewrapped.</li> <li>The Vault can send the data encrypted with an older version of the key to have it re-encrypted with the latest version</li> </ul> <ul> <li>This will produce a new ciphertext for the data in accordance with the required key version - allowing you to successfully decrypt the data.</li> </ul>"},{"location":"certifications/vault-associate/02_Getting-Started-with-Vault.html#211-totp-secrets-engine","title":"2.11 - TOTP Secrets Engine","text":"<ul> <li>TOTP = Time-Based One-Time-Passwords</li> <li>These are passwords that typically expire within 30, 60 seconds, etc.</li> <li>An example of this is Google Authenticator.</li> <li>Typically these TOTPs will regenerate with new values after the set timeframe.</li> <li>Vault offers both generation and provision services for TOTPs.</li> <li>To create, simply select TOTP from the list of secrets engines available. Note that you cannot now select it to open.</li> </ul> <ul> <li>For usage e.g. for a barcode:</li> <li> <p>Generate data for bar code:</p> <ul> <li><code>vault write --field=barcode totp/keys/&lt;keyname&gt; generate=true issuer=vault account_name=&lt;user&gt;</code></li> <li> <p>The resultant output should follow - note that you must have the VAULT_ADDR environment variable for this to work</p> <p></p> </li> </ul> </li> <li> <p>Write the data to a file, decode the data and store it in an image file:     <code>cat &lt;file containing code&gt; | base64 -d &gt; totp.jpg</code></p> </li> <li>This will result in a barcode data that can be used by authentication apps like google authenticator.</li> <li> <p>The code can also be read from the Vault itself:</p> <p></p> </li> </ul>"},{"location":"certifications/vault-associate/02_Getting-Started-with-Vault.html#212-pki-secrets-engine","title":"2.12 - PKI Secrets Engine","text":"<ul> <li>Disclaimer - PKI is a huge topic in itself, however for the purposes of the certification, you can only consider it at a high level.</li> <li>Certificate Authority:</li> <li>Any entity that issues digital certificates</li> <li>Both the receiver and sender have a trusted relationship with the CA during the process.</li> <li>The certificate authority validates the address/domain being accessed and the users accessing the domain.</li> <li>Typically, CAs work via the following process:<ol> <li>Users/Entities generate a Certificate Signing Request (CSR)<ol> <li>Generate public/private keys</li> <li>Generate CSR and have it signed with a private key</li> </ol> </li> <li>Submit the CSR to the CA to get it signed</li> </ol> </li> <li>The above steps are typically achieved via the openssl tool:</li> </ul> <pre><code>## Create base directory for certificates and keys\nmkdir /root/certificates\ncd /root/certificates\n\n## Create a private key for the CA\nopenssl genrsa -out ca.key 2048\n\n## Create a CSR\nopenssl req -new -key ca.key -subj \"/CN-KUBERNETES-CA\" -out ca.csr\n\n## Self-sign the CSR\nopenssl x509 -req -in ca.csr -signkey ca.key -CAcreateserial -out ca.crt -days 1000\n</code></pre> <ul> <li>The PKI Secrets Engine Provided by HashiCorp Vault aims to simplify this process by generating dynamic X509 certificates.</li> <li>This removes the need for manual generation of certificates as outlined in the steps above - Vault acts as the CA.</li> <li>PKI Secrets Engines can be implemented easily enough via the Vault UI. Once done, selecting \"issue certificate\" and the common name will allow generation of the certificate.</li> </ul>"},{"location":"certifications/vault-associate/02_Getting-Started-with-Vault.html#benefits-of-pki-in-vault","title":"Benefits of PKI In Vault","text":"<ul> <li>Vault can act as an Intermediate CA i.e. a liaising certificate authority between the identity certificate authority and a third-party root CA.</li> <li>It reduces or eliminates certificate revocations</li> <li>Reduces time to get certificate by eliminating the need to generate a private key and a CSR.</li> </ul>"},{"location":"certifications/vault-associate/03_Authentication-and-Authorization.html","title":"3.0 - Authentication and Authorization","text":""},{"location":"certifications/vault-associate/03_Authentication-and-Authorization.html#301-authentication-in-vault","title":"3.01 - Authentication in Vault","text":""},{"location":"certifications/vault-associate/03_Authentication-and-Authorization.html#notes","title":"Notes","text":"<ul> <li>Authentication is the process of recognizing a user's identity. It's typically based on various approaches e.g. username/passwords or tokens.</li> <li>In Vault it's no different - you have multiple ways to authenticate, including:</li> <li>Token</li> <li>Username</li> <li>JWT</li> <li>GitHub</li> <li>Any of these authentication methods must be matched when authenticating to Vault. You can however control what authentication methods are usable via the Access tab in the UI.</li> </ul> <ul> <li>Based on the above screenshot, one can see that only token-based authentication is allowed for this Vault instance.</li> <li>Selecting \"Enable an Authentication method\" - you are presented with a multitude of other options, including JWT and TLS Certificates as discussed, but as well as:</li> <li>AWS</li> <li>Azure</li> <li>GitHub</li> <li>Kubernetes</li> <li>Once enabled, each authentication method can be configured for various users e.g. in username and password, you can generate new credentials on the fly:</li> </ul> <ul> <li>To authenticate as the user in the CLI:</li> </ul> <pre><code>vault login -method=userpass username=admin password=password\nvault login -method=&lt;method path&gt; &lt;parameters&gt;\n</code></pre> <ul> <li>Once authenticated, you will be presented with details regarding the user's access credentials e.g. token, token duration, policies assigned, etc.</li> <li>Note: any tokens displayed will be stored in the <code>token helper</code> - this will prevent the need to re-provide the token.</li> </ul>"},{"location":"certifications/vault-associate/03_Authentication-and-Authorization.html#302-overview-of-vault-policies","title":"3.02 - Overview of Vault Policies","text":""},{"location":"certifications/vault-associate/03_Authentication-and-Authorization.html#vault-policies-01","title":"Vault Policies 01","text":""},{"location":"certifications/vault-associate/03_Authentication-and-Authorization.html#introduction","title":"Introduction","text":"<ul> <li>By default, any users created / assigned to authentication methods doesn't have any/many capabilities. Permissions come as a result of authorization in Vault, which is determined by Vault policies.</li> <li>Examples of policies include:</li> <li>Read from secret/</li> <li>Read and write from secret/</li> <li>Read, write, update and delete from secret/, as well as work with auth methods</li> </ul>"},{"location":"certifications/vault-associate/03_Authentication-and-Authorization.html#predefined-policies","title":"Predefined Policies","text":"<ul> <li>When first initialized, Vault creates a root policy by default and assigns it to the root / superuser allowing them to do anything in Vault; including setup additional policies and authentication methods.</li> <li>Another policy titled default is also created upon initialization - this is attached to all tokens and provides minimum common permissions.</li> </ul> <p>Policies can be easily viewed from the top toolbar (above).</p>"},{"location":"certifications/vault-associate/03_Authentication-and-Authorization.html#basics-of-policy-writing","title":"Basics of Policy Writing","text":"<ul> <li>Policies are written in HCL format, referred to as ACL policies.</li> <li>Typically, policies are written based on secret engine paths. Admins write policies to allow/deny certain paths and operations.</li> <li>Example:</li> </ul> <pre><code>path \"auth/*\"\n{\n capabilities = [\"create\", \"read\", \"update\", \"delete\", \"list\", \"sudo\"]\n{\n</code></pre> <ul> <li>If a policy capabiloities list is empty, the user cannot do anything</li> </ul> <pre><code>path \"&lt;path&gt;\" {\n capabilities = [ \"capability 1\", \"capability 2\", ..... , \"capability N\"]\n}\n</code></pre>"},{"location":"certifications/vault-associate/03_Authentication-and-Authorization.html#primary-capabilities","title":"Primary Capabilities","text":"<ul> <li>Create</li> <li>Read</li> <li>Update</li> <li>Delete</li> <li>List</li> <li>Sudo</li> <li>Deny</li> </ul>"},{"location":"certifications/vault-associate/03_Authentication-and-Authorization.html#example","title":"Example","text":"<ul> <li>Ensuring a user has been created, authenticate to Vault via the userpass method</li> </ul> <pre><code>vault login -method=userpass username=admin password=password\nvault login -method=&lt;method path&gt; &lt;parameters&gt;\n</code></pre> <ul> <li>When authenticated, this user should have \"default\" as the policy. To verify, run <code>vault secrets list</code> - this should provide an error as it goes against the default policy.</li> <li>Typically the error will be of the form <code>URL &lt;REQUEST TYPE&gt; &lt;vault address&gt;/version/path</code></li> <li> <p>To fix this, one can create a new policy. Navigate to policies and select \"Create ACL Policy\" from the menu - you will be presented with an editor to write an ACL policy.</p> <p></p> <p>\"Create New ACL Policy\" Menu</p> </li> <li> <p>Write the policy as desired, in this case, we just want to allow \"read\" operations on a particular path</p> </li> </ul> <p></p> <ul> <li>The policy must then be assigned the policy(ies). To do so, navigate to the user under \"access\" and select \"edit user\".</li> <li> <p>Under \"Generated Token's Policies\", add the name of the policy(ies) you which to attach, then click save.</p> </li> <li> <p>Note: Once the policy is assigned to the user, it will NOT automatically work. You must re-authenticate via the vault login command to generate a new token for the user.</p> </li> <li> <p>Re-authenticate the user and verify the user has the capability(ies) required.</p> </li> <li> <p>Repeat this process for any other issues / capability problems as required for the user.</p> </li> <li> <p>Note: Once a policy is assigned to a user, any changes made to it will automatically apply to that user.</p> </li> <li> <p>In some cases, there may be an unexpected change to the path e.g. <code>secret/data/secret1</code>  to fix this, the path in the policy could be changed to <code>secret/*</code> - using the * as a wildcard operator.</p> </li> <li>This poses a security risk - as multiple secrets may be under the path <code>secret/</code> that users shouldn't have access to.</li> <li>To fix, either explicitly define the secret path provided i.e. <code>full path to secret</code>  OR add an additional policy to deny access to the other secret(s), an example follows:</li> </ul> <pre><code>path \"secret/*\" {\n  capabilities = [\"read\"]\n}\n\npath \"secret/data/&lt;secret not to be seen&gt;\" {\n  capabilities = [\"deny\"]\n}\n</code></pre> <ul> <li>Note: If you did not wish to add the path information between the endpoints of the path, such as <code>data</code> in the above example, you can replace it with <code>+</code> , this will automatically apply to any secrets ending in the secret path prefix and suffix.</li> </ul>"},{"location":"certifications/vault-associate/03_Authentication-and-Authorization.html#root-protected-api-endpoints","title":"Root-Protected API Endpoints","text":"<ul> <li>Some paths are more restrictive than others, requiring root token or sudo capabilities in the policy to allow the operations. Examples include:</li> <li>auth/token/accessors</li> <li>auth/token</li> <li>sys/audit</li> <li>sys/rotate</li> <li>sys/seal</li> </ul>"},{"location":"certifications/vault-associate/03_Authentication-and-Authorization.html#vault-policies-02","title":"Vault Policies 02","text":""},{"location":"certifications/vault-associate/03_Authentication-and-Authorization.html#acl-rules-format-kv-secret-engine-version-2","title":"ACL Rules Format - KV Secret Engine Version 2","text":"<ul> <li>The version 2 key-value store uses a prefixed API that differs from version 1</li> <li>Writing and reading versions are prefixed with the <code>data/</code> path.</li> <li>Example:</li> </ul> Path KV Version 1 KV Version 2 /secret/first <code>path \"secret/first\" { capabilities = [\"create\"] }</code> <code>path \"secret/data/first\" { capabilities = [\"create\"] }</code>"},{"location":"certifications/vault-associate/03_Authentication-and-Authorization.html#practical-example-acl-policies","title":"Practical Example - ACL Policies","text":"<ul> <li>Create a policy for KV Version 1</li> </ul> <ul> <li>Additionally, create a test-user to test the policy out. Be sure to assign the policy to the user under <code>token-generated policy</code></li> </ul> <ul> <li>Authenticate to the Vault via the CLI for the user</li> </ul> <pre><code>vault login -method=userpass username=\"demo-user\" password=\"demo-password\"\n</code></pre> <ul> <li>As expected, the authentication works, and the policy is shown to be attached to the user:</li> </ul> <ul> <li>Create a secret under the key-value secret engine for version 2 to test</li> </ul> <ul> <li>Test the policy: <code>vault kv get secret/demo01</code></li> </ul> <ul> <li>As expected, the get request is denied in line with the policy. Note the path is <code>/secret/data/demo01</code> as the secret is under key vault version 2.</li> <li>Editing the policy to add the /data prefix will fix this.</li> </ul>"},{"location":"certifications/vault-associate/03_Authentication-and-Authorization.html#listing-secrets","title":"Listing Secrets","text":"<ul> <li>The <code>metadata/</code> endpoint returns a list of key names at the specified location.</li> <li>Any input for this must be a folder</li> <li>The values aren't accessible via this command.</li> </ul> Path KV Version 2 <code>/secret/</code> <code>path \"secret/metadata/\" { capabilities =[\"list\"] }</code>"},{"location":"certifications/vault-associate/03_Authentication-and-Authorization.html#practical-example-listing-secrets","title":"Practical Example - Listing Secrets","text":"<ul> <li>Add some more secrets to the key-value secrets engine</li> </ul> <ul> <li>If the user logs into the UI, they would not be able to see the contents of these secrets, but they CAN see the names of the secrets if they are allowed access to the metadata endpoint</li> <li>Add the required information to the policy</li> </ul> <ul> <li>This can then be tested in the UI by logging in as the user and verifying if you can <code>list</code> the secrets but not <code>read</code> them.</li> </ul>"},{"location":"certifications/vault-associate/03_Authentication-and-Authorization.html#reading-secret-metadata","title":"Reading Secret Metadata","text":"<ul> <li>To retrieve the metadata and versions for the secret at the specified path:</li> </ul> Path KV Version 2 <code>/secret/firstsecret</code> <code>path \"secret/metadata/firstsecret\" { capabilities = [\"read\"] }</code>"},{"location":"certifications/vault-associate/03_Authentication-and-Authorization.html#summary","title":"Summary","text":"Operation Path [Capability(ies)] Writing and Reading Versions /data Listing Keys metadata/ [list] Reading versions metadata [read] Destroy versions of secret destroy/ [update] Destroy all versions of metadata for key metadata/ [delete] <p>Examples for the latter two:</p> <p></p>"},{"location":"certifications/vault-associate/03_Authentication-and-Authorization.html#303-approle-authentication-method","title":"3.03 - AppRole Authentication Method","text":"<ul> <li>Before a client can interact with Vault, it must authenticate against a particular auth method as outlined previously.</li> <li>Auth methods are generally targeted for one of two types of users:</li> <li>Human users e.g. userpass</li> <li>Machine/App users</li> <li>Once authenticated, a token is generated, which may have a policy associated.</li> <li>For machine/app users, the most common one is AppRole.</li> <li>This allows multiple \"roles\" to be defined corresponding to different applications, each with different access levels e.g. one role for MySQL or a database application, another for a particular CI/CD application e.g. Jenkins.</li> <li>When authenticating via the AppRole method, applications will need to take note of:</li> <li>Role ID</li> <li>Secret ID</li> </ul>"},{"location":"certifications/vault-associate/03_Authentication-and-Authorization.html#configuring-approle-authentication-method","title":"Configuring AppRole Authentication Method","text":"<ol> <li>Create policy for role and apps</li> <li>Get Role ID</li> <li>Generate new Secret ID</li> <li>Provide Role and Secret ID to application</li> <li>Application authenticates with the provided role and secret IDs</li> <li>Authentication token is returned.</li> </ol> <p>To enable:</p> <ol> <li>Enable AppRole authentication method under \"access\"</li> <li>Under Policies, create a desired policy for the application e.g. allowing all non-destructive capabilities to the application</li> <li>Create the role: <code>vault write /auth/approle/role/&lt;name&gt; token_policies=\"&lt;role-name&gt;\"</code></li> <li>Test via <code>vault read auth/approle/role/&lt;rolename&gt;</code></li> <li>Read the role ID for provisioning<code>vault read auth/approle/role/&lt;rolename&gt;/role-id</code></li> <li>Generate the new secret ID: <code>vault write -f auth/approle/&lt;role&gt;/secret-id</code></li> <li>Using the IDs generated in steps 5 and 6, authenticate to vault: <code>vault write auth/approle/login role_id=\"\" secret_id=\"\"</code></li> <li>Verify the login operation is successful, a token is provisioned and the required policies are provided.</li> </ol> <ul> <li>The AppRole auth method is specifically designed for use by machines and applications.</li> <li>Role and Secret IDs can effectively be viewed as the application's username and passwords</li> </ul>"},{"location":"certifications/vault-associate/03_Authentication-and-Authorization.html#304-http-apis-in-vault","title":"3.04 - HTTP APIs in Vault","text":"<ul> <li>All of Vault's capabilities are accessible via the HTTP API.</li> <li>Most CLI commands invoke the HTTP API, however, some Vault features can only be accessed via the HTTP API.</li> <li>Tools such as <code>cURL</code> can be used to make calls to the HTTP API. This requires the use of a client token, settable via the X-Vault-Token HTTP Header.</li> <li>Example: <code>curl -h \"X-Vault-Token: &lt;token&gt; -X GET http://127.0.0.1:8200/v1/secret/foo</code></li> <li>All API Routes are prefixed with v1 bar a few documented exceptions.</li> <li> <p>All response data from Vault is via JSON.</p> </li> <li> <p>The HTTP request types correspond to a particular Vault capability or operation</p> Capability HTTP Request Type create POST/PUT list GET update POST/PUT delete DELETE list LIST </li> </ul> <ul> <li>Sample requests are well-documented for multiple secret types at the following link</li> </ul>"},{"location":"certifications/vault-associate/03_Authentication-and-Authorization.html#305-token-capabilities","title":"3.05 - Token Capabilities","text":"<ul> <li>Users can check the capabilities of a token for a particular path using the <code>token capabilities</code> command</li> <li>Example:</li> <li><code>vault token capabilities sys</code></li> <li><code>vault token capabilities &lt;path&gt;</code></li> <li>When using this, if you you do not explicitly specify a token, Vault will assume the token of the user making the request is the token to be checked.</li> <li>If a token is provided as an argument, the \"/sys/capabilities\" endpoint and permission is used.</li> <li>If no token is provided, the \"/sys/capabilities-self\" endpoint and permission is used with the locally authenticated token</li> <li>To provide the token as an argument, simply add the token prior to the <code>&lt;path&gt;</code></li> </ul>"},{"location":"certifications/vault-associate/03_Authentication-and-Authorization.html#306-entities-and-aliases","title":"3.06 - Entities and Aliases","text":""},{"location":"certifications/vault-associate/03_Authentication-and-Authorization.html#authentication-for-multiple-users","title":"Authentication for Multiple Users","text":"<ul> <li>Vault supports multiple authentication methods, as well as allowing the same type of authentication method for different mount paths.</li> <li>Each Vault client may have multiple accounts with various identity providers and are enabled on the Vault server.</li> <li>For example, a user may have an account each in Active Directory and GitHub</li> <li>Rather than have policies set up for each of these, they can be collated by usage of an entity and aliases.</li> <li>The aliases can be mapped to the particular entity as entity entries.</li> <li>Policies can now be set at entity level AND per account.</li> <li>Policies can be set to be inherited from entity level - any account added to the entity as an entity member will automatically have the particular policy applied.</li> <li>To create an entity: Access \u2192 Entities</li> <li>ENSURE A SUFFICIENT USER AUTH METHOD IS ENABLED</li> <li>Entities \u2192 Create Entity<ul> <li>If there are any policies already existing, add them to the entity.</li> <li>It may be advisable to have a policy per user account.</li> <li>Select create</li> </ul> </li> <li>Add Aliases:</li> <li>entities \u2192 create alias<ul> <li>Provide alias name and auth backend per alias/entry</li> </ul> </li> <li>To test:</li> <li><code>vault login -method=userpass username=bob password=password</code><ul> <li>Entity policy, alias policy should be viewable under policies</li> <li>Under identity_policies, the entity policy should be listed</li> </ul> </li> <li><code>vault login -method=userpass username=bsmith password=password</code><ul> <li>The entity policy should still be listed under identity_policies, however the policies list should be specific to the account/alias.</li> </ul> </li> </ul>"},{"location":"certifications/vault-associate/03_Authentication-and-Authorization.html#the-identity-secret-engine","title":"The Identity Secret Engine","text":"<ul> <li>This maintains the clients recognised by Vault</li> <li>Each client is internally termed as an entity, which can have multiple aliases</li> <li>This engine is mounted by default, and cannot be disabled or moved.</li> </ul>"},{"location":"certifications/vault-associate/03_Authentication-and-Authorization.html#307-identity-groups","title":"3.07 - Identity Groups","text":""},{"location":"certifications/vault-associate/03_Authentication-and-Authorization.html#identity-groups","title":"Identity Groups","text":"<ul> <li>Groups can contain multiple entities as members</li> <li>Policies set at group level will be applied to all members of the group i.e. all entities and the associated aliases</li> <li>I.e. all users and all associated accounts as part of the group will have the policies be applied</li> </ul>"},{"location":"certifications/vault-associate/03_Authentication-and-Authorization.html#demonstration","title":"Demonstration","text":"<ul> <li>Assuming a entity has already been created (refer to section 3.06), create a new policy to be applied to the team e.g. allow read capabilities to all secrets under the secret/ path</li> <li>From Access \u2192 Groups, select \"Create Group\"</li> <li>Provide the group name</li> <li>Select type (internal or external)</li> <li>Select any policy(ies) to be associated with the group</li> <li>Add the member group IDs or the Entity IDs to be added to the group</li> <li>Create</li> <li>To verify, login as a user as part of the group using <code>vault login</code></li> <li>The group policy should be included under the key <code>identity_policies</code> AND the <code>policies</code> key</li> </ul>"},{"location":"certifications/vault-associate/03_Authentication-and-Authorization.html#internal-and-external-groups","title":"Internal and External Groups","text":"<ul> <li>By default, Vault creates an internal group</li> <li>Many organizations have groups predefined with their external identity providers, such as Active Directory</li> <li>External groups allows these providers to be linked to Vault via the <code>external identity</code> provider (auth provider) such that appropriate policies can be attached to the group</li> </ul>"},{"location":"certifications/vault-associate/03_Authentication-and-Authorization.html#308-tools-in-vault","title":"3.08 - Tools in Vault","text":""},{"location":"certifications/vault-associate/03_Authentication-and-Authorization.html#vault-tools","title":"Vault Tools","text":"<ul> <li>Vault contains tools allowing specific functions</li> <li>They are available at the <code>/sys/tools</code> endpoint</li> <li>Examples of tools include:</li> <li>Wrap</li> <li>Lookup</li> <li>Unwrap</li> <li>Rewrap</li> <li>Random - Used to generate a secret of random bytes of a particular size and format</li> <li>Hash - Used to hash data using a particular format and output it as a particular format<ul> <li>Note, sometimes input data may need to be encoded as base64 before hashing</li> </ul> </li> <li>Random can be called via a POST request to <code>/sys/tools/random/{bytes number}</code></li> <li>Hash can be called via a POST request to <code>/sys/tools/hash/{algorithm}</code></li> <li>Algorithm may be any of <code>sha2-224</code>, <code>sha2-256</code>, and so on.</li> <li>Additional details are available in the documentation, along with sample curl requests.</li> <li>Documentation.</li> <li>Common exam questions are \"what does X Call aim to achieve?\"</li> </ul>"},{"location":"certifications/vault-associate/03_Authentication-and-Authorization.html#309-vault-auto-complete","title":"3.09 - Vault Auto-Complete","text":""},{"location":"certifications/vault-associate/03_Authentication-and-Authorization.html#auto-completion-overview","title":"Auto-Completion Overview","text":"<ul> <li>Vault auto-complete allows automatic completion for input flags, subcommands and arguments in the vault CLI</li> <li>This would be achieved by pressing <code>[TAB]</code> when writing any commands like <code>seal</code></li> <li>Note, this is is very similar to the autocomplete feature in Linux.</li> <li>To enable autocomplete, simply run the following command: <code>vault -autocomplete-install</code></li> <li>Note: When a command hasn't been written following <code>vault &lt;x&gt;</code>, the autocomplete would provide options for the command.</li> </ul>"},{"location":"certifications/vault-associate/03_Authentication-and-Authorization.html#310-acl-policy-path-templating","title":"3.10 - ACL Policy Path Templating","text":""},{"location":"certifications/vault-associate/03_Authentication-and-Authorization.html#contexts","title":"Contexts","text":"<ul> <li>Consider <code>/secret</code> has two prefixes for two users:</li> <li><code>/alice</code></li> <li><code>/bob</code></li> <li>Each user should only be able to view what is available to them at their particular path i.e. a policy would be required for each:</li> </ul> <pre><code>path \"secret/data/alice/\" {\n capabilities = [\"create\", \"update\", \"delete\"]\n}\n\npath \"secret/data/bob/\" {\n capabilities = [\"create\", \"update\", \"delete\"]\n}\n</code></pre> <ul> <li>For two users only, this is easy to manage, however if there are hundreds of users it will be pointless to write effectively the same policy over and over.</li> <li>Path Templating can be used to help here.</li> </ul>"},{"location":"certifications/vault-associate/03_Authentication-and-Authorization.html#path-templating-overview","title":"Path Templating Overview","text":"<ul> <li>Path templating allows variable replacement based on information provided by the entity associated.</li> </ul> <pre><code>path \"secret/data/{{identity.entity.name}}/*\" {\n capabilities = [\"create\", \"update\", \"delete\"]\n}\n</code></pre> <ul> <li>So suppose this policy is assigned to an entity which the <code>alice</code> and <code>bob</code> users are part of, <code>{{identity.entity.name}}</code> would be replaced by <code>alice</code> or <code>bob</code>.</li> </ul>"},{"location":"certifications/vault-associate/03_Authentication-and-Authorization.html#practical-example","title":"Practical Example","text":"<ol> <li>Generate a sample policy to <code>\"secret/data/{{identity.entity.name}}/*</code><ol> <li>Add <code>secret/metadata</code> as well if desired</li> </ol> </li> <li>Create a sample user set e.g. Alice and Bob via the desired authentication method (use userpass for ease)</li> <li>Create an entity for each user and attach the template policy from step 1 to each, then attach the user alias</li> <li>Verify the configuration via <code>vault login</code> for each of the users.</li> <li>Test the capabilities for each user by creating a new secret at <code>/secret/....</code><ol> <li>Alice-user should only be able to review secrets under <code>/alice</code> and not <code>/bob</code> and vice versa</li> </ol> </li> <li>This will be applicable for any additional users created in this manner.</li> </ol>"},{"location":"certifications/vault-associate/03_Authentication-and-Authorization.html#supported-parameters","title":"Supported Parameters","text":"Name Description <code>identity.entity.id</code> Entity ID <code>identity.entity.name</code> Entity Name <code>identity.entity.metadata.&lt;metadata-key&gt;</code> Metadata associated with an entity for a given key <code>identity.entity.aliases.&lt;mount-accessor&gt;.id</code> Entity alias ID for a particular mount <code>identity.entity.aliases.&lt;mount-accessor&gt;.name</code> Entity alias name for particular mount <code>identity.entity.aliases.&lt;mount-accessor&gt;.metadata.&lt;metadada-key&gt;</code> Metadata associated with an alias for a given mount and metadata key <code>identity.groups.ids.&lt;groupid&gt;.name</code> Group name for a particular group ID <code>identity.groups.names.&lt;group name&gt;.id</code> Group ID for a particular group name <code>identity.groups.ids.&lt;group id&gt;.metadata.&lt;metadata key&gt;</code> Metadata associated with a particular group id for a particular metadata key <code>identity.groups.names.&lt;group name&gt;.metadata.&lt;metadata key&gt;</code> Metadata associated with a particular group name for a particular metadata key"},{"location":"certifications/vault-associate/03_Authentication-and-Authorization.html#311-vault-policy-transit-secret-engine","title":"3.11 - Vault Policy - Transit Secret Engine","text":""},{"location":"certifications/vault-associate/03_Authentication-and-Authorization.html#vault-policy-rules-transit-engine","title":"Vault Policy Rules - Transit Engine","text":"<ul> <li>For Vault clients to be able to perform encryption and decryption operations, two policy rules are required:</li> </ul> <pre><code>path \"transit/encrypt/&lt;key_name&gt;\" {\n  capabilities = [\"update\"]\n}\n\npath \"transit/decrypt/&lt;key_name&gt;\" {\n  capabilities = [\"update\"]\n}\n</code></pre>"},{"location":"certifications/vault-associate/03_Authentication-and-Authorization.html#practical-demo","title":"Practical Demo","text":"<ul> <li>Enable the transit secret engine</li> <li>Create an example encryption key</li> <li>Enable a suitable authentication method for a new user (create if required)</li> <li>Sign in to the Vault UI via the assigned method</li> <li>Note that based on the default policy, the user can't do much</li> <li>Via admin,  create a policy with the rules above and associate it with the sample user.</li> <li>Upon signing out and signing in, the sample user should now see the transit/ secret path</li> <li>The secrets won't be listable however, add a rule with the <code>list</code> capability to the path <code>transit/keys</code></li> <li>Still not done..... add another rule with the <code>read</code> capability to the path <code>/transit/keys/&lt;key-name&gt;</code></li> </ul>"},{"location":"certifications/vault-associate/04_Vault-Tokens.html","title":"4.0 - Vault Tokens","text":""},{"location":"certifications/vault-associate/04_Vault-Tokens.html#401-overview-of-vault-tokens","title":"4.01 - Overview of Vault Tokens","text":""},{"location":"certifications/vault-associate/04_Vault-Tokens.html#tokens-overview","title":"Tokens Overview","text":"<ul> <li>Tokens are the core method for authentication within Vault</li> <li>With <code>vault server -dev</code> the root token has been used</li> <li>This is the first method of authentication for Vault and the only auth method that cannot be disabled</li> </ul>"},{"location":"certifications/vault-associate/04_Vault-Tokens.html#mapping-of-tokens-to-policies","title":"Mapping of Tokens to Policies","text":"<ul> <li>Within Vault, tokens map to information. The key mapping noted is a set of one or more policies e.g. the default policy maps to all users.</li> </ul>"},{"location":"certifications/vault-associate/04_Vault-Tokens.html#practical","title":"Practical","text":"<ul> <li>Create a sample user under the userpass method e.g. demouser / password</li> <li>Under \"tokens\" you can select \"do not attach default policy\" - typically don't want it assigned anyways as it offers minimal privileges</li> <li>Can see upon login via <code>vault login -method=userpass username= password=</code> that no policy will be assigned</li> <li>Creating a sample policy via Policies \u2192 Create:</li> <li>Under the user the desired policy(ies) can be assigned under \"generated token's policies\"</li> <li>This can then be verified via re-logging in and attempting operations on any secrets specified</li> <li>Note: no policy changes can take effect unless the user with the token logs out and back in so a new token can be generated with the updated mapping.</li> </ul>"},{"location":"certifications/vault-associate/04_Vault-Tokens.html#lookup-token-information","title":"Lookup Token Information","text":"<ul> <li>Users can explore the details of a token with the command <code>vault token lookup</code></li> <li>This will work for the user logged in to Vault</li> <li>Note: to use - the read capabilities must be added to a policy for the path <code>auth/token/lookup-self</code></li> <li>Information provided include policies, paths, ttl</li> <li>For root-related ops:</li> <li><code>vault login &lt;root token&gt;</code></li> <li><code>vault token lookup</code></li> <li>For any other tokens - append the token desired to <code>vault token lookup</code></li> </ul>"},{"location":"certifications/vault-associate/04_Vault-Tokens.html#402-token-helper","title":"4.02 - Token Helper","text":""},{"location":"certifications/vault-associate/04_Vault-Tokens.html#token-helper","title":"Token Helper","text":"<ul> <li>By default, Vault CLI uses a \"token helper\" to cache any tokens used in authentication</li> <li>The tokens are stored at <code>~/.vault-token</code></li> <li>This can be deleted at any time to force-logout of Vault</li> </ul>"},{"location":"certifications/vault-associate/04_Vault-Tokens.html#403-tokens-time-to-live","title":"4.03 - Tokens Time-to-Live","text":""},{"location":"certifications/vault-associate/04_Vault-Tokens.html#overview","title":"Overview","text":"<ul> <li>Time-to-Live (TTL) defines the lifetime of data</li> <li>Extensively used in regards to DNS mappings</li> </ul>"},{"location":"certifications/vault-associate/04_Vault-Tokens.html#ttl-for-tokens","title":"TTL For Tokens","text":"<ul> <li>Every non-root token has a TTL</li> <li>After the TTL expires, the token will no longer function and any associated leases will be revoked</li> </ul>"},{"location":"certifications/vault-associate/04_Vault-Tokens.html#token-renewal","title":"Token Renewal","text":"<ul> <li>Achieved by using the <code>vault token renew</code> command whilst logged in as a particular user</li> <li>Example: <code>vault token renew &lt;token&gt;</code></li> <li>Additional options available such as <code>--increment=&lt;time&gt;</code></li> </ul>"},{"location":"certifications/vault-associate/04_Vault-Tokens.html#token-defaults","title":"Token Defaults","text":"<ul> <li>Where does the default 768hr TTL come from?</li> <li>This is defined via the vault secret at <code>sys/auth/token/tune</code></li> <li>Logging into root and reading this via <code>vault read sys/auth/token/tune</code>, the default lease TTL and maximum lease TTL is set to 768h</li> </ul>"},{"location":"certifications/vault-associate/04_Vault-Tokens.html#404-lifecycle-of-service-tokens","title":"4.04 - Lifecycle of Service Tokens","text":""},{"location":"certifications/vault-associate/04_Vault-Tokens.html#service-lifecycle-of-tokens","title":"Service Lifecycle of Tokens","text":"<ul> <li>When a token holder creates a new token e.g. you create a token, the subsequent token is created as a \"child\" of the original token.</li> <li>If the parent is revoked or expires, as will all its children regardless of their own TTLs.</li> <li>E.g. suppose token 1 is the parent of token 2.</li> <li>If token 1 is not renewed, it is revoked by Vault, and as a result token 2 will be revoked.</li> </ul>"},{"location":"certifications/vault-associate/04_Vault-Tokens.html#405-token-accessories","title":"4.05 - Token Accessories","text":"<ul> <li>When a token is created, a token accessor is also created and returned.</li> <li>This accessor is a value that acts as a reference to a token, and can only be used to carry out certain actions regarding it e.g.:</li> <li>Look up a tokens properties</li> <li>Look up a tokens capabilities for a particular path</li> <li>Renew the token</li> <li>Revoke the token</li> </ul>"},{"location":"certifications/vault-associate/04_Vault-Tokens.html#example-use-case","title":"Example Use Case","text":"<ul> <li>Accessors can be used to save central services from storing and managing multiple tokens used as they can be used to renew and revoke any tokens associated with the processes</li> <li>To view the accessors: <code>vault list auth/token/accessors</code></li> <li>For a particular token's accessors: <code>vault token lookup -accessor &lt;token accessor ID&gt;</code></li> <li>Note: Tokens cannot be generated by using the token accessor!</li> </ul>"},{"location":"certifications/vault-associate/04_Vault-Tokens.html#406-overview-of-orphaned-tokens","title":"4.06 - Overview of Orphaned Tokens","text":"<ul> <li>Orphan tokens are not children of any parent token - they therefore do not expire when the \"parent\" does</li> <li>They are the root of their own token tree, however orphan tokens still expire when their own max TTL is reached</li> <li>One can determine if a token is an orphan by checking the field of the same name when running a <code>vault token lookup</code> command</li> <li>To create an orphaned token:</li> <li><code>vault token create -orphan</code> as whichever user you desire that is capable of token creation.</li> <li> <p>Note: this would require a user with the following capabilities outlined in a policy:</p> <p>```go path \"auth/token/create\" {   capabilities = [\"create\", \"read\", \"update\", \"sudo\"] }</p> <p>path \"auth/token/lookup\" {   capabilities = [\"create\", \"read\", \"update\"] } ```</p> </li> </ul>"},{"location":"certifications/vault-associate/04_Vault-Tokens.html#407-cubbyhole-secret-engine","title":"4.07 - Cubbyhole Secret Engine","text":"<ul> <li>Cubbyhole secrets engine provides a private secret storage space unreadable by any other users (including root).</li> <li>One cannot reach into another token's cubbyhole, even as the root user.</li> <li>This secrets engine is enabled by default. It cannot be disabled/moved/enabled.</li> <li>In the cubbyhole secrets engine, paths are scoped per token.</li> <li>When the token is expired, the associated cubbyhole is destroyed.</li> <li>By DEFAULT - Cubbyhole secrets engine will already be in place for any user that has the default policy assigned.</li> <li>The default policy allows users to create/read/update/delete/list all secrets in their particular cubbyhole</li> <li>All operations in the cubbyhole user is facilitated using the user's particular token at the time of login.</li> <li>Suppose a user logs in and has a new token generated - that token will not be able to do anything with the secrets.</li> <li>Verify via <code>vault read cubbyhole/&lt;path&gt;</code></li> </ul>"},{"location":"certifications/vault-associate/04_Vault-Tokens.html#-one-must-use-the-same-token-used-creating-the-secret-eg-if-creating-it-in-the-ui-that-token-must-be-used-via-vault-login-token","title":"- One must use the same token used creating the secret e.g. if creating it in the UI, that token must be used via <code>vault login &lt;token&gt;</code>","text":""},{"location":"certifications/vault-associate/04_Vault-Tokens.html#408-response-wrapping","title":"4.08 - Response Wrapping","text":"<ul> <li>As outlined in AppRole usage i.e.</li> <li>Policy and role for app created</li> <li>Role ID and Secret ID are generated</li> <li>Role ID and Secret ID are passed to and used by the app to authenticate to Vault, which returns a token.</li> <li>Response Wrapping aims to secure this process. The steps are outlined as follows:<ol> <li>AppRole Auth Backend is mounted</li> <li>Policy and Role created for app</li> <li>Role ID received by Trusted Entity (Terraform, Kubernetes, Ansible, etc.)</li> <li>Role ID Delivered to app</li> <li>The Trusted Entity (Terraform, Kubernetes, etc.) receives the wrapped secret ID from Vault</li> <li>Vault returns the wrapping token to the trusted entity</li> <li>The trusted entity delivers the wrapping token to the app</li> <li>The app uses the unwrapping token to unwrap the secret ID stored in the Vault</li> <li>The app logs in via the Role ID and Secret ID</li> </ol> </li> </ul>"},{"location":"certifications/vault-associate/04_Vault-Tokens.html#working","title":"Working","text":"<ul> <li>When the response wrapping is requested, Vault creates a temporary single-use token (wrapping token)</li> <li>The wrapping token is inserted into the token's cubbyhole with a short TTL</li> <li>Only the expecting client who has the wrapping token can unwrap this secret.</li> <li>If the wrapping token is compromised and the attacker unwraps the secret, the application will not be able to unwrap again</li> <li>This can be used in conjunction with monitoring tools to implore admins to revoke the appropriate tokens.</li> </ul>"},{"location":"certifications/vault-associate/04_Vault-Tokens.html#in-practice","title":"In Practice","text":"<ul> <li>New tokens can be created by <code>vault token create</code></li> <li>This displays the token in plaintext</li> <li>Use <code>vault token create -wrap-ttl=&lt;time in seconds&gt;</code></li> <li>This displays the wrapping token that can be used to unwrap the secret associated</li> <li><code>vault unwrap &lt;wrapping token&gt;</code></li> </ul>"},{"location":"certifications/vault-associate/04_Vault-Tokens.html#409-overview-of-batch-tokens","title":"4.09 - Overview of Batch Tokens","text":"<ul> <li>Batch tokens are encrypted blobs to carry information to be used in Vault actions - but they do not require any on-disk storage to track them.</li> <li>With Vault Replcication enabled, the pressure on storage backend increases as the number of token or lease generation requests come in.</li> <li>As batch tokens don't require disk storage, making them very lightweight and scalable, they serve as a strong solution to the problem.</li> <li>The caveat is however, they lack a lot of flexibility in comparison to a standard service token.</li> </ul>"},{"location":"certifications/vault-associate/04_Vault-Tokens.html#analysis","title":"Analysis","text":"Feature Service Token Batch Token Can be root tokens Yes No Can create child tokens Yes No Renewable Yes No Periodic Yes No Can have particular max TTL Yes No (fixed TTL always) Has Accessors Yes No Has CubbyHole Yes No Revoked with Parent if not orphan Yes Stops Working Dynamic Secrets Lease Assignment Self Parent (if not orphan) Can be used across performance replication clusters No Yes (if orphan) Creation scales with performance standby node count No Yes Cost Heavy Weight - Multiple Storage Writes per token creation Lightweight - No storage cost for token creation"},{"location":"certifications/vault-associate/04_Vault-Tokens.html#batch-token-example","title":"Batch Token Example","text":"<ul> <li>To create a batch token, the <code>-type</code> flag is used to specify a <code>batch</code> token. Note that by default, if this is not provided a service token is provided.</li> <li>Example: <code>vault token create -type=batch -policy=default</code></li> </ul>"},{"location":"certifications/vault-associate/04_Vault-Tokens.html#410-token-ttl-configuration","title":"4.10 - Token TTL Configuration","text":""},{"location":"certifications/vault-associate/04_Vault-Tokens.html#recap","title":"Recap","text":"<ul> <li>Every non-root token has a Time to Live (TTL) - defining how long the token remains valid for.</li> <li>After the TTL expires, the token no longer functions and any associated leases are revoked.</li> </ul>"},{"location":"certifications/vault-associate/04_Vault-Tokens.html#ttl-dependency","title":"TTL Dependency","text":"<ul> <li>TTLs are dependent on a combination of multiple factors, such as:</li> <li>The system's max TTL - 32 days<ul> <li>This can be configured in Vaults configuration file.</li> </ul> </li> <li>The max TTL set on a mount using mount tuning</li> <li>A value suggested by the auth method issuing the token.</li> <li>The default and max lease ttl can be configured via the following: <code>vault write sys/mounts/auth/token/tune</code> and appending either or:</li> <li><code>default_lease_ttl=&lt;time&gt;</code></li> <li><code>max_lease_ttl=&lt;time&gt;</code></li> <li>To configure a new default and max lease TTL for an auth method, this can be configured during setup of the auth method</li> </ul>"},{"location":"certifications/vault-associate/04_Vault-Tokens.html#411-periodic-token","title":"4.11 - Periodic Token","text":"<ul> <li>Periodic tokens = tokens that never expire (so long as they are renewed)</li> <li>Aside from root tokens, periodic tokens are currently the only way for a token in Vault to have an unlimited lifetime e.g. to work so long as a service is running.</li> <li>To create a periodic token - <code>vault token create -period=&lt;time&gt;</code></li> <li>Note - a similar scenario is NOT achievable via repeatedly doing <code>vault token renew</code> as this will conflict with the max ttl of the associated auth method.</li> <li>Note - warnings are always provided with periodic tokens - this may be due to the period value exceeding the max TTL.</li> <li>Note: Periodic tokens should only exist for as long as required e.g. until a service has done its job!</li> </ul>"},{"location":"certifications/vault-associate/05_Vault-Architecture.html","title":"5.0 - Vault Architecture","text":""},{"location":"certifications/vault-associate/05_Vault-Architecture.html#501-vault-for-production-environments","title":"5.01 - Vault for Production Environments","text":""},{"location":"certifications/vault-associate/05_Vault-Architecture.html#overview","title":"Overview","text":"<ul> <li>Up until this point, Vault has been used in development mode only.</li> <li>In this, all data is stored in-memory ONLY.</li> <li>This is obviously not suitable for production, a new storage class is required. Vault supports a multitude of storage class options, such as:</li> <li>Filesystem</li> <li>S3 Bucket (AWS)</li> <li>Databases (MySQL, PostgreSQL, etc.)</li> </ul>"},{"location":"certifications/vault-associate/05_Vault-Architecture.html#deploying-in-production-mode","title":"Deploying in Production Mode","text":"<ul> <li>Vault servers are configured using a particular file in JSON or HCL.</li> <li>Example config:</li> </ul> <pre><code>storage \"file\" {\n path = \"/root/vault-data\"\n}\n\nlistener \"tcp\" {\n  address      = \"0.0.0.0:8200\"\n  tls_disable  = 1\n}\n</code></pre> <ul> <li>Once setup, the Vault can be started using the config file: <code>vault server -config /path/to file</code></li> <li>As we're using a new backend, once the Vault server is started, it must be initialized.</li> <li>In this step, encryption keys are generated, as well as unseal keys and the initial root token.</li> <li>To do so: <code>vault operator init</code></li> <li>This will output the Unseal keys and Root Tokens.</li> <li>Once initialized, the Vault must be unsealed. This is true of every initialized Vault.</li> <li>Unsealing is required as Vault knows where to look in the particular storage backend, but it does not have the key to decrypting it and reading the data.</li> <li>To unseal the Vault: <code>vault operator unseal</code></li> </ul>"},{"location":"certifications/vault-associate/05_Vault-Architecture.html#practical-example","title":"Practical Example","text":"<ul> <li>For any VMs used, ensure that SSH port 22 is open for ease of use.</li> <li> <p>This example will be set up for Linux.</p> </li> <li> <p>Creating the config file. The following areas should be added:</p> <ol> <li>Storage (required)</li> <li>Listener (required)</li> <li>Telemetry (optional)</li> </ol> <p>```go storage \"file\" {   path = \"/path/to/vault/data\" # path will be created if it doesn't exist! }</p> <p>listener \"tcp\" {   address      = \"0.0.0.0:8200\"   tls_disable  = 1 # shouldn't be disabled when running in production } ```</p> </li> <li> <p>Starting the server: <code>vault server -config /path/to/config.hcl</code></p> </li> <li>Initialize the Vault:<ol> <li>Ensure <code>VAULT_ADDR</code> environment variable is set</li> <li>Verify vault operations by running <code>vault status</code></li> <li>Run <code>vault operator init</code></li> <li>Save the unseal keys and root tokens - they will disappear and be inaccessible after this!</li> </ol> </li> <li>Unseal the Vault: <code>vault operator unseal</code><ol> <li>Provide 1 of the 5 unseal keys provided</li> <li>Repeat two more times such that 3 of the 5 unseal keys have been provided.</li> </ol> </li> <li>You can test Vault operations by using e.g. <code>vault list auth/token/accessors</code><ol> <li>This will fail though, as Vault in Production mode by default does not set the root token as the token for usage.</li> <li>Login via root <code>vault login</code> and provide the root token<ol> <li>Future requests will automatically use the token.</li> <li>The same will be applied for any other tokens created (so long as the user has sufficient permission!)</li> </ol> </li> </ol> </li> </ul>"},{"location":"certifications/vault-associate/05_Vault-Architecture.html#verifying-storage-persistence","title":"Verifying Storage Persistence","text":"<ul> <li>As production mode persists storage, if Vault is shut down, the data isn't deleted.</li> <li> <p>Upon restart of Vault, the process outlined previously must be followed to unseal the vault and verify data persistence.</p> </li> <li> <p>Note: THE UNSEAL KEYS SHOULD NOT BE STORED WITH ONLY 1 PERSON - THEY SHOULD BE DISTRIBUTED AMONGST TEAM MEMBERS</p> </li> </ul>"},{"location":"certifications/vault-associate/05_Vault-Architecture.html#502-vault-ui-for-production","title":"5.02 - Vault UI for Production","text":"<ul> <li>By default, the Vault GUI is disabled for production. It can be enabled via the Vault config file however.</li> <li>All that is needed is adding the following to the config file:</li> </ul> <pre><code>ui = true # boolean\n</code></pre> <ul> <li>Note: For production environments, you may wish to update the private address of the server under <code>listener</code>.</li> <li>You can verify the system is listening to the desired address using <code>netstat -ntlp</code></li> <li> <p>Once the Vault is unsealed, ensure that the desired port for HashiCorp Vault is open - in this case <code>8200</code>, to allow the user to access the Vault UI.</p> </li> <li> <p>Note: When accessing the UI whilst the Vault is in a sealed state, users are able to provide the unseal keys to unseal the vault.</p> </li> </ul>"},{"location":"certifications/vault-associate/05_Vault-Architecture.html#503-understanding-vault-agent","title":"5.03 - Understanding Vault Agent","text":""},{"location":"certifications/vault-associate/05_Vault-Architecture.html#challenges","title":"Challenges","text":"<ul> <li>For applications needing to interact with Vault, they must first authenticate to Vault and then use the tokens for their required tasks.</li> <li>Outside of this, logic related to token renewal, etc may be required with the application.</li> <li>Rather than building custom logic for the application(s), the Vault agent can be utilised.</li> </ul>"},{"location":"certifications/vault-associate/05_Vault-Architecture.html#vault-agent-overview","title":"Vault Agent Overview","text":"<ul> <li>A client daemon that automates the workflow of client login and token refresh.</li> <li>It automatically authenticates to Vault for supported auth methods.</li> <li>It ensures tokens are renewed by re-authenticating as required, until renewal is no longer allowed.</li> <li>Additionally, it is designed with robustness and fault-tolerance in mind.</li> </ul> <p>The Vault agent works as follows:</p> <ol> <li>Authenticates and acquires a token via the configured auth method</li> <li>The token is written to the backend</li> <li>The token is used to authenticate to Vault</li> </ol>"},{"location":"certifications/vault-associate/05_Vault-Architecture.html#running-vault-agent","title":"Running Vault Agent","text":"<ul> <li>To use the Vault agent, the binary can be ran in \"agent mode\"</li> <li>To do so, run <code>vault agent config=&lt;config file&gt;</code></li> <li>The agent configuration file must specify the auth method and sink locations where the tokens are to be written.</li> </ul>"},{"location":"certifications/vault-associate/05_Vault-Architecture.html#working-of-vault-agent","title":"Working of Vault Agent","text":"<ul> <li>When Vault is started in agent mode, it will attempt to get a Vault token via the auth method specified in the agent config file.</li> <li>Upon successful authentication, the token is written to the sink locations.</li> <li>Whenever the current token's value changes; the agent writes to the sinks.</li> </ul>"},{"location":"certifications/vault-associate/05_Vault-Architecture.html#vault-agent-example","title":"Vault Agent Example","text":"<ul> <li>Various Auth methods are available, including AppRole, Azure, and AWS.</li> <li>Ensure the AppRole Auth method is enabled either via the CLI or UI</li> <li><code>vault auth enable approle</code></li> <li>Create a policy for the agent. An example follows:</li> </ul> <pre><code>path \"auth/token/create\" {\n  capabilities = [\"update\"]\n}\n</code></pre> <ul> <li>Create an approle to use the policy defined above: <code>vault write auth/approle/role/&lt;role name&gt; token_policies=\"&lt;policy name&gt;\"</code></li> <li>Fetch the role ID: <code>vault read auth/approle/role/&lt;role name&gt;/role-id</code></li> <li>Obtain the secret ID: <code>vault read auth/approle/role/&lt;role name&gt;/secret-id</code></li> <li>Add the following code (example) at minimum to the agent config file (written in HCL):</li> </ul> <pre><code>exit_after_auth = false\npid_file = \"./pidfile\"\n\nauto_auth {\n method \"approle\" {\n  mount_path = \"auth/approle\"\n  config = {\n    role_id_file_path = \"/path/to/role-id\" # files must exist!\n    secret_id_file _path = \"/path/to/secret-id\" # files must exist!\n    remove_secret_id_file_after_reading = false\n  }\n}\n\nsink \"file\" {\n config = {\n  path = \"/path/to/token\"\n }\n}\n\nvault {\n address = \"http://127.0.0.1:8200\n}\n</code></pre> <ul> <li>Start the Vault in agent mode via <code>vault agent -config=/path/to/file.hcl</code></li> <li>Information provided regarding the sink file with the token</li> <li>This can be looked up via <code>vault token lookup &lt;token&gt;</code></li> <li>Now, any application wishing to use the token must fetch it from the sink file location. As an example, a Kubernetes secret could be based on this.</li> </ul>"},{"location":"certifications/vault-associate/05_Vault-Architecture.html#504-vault-agent-caching","title":"5.04 - Vault Agent Caching","text":""},{"location":"certifications/vault-associate/05_Vault-Architecture.html#primary-functionalities-of-vault-agent","title":"Primary Functionalities of Vault Agent","text":"<ul> <li>Auto-Auth</li> <li>Automatically authenticates to Vault and manages the token removal process</li> <li>Caching</li> <li>Allows client-side caching of responses containing newly-created tokens.</li> </ul>"},{"location":"certifications/vault-associate/05_Vault-Architecture.html#overview-of-caching","title":"Overview of Caching","text":"<ul> <li> <p>Vault Caching follows a particular general process:</p> </li> <li> <p>Application/Service requests a lease or token from Vault Agent</p> </li> <li>Vault Agent Response: Upon Receiving the Request, Vault checks its Cache<ol> <li>If the requested lease or token is in cache, it is returned to the app/service</li> <li>If not found in the cache, a request is forwarded to Vault Server to obtain the lease or token</li> </ol> </li> <li>If step 2b occurred, Vault Server returns the requested lease or token</li> <li>The returned lease or token is stored in the Agent Cache of the Vault client; then returned to the application or service for usage.</li> </ul>"},{"location":"certifications/vault-associate/05_Vault-Architecture.html#in-practice","title":"In-Practice","text":"<ul> <li>When looking to utilise the caching mechanism of the Vault agent, two particular areas must be added to the config:</li> </ul> <pre><code>cache {\n use_auto_auth_token = true\n}\n\nlistener \"tcp\" {\n address = \"127.0.0.1:8007\"\n tls_disable = true\n}\n</code></pre> <ul> <li>Starting the vault using the config: <code>vault agent -config=/path/to/config.hcl</code></li> <li>Run a test command: <code>&lt;token&gt; vault token create</code></li> <li>No logs will appear in the agent logs as this request goes directly to the Vault Server</li> <li>To resolve, <code>export VAULT_AGENT_ADDR=127.0.0.1:8007</code></li> <li>If the request is made again, the same token will be returned rather than a new one generated - this is because the original token was stored in the cache of the agent.</li> <li>In separate users, so long as they have the Vault Agent Address variable set, they will not require a token to run commands as the client token is used.</li> </ul>"},{"location":"certifications/vault-associate/05_Vault-Architecture.html#505-shamirs-secret-for-unsealing-process","title":"5.05 - Shamirs Secret for Unsealing Process","text":"<ul> <li>Storage backend in Vault is considered to be untrusted.</li> <li>Any data stored is within the encrypted state.</li> <li>When vault is initialized, it generates an encryption key to protect all data stored within.</li> <li>This key is then protected by a master key</li> <li>Vault then uses Shamir's secret sharing algorithm to split the master key into 5 separate shares, where any 3 of which can be used to reconstruct the master key.</li> <li>This is why 3 keys must be used to unseal the vault.</li> </ul>"},{"location":"certifications/vault-associate/05_Vault-Architecture.html#notes","title":"Notes","text":"<ul> <li>The number of shares and minimum threshold required can both be specified.</li> <li>Shamirs technique can be disabled such that the master key can be used directly for unsealing.</li> <li>Once Vault receives the encryption key, it can decrypt the data in the storage backend and the Vault becomes unsealed</li> </ul>"},{"location":"certifications/vault-associate/05_Vault-Architecture.html#seal-stanza","title":"Seal Stanza","text":"<ul> <li>In the Vault server config file, the particular seal type can be configured if usage of Shamir's technique is not desired.</li> </ul> <pre><code>seal [name] {\n # seal details\n}\n</code></pre> <ul> <li>This is optional configuration -  by default Shamir's algorithm will be used if not configured.</li> <li>Most people tend to use this by default.</li> <li>Alternatives could include HSM or Cloud Key Management Solutions e.g. AWS KMS.</li> </ul>"},{"location":"certifications/vault-associate/05_Vault-Architecture.html#506-vault-auto-unseal-overview","title":"5.06 - Vault Auto-Unseal Overview","text":""},{"location":"certifications/vault-associate/05_Vault-Architecture.html#background","title":"Background","text":"<ul> <li>During Vault Unseal process, users must enter they key(s) required.</li> <li>This allows each share of the master key to be stored separately for improved security.</li> <li>This can lead to issues:</li> <li>If there are multiple Vault clusters in an organization, unsealing them can be a tiresome process heavily dependent upon the availability of users with the keys.</li> <li>Unsealing makes the process of automating a Vault install difficult.<ul> <li>Automation tools can easily install, configure and start Vault, but unsealing it via Shamir's technique is heavily manual.</li> </ul> </li> <li>To resolve these issues, Vault's Auto-Unseal mechanism can be utilised.</li> </ul>"},{"location":"certifications/vault-associate/05_Vault-Architecture.html#auto-unseal-overview","title":"Auto-Unseal Overview","text":"<ul> <li>This delegates the responsibility of securing the unseal key from users to a trusted device or service.</li> <li>At startup, Vault will connect to the device or service implementing the seal, then request it decrypt the master key, such that Vault can read from storage.</li> <li>Cloud-based key(s) provides master key information \u2192 Provided master key used to access encryption key and decrypt.</li> <li>This provides pre-setup via the desired unseal method e.g.:</li> <li>AWS KMS</li> <li>Transit Secret Engine</li> <li>Azure Key Vault</li> <li>HSM</li> <li>GCP Cloud KMS</li> </ul>"},{"location":"certifications/vault-associate/05_Vault-Architecture.html#507-auto-unseal-with-aws-kms","title":"5.07 - Auto-Unseal with AWS KMS","text":""},{"location":"certifications/vault-associate/05_Vault-Architecture.html#setting-up-aws-auto-unseal-from-scratch","title":"Setting up AWS Auto-Unseal From Scratch","text":"<ul> <li>As a prerequisite when implementing auto-unseal, you should have an understanding of the mechanism to be used.</li> <li>From Services \u2192 Key Management Service (KMS)</li> <li>Create a symmetric key</li> <li>Leave all settings as default (unless otherwise)</li> <li>Create an IAM user for authentication.</li> <li>Provide sufficient permissions (Admin for demo purposes)</li> <li>Note the access and secret keys associated with the IAM users.<ul> <li>Typically, one would export them as environment variables for security purposes.</li> </ul> </li> <li>Add the following to the Vault Server Config file:</li> </ul> <pre><code>seal \"awskms\" {\n region = \"&lt;region code&gt;\"\n access_key = \"&lt;access key&gt;\" # typically included as env variable instead\n secret_key = \"&lt;secret key&gt;\" # typically included as env variable instead\n kms_key_id = \"&lt;kms key id&gt;\"\n endpoint = \"&lt;KMS API endpoint&gt;\" # optional - typically used for private connactions over vpc from an EC2 instance\n}\n</code></pre> <ul> <li>Starting the Vault using this config: <code>vault server -config &lt;config&gt;.hcl</code></li> <li>Upon first starting, an error/warning will be provided saying that no keys were found. Vault must be initialized to provide the master key and key shares.</li> <li><code>vault operator init</code></li> <li>Provides the root token and recovery keys</li> <li>This will automatically unseal the vault.</li> <li>The keys generated are automatically stored into the kms - they can now be called automatically for future usage of Vault.</li> </ul>"},{"location":"certifications/vault-associate/05_Vault-Architecture.html#508-vault-plugin-mechanism","title":"5.08 - Vault Plugin Mechanism","text":"<ul> <li>Plugins are the building blocks of Vault. All auth and secret backends are considered plugins.</li> <li>This allows Vault to extensible to suit user needs.</li> <li>This doesn't just apply to pre-existing plugins, custom plugins can also be developed.</li> <li>In general, to integrate a 3rd-party plugin, the following steps need to be done:</li> <li>Create the plugin</li> <li>Compile the plugin</li> <li>Start Vault pointing towards where the plugin is stored.</li> </ul>"},{"location":"certifications/vault-associate/05_Vault-Architecture.html#in-practice_1","title":"In Practice","text":"<ul> <li>Install any desired packages e.g. go, git, etc. (Use Apt, Yum, etc as appropriate)</li> <li>Clone the code down for the plugin using GIT e.g. vault-guides/secrets/mock</li> <li>Compile the plugin using go: <code>go build -o /path/to/output/dir</code></li> <li>Start Vault, pointing to the plugin directory: <code>vault server -dev -dev-root-token-id=root -dev-plugin-dir=/path/to/dir</code></li> <li>Test the plugin functionality:</li> <li><code>export VAULT_ADDR=\"http://127.0.0.1:8200</code></li> <li><code>vault login root</code></li> <li><code>vault secrets enable my-mock-plugin</code></li> <li><code>vault secrets list</code><ul> <li>Verify the plugin is working and the secret path exists.</li> </ul> </li> <li>Read and Write operations to the plugin path:<ul> <li><code>vault write my-mock-plugin/test message=\"Hello World\"</code></li> <li><code>vault read my-mock-plugin/test</code></li> </ul> </li> </ul>"},{"location":"certifications/vault-associate/05_Vault-Architecture.html#production","title":"Production","text":"<ul> <li>In production, to use plugins, the following should be added to the config file: <code>plugin_directory = /path/to/directory\"</code></li> </ul>"},{"location":"certifications/vault-associate/05_Vault-Architecture.html#509-audit-devices","title":"5.09 - Audit Devices","text":"<ul> <li>Components in Vault that log requests and responses to Vault.</li> <li>As each operation is an API request/response - the audit log includes every authenticated interaction with Vault; including errors.</li> <li>By default, auditing is not enabled. It must be enabled by a root user using <code>vault audit enable</code>.</li> </ul>"},{"location":"certifications/vault-associate/05_Vault-Architecture.html#audit-devices-example","title":"Audit Devices Example","text":"<ul> <li>To enable file audit: <code>vault audit enable file file_path=path/to/file.log</code></li> <li>Audit mechanisms in place can be viewed by <code>vault audit list</code></li> <li>Note: The audit log is by default in JSON format.</li> <li>Details logged include:</li> <li>Client token used</li> <li>Accessor</li> <li>User</li> <li>Policies involved</li> <li>Token type.</li> <li>Note: Other options are available:</li> <li>Syslog</li> <li>Socket</li> <li>Enabling in Linux:</li> <li><code>vault audit enable -path=\"audit_path\" file file_path=/var/log/vault-audit.log</code><ul> <li>Audit log stored at /var/log</li> <li>viewable via <code>cat &lt;audit log&gt; | jq</code></li> </ul> </li> <li>Note: The client token value is hashed - this can be computed via the endpoint <code>/sys/audit-hash</code></li> <li>Sample request:<ul> <li> <p><code>vault print token</code> - gets the original token in plaintext</p> <p><code>go curl --header \"X-Vault-Token: &lt;vault-token&gt;\" --request POST --data @audit.json http://127.0.0.1:8200/v1/sys/audit-hash/&lt;path&gt;</code></p> </li> </ul> </li> </ul>"},{"location":"certifications/vault-associate/05_Vault-Architecture.html#important-pointers","title":"Important Pointers","text":"<ul> <li>If there are any audit devices enabled, Vault requires at least one to persist the logs before completing a request.</li> <li>If only one device is enabled and is blocking; Vault will be unresponsive until the audit device can write.</li> <li>If more than one audit device is enabled in addition to the blocking one, Vault will be unaffected.</li> </ul>"},{"location":"certifications/vault-associate/05_Vault-Architecture.html#511-vault-enterprise-overview","title":"5.11 - Vault Enterprise Overview","text":"<ul> <li>Includes a number of features benefitting organizational workflows.</li> <li>Features include:</li> <li>Disaster Recovery</li> <li>Namespaces</li> <li>Monitoring</li> <li>Multi-Factor Authentication</li> <li>Auto-Unseal with HSM</li> <li>Once Vault Enterprise is acquired - instructions are provided for installation.</li> <li>In terms of appearance, there is little difference between Enterprise and Open-Source versions.</li> </ul>"},{"location":"certifications/vault-associate/05_Vault-Architecture.html#512-vault-namespaces","title":"5.12 - Vault Namespaces","text":""},{"location":"certifications/vault-associate/05_Vault-Architecture.html#background-context","title":"Background Context","text":"<ul> <li>In organizations, there are typically multiple teams wanting to use services such as Vault for their own particular purposes.</li> <li>In the case of Vault, they would need to manage their own secrets, auth methods, etc.</li> <li>To allow for this segregation and minimizing impacts teams may have on one another, Namespaces can be used.</li> <li>Each Vault namespace can have its own:</li> <li>Policies</li> <li>Auth Methods</li> <li>Secrets Engines</li> <li>Tokens</li> <li>Identity entities and groups.</li> </ul>"},{"location":"certifications/vault-associate/05_Vault-Architecture.html#namespaces-example","title":"Namespaces Example","text":"<ul> <li>When logging into Vault, you can specify the namespace to log into if desired.</li> <li>Namespaces are created under <code>Access</code> in the Vault GUI.</li> </ul>"},{"location":"certifications/vault-associate/05_Vault-Architecture.html#513-vault-replication","title":"5.13 - Vault Replication","text":"<ul> <li>Having a single Vault cluster can impose various challenges, such as:</li> <li>High latency</li> <li>Connection issues</li> <li>Availability loss</li> <li>It is therefore beneficial to have multiple clusters across different regions; allowing users in region 1 to manage their own secrets, etc. in their closer region.</li> <li>Replication serves to resolve this. There are multiple types available.</li> </ul>"},{"location":"certifications/vault-associate/05_Vault-Architecture.html#performance-replication","title":"Performance Replication","text":"<ul> <li>Secondary regions keep track of their own tokens and leases, but share the same underlying configuration, policies, and supporting secrets (K/V values, encryption keys for transit, etc.) with the primary region.</li> </ul>"},{"location":"certifications/vault-associate/05_Vault-Architecture.html#disaster-recovery-replication","title":"Disaster Recovery Replication","text":"<ul> <li>Allows for a full restoration of all types of data (including local and cluster data)</li> <li>Service tokens and leases are valid across both clusters.</li> <li>The secondary cluster does not handle any client requests, and can be promoted to the new primary in the event of disaster.</li> </ul>"},{"location":"certifications/vault-associate/05_Vault-Architecture.html#514-monitoring-telemetry-in-vault","title":"5.14 - Monitoring Telemetry in Vault","text":"<ul> <li>Telemetry covers any data being output by a particular device.</li> <li>Analysis of this can be useful for areas such as:</li> <li>Performance</li> <li>Troubleshooting</li> <li>In Vault, there are two data sets to note:</li> <li>Metrics - Output via Telegraf</li> <li>Vault Audit Logs - Output via Fluentd</li> <li>These metrics can be output to various tools such as Prometheus.</li> </ul>"},{"location":"certifications/vault-associate/05_Vault-Architecture.html#metrics-output","title":"Metrics Output","text":"<ul> <li>Further details for each metric are available in the documentation, however metrics covered include:</li> <li>Policy-based</li> <li>Token-based</li> <li>Resource usage</li> </ul>"},{"location":"certifications/vault-associate/05_Vault-Architecture.html#configuration","title":"Configuration","text":"<ul> <li>In the Vault Server config, one can add a config block to the file to point to a particular location for the metrics to be exported to.</li> </ul> <pre><code>telemetry {\n  statsite_address = \"statsite.company.local:8125\"\n}\n</code></pre> <ul> <li>Metrics can also be fetched from the <code>/sys/metrics</code> endpoint</li> <li>Sample curl requests are available in the documentation</li> <li>The format of the metrics can be configured e.g. JSON, Prometheus</li> <li>e.g. <code>curl --header \"X-Vault-Token: &lt;token&gt;\" 127.0.0.8200:/v1/sys/metrics</code></li> <li>Additional documentation is available for configuring Vault with various monitoring integrations; including Splunk.</li> </ul>"},{"location":"certifications/vault-associate/05_Vault-Architecture.html#515-high-availability-setup-implementation-in-vault","title":"5.15 - High-Availability Setup &amp; Implementation in Vault","text":""},{"location":"certifications/vault-associate/05_Vault-Architecture.html#overview-of-ha","title":"Overview of HA","text":"<ul> <li>In general, running a single instance of anything is risky. Vault is no exception.</li> <li>Vault supports a multi-server mode for High-Availability. This further protects organisations against outages by running multiple servers.</li> <li> <p>The data will be replicated across each based on the \"leader\".</p> </li> <li> <p>Note: High-Availability IS STORAGE BACKEND DEPENDENT - Integrated Storage is also available to support this.</p> </li> <li> <p>Out-of the box, Raft is available for use, an integrated storage backend.</p> </li> <li>To configure:</li> </ul> <pre><code>storage \"raft\" {\n  path = \"/path/to/raft/data\" # defines where the data will be stored\n  node_id = \"raft_node_1\"\n}\ncluster_addr = \"http://127.0.0.1:8201\"\n</code></pre>"},{"location":"certifications/vault-associate/05_Vault-Architecture.html#high-availability-example","title":"High Availability Example","text":"<ul> <li>Start three separate instances of Vault with the above config: <code>vault server -config=/path/to/config.hcl</code></li> <li>List the raft peers: <code>vault operator raft list-peers</code></li> <li>One will be noted as \"leader\" under state</li> <li>Run a test command in the leader node to store data: <code>vault kv put secret/dbcreds admin=password</code></li> <li>On one of the follower nodes, test access to the secrets: <code>vault kv get secret/dbcreds</code></li> <li>The secret data should be displayed.</li> <li>In the event that the leader node goes down, the data will still be accessible AND a new leader node will be assigned.</li> </ul>"},{"location":"certifications/vault-associate/05_Vault-Architecture.html#implementing-vault-ha","title":"Implementing Vault HA","text":"<ul> <li>Raft Storage can be configured by adding the following to the config file:</li> </ul> <pre><code>storage \"raft\" {\n  path = \"/path/to/raft/data\" # defines where the data will be stored\n  node_id = \"raft_node_1\"\n}\ncluster_addr = \"http://127.0.0.1:8201\"\n</code></pre> <ul> <li>You are advised to set <code>disable_mlock</code> to true and disable memory swapping on the system.</li> <li>Start the server with <code>vault server -config=/path/to/config</code></li> <li>The vault will be shown to operational with Raft storage ready</li> <li>In a separate terminal, initialise the Vault:</li> <li><code>vault operator init -key-shares=&lt;value&gt; -key-threshold=&lt;value&gt; &gt; key.txt</code></li> <li>Unseal: <code>vault operator unseal &lt;unseal key&gt;</code></li> <li>Login: <code>vault login &lt;token&gt;</code></li> <li>Check the Raft nodes: <code>vault operator raft list-peers</code></li> <li>Repeat for each node/vault server as required up until initialisations:</li> <li><code>export VAULT_ADDR='address'</code></li> <li>Join the node to the server <code>vault operator raft join &lt;leader IP address&gt;</code></li> <li>To verify, put a secret to a particular path e.g.:</li> <li><code>vault secrets enable -path=secret kv</code></li> <li><code>vault kv put secret/creds admin=password</code></li> <li>On another node: <code>vault kv get secret/creds</code> - should return the secret.</li> <li> <p>Test the replication and leadership transfer by restarting the leader node and running <code>vault operator raft list-peers</code></p> </li> <li> <p>To be highly available, one of the Vault server nodes grabs a lock within the data store.</p> </li> <li>The successful server node becomes the \"active\" node - all others become standby nodes.</li> <li>At this point, if the standby nodes receive a request, they will either forward the request or redirect the client depending on the configuration.</li> <li>Nodes can be stepped down from active duty by using the <code>vault operator step-down &lt;address&gt;</code> command.</li> </ul>"},{"location":"certifications/vault-associate/05_Vault-Architecture.html#516-raft-storage-snapshot-and-restore","title":"5.16 - Raft Storage - Snapshot and Restore","text":"<p>Snapshot and restore operations can be carried out via raft for the following commands:</p> <p><code>vault operator raft snapshot save &lt;file&gt;.snap</code></p> <p><code>vault operator raft snapshot restore &lt;file&gt;.snap</code></p>"},{"location":"certifications/vault-associate/06_Exam-Preparation.html","title":"6.0 - Exam Preparation","text":""},{"location":"certifications/vault-associate/06_Exam-Preparation.html#601-important-pointers-for-the-exam","title":"6.01 - Important Pointers for the Exam","text":""},{"location":"certifications/vault-associate/06_Exam-Preparation.html#official-certification-guide","title":"Official Certification Guide","text":"<p>https://developer.hashicorp.com/vault/tutorials/associate-cert</p>"},{"location":"certifications/vault-associate/06_Exam-Preparation.html#dynamic-secrets","title":"Dynamic Secrets","text":"<ul> <li>Dynamic secrets allow on-demand credential generation dynamically. They are automatically revoked after a particular amount of time (lease).</li> <li>Not all secrets engines support dynamic credentials. Primary engines that can utilise it include:</li> <li>AWS</li> <li>Database</li> <li>Google Cloud</li> <li>Azure</li> <li>Dynamic secrets do not provide any stroger cryptographic key generation.</li> </ul>"},{"location":"certifications/vault-associate/06_Exam-Preparation.html#lease-management","title":"Lease Management","text":"<ul> <li>With every dynamic secret and service type authentication token, Vault will create a lease.</li> <li>Lease - Metadata containing information regarding a secret or service type authentication. Contains information such as time duration, renewability, etc.</li> <li>Once the lease expires, Vault can automatically revoke the data, preventing future access.</li> <li>When working with Leases, Vault has two main operations that can be achieved via the CLI or UI:</li> <li>Renew - renews the lease on a secret, extending its usage time before automatic revocation</li> <li>Revoke - Force-invalidates a secret with immediate effect, preventing any further renewals.</li> <li>Example commands:</li> <li><code>vault lease renew -increment=3600 &lt;lease id&gt;</code> - Requests an adjustment of a lease's TTL to 1 hour</li> <li><code>vault lease revoke &lt;lease id&gt;</code> - Revokes a lease</li> <li><code>vault lease revoke -prefix aws/</code> - Revokes all secrets under the path aws</li> </ul>"},{"location":"certifications/vault-associate/06_Exam-Preparation.html#transit-secret-engine","title":"Transit Secret Engine","text":"<ul> <li>Handles cryptographic functions on data in-transit.</li> <li>All plaintext data must be base64-encoded.</li> <li>This is because Vault does not require that the plaintext is \"text\" it could be a binary file, such as a PDF or image.</li> <li>Encryption keys can be rotated at regular intervals to ensure that not all data is encrypted with one static encryption key.</li> </ul>"},{"location":"certifications/vault-associate/06_Exam-Preparation.html#key-version","title":"Key Version","text":"<ul> <li>Transit engine supports versioning of keys</li> <li>Key versions earlier than a key's specified <code>min_decryption_version</code> will be archived. Any later belong to the working set.</li> <li>This facilitates enhanced performance and security.</li> <li>By disallowing decryption of old versions, any found ciphertext associated with obsolete data cannot be decrypted. The only way that this could be achieved would be if <code>min_decryption_version</code> was manually lowered.</li> </ul>"},{"location":"certifications/vault-associate/06_Exam-Preparation.html#vault-policies","title":"Vault Policies","text":"<ul> <li>Used to govern access to Vault for users and roles (authorization)</li> <li>When first initialized, the root and default policies are created by default.</li> <li>Policies operate on a \"deny by default\" manner, so an empty policy grants no permissions</li> </ul>"},{"location":"certifications/vault-associate/06_Exam-Preparation.html#default-policy","title":"Default Policy","text":"<ul> <li>A built-in policy that cannot be removed.</li> <li>Attached to all tokens by default, but may be explicitly be excluded at token creation time by particular authentication methods.</li> <li>Covers basic functionality such as the ability for a token to look up data about itself and use its cubbyhole data.</li> <li>Vault is not prescriptive about its contents - it can be modified to suit.</li> </ul>"},{"location":"certifications/vault-associate/06_Exam-Preparation.html#root-policy","title":"Root Policy","text":"<ul> <li>A root user can do ANYTHING within Vault. Therefore, it is highly recommended that any root tokens are revoked before running Vault in production.</li> <li>When a Vault server is first initialized, one root user always exists. This user is primarily used to do the initial configuration and setup of Vault.</li> <li>After configuration, the initial root token should be revoked and more strictly controlled users and authentication should be used.</li> <li>E.g. have an administrator user as the primary usage point.</li> </ul>"},{"location":"certifications/vault-associate/06_Exam-Preparation.html#token-accessor","title":"Token Accessor","text":"<ul> <li>The accessor is a value that acts as a reference to a token - used to perform limited actions such as:</li> <li>Lookup a token's properties</li> <li>Lookup a tokens capabilities for a particular path</li> <li>Renew and revoke the token</li> </ul>"},{"location":"certifications/vault-associate/06_Exam-Preparation.html#policy-association","title":"Policy Association","text":"<ul> <li>During token creation, a policy will be associated with the token.</li> <li>If a new policy is attached to a user or role, it will not affect the existing tokens. New tokens must be created.</li> <li>For any policies that are updated and attached to a token, the rules will be reflected accordingly as part of the token's permission.</li> </ul>"},{"location":"certifications/vault-associate/06_Exam-Preparation.html#token-capabilities","title":"Token Capabilities","text":"<ul> <li>To check a token's capabilities for a particular path, use the following sample command: <code>vault token capabilities secret</code></li> <li>Note: the same operation can be achieved using token accessor.</li> </ul>"},{"location":"certifications/vault-associate/06_Exam-Preparation.html#authentication-methods","title":"Authentication Methods","text":"<ul> <li>There are multiple authentication methods available</li> <li>Upon successful authentication, a token will be generated for the user to interact with Vault.</li> <li>Note: GitHub auth method is user-oriented and easiest to use for developer machines.</li> <li>For servers/applications, AppRole is recommended.</li> <li>To enable an authentication method, utilise the following example command for userpass method: <code>vault auth enable -path=my-login userpass</code></li> <li>To disable an auth method: <code>vault auth disable &lt;method name&gt;</code></li> <li>This will auto-logout any users logged in via that method.</li> </ul>"},{"location":"certifications/vault-associate/06_Exam-Preparation.html#storage-backend","title":"Storage Backend","text":"<ul> <li>Multiple storage backends are available for Vault depending on the use cases</li> <li>The backend determines the location for storage of Vault information and data.</li> <li>Note: not all storage backends support high-availability. Typically this is cloud-only.</li> </ul>"},{"location":"certifications/vault-associate/06_Exam-Preparation.html#disabling-a-secrets-engine","title":"Disabling a Secrets Engine","text":"<ul> <li>To disable a secrets engine at a particular path: <code>vault secrets disable &lt;path prefix&gt;</code></li> <li>Once disabled, any secrets associated are immediately revoked.</li> </ul>"},{"location":"certifications/vault-associate/06_Exam-Preparation.html#userpass-auth-method","title":"UserPass Auth Method","text":"<ul> <li>Login achievable via <code>vault login -method=userpass username=user</code></li> <li>Passwords should not be used directly in the CLI - it will auto hide it if not included.</li> </ul>"},{"location":"certifications/vault-associate/06_Exam-Preparation.html#vault-unseal","title":"Vault Unseal","text":"<ul> <li>Vault starts in a sealed state any time it is started</li> <li>Unsealing involves constructing the master key necessary to read the decryption key to decrypt the data; allowing access to Vault.</li> </ul>"},{"location":"certifications/vault-associate/06_Exam-Preparation.html#vault-agent","title":"Vault Agent","text":"<ul> <li>The agent doesn't persist anything to storage - all data is stored in memory</li> <li>The agent looks to utilise two main functionalities:</li> <li>Auto-Auth: Facilitates automatic authentication to Vault and management of token renewal processes</li> <li>Caching: Allows client-side caching of responses containing newly-created tokens.<ul> <li>If configured with <code>use_auto_auth_token</code>, clients will not be required to provide a Vault token to the requests made to the agent.</li> </ul> </li> </ul>"},{"location":"certifications/vault-associate/06_Exam-Preparation.html#response-wrapping-token","title":"Response Wrapping Token","text":"<ul> <li>When response wrapping is requested, Vault creates a temporary single-use token (Wrapping Token) and the response is inserted into the token's cubbyhole with a short TTL</li> <li>If the wrapping token is compromised, the application will not be able to access the secret, and security actions can be taken accordingly.</li> </ul>"},{"location":"certifications/vault-associate/06_Exam-Preparation.html#shamir-secret-sharing-for-unsealing-vault","title":"Shamir Secret Sharing for Unsealing Vault","text":"<ul> <li>Vault considers all storage backends untrusted by default. Vault therefore uses an encryption key to protect all data; which is then protected by a master key.</li> <li>Vault utilises Shamirs secret sharing algorithm to split the master key into 5 shares. Any 3 of the 5 shares are required to reconstruct the master key.</li> <li>In terms of best practices - the key shares that need to be entered should be done from different workstations by different users; each with their own individual keys.</li> </ul>"},{"location":"certifications/vault-associate/06_Exam-Preparation.html#seal-stanza","title":"Seal Stanza","text":"<ul> <li>The seal stanza configures the seal type used for additional data protection e.g. HSM or Cloud-based KMS solutions to encrypt and decrypt the master key.</li> <li>This is OPTIONAL. In the case of the master key, Vault will by default use the Shamir algorithm to cryptographically split the master key if not provided.</li> </ul>"},{"location":"certifications/vault-associate/06_Exam-Preparation.html#vault-replication","title":"Vault Replication","text":"<ul> <li>For performance replication - secondary clusters will service reads locally</li> <li>Some data is also stored locally and not replicated from the primary cluster</li> <li>For DR Replication - ALL data is replicated, but the secondary cluster cannot accept client requests.</li> </ul>"},{"location":"certifications/vault-associate/06_Exam-Preparation.html#entities-and-aliases","title":"Entities and Aliases","text":"<ul> <li>Each client is internally termed as an entity - entities can have multiple aliases.</li> <li>Policies defined at entity-level will be associated with all aliases associated with the entities.</li> <li>E.g. users under the  \"dev team\" entity</li> </ul>"},{"location":"certifications/vault-associate/06_Exam-Preparation.html#identity-groups","title":"Identity Groups","text":"<ul> <li>A group can contain multiple entities as its members</li> <li>Any policies applied to a group will be applied to all entities within the group.</li> </ul>"},{"location":"certifications/vault-associate/06_Exam-Preparation.html#vault-output","title":"Vault Output","text":"<ul> <li>Vault data output can be configured for a variety of output formats, including:</li> <li>Table</li> <li>JSON</li> <li>YAML</li> <li>Table is the default output format.</li> <li>Output can be specified when running vault commands e.g. <code>vault secrets list -format table</code></li> </ul>"},{"location":"certifications/vault-associate/06_Exam-Preparation.html#multiple-encryption-keys","title":"Multiple Encryption Keys","text":"<ul> <li>For transit engine - it is good practice to regularly rotate the encryption key</li> <li>This limits the number of data encrypted via a single key</li> <li>All data should NEVER be encrypted via a single encryption key - this is heavily risky</li> </ul>"},{"location":"certifications/vault-associate/06_Exam-Preparation.html#reading-output-from-kv-path","title":"Reading Output from KV Path","text":"<ul> <li>Suppose you want to read a secret at a particular path, what capability is needed?</li> <li>LIST - Allows for listing values at the particular paths</li> <li>READ - Allows reading of data at a particular path</li> </ul>"},{"location":"certifications/vault-associate/06_Exam-Preparation.html#audit","title":"Audit","text":"<ul> <li>Audit devices are the components in Vault that keep a log of all requests and response to Vault</li> <li>When Vault servers are first initialized, no auditing is enabled</li> <li>Any audit devices must be enabled by a root user using <code>vault audit enable</code></li> </ul>"},{"location":"certifications/vault-associate/06_Exam-Preparation.html#vault-browser-cli","title":"Vault Browser CLI","text":"<ul> <li>Allows the running of basic CLI commands e.g read/write/delete/list</li> <li>More advanced tasks e.g creating new authentication methods are not available via this.</li> </ul>"},{"location":"certifications/vault-associate/06_Exam-Preparation.html#vault-token-lookup","title":"Vault Token Lookup","text":"<ul> <li>Via the <code>vault token lookup &lt;command&gt;</code>, the following parameters are output:</li> <li>Creation_TTL - How long is the token valid for upon first creation?</li> <li>Orphan - True/False - Does the token have a parent token?</li> <li>TTL - How long is the token valid for at the moment in time?</li> </ul>"},{"location":"certifications/vault-associate/06_Exam-Preparation.html#orphan-tokens","title":"Orphan Tokens","text":"<ul> <li>Orphan tokens aren't the children of their parents - they therefore do not expire when their \"parent\" does.</li> <li>They are the root of their own token tree</li> <li>Orphan tokens still expire when their own max TTL is reached.</li> </ul>"},{"location":"certifications/vault-associate/06_Exam-Preparation.html#create-token-with-explicit-ttl","title":"Create Token with Explicit TTL","text":"<ul> <li>TTL = Initial TTL to associate with the token</li> <li>Explicit Max TTL = Maximum lifetime for the token - a hard limit that cannot be exceeded</li> <li>The systems max TTL is 32 days by default, but this can be adjusted in the Vault config file</li> <li>To create a token with an explicit TTL, use the following example command: <code>vault token create --ttl=&lt;time&gt; --explicit-max-ttl=&lt;time&gt;</code></li> </ul>"},{"location":"certifications/vault-associate/06_Exam-Preparation.html#renewing-a-token","title":"Renewing a Token","text":"<ul> <li><code>vault token renew</code> is used to extend the validity of a renewable token</li> <li><code>vault token renew -increment=&lt;time&gt; &lt;token&gt;</code></li> </ul>"},{"location":"certifications/vault-associate/06_Exam-Preparation.html#basic-environment-variables","title":"Basic Environment Variables","text":"<ul> <li>VAULT_ADDR defines the Vault server's address and port e.g. https://127.0.0.1:8200</li> </ul>"},{"location":"certifications/vault-associate/06_Exam-Preparation.html#gui-related-questions","title":"GUI Related Questions","text":"<ul> <li>Should need to understand how to do particular actions within the GUI e.g. delete a particular version of a secret.</li> </ul>"},{"location":"certifications/vault-associate/06_Exam-Preparation.html#secrets-engines","title":"Secrets Engines","text":"<ul> <li>Multiple secrets engine of the same type can be enabled ata given time</li> <li>They can be distinguished uniquely be separating them via path</li> <li>Example:<ul> <li>Key-Value engine at /secret</li> <li>Key-value engine at /kv</li> </ul> </li> </ul>"},{"location":"certifications/vault-associate/06_Exam-Preparation.html#supported-backend-hashicorp-support","title":"Supported Backend - HashiCorp Support","text":"<ul> <li>The following backends are officially supported by HashiCorp:</li> <li>In-Memory</li> <li>Filesystem</li> <li>Consul</li> <li>Raft</li> <li>Technical support is available from HashiCorp community and technical team.</li> </ul>"},{"location":"certifications/vault-associate/06_Exam-Preparation.html#vault-enterprise-features","title":"Vault Enterprise Features","text":"<ul> <li>Vault Enterprise includes features to utilise particular workflows such as:</li> <li>Disaster recovery</li> <li>Namespaces</li> <li>Monitoring</li> <li>Multi-Factor Authentication</li> <li>Auto-unseal with HSM</li> </ul>"},{"location":"certifications/vault-associate/06_Exam-Preparation.html#vault-namespace","title":"Vault Namespace","text":"<ul> <li>Namespaces are isolated environments that act as \"vaults within Vault\"</li> <li>Each namespace has separate login paths and supports creating and managing data isolated to their namespace</li> <li>Each namespace can have its own:</li> <li>Policies</li> <li>Authentication methods</li> <li>Secrets engines</li> <li>tokens</li> <li>identify entities and groups</li> </ul>"},{"location":"certifications/vault-associate/06_Exam-Preparation.html#vault-replication-2","title":"Vault Replication 2","text":"<ul> <li>When replication is enabled, all of the secondary clusters existing data will be destroyed</li> <li>Vault does not support an automatic failover/promotion of a DR secondary cluster</li> <li>Vault replication is an Enterprise-only feature</li> <li>DR replicated clusters will replicate all data from the primary cluster, including tokens.</li> <li>Performance-replicated clusters will not replicated tokens from the primary</li> </ul>"},{"location":"certifications/vault-associate/06_Exam-Preparation.html#auto-completion-feature","title":"Auto-Completion Feature","text":"<ul> <li>Allows automatic completion for flags, subcommands and arguments</li> <li>Note: Requires shell restart after install</li> <li>Installation via: <code>vault -autocomplete-install</code></li> </ul>"},{"location":"certifications/vault-associate/06_Exam-Preparation.html#vault-cli-commands","title":"Vault CLI Commands","text":"Use Case Example Command Enable Secrets Engine vault secrets enable kv-2 Store Data vault kv put secret/my-secret admin=password List Key Names in Secrets vault kv list secret/my-app/ Delete Version of Secret vault kv delete secret/my-app/ Delete all version &amp; metadata vault kv metadata delete secret/my-app/"},{"location":"certifications/vault-associate/06_Exam-Preparation.html#auto-unseal","title":"Auto-Unseal","text":"<ul> <li>Auto unseal delegates the responsibility of securing the unseal key from users to a trusted device or service</li> <li>Following are some of the supported services:</li> <li>AWS KMS</li> <li>Transit Secret Engine</li> <li>Azure Key Vault</li> <li>HSM</li> <li>GCP Cloud KMS</li> <li>For private connectivity, VPC endpoints can be used</li> </ul>"},{"location":"certifications/vault-associate/06_Exam-Preparation.html#identify-output-of-transit-engine","title":"Identify Output of Transit Engine","text":"<ul> <li><code>vault write encryption/encrypt/demo plaintext=$(base64 &lt;&lt;&lt; \"Sample Data\")</code></li> <li>Output: <code>vault:v3&lt;stuff&gt;</code></li> <li>V3 Indicates the key version used to encrypt the plain text</li> <li>Name of the keyring is demo</li> <li>Transit secret engine is mounted at <code>/encryption</code></li> </ul>"},{"location":"certifications/vault-associate/06_Exam-Preparation.html#pki-secrets-engine","title":"PKI Secrets Engine","text":"<ul> <li>Allows generation of dynamic X509 certificates</li> <li>Benefits include:</li> <li>Allows vault to act as an intermediate Certificate Authority</li> <li>Reduces or eliminates the amount of certificate revocations required</li> </ul>"},{"location":"certifications/vault-associate/06_Exam-Preparation.html#totp-secrets-engine","title":"TOTP Secrets Engine","text":"<ul> <li>TOTP = Time-based one-time passwords</li> <li>These are temporary passcodes and typically expires after 30, 60, 120 seconds etc</li> <li>The TOTP secrets engine can act as both a generator (e.g. Google Authenticator) and a providr (e.g. google sign-on)</li> <li>To view: <code>vault read totp/code/zeal</code></li> </ul>"},{"location":"certifications/vault-associate/06_Exam-Preparation.html#vault-plugins","title":"Vault Plugins","text":"<ul> <li>All Vault Auth and secret backends are plugins</li> <li>This allows for easier customisation and extensability of Vault</li> </ul>"},{"location":"certifications/vault-associate/06_Exam-Preparation.html#telemetry-in-vault","title":"Telemetry in Vault","text":"<ul> <li>All telemetry related metrics are available at <code>/sys/metrics</code> endpoint</li> <li>The primary metrics analysed are:</li> <li>Metrics</li> <li>Vault Audit Logs</li> </ul>"},{"location":"certifications/vault-associate/06_Exam-Preparation.html#security-best-practices-root-tokens","title":"Security Best Practices - Root Tokens","text":"<ul> <li>It's best practice to not persist root tokens</li> <li>Root tokens should be generated using vault's <code>operator generate-root</code> command only when absolutely necessary</li> <li>For day-to-day operations, the root token should be deleted after configuring other auth methods or configuration settings etc</li> </ul>"},{"location":"certifications/vault-associate/06_Exam-Preparation.html#enabling-versioning-in-kv-version-1","title":"Enabling Versioning in KV - Version 1","text":"<ul> <li>When enabling Key-Value secret engine version 1, the versioning feature is not enabled by default.</li> <li>Versioning can be enabled by using the <code>kv enable-versioning</code> for an existing non-versioned key/value secrets engine at its path.</li> </ul>"},{"location":"certifications/vault-associate/06_Exam-Preparation.html#response-wrapping-from-ui","title":"Response Wrapping from UI","text":"<ul> <li>The feature under Vault Tools \u2192 Wrap allows response wrapping functionality</li> </ul>"},{"location":"certifications/vault-associate/06_Exam-Preparation.html#path-templating","title":"Path Templating","text":"<ul> <li>Allows variable replacement based on information of the entity.</li> <li>Example entity policy:</li> </ul> <pre><code>path \"secret/data/{{identity.entity.name}}/\" {\n  capabilities = [\"create\", \"update\"]\n}\n</code></pre> <ul> <li>Example alias policy based on entity policy with path templating:</li> </ul> <pre><code>path \"secret/data/alice/\" {\n  capabilities = [\"create\", \"update\"]\n}\n</code></pre>"},{"location":"certifications/vault-associate/06_Exam-Preparation.html#service-tokens-vs-batch-tokens","title":"Service Tokens vs Batch Tokens","text":"Feature Service Token Batch Token Can be root tokens Yes No Can create child tokens Yes No Renewable Yes No Periodic Yes No Can have particular max TTL Yes No (fixed TTL always) Has Accessors Yes No Has CubbyHole Yes No Revoked with Parent if not orphan Yes Stops Working Dynamic Secrets Lease Assignment Self Parent (if not orphan) Can be used across performance replication clusters No Yes (if orphan) Creation scales with performance standby node count No Yes Cost Heavy Weight - Multiple Storage Writes per token creation Lightweight - No storage cost for token creation <ul> <li>Key points to note are:</li> <li>Can be root tokens</li> <li>Cost</li> </ul>"},{"location":"certifications/vault-associate/06_Exam-Preparation.html#vault-tools","title":"Vault Tools","text":"<ul> <li>Vault contains certain tools for particular functions at the <code>/sys/tools</code> endpoint</li> </ul> Endpoints Description /sys/tools/random/164 Generate a random 164-byte value /sys/tools/hash/sha2-512 Hash some input data using SHA2 algorithm"},{"location":"certifications/vault-associate/06_Exam-Preparation.html#miscellaneous-pointers","title":"Miscellaneous Pointers","text":"<ul> <li>Root and Default are two default policies available in Vault.</li> <li>When a lease is revoked, it invalidates that secret immediately, preventing further renewals</li> <li>To remove all secrets under a particular path: <code>vault lease revoke -prefix &lt;path&gt;</code></li> <li>Userpass auth method cannot read usernames and passwords from an external source</li> <li>The <code>/sys/leader</code> endpoint is used to check HA status and current leader of vault</li> <li><code>vault operator init</code> initializes a vault server</li> <li>Vault config files can be used to configure various settings e.g.:</li> <li>Cluster name</li> <li>Storage backends</li> <li>seal settings</li> <li>Configuration for setup like namespaces and auth methods are handled directly within Vault itself.</li> <li>Identity secrets engine is mounted by default in Vault</li> <li>Storage backends are not trusted by default in Vault.</li> <li>Port Numbers:</li> <li>8200 - Vault API and UI</li> <li>8201 - Cluster-cluster communication</li> <li>Root tokens in Vault are not associated with TTLs - non-root tokens are</li> <li>The default max TTL is 32 days for tokens, but this can be modified.</li> <li>Transit secret engine stores no data</li> <li>VAULT_ADDR needs to be set to allow for any commands to be ran.</li> <li>Post-authentication,  the CLI and UI automatically assumes the token used in authentication for subsequent requests.</li> <li>For the API, the token must be provided for all subsequent requests.</li> <li>Vault Secrets engines supports multiple cloud providers e.g. AWS, Azure, GCP</li> <li>Cubbyhole is a default secrets engine enabled for all users.</li> <li>When generating dynamic secrets, Vault returns the <code>lease_id</code></li> <li>This can be used with commands for lease renew, revoke, etc</li> <li>Vault login command is used for CLI authentication</li> <li>After initializing, Vault provides the root token to the user. This is the only way to login to Vault to configure additional auth methods</li> <li><code>+</code> and <code>*</code> are associated with wildcard path in policies.</li> <li>Config files for Vault can be done in either JSON or HCL.</li> <li>When data is decrypted via the transit engine, the output is base64 format</li> <li>To get the original data, it must be decoded via base64</li> <li>Vault UI needs to be enabled from the configuration file ONLY and not the CLI</li> <li>For token renewal, <code>vault token renew &lt;token&gt;</code> can be utilised to extend the TTL.</li> <li>When the Vault is sealed, users can only:</li> <li>View the vault status</li> <li>Attempt to unseal the Vault</li> <li>If the Vault CLI can access a specific path but a user cannot access it via the GUI, the issue is that the user is missing the LIST capability against that path.</li> <li>If a secret has been manually removed, <code>vault lease revoke</code> will result in an error (which can be avoided by appending the <code>-force</code> flag.</li> <li>Storage backend and HTTP API are outside of the security barrier and cannot be pretected</li> <li>WAL = Write-Ahead Logging</li> <li>Changes are first recorded in the log (which has to be written to stable storage) before changes are written/applied to any data stores.</li> <li>This is commonly noted in HA setups.</li> </ul>"},{"location":"certifications/vault-associate/06_Exam-Preparation.html#dealing-with-large-data-sizes-transit-engine","title":"Dealing with Large Data Sizes - Transit Engine","text":"<ul> <li>When data size is large, it is not advised to send it over the network to the Vault</li> <li>Via Vault, one can generate a data key, encrypt the data locally, and use it to decrypt the data when required.</li> </ul>"},{"location":"certifications/vault-associate/06_Exam-Preparation.html#vault-policy-rules-transit-engine","title":"Vault Policy Rules - Transit Engine","text":"<ul> <li>Common / Sample Rules associated with the Transit engine are:</li> </ul> <pre><code>## encrypt particular key\npath \"transit/encrypt/demo-key\" {\n  capabiltities = [ \"update\" ]\n}\n\n## decrypt particular key\npath \"transit/decrypt/demo-key\" {\n  capabiltities = [ \"update\" ]\n}\n\n## list values under /tranist/keys\npath \"transit/keys\" {\n  capabiltities = [ \"list\" ]\n}\n\n#view values associated with particular keys\npath \"transit/keys/demo-key\" {\n  capabiltities = [ \"read\" ]\n}\n</code></pre>"},{"location":"certifications/vault-associate/06_Exam-Preparation.html#key-rotation-in-vault","title":"Key Rotation in Vault","text":"<ul> <li>It's ill-advised to encrypt all data with one encryption key</li> <li>Transit engine allows encryption key rotation.</li> <li>Vault maintains the versioned \"keyring\" and the operator can configure the minimum allowed version for decryption tasks</li> </ul>"},{"location":"certifications/vault-associate/06_Exam-Preparation.html#periodic-tokens","title":"Periodic Tokens","text":"<ul> <li>Never expire so long as they're renewed</li> <li>Aside from root tokens, these are the only way for unlimited lifetime tokens to exist in Vault</li> <li>Created via <code>vault token create -period=&lt;time&gt; -policy=&lt;policy&gt;</code></li> </ul>"},{"location":"certifications/vault-associate/06_Exam-Preparation.html#vault-ha","title":"Vault HA","text":"<ul> <li>Vault supports multi-server mode for High-Availability.</li> <li>This protects against outages by running multiple Vault servers.</li> <li>Vault servers/nodes can be force-removed from active duty using <code>vault operator step-down</code> whilst within the server.</li> <li>When utilising HA, Consul and Integrated Storage (Raft) are advised for use as storage backends.</li> </ul>"},{"location":"certifications/vault-associate/06_Exam-Preparation.html#guide-in-vault-gui","title":"Guide in Vault GUI","text":"<ul> <li>Note that Vault's GUI provides guidance for various operations e.g. restart, secrets, authentication, etc.</li> <li>It will provide links to the related documentation involved e.g. auth method documentation.</li> </ul>"},{"location":"certifications/vault-associate/06_Exam-Preparation.html#vault-policy-format","title":"Vault Policy Format","text":"<ul> <li>HCL Policy files can be auto-formatted by <code>vault policy fmt &lt;filename&gt;.hcl</code></li> </ul>"},{"location":"certifications/vault-associate/06_Exam-Preparation.html#misc-pointers","title":"Misc Pointers","text":"<ul> <li>Kubernetes authentication is required for any kubernetes-based workloads</li> <li>For kubernetes-based authentication, passing JWT token is required for all API requests.</li> <li><code>/sys/seal</code> endpoints are used to seal the Vault. It requires a token with root policy or sudo capability on the path.</li> <li>For EC2-based workloads, AWS Authentication method can be used.</li> <li>It is not mandatory, AppRole can also be used.</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/00_Introduction/01_Course-Introduction.html","title":"0.1 - Course Introduction","text":""},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/00_Introduction/01_Course-Introduction.html#course-introduction","title":"Course Introduction","text":"<ul> <li>Course aims to provide the understanding of how to use Tanzu Kubernetes Grid (TKG) in organizations to maximise the benefits of it in application deployment.</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/00_Introduction/01_Course-Introduction.html#course-goals","title":"Course Goals","text":"<ul> <li>Describe Tanzu Kubernetes Grid</li> <li>Differentiate between the Kubernetes lifecycle management options in the VMWare Tanzu Portfolio</li> <li>Explain how to prepare a vSphere environment to install Tanzu Kubernetes Grid</li> <li>Describe how to initialize a Tanzu Kubernetes Grid Instance</li> <li>Detail how to create Tanzu Kubernetes Clusters</li> <li>Explain how to deploy the Tanzy Kubernetes Grid extensions on a Tanzu Kubernetes cluster e.g. monitoring &amp; networking extensions</li> <li>Describe how to troubleshoot a Tanzu Kubernetes Grid instance.</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/00_Introduction/01_Course-Introduction.html#additional-notes","title":"Additional Notes","text":"<ul> <li>Intermediate knowledge of Kubernetes is required as a prerequisite.</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/00_Introduction/01_Course-Introduction.html#references","title":"References","text":"<ul> <li>Cluster API</li> <li>Kubernetes Cluster API Provider vSphere</li> <li>Fluentbit</li> <li>Pinniped</li> <li>Contour</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/00_Introduction/01_Course-Introduction.html#additional-links","title":"Additional Links","text":"<ul> <li>VMWare Communities</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/01_VMware_Tanzu_Portfolio/01_VMWare-Tanzu-Portfolio.html","title":"1.1 - The VMWare Tanzu Portfolio","text":""},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/01_VMware_Tanzu_Portfolio/01_VMWare-Tanzu-Portfolio.html#introduction-and-background","title":"Introduction and Background","text":"<ul> <li>Kubernetes is a key technology in VMWare's Tanzu Portfolio</li> <li>Tanzu Kubernetes grid is the common implementation of Kubernetes across supported cloud platforms e.g. Azure, AWS, vSphere.</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/01_VMware_Tanzu_Portfolio/01_VMWare-Tanzu-Portfolio.html#objectives","title":"Objectives","text":"<ul> <li>Describe the VMWare Tanzu Portfolio</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/01_VMware_Tanzu_Portfolio/01_VMWare-Tanzu-Portfolio.html#vmware-tanzu-portfolio","title":"VMWare Tanzu Portfolio","text":"<ul> <li>The VMware Tanzu portfolio aims to provide a modern applications platform.</li> <li>In doing so, the following operations can be carried out:</li> <li>Build</li> <li>Run</li> <li>Manage</li> <li> <p>Each of these operations is handled or  facilitated by a particular Tanzu-based service.</p> <p></p> </li> <li> <p>Typically, the services under RUN and Manage are handled by SRE, those in Build are primarily handled by Developers</p> </li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/01_VMware_Tanzu_Portfolio/01_VMWare-Tanzu-Portfolio.html#kubernetes-lifecycle-management","title":"Kubernetes Lifecycle Management","text":"<ul> <li>The focus of the course will be on the following:</li> <li>Run:<ul> <li>VMWare Tanzu Kubernetes Grid</li> <li>VMware vSphere with Tanzu</li> <li>VMware Tanzu Kubernetes Grid Integrated Edition</li> </ul> </li> <li>Manage<ul> <li>VMWare Tanzu Mission Control (works in conjunction with the above)</li> </ul> </li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/01_VMware_Tanzu_Portfolio/01_VMWare-Tanzu-Portfolio.html#tanzu-kubernetes-grid","title":"Tanzu Kubernetes Grid","text":"<ul> <li>A multicloud Kubernetes distribution that can run on:</li> <li>vSphere, VMWare Cloud on AWS, Azure VMware Solution (any vSphere-based solution)</li> <li>AWS (in an EC2 instance)</li> <li>Microsoft Azure</li> <li>Aims to automate the lifecycle management of multiple Tanzu Kubernetes clusters using Cluster API</li> <li>An open-source Kubernetes distribution</li> <li>Includes tested, signed Kubernetes binaries supported by VMWare</li> <li>Includes signed and supported versions of open-source apps to support the following operations in a production Kubernetes environment:</li> <li>networking</li> <li>authentication</li> <li>ingress</li> <li>logging</li> <li>monitoring</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/01_VMware_Tanzu_Portfolio/01_VMWare-Tanzu-Portfolio.html#vsphere-with-tanzu","title":"vSphere with Tanzu","text":"<ul> <li>Provides a Kubernetes experience that only runs on vSphere 7 - a tight integration</li> <li>Contains multiple services which provide access to infrastructure via a Kubernetes API</li> <li>Contains the Tanzu Kubernetes Grid service</li> <li>Runs on supervisor vSphere with Tanzu</li> <li>Creates Tanzu Kubernetes clusters optimized for vSphere</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/01_VMware_Tanzu_Portfolio/01_VMWare-Tanzu-Portfolio.html#tanzu-kubernetes-grid-integrated-edition","title":"Tanzu Kubernetes Grid Integrated Edition","text":"<ul> <li>A multicloud Kubernetes distribution that runs on the same platforms as Tanzu Kubernetes Grid</li> <li>Previously known as VMware Enterprise PKS</li> <li>Automates the lifecycle management of multiple Kubernetes clusters using BOSH</li> <li>Includes Kubernetes binaries that are tested, signed, and supported by VMware</li> <li>Provides advanced networking with VMWare NSX-T Data Center</li> <li>Provides integrations to vRealize Log Insight, vRealize Operations and Tanzu Observability</li> <li>Supports Microsoft Windows workloads</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/01_VMware_Tanzu_Portfolio/01_VMWare-Tanzu-Portfolio.html#vmware-tanzu-mission-control","title":"VMware Tanzu Mission Control","text":"<ul> <li>Provides a centralized management platform for operating and securing multiple Kubernetes clusters and applications across multiple teams and cloud environments.</li> <li>Available via VMware cloud services</li> <li>Provides a hosted Tanzu Kubernetes Grid implementation as a managed service.</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/01_VMware_Tanzu_Portfolio/01_VMWare-Tanzu-Portfolio.html#vmware-tanzu-editions","title":"VMware Tanzu Editions","text":"<ul> <li>Tanzu editions are groupings of the VMware Tanzu products that are designed for organizations at different stages of Kubernetes Adoption:</li> <li>Basic - Run Kubernetes in vSphere</li> <li>Standard - Run and manage Kubernetes across multiple clouds</li> <li>Advanced - Simplify and secure the container lifecycle at scale and enhance app delivery.</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/01_VMware_Tanzu_Portfolio/02_Tanzu-Kubernetes-Grid-Concepts.html","title":"1.2 - Tanzu Kubernetes Grid Concepts","text":""},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/01_VMware_Tanzu_Portfolio/02_Tanzu-Kubernetes-Grid-Concepts.html#goals","title":"Goals","text":"<ul> <li>Describe the Tanzu Kubernetes Grid Concepts</li> <li>Describe the components of a Tanzu Kubernetes Grid instance</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/01_VMware_Tanzu_Portfolio/02_Tanzu-Kubernetes-Grid-Concepts.html#bootstrap-machine","title":"Bootstrap Machine","text":"<ul> <li>Typically a VM that runs the Tanzu CLI</li> <li>Used to initialize a Tanzu Kubernetes Grid instance by bootstrapping a management cluster on the cloud infrastructure of choice</li> <li>After bootstrapping the management cluster, the bootstrap machine is used to manage the Tanzu Kubernetes Grid Instance</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/01_VMware_Tanzu_Portfolio/02_Tanzu-Kubernetes-Grid-Concepts.html#tanzu-cli-and-installer-interface","title":"Tanzu CLI and Installer Interface","text":"<ul> <li>Used to initialize a Tanzu Kubernetes Grid instance by creating a management clusters</li> <li>After the management cluster's created, the Tanzu CLI is used to create/scale/upgrade/delete Tanzu Kubernetes Clusters</li> <li>The installer interface is launched from the Tanzu CLI</li> <li>A 9-step GUI wizard where you specify areas such as credentials, deployment locations, networking, etc.</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/01_VMware_Tanzu_Portfolio/02_Tanzu-Kubernetes-Grid-Concepts.html#tanzu-kubernetes-cluster-plans","title":"Tanzu Kubernetes Cluster Plans","text":"<ul> <li>A cluster plan = blueprint that describes the configuration with which to deploy a Tanzu Kubernetes cluster.</li> <li>Provides a set of configurable values that determine its settings e.g.:</li> <li>Control plane machine numbers</li> <li>worker machine numbers</li> <li>CPU and RAM values</li> <li>Two plans are provided by default:</li> <li>Dev</li> <li>Prod</li> <li>Existing cluster plans can be customised and new ones can be developed.</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/01_VMware_Tanzu_Portfolio/02_Tanzu-Kubernetes-Grid-Concepts.html#management-clusters","title":"Management Clusters","text":"<ul> <li>Management cluster - the first element deployed when creating a Tanzu Kubernetes grid instance</li> <li>A cluster dedicated to running Cluster API, which provides lifecycle management support for Tanzu Kubernetes workload clusters</li> <li>The cluster is composed of one or more control plane nodes and one or more worker nodes.</li> <li>Tn vSphere with Tanzu, the supervisor cluster performs the role of the management cluster</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/01_VMware_Tanzu_Portfolio/02_Tanzu-Kubernetes-Grid-Concepts.html#tanzu-kubernetes-clusters","title":"Tanzu Kubernetes Clusters","text":"<ul> <li>These are Kubernetes clusters deployed from the management cluster by using the Tanzu CLI</li> <li> <p>Composed of one or more control plane nodes and one or more worker nodes</p> </li> <li> <p>Note: Tanzu Kubernetes cluster and workload cluster are used interchangeably</p> </li> <li> <p>These clusters are created based on cluster plans</p> </li> <li>The entire lifecycle of these Tanzu Kubernetes clusters can be managed by the Tanzu CLI e.g. version upgrades.</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/01_VMware_Tanzu_Portfolio/02_Tanzu-Kubernetes-Grid-Concepts.html#shared-and-in-cluster-services","title":"Shared and In-Cluster Services","text":"<ul> <li>These are services that run in a Tanzu Kubernetes Grid instance. They aim to support operations such as:</li> <li>Authentication</li> <li>Ingress</li> <li>Logging</li> <li>Service discovery</li> <li>Shared services run on the management cluster or a dedicated shared-services cluster, and can be used by multiple Tanzu Kubernetes clusters</li> <li>The Tanzu Kubernetes Grid extensions bundle is downloaded from the VMware website and provides the required YAML configurations</li> <li>In-cluster services are typically deployed to specific Tanzu Kubernetes clusters.</li> <li>Shared services can be deployed typically to their own clusters.</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/01_VMware_Tanzu_Portfolio/02_Tanzu-Kubernetes-Grid-Concepts.html#tanzu-kubernetes-grid-instances","title":"Tanzu Kubernetes Grid Instances","text":"<ul> <li>A Tanzu Kubernetes Grid instance is a full deployment of Tanzu Kubernetes Grid i.e.:</li> <li>The management cluster</li> <li>The deployed Tanzu Kubernetes clusters</li> <li>The shared and in-cluster services configured for usage.</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/01_VMware_Tanzu_Portfolio/02_Tanzu-Kubernetes-Grid-Concepts.html#bootstrapping-many-instances","title":"Bootstrapping Many Instances","text":"<ul> <li>A single bootstrap machine can bootstrap many instances of Tanzu Kubernetes Grid e.g.:</li> <li>Environments e.g. production/development</li> <li>IaaS providers e.g. vSphere, AWS</li> <li>Failure domains e.g. ,"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/01_VMware_Tanzu_Portfolio/02_Tanzu-Kubernetes-Grid-Concepts.html#registering-with-tanzu-mission-control","title":"Registering with Tanzu Mission Control","text":"<ul> <li>When bootstrapping multiple Tanzu Kubernetes Grid Instances, one can register all the instances with VMware Tanzu Mission Control to make the process easier.</li> <li>Tanzu Mission Control provides a centralized control plane for:</li> <li>Applying consistent Kubernetes policies across clusters</li> <li>Self-service provisioning and lifecycle management of clusters</li> <li>Data protection of Kubernetes cluster workloads</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/01_VMware_Tanzu_Portfolio/03_The-Tanzu-CLI.html","title":"1.3 - The Tanzu CLI","text":""},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/01_VMware_Tanzu_Portfolio/03_The-Tanzu-CLI.html#objectives","title":"Objectives","text":"<ul> <li>Describe Tanzu CLI</li> <li>List requirements for a bootstrap machine</li> <li>Describe the Carvel Tool Set</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/01_VMware_Tanzu_Portfolio/03_The-Tanzu-CLI.html#tanzu-cli","title":"Tanzu CLI","text":"<ul> <li>Available on all major OS (Windows/Linux/Mac OS)</li> <li>Used to interact with different products in the VMWare Tanzu Portfolio</li> <li>Used with Tanzu Kubernetes Grid to:</li> <li>Initialize a Tanzu Kubernetes Grid Instance</li> <li>Create, scale, upgrade, and delete Tanzu Kubernetes clusters</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/01_VMware_Tanzu_Portfolio/03_The-Tanzu-CLI.html#tanzu-cli-plugins","title":"Tanzu CLI Plugins","text":"<ul> <li>Each product associated with Tanzu makes up a plugin that works with the Tanzu CLI</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/01_VMware_Tanzu_Portfolio/03_The-Tanzu-CLI.html#installing-plugins","title":"Installing Plugins","text":"<ul> <li>Various methods available:</li> <li>From the internet: <code>tanzu plugin install &lt;plugin name&gt;</code></li> <li>From a local directory: <code>tanzu plugin install &lt;plugin name&gt; --local &lt;dir&gt;</code></li> <li>All available plugins: <code>tanzu plugin install all |--local &lt;directory&gt;|</code></li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/01_VMware_Tanzu_Portfolio/03_The-Tanzu-CLI.html#tanzu-cli-syntax","title":"Tanzu CLI Syntax","text":"<ul> <li>General syntax format: <code>tanzu &lt;resource&gt; |&lt;sub-resource&gt;| &lt;verbs&gt; --&lt;options&gt;</code></li> <li>Examples:</li> <li><code>tanzu management-cluster get</code></li> <li><code>tanzu cluster create --file &lt;/path/to/file.yaml&gt;</code></li> <li><code>tanzu cluster list</code></li> <li><code>tanzu cluster get &lt;cluster name&gt;</code></li> <li><code>tanzu cluster kubeconfig get --admin</code></li> <li><code>tanzu kubernetes-release get</code></li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/01_VMware_Tanzu_Portfolio/03_The-Tanzu-CLI.html#cli-configuration-files","title":"CLI Configuration Files","text":"<ul> <li>The following CLI configuration files are created via the Tanzu CLI:</li> </ul> <ul> <li>TLDR:</li> <li>config - stores Kubernetes contexts for management clusters managed by the Tanzu CLI</li> <li>config.yaml - List of management clusters managed by the Tanzu CLU</li> <li>providers - Sores YAMLs templated and merged when cluster configuration is generated</li> <li>clusterconfigs - management cluster configuration files created by the installer UI</li> <li>bom - Bill of Materials files - list container image versions provided by installed TKG version e.g Pod images</li> <li>sessions.yaml - used by pinniped-auth plugin to store authenticated session details when accessing clusters.</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/01_VMware_Tanzu_Portfolio/03_The-Tanzu-CLI.html#bootstrap-machine-requirements","title":"Bootstrap Machine Requirements","text":"<ul> <li>as this is where a Tanzu Kubernetes Grid instance will be initialized, there are prerequisites:</li> <li>Kubectl must be installed</li> <li>Docker must be installed (CLI for Linux), Desktop for Mac or Windows</li> <li>Minimum 6Gb memory for the containers used during the bootstrapping process</li> <li>IP network connectivity to the vCenter Server instance and the network where the management cluster is to be deployed.</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/01_VMware_Tanzu_Portfolio/03_The-Tanzu-CLI.html#installer-interface","title":"Installer Interface","text":"<ul> <li>The TKG installer interface is a GUI wizard that guides the configuration of a management cluster.</li> <li>To start it up: <code>tanzu management-cluster create --ui</code></li> <li>First step to select IaaS system e.g. vSphere, AWS, Azure</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/01_VMware_Tanzu_Portfolio/03_The-Tanzu-CLI.html#carvel-tools","title":"Carvel Tools","text":"<ul> <li>A collection of CLI tools that help in building, configuring and deploying Kubernetes/Kubernetes-based workloads</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/01_VMware_Tanzu_Portfolio/04_%20Lab-1-Setting-Up-a-Bootstrap-Machine.html","title":"1.4 - Lab 1 - Setting Up a Bootstrap Machine","text":""},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/01_VMware_Tanzu_Portfolio/04_%20Lab-1-Setting-Up-a-Bootstrap-Machine.html#verify-the-installation-of-docker","title":"Verify the Installation of Docker","text":"<ul> <li><code>sudo systemctl status docker</code> - Output should be <code>Active (running)</code></li> <li>Run a docker command e.g. <code>docker ps</code></li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/01_VMware_Tanzu_Portfolio/04_%20Lab-1-Setting-Up-a-Bootstrap-Machine.html#install-kubectl-cli","title":"Install Kubectl CLI","text":"<ul> <li>Donwload kubectl from the appropriate source</li> <li>Unzip: <code>gunzip &lt;kubectl zip file&gt;</code></li> <li>Install to <code>/bin</code> directory: <code>sudo install kubectl-linux-v1.20.5-vmware.1 /usr/local/bin/kubectl</code></li> <li>Verify installation: <code>kubectl version --short --client=true</code></li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/01_VMware_Tanzu_Portfolio/04_%20Lab-1-Setting-Up-a-Bootstrap-Machine.html#enable-kubernetes-autocomplete","title":"Enable Kubernetes AutoComplete","text":"<ol> <li> <p>Run the following command to insert the Kubernetes CLI autocompletion data into the .bash_profile file.</p> <p><code>echo 'source &lt;(kubectl completion bash)' &gt;&gt; ~/.bash_profile</code></p> <p>Autocompletion will be enabled when a new terminal is opened.</p> </li> <li> <p>Close the terminal window.</p> </li> <li>Reopen the terminal window.</li> <li> <p>Enter the <code>kubectl</code> command and press the Tab key twice.</p> <p><code>kubectl</code></p> <p>The autocompletion displays the available kubectl options.</p> <p>Use the Tab Tab keystroke sequence, in future labs, to reduce the amount of commands that you need to enter.</p> </li> </ol>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/01_VMware_Tanzu_Portfolio/04_%20Lab-1-Setting-Up-a-Bootstrap-Machine.html#install-the-tanzu-cli","title":"Install the Tanzu CLI","text":"<ol> <li> <p>Using the terminal, navigate to the <code>Downloads</code> directory.</p> <p><code>cd ~/Downloads</code></p> </li> <li> <p>Unzip the tanzu CLI.</p> <p><code>tar -xvf tanzu-cli-bundle-v1.3.1-linux-amd64.tar</code></p> </li> <li> <p>Install the Tanzu CLI to the <code>Bin</code> directory.</p> <p><code>sudo install cli/core/v1.3.1/tanzu-core-linux_amd64 /usr/local/bin/tanzu</code></p> </li> <li> <p>Display the Tanzu CLI version to ensure that the installation was successful.</p> <p><code>tanzu version</code></p> <p>The output displays the version number.</p> </li> </ol>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/01_VMware_Tanzu_Portfolio/04_%20Lab-1-Setting-Up-a-Bootstrap-Machine.html#install-the-tanzu-cli-plugins","title":"Install the Tanzu CLI Plugins","text":"<ol> <li> <p>Using the terminal, navigate to the <code>Downloads</code> directory.</p> <p><code>cd ~/Downloads</code></p> </li> <li> <p>Display the installed plug-ins.</p> <p><code>tanzu plugin list</code></p> <p>The STATUS column displays <code>not installed</code>.</p> </li> <li> <p>Install the Tanzu CLI plug-in from a local <code>CLI</code> subdirectory.</p> <p><code>tanzu plugin install all --local cli</code></p> </li> <li> <p>Display the installed plug-ins.</p> <p><code>tanzu plugin list</code></p> <p>The STATUS column for each plug-in, except for alpha, displays <code>installed</code>.</p> </li> <li> <p>Run the Tanzu CLI to view the new options.</p> <p><code>tanzu --help</code></p> <p>The cluster, kubernetes-release, management-cluster, and login commands display.</p> </li> </ol>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/01_VMware_Tanzu_Portfolio/04_%20Lab-1-Setting-Up-a-Bootstrap-Machine.html#enable-tanzu-cli-autocomplete","title":"Enable Tanzu CLI Autocomplete","text":"<ol> <li> <p>Run the following command to insert the Tanzu CLI autocompletion data into the .bash_profile file.</p> <p><code>echo 'source &lt;(tanzu completion bash)' &gt;&gt; ~/.bash_profile</code></p> <p>Autocompletion will be enabled when a new terminal is opened.</p> </li> <li> <p>Close the terminal window.</p> </li> <li>Reopen the terminal window.</li> <li> <p>Enter the <code>tanzu</code> command and then press the Tab key twice.</p> <p><code>tanzu</code></p> <p>Autocompletion displays the available <code>tanzu</code> options.</p> <p>Use the Tab Tab keystroke sequence in future labs to reduce the amount of commands that you need to enter.</p> </li> </ol>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/01_VMware_Tanzu_Portfolio/04_%20Lab-1-Setting-Up-a-Bootstrap-Machine.html#install-the-carvel-tools","title":"Install the Carvel Tools","text":"<ol> <li> <p>Using the terminal, navigate to the <code>Downloads</code> directory.</p> <p><code>cd ~/Downloads</code></p> </li> <li> <p>List the tools.</p> <p><code>ls -l cli/*.gz</code></p> <p>Tools imgpkg, kapp, kbld, vendir, and ytt display.</p> </li> <li> <p>Run the following script to unzip and install the tools.</p> <p><code>./install-carvel.sh</code></p> <p>The version of each tool displays.</p> </li> <li> <p>Using the terminal, navigate to the <code>Downloads</code> directory.</p> <p><code>cd ~/Downloads</code></p> </li> <li> <p>List the tools.</p> <p><code>ls -l cli/*.gz</code></p> <p>Tools imgpkg, kapp, kbld, vendir, and ytt display.</p> </li> <li> <p>Run the following script to unzip and install the tools.</p> <p><code>./install-carvel.sh</code></p> <p>The version of each tool displays.</p> </li> </ol>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/01_VMware_Tanzu_Portfolio/05_Cluster-API.html","title":"1.5 - Cluster API","text":""},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/01_VMware_Tanzu_Portfolio/05_Cluster-API.html#objectives","title":"Objectives","text":"<ul> <li>Describe Cluster API</li> <li>List infrastructure providers</li> <li>Detail Cluster API controllers</li> <li>List the Cluster API Custom Resource definitions</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/01_VMware_Tanzu_Portfolio/05_Cluster-API.html#introduction","title":"Introduction","text":"<ul> <li>Cluster API is a Kubernetes project to provide declarative Kubernetes-styled APIs to cluster provisioning, configuration and management</li> <li>Cluster API controllers running on a Kubernetes cluster receives Cluster API definitions that specify the desired state of the new cluster</li> <li>They also send requests to the cloud/IaaS provider(s) to create the new cluster by using the required provider configurations.</li> </ul> <pre><code>CRD = Custom Resource Definition\n</code></pre>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/01_VMware_Tanzu_Portfolio/05_Cluster-API.html#cluster-api-custom-resource-definitions-crds","title":"Cluster API Custom Resource Definitions (CRDs)","text":"<ul> <li>Cluster API provides custom resource definitions (CRDs) to management clusters to help the management cluster know what to deploy to the particular workload cluster(s).</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/01_VMware_Tanzu_Portfolio/05_Cluster-API.html#machine-health-checks","title":"Machine Health Checks","text":"<ul> <li>Allows cluster API to monitor the health of cluster nodes</li> <li>A timeout is specified for each of the conditions that can be checked</li> <li>If any of the conditions are met for the duration of the timeout, the machine is remediated</li> <li>The action of remediating a machine triggers a new machine to be created to replace the old one.</li> </ul> <ul> <li>**Note: This is equivalent to the traditional Readiness and Liveness probe functionalities available in Kubernetes for deployments.</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/01_VMware_Tanzu_Portfolio/05_Cluster-API.html#cluster-api-components","title":"Cluster API Components","text":"<ul> <li>Logic in the Cluster API is divided per component</li> <li>Bootstrap providers - perform the steps to configure and start the Kubernetes processes that make up a cluster</li> <li>Infrastructure providers - perform the steps to create cloud resources e.g. VMs</li> <li>Basically like how providers work in Terraform</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/01_VMware_Tanzu_Portfolio/05_Cluster-API.html#bootstrap-provideers","title":"Bootstrap Provideers","text":"<ul> <li>Responsible for turning a machine into a Kubernetes Node</li> <li>Cluster API bootstrap provider <code>kubeadm</code> is a bootstrap provider implementation that utilises <code>kubeadm</code> to perform machine configuration to set up a Kubernetes Cluster (management in this case).</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/01_VMware_Tanzu_Portfolio/05_Cluster-API.html#infrastructure-providers","title":"Infrastructure Providers","text":"<ul> <li>All in format CAP <li>E.g. CAPV = vSphere, CAPA = AWS, CAPD = Docker</li> <li>Only some of these are supported by Tanzu Kubernetes Grid 1.3 directly:</li> <li>CAPV - vSPhere</li> <li>CAPA - AWS</li> <li>CAPZ - Azure</li>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/01_VMware_Tanzu_Portfolio/05_Cluster-API.html#cluster-api-controllers","title":"Cluster API Controllers","text":"<ul> <li>Responsible for watching the Kubernetes API for cluster API (CAPI) resources and perform the required steps to provision clusters</li> </ul> <p>Note: This is analagous to the traditional kube-system deployments like kube-scheduler, kube-controller-manager, etc.</p>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/01_VMware_Tanzu_Portfolio/05_Cluster-API.html#vsphere-cluster-api-provider-capv","title":"vSphere Cluster API Provider (CAPV)","text":"<ul> <li>An infrastructure provider that provides the following controller and custom resource definitions</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/01_VMware_Tanzu_Portfolio/05_Cluster-API.html#summary","title":"Summary","text":"<ul> <li> <p>Note: Cluster API ONLY runs on the Management Cluster</p> </li> <li> <p>Note: You can think of them equivalent to the traditional kube-system deployments</p> </li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/01_VMware_Tanzu_Portfolio/06_End-of-Module-Assessment.html","title":"1.6 - End of Module Assessment","text":"<ol> <li>What component of TKG manages cluster lifecycle?<ol> <li>Cluster API</li> </ol> </li> <li>What's the purpose of the bootstrap machine?<ol> <li>Used to initialize a management cluster</li> </ol> </li> <li>Which configuration file stores management cluster kube contexts?     1.</li> </ol>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/02_Management_Clusters/01_Overview-of-Management-Clusters.html","title":"2.1 - Overview of Management Clusters and Preparing the vSphere","text":""},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/02_Management_Clusters/01_Overview-of-Management-Clusters.html#importance","title":"Importance","text":"<ul> <li>Tanzu Kubernetes Grid management clusters offer Kubernetes lifecycle functionality of Tanzu Kubernetes Grid for the supported platforms e.g. vSphere and the workload clusters within.</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/02_Management_Clusters/01_Overview-of-Management-Clusters.html#vsphere-requirements","title":"vSphere Requirements","text":"<ul> <li>For vSphere deployment, Tanzu Kubernetes Grid requires:</li> <li>vSphere 6.7 Update 3, vSphere 7, VMWare Cloud on AWS or Azure VMware Solution</li> <li>A vSphere cluster with minimum 2 ESXi hosts and vSphere DRS enabled</li> <li>A DHCP server to provide IP addresses to Tanzu Kubernetes grid management and workload clusters</li> <li>Traffic to vCenter server allowed from the network on which the clusters run</li> <li>A dedicated vCenter SSO account with permissions required for Tanzu Kubernetes Grid</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/02_Management_Clusters/01_Overview-of-Management-Clusters.html#port-requirements","title":"Port Requirements","text":""},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/02_Management_Clusters/01_Overview-of-Management-Clusters.html#dedicated-vsphere-sso-user","title":"Dedicated vSphere SSO User","text":"<ul> <li>When deploying TKG using a dedicated vSphere SSO user, the TKG user should be assigned a role allowing access to the the following:</li> <li>Datacenters or data center folders</li> <li>Datastores or datastore folders</li> <li>Hosts, clusters or resource pools</li> <li>Networks to which the clusters are assigned</li> <li>VM and template folders</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/02_Management_Clusters/01_Overview-of-Management-Clusters.html#ssh-key-pairs","title":"SSH Key Pairs","text":"<ul> <li>An optional SSH public key is added to all VMs created by Tanzu Kubernetes Grid</li> <li>The SSH key is used to remotely connect to a VM using SSH to troubleshoot/manage the VMs</li> <li>If not provided, connecting to a control plane or worker node is not possible</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/02_Management_Clusters/01_Overview-of-Management-Clusters.html#ova-templates","title":"OVA Templates","text":"<ul> <li>TKG provides base OS image templates in OVA format to import to vSphere:</li> <li>Ubuntu v20.04 Kubernetes v1.xx.yy OVA (Default)</li> <li>Photon v3 Kubernetes v1.xx.yy OVA</li> <li>TKG creates the management cluster and TK cluster node VMs from these templates</li> <li>After importing the OVA files, they must be converted to VM templates</li> <li>The base OS image template includes the version of Kubernetes that TKG uses to create clusters</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/02_Management_Clusters/01_Overview-of-Management-Clusters.html#deploying-in-internet-restricted-environments","title":"Deploying in Internet-Restricted Environments","text":"<ul> <li>Deploying in an Internet-Restricted environment is supported by loading all container images into a private Docker registry</li> <li>Within firewall - install and configure a private Docker registry e.g. Harbor</li> <li>Run <code>docker pull</code> to pull all the images required</li> <li>Use <code>docker tag</code> and <code>docker push</code> to push them into the private registry</li> <li>Set any required environment variables e.g. <code>TKG_CUSTOM_IMAGE_REPOSITORY=\"registry URL\"</code></li> <li>Proceed with the normal installation steps</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/02_Management_Clusters/02_Lab-2-TKG_Container-Images.html","title":"2.2 - Lab 2","text":""},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/02_Management_Clusters/02_Lab-2-TKG_Container-Images.html#objectives","title":"Objectives","text":"<ol> <li>Create a Library in Harbor</li> <li>Push Tanzu Kubernetes Grid Container Images to Harbor</li> </ol>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/02_Management_Clusters/02_Lab-2-TKG_Container-Images.html#create-a-library-in-harbor","title":"Create a Library in Harbor","text":"<ol> <li>Using Firefox, open the Harbor bookmark in a new tab.</li> <li>Log in to Harbor.<ul> <li>User name: admin</li> <li>Password: VMware1!</li> </ul> </li> <li>On the Projects page, click NEW PROJECT.<ol> <li>Enter tkg for Project Name.</li> <li>For Access Level select Public.</li> </ol> </li> <li>Click OK.</li> </ol>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/02_Management_Clusters/02_Lab-2-TKG_Container-Images.html#push-tanzu-kubernetes-grid-container-images-to-harbor","title":"Push Tanzu Kubernetes Grid Container Images to Harbor","text":"<ol> <li> <p>Using the terminal, navigate to the <code>Workspace</code> directory.</p> <p><code>cd ~/Workspace</code></p> </li> <li> <p>Log in to Harbor using the docker CLI.</p> <p><code>docker login harbor.vclass.local</code></p> <ul> <li>User name: admin</li> <li>Password: VMware1!</li> <li>Run the publish script to push container images from the student desktop to Harbor.</li> </ul> <p><code>./tag-push-images.sh</code></p> <p>This process will take approximately 20 minutes.</p> </li> <li> <p>In Harbor, click Projects.</p> </li> <li> <p>Click tkg.</p> <p>The Tanzu Kubernetes Grid container images display.</p> </li> </ol>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/02_Management_Clusters/03_Lab-3-Create-VM-Templates.html","title":"2.3 - Lab 3","text":""},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/02_Management_Clusters/03_Lab-3-Create-VM-Templates.html#objectives","title":"Objectives","text":"<ol> <li>Log in to the vSphere Client</li> <li>Create Resource Pools</li> <li>Create a VM Folder</li> <li>Import the Base OS Template OVA Files</li> <li>Convert the VMs to Templates</li> </ol>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/02_Management_Clusters/03_Lab-3-Create-VM-Templates.html#login-to-the-vsphere-client","title":"Login to the vSphere Client","text":"<ol> <li>Using Firefox, open the vSphere Client bookmark in a new tab.</li> <li>Log in to the vSphere Client.<ul> <li>User name: administrator@vsphere.local</li> <li>Password: VMware1!</li> </ul> </li> <li>Expand SA-Datacenter and expand SA-Cluster-01.</li> <li>Verify that nsx-adv-lb-controller is powered on.<ol> <li>If nsx-adv-lb-controller is not powered on, right-click nsx-adv-lb-controller and select Power &gt; Power On.</li> </ol> </li> </ol>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/02_Management_Clusters/03_Lab-3-Create-VM-Templates.html#create-the-resource-pools","title":"Create the Resource Pools","text":"<p>You create resource pools for the management cluster and workload clusters.</p> <ol> <li>In the vSphere Client, click Menu.</li> <li>Click Hosts and Clusters.</li> <li>In the navigation pane, expand SA-Datacenter.</li> <li>Right-click SA-Cluster-01.</li> <li>Click New Resource Pool.</li> <li>Enter rp-tkg-management in the Name text box and click OK.</li> <li>Right-click SA-Cluster-01.</li> <li>Click New Resource Pool.</li> <li>Enter rp-tkg-production in the Name text box and click OK.</li> </ol>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/02_Management_Clusters/03_Lab-3-Create-VM-Templates.html#create-a-vm-folder","title":"Create a VM Folder","text":"<p>You create a VM folder to place VMs created by Tanzu Kubernetes Grid.</p> <ol> <li>In the vSphere Client, click Menu.</li> <li>Click VMs and Templates.</li> <li>In the navigation pane, right-click SA-Datacenter.</li> <li>Select New Folder &gt; New VM and Template Folder.</li> <li>In the New Folder window, enter tkg-vms in the Enter a name for the folder text box and click OK.</li> </ol>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/02_Management_Clusters/03_Lab-3-Create-VM-Templates.html#import-the-base-os-template-ova-files","title":"Import the Base OS Template OVA Files","text":"<ol> <li>In the vSphere Client, click Menu.</li> <li>Click Hosts and Clusters.</li> <li>In the navigation pane, expand SA-Datacenter.</li> <li>Right-click SA-Cluster-01.</li> <li>Click Deploy OVF Template.</li> <li>In the Deploy OVF Template window, select Local file &gt; UPLOAD FILES under Select an OVF template.</li> <li>In the Open window, select Downloads.</li> <li>Select ubuntu-2004-kube-v1.20.5-vmware.1-tkg.1-16555584836258482890.ova and click Open.</li> <li>In the Deploy OVF Template window, click NEXT.</li> <li>On the Select a name and folder form, leave the default value for the virtual machine name.</li> <li>Select tkg-vms for the virtual machine location and click NEXT.</li> <li>Select SA-Cluster-01 for the compute resource and click NEXT.</li> <li>On the Review details form, click NEXT.</li> <li>Select I accept all license agreements and click NEXT.</li> <li>Select SA-Shared-01 for storage.</li> <li> <p>Select Thin Provision as the virtual disk format and click NEXT.</p> <p>IMPORTANT:</p> <p>Ensure that you select Thin Provision after selecting the storage.</p> </li> <li> <p>Select pg-SA-Production for the destination network and click NEXT.</p> </li> <li> <p>Click FINISH.</p> <p>Wait for the OVF deployment to finish before proceeding to the next task.</p> </li> </ol>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/02_Management_Clusters/03_Lab-3-Create-VM-Templates.html#convert-the-vms-to-templates","title":"Convert the VMs to Templates","text":"<ol> <li>In the vSphere Client, click Menu.</li> <li>Click VMs and Templates.</li> <li>Right-click ubuntu-2004-kube-v1.20.5+vmware.1.</li> <li>Click Template.</li> <li>Click Convert to Template.</li> <li>In the Confirm Convert window, click YES.</li> </ol>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/02_Management_Clusters/04_NSX-Advanced-Load-Balancer.html","title":"2.4 - NSX Advanced Load Balancer","text":""},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/02_Management_Clusters/04_NSX-Advanced-Load-Balancer.html#objectives","title":"Objectives","text":"<ul> <li>Describe the NSX Advanced Load balancer components</li> <li>Explain how Tanzu Kubernetes Grid integrates with NSX Load Balancer</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/02_Management_Clusters/04_NSX-Advanced-Load-Balancer.html#load-balancing-kubernetes-services","title":"Load Balancing Kubernetes Services","text":"<ul> <li>TKG supports optional integration with NSX advanced load balancer</li> <li>NSX Advanced Load Balancer doesn't require any licenses to be used with TKG</li> <li>The integration allows NSX Advanced Load Balancer to provide layer-4 load balancing services when a load balancer service type is created on a TK workload cluster.</li> <li>NSX Advanced Load Balancer does not perform load balancing for the Kubernetes API on control plane nodes - this is handled by kube-vip.</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/02_Management_Clusters/04_NSX-Advanced-Load-Balancer.html#nsx-advanced-load-balancer","title":"NSX Advanced Load Balancer","text":""},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/02_Management_Clusters/04_NSX-Advanced-Load-Balancer.html#components","title":"Components","text":""},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/02_Management_Clusters/04_NSX-Advanced-Load-Balancer.html#architecture","title":"Architecture","text":"<ul> <li>When applications running on a workload cluster are exposed using a load balancer service type, traffic inbound to the apps is routed to the Service Engines, from which they're routed to a NodePort IP address on the workload cluster.</li> </ul> <ul> <li>The AVI controller determines the configuration and operations of the Service engines</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/02_Management_Clusters/04_NSX-Advanced-Load-Balancer.html#ako-operator-functionality","title":"AKO Operator Functionality","text":"<ul> <li>The AKO Operator (AKOO) runs on management clusters and is reponsible for deploying the AKO on workload clusters.</li> <li>When any new workload cluster is deployed, the AKOO:</li> <li>Creates a user dedicated to the workload cluster in the NSX Advanced LoadBalancer controller</li> <li>Creates a Kubernetes secret in the workload cluster; containing the new user credentials</li> <li>Deploys the AKO Controller to the workload cluster</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/02_Management_Clusters/04_NSX-Advanced-Load-Balancer.html#avi-kubernetes-operator-functionality","title":"AVI Kubernetes Operator Functionality","text":"<ul> <li>Avi Kubernetes Operator = AKO</li> <li>When a service of type load balancer is created on the workload cluster, the AKO Operator sends a request to the NSX Advanced Load Balancer controller to create a virtual service</li> <li>The NSX Advanced Load Balancer controller sends a request to the SE to configure a virtual service</li> <li>The SE load balances traffic to pods running in the cluster.</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/02_Management_Clusters/05_Cluster-Authentication.html","title":"2.5 - Cluster Authentication","text":""},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/02_Management_Clusters/05_Cluster-Authentication.html#objectives","title":"Objectives","text":"<ul> <li>Describe how Kubernetes manages authentication</li> <li>Explain what Pinniped is</li> <li>Explain what Dx is</li> <li>Describe the Pinniped authentication workflow</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/02_Management_Clusters/05_Cluster-Authentication.html#external-authentication","title":"External Authentication","text":"<ul> <li>Users aren't directly managed or authenticated by Kubernetes</li> <li>An external identity service must be used to generate trusted tokens or certificates for users to communicate with the Kubernetes API server</li> <li>Kubernetes supports OIDC tokens as a way to identify users accessing the cluster.</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/02_Management_Clusters/05_Cluster-Authentication.html#user-authentication-pinniped","title":"User Authentication - Pinniped","text":"<ul> <li>In TKG, user authentication is provided by Pinniped by default</li> <li>Allows integration of external OIDC OR LDAP identity providers into Tanzu Kubernetes clusters so access to said clusters can be controlled.</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/02_Management_Clusters/05_Cluster-Authentication.html#ldap-authentication-dex","title":"LDAP Authentication - Dex","text":"<ul> <li>Dex is an identity service that supports LDAP authentication</li> <li>Pinniped uses Dex if LDAP authentication is required.</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/02_Management_Clusters/05_Cluster-Authentication.html#kubeconfig-file-setup-pinniped","title":"KubeConfig File Setup - Pinniped","text":"<ul> <li>When Pinniped authentication is enabled, the <code>tanzu cluster kubeconfig get</code> command returns a kubeconfig file that's configured to run the Tanzu pinniped-auth plugin</li> <li>The plugin initiates the Pinniped authentication workflow</li> <li>Users will be redirected to the Pinniped supervisor endpoint on the management cluster</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/02_Management_Clusters/05_Cluster-Authentication.html#pinniped-authentication-workflow","title":"Pinniped Authentication Workflow","text":"<ol> <li>Kubectl calls the Tanzu CLI</li> <li>Tanzu CLI realises that the user requesting doesn't have a token for authentication; opens a web browser to the Pinniped supervisor - redirecting to the Dex Login Page</li> <li>The user provides their LDAP credentials and Dex authenticates with the LDAP server \u2192 Redirecting to Pinniped Supervisor</li> </ol> <ol> <li>Pinniped Supervisor generates an ID token and passes it to the Tanzu CLI</li> <li>The Tanzu CLI sends the token to Pinniped concierge on the workload cluster</li> <li>Pinniped concierge swaps the ID token for client certificate via the Pinniped supervisor</li> </ol> <ol> <li>The client certificate is passed to the Tanzu CLI</li> <li>The client certificate is passed to kubectl</li> <li>kubectl sends the client certificate with its request, the client certificate is already now trusted by the cluster and the user is authenticated.</li> </ol>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/02_Management_Clusters/05_Cluster-Authentication.html#pinniped-session-cache","title":"Pinniped Session Cache","text":"<ul> <li>On successful login, the Tanzu CLI stores the client certificate to <code>~/.tanzu/pinniped/session.yaml</code></li> <li>Deleting this file will require users to re-authenticate via the process outlined above.</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/02_Management_Clusters/06_Creating-Management-Clusters.html","title":"2.6 - Creating Management Clusters","text":""},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/02_Management_Clusters/06_Creating-Management-Clusters.html#objectives","title":"Objectives","text":"<ul> <li>List the steps involved in installing a TKG Management Cluster</li> <li>Describe what occurs when a management cluster is created</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/02_Management_Clusters/06_Creating-Management-Clusters.html#installing-tkg-instances","title":"Installing TKG Instances","text":"<ul> <li> <p>The Tanzu CLI creates a temporary bootstrap cluster, which is used to create the management cluster.</p> </li> <li> <p>Note: Cluster = Collection of VMs (Nodes)</p> </li> </ul> <p></p> <ul> <li>The boostrap cluster will use the appropriate Cloud Infrastructure Provider to deploy the required resources</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/02_Management_Clusters/06_Creating-Management-Clusters.html#deploying-management-clusters-vsphere-7","title":"Deploying Management Clusters - vSphere 7","text":"<ul> <li>When deploying management clusters on vSphere 7, there are two options:</li> <li>vSphere (Tanzu Enabled)<ul> <li>Deploying a management cluster isn't supported</li> <li>The supervisor cluster performs the role of the management cluster</li> <li>The Tanzu CLI connects to the Supervisor cluster</li> <li>Carries out the roles associated with the management cluster</li> </ul> </li> <li>vSphere (Tanzu Disabled)<ul> <li>Deployed as per vSphere 6.7</li> </ul> </li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/02_Management_Clusters/06_Creating-Management-Clusters.html#deploying-management-clusters-installer-ui-or-cli","title":"Deploying Management Clusters: Installer UI or CLI","text":"<ul> <li>Management clusters can be deployed via two main methods:</li> <li>Run the Tanzu Kubernetes Grid Installer<ul> <li>Wizard providing guidance through the deployment process</li> <li>Recommended method - especially for platform operators new to TKG</li> <li>Configuration generated by the installer UI is saved to a YAML configuration file</li> </ul> </li> <li>Create and edit YAML configuration files generated by the installer UI<ul> <li>More advanced configuration options can be added prior to deployment</li> </ul> </li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/02_Management_Clusters/06_Creating-Management-Clusters.html#tanzu-cli-management-cluster-plugin","title":"Tanzu CLI Management Cluster Plugin","text":"<ul> <li>When deploying a management cluster using the Tanzu CLI, the <code>Tanzu Management-Cluster</code> plugin will be used</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/02_Management_Clusters/06_Creating-Management-Clusters.html#tanzu-cli-management-cluster-create","title":"Tanzu CLI Management Cluster Create","text":"<ul> <li>When using <code>tanzu management-cluster create</code>:</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/02_Management_Clusters/06_Creating-Management-Clusters.html#tanzu-kubernetes-grid-installer","title":"Tanzu Kubernetes Grid Installer","text":"<ol> <li>Access the installer UI - <code>tanzu management-cluster create --ui</code><ol> <li>Select the desired IaaS provider</li> </ol> </li> </ol> <ol> <li>Assuming vSphere's selected, provide vCenter FQDN, username and password<ol> <li>Auto-determines version of vSphere</li> <li>For vSphere 7 and above - additional dialog will be added questioning if Tanzu is already enabled on the cluster \u2192 Determines whether TKG Management cluster can be deployed or not.</li> <li>Specify the SSH public key associated with the datacenter</li> </ol> </li> </ol> <ol> <li>Choose cluster plan to be developed and any additional configuration - development and production are pre-available by default<ol> <li>Provide management cluster name</li> <li>Control plane endpoint - IP address for Kube-VIP to connect to</li> <li>Configure control plane and worker node size/resource configuration</li> </ol> </li> </ol> <ol> <li>Optional - Configure the NSX Load Balancer e.g.<ol> <li>FQDN Name (VM to connect to)</li> <li>Credentials</li> <li>Certificate Authority</li> <li>Load Balancing-specific configuration<ol> <li>VIP Network name</li> <li>VIP Network CIDR</li> </ol> </li> </ol> </li> <li>Specify any metadata and labels to be applied to the cluster e.g. location, labels, description</li> <li>Specify the VM folder, datastore and compute resource<ol> <li>Outline where the VMs should be stored</li> <li>What cluster, hosts and resource pools should be utilised?</li> <li>What datastore should be used?</li> </ol> </li> </ol> <ol> <li>Specify the vSphere network, Kubernetes service and pod CIDR ranges<ol> <li>Configure the CNI (Container Network Interface)</li> <li>Cluster CIDR - For Kubernetes services</li> <li>Cluster Pod CIDR</li> <li>Network name</li> </ol> </li> </ol> <ol> <li>Configure Identity Management<ol> <li>What provider is being used?</li> <li>What configuration is required for that provider? E.g. LDAP has its own particular configuration requirements</li> </ol> </li> </ol> <ol> <li>Specify the base OS for Kubernetes to run on<ol> <li>This will be the OS OVA file that was converted into a template</li> </ol> </li> <li>Register the Management cluster with Tanzu Mission control if desired</li> <li>Deploy the cluster and wait<ol> <li>Note the location where the configuration is stored.</li> </ol> </li> </ol>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/02_Management_Clusters/06_Creating-Management-Clusters.html#post-installation","title":"Post-Installation","text":"<ul> <li>Once installation is complete, the browser can be closed.</li> <li>The management cluster configuration file generated by the installer is saved to <code>.tanzu/tkg/clusterconfigs</code></li> <li>In the file are configuration parameters identical to those passed in the Wizard</li> <li>E.g. Plan chosen</li> <li>Kubernetes CIDRs</li> <li>VM Characteristics</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/02_Management_Clusters/06_Creating-Management-Clusters.html#cluster-api-configuration","title":"Cluster API Configuration","text":"<ul> <li>The cluster API components deployed to the management cluster are found at <code>.tanzu/tkg/providers</code></li> <li><code>cluster-api/v0.3.13/core-components.yaml</code></li> <li><code>control-plane-kubeadm/v0.3.14/control-plane-components.yaml</code></li> <li><code>bootstrap-kubeadm/v0.3.14/bootstrap-components.yaml</code></li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/02_Management_Clusters/06_Creating-Management-Clusters.html#infrastructure-configuration","title":"Infrastructure Configuration","text":"<ul> <li>Cluster API for vSphere controllers and CRDs listed in the following YAML file are deployed on the management cluster when running vSphere 6.7: <code>.tanzu/tkg/providers/infrastructure-vsphere/v0.7.7/infrastructure-components.yaml</code></li> <li>For vSphere 7 - no configuration is required as the supervisor cluster is preconfigured with Cluster API for vSphere controllers and CRDs</li> <li>AWS and Azure's provider deployment YAMLs are stored under the <code>.tanzu/tkg/providers/</code> folder if chosen as well</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/02_Management_Clusters/06_Creating-Management-Clusters.html#management-cluster-deployment-workflow","title":"Management Cluster Deployment Workflow","text":"<ul> <li> <p>The <code>tanzu management-cluster create</code> command:</p> <ol> <li>Validates the provided configuration</li> <li>Using <code>kind</code> it creates a local boostrap cluster on the boostrap machine</li> <li>The cluster API pods are deployed to the bootstrap cluster</li> <li>The requested management cluster configuration are deployed to the boostrap cluster using the Cluster API Custom Resource Definitions</li> <li>The cluster API provider for vSphere on the bootstrap cluster communicates with vSphere to clone and configure the cluster VMs in vSphere</li> <li>When the cluster is available, the Cluster API pods and configuration are moved to the vSphere management cluster</li> <li>The local bootstrap cluster is deleted</li> </ol> </li> <li> <p>Note: Deployment will output particular logs associated with each step</p> </li> <li> <p>Once deployment is completed, two VM types will be created,one for control plane nodes, another type for worker nodes.</p> </li> </ul> <p></p>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/02_Management_Clusters/07_Lab-4-Create-Management-Clusters.html","title":"2.7 - Lab 4","text":""},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/02_Management_Clusters/07_Lab-4-Create-Management-Clusters.html#objectives","title":"Objectives","text":"<ol> <li>Create a Management Cluster</li> <li>Rename the Management Cluster Configuration File</li> <li>Examine the Management Cluster</li> </ol>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/02_Management_Clusters/07_Lab-4-Create-Management-Clusters.html#create-a-management-cluster","title":"Create a Management Cluster","text":"<ol> <li> <p>Using the terminal, set the environment variables to install from the local Harbor registry.</p> <p><code>source ~/Workspace/harbor-vars.sh</code></p> </li> <li> <p>Run the tanzu command with the Tanzu Kubernetes Grid installer enabled.</p> <p><code>tanzu management-cluster create --ui</code></p> <p>The Tanzu Kubernetes Grid Installer opens in the browser.</p> </li> <li> <p>Under VMware vSphere, click DEPLOY.</p> </li> <li>In the IaaS Provider panel, enter the following values and then click CONNECT.</li> <li>On the Verify SSL Thumbprint page, click CONTINUE.</li> <li>On the vSphere 7.0.2 Environment Detected window, click DEPLOY TKG MANAGEMENT CLUSTER.</li> <li>Select /SA-Datacenter for DATACENTER.</li> <li>Using the Files application, retrieve the SSH Public Key from the student desktop.<ol> <li>Navigate to the ~/.ssh folder.</li> <li>Open the id_rsa.pub file.</li> <li>Copy the file contents and paste them in the SSH PUBLIC KEY text box.</li> <li>Close the Text Editor window and click NEXT.</li> </ol> </li> <li>In the Management Cluster Settings panel, enter the following values and click NEXT.</li> <li>In the VMware NSX Advanced Load Balancer panel, enter the following values.</li> <li>Using the Files application, retrieve the NSX CA certificate.<ol> <li>Navigate to the ~/Workspace/certs folder.</li> <li>Open the nsx-ca.pem file.</li> <li>Copy the file contents and paste them in the CONTROLLER CERTIFICATE AUTHORITY text box.</li> <li>Close the Text Editor window.</li> </ol> </li> <li>Click VERIFY CREDENTIALS.</li> <li>In the VMware NSX Advanced Load Balancer panel, enter the following values and click NEXT.</li> <li>In the Metadata panel, click NEXT.</li> <li>Under Specify the Resources, enter the following values and click NEXT.</li> <li>Under Kubernetes Network Settings, select pg-SA-Management for NETWORK NAME and click NEXT.</li> <li>In the Identity Management panel, select LDAPS.</li> <li>Under LDAPS Identity Management Source, enter the following values.</li> <li>Under User Search Attributes, enter the following values.</li> <li>Under Group Search Attributes, enter the following values.</li> <li>Using the Files application, retrieve the LDAP certificate.<ol> <li>Navigate to the ~/Workspace/certs folder.</li> <li>Open the ldap.pem file.</li> <li>Copy the file contents and paste them in the ROOT CA text box.</li> <li>Close the Text Editor window and click NEXT.</li> </ol> </li> <li>In the OS Image panel, select /SA-Datacenter/vm/ubuntu-2004-kube-v1.20.5-vmware.1 for OS IMAGE and click NEXT.</li> <li>In the Register TMC panel, Click NEXT.</li> <li>In the CEIP Agreement panel, deselect Participate in the Customer Experience Improvement Program and click NEXT.</li> <li>Click REVIEW CONFIGURATION.</li> <li> <p>Under CLI Command Equivalent, click COPY and paste the command into commands.txt on the student desktop.</p> <p>The command includes a file with an autogenerated file name.</p> <p><code>/home/student01/.tanzu/tkg/clusterconfigs/&lt;AUTOGENERATED_FILENAME&gt;.yaml</code></p> <p>This file contains all the parameters you provided in the installer UI.</p> </li> <li> <p>Verify that your configuration matches the configuration from the lab guide.</p> <ol> <li>Using the Terminal, open a new terminal tab.</li> <li> <p>Navigate to the <code>clusterconfigs</code> directory.</p> <p><code>cd ~/.tanzu/tkg/clusterconfigs</code></p> </li> <li> <p>Run the <code>checkconfig</code> script.</p> <p><code>checkconfig &lt;AUTOGENERATED_FILENAME&gt;.yaml</code></p> <p>If there is a configuration mismatch, Visual Studio Code opens with your configuration file on the left panel and the reference configuration file on the right panel. Differences are highlighted in red.</p> <p>Do not close the Visual Studio Code window yet.</p> </li> <li> <p>In Firefox, click EDIT CONFIGURATION and correct the parameters.</p> </li> <li>Click REVIEW CONFIGURATION.</li> <li> <p>In Visual Studio Code, verify no differences are found between your configuration file and the reference configuration file.</p> <p>Close the Visual Studio Code window.</p> </li> </ol> </li> <li> <p>Deploy the management cluster.</p> <ol> <li> <p>Click DEPLOY MANAGEMENT CLUSTER.</p> <p>The deploy progress screen displays.</p> <p>CAUTION:</p> <p>Do not close the browser window yet.</p> </li> <li> <p>Wait for the <code>Management cluster created!</code> message to display and then close the Tanzu Kubernetes Grid installer browser window.</p> <p>The management cluster takes approximately 20 minutes to deploy.</p> </li> </ol> </li> <li> <p>In the terminal, review the output of the Tanzu CLI.</p> <p>Both the <code>Management cluster created!</code> message and the command to create a Tanzu Kubernetes cluster display.</p> </li> </ol>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/02_Management_Clusters/07_Lab-4-Create-Management-Clusters.html#rename-the-management-cluster-configuration-file","title":"Rename the Management Cluster Configuration File","text":"<ol> <li> <p>Using the terminal, navigate to the <code>clusterconfigs</code> directory.</p> <p><code>cd ~/.tanzu/tkg/clusterconfigs</code></p> </li> <li> <p>Rename the autogenerated filename captured in the previous task to match the management cluster name.</p> <p><code>mv &lt;AUTOGENERATED_FILENAME&gt;.yaml sa-compute-01-mgmt.yaml</code></p> </li> </ol>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/02_Management_Clusters/07_Lab-4-Create-Management-Clusters.html#examine-the-management-cluster","title":"Examine the Management Cluster","text":"<ol> <li> <p>Using the terminal, retrieve the admin kubeconfig file for the management cluster.</p> <p><code>tanzu management-cluster kubeconfig get --admin</code></p> <p>NOTE:</p> <p>The Tanzu CLI automatically adds the management cluster kubeconfig data to your local kubeconfig file. These steps are provided for learning purposes.</p> </li> <li> <p>List the kubectl contexts.</p> <p><code>kubectl config get-contexts</code></p> </li> <li> <p>Set the current kubectl context to point to the management cluster.</p> <p><code>kubectl config use-context sa-compute-01-mgmt-admin@sa-compute-01-mgmt</code></p> </li> <li> <p>Display all pods running on the cluster.</p> <p><code>kubectl get pods -A</code></p> <p>The status of some <code>pinniped-post-deploy-job</code> pods might display as error, which is expected.</p> </li> <li> <p>Display the status of the management cluster nodes.</p> <p><code>kubectl get nodes</code></p> <p>The output displays the following nodes with a Ready status.</p> </li> <li> <p>In the vSphere Client, click Menu.</p> </li> <li>Click Hosts and Clusters.</li> <li> <p>Expand rp-tkg-management.</p> <p>The VMs correspond to the <code>kubectl get nodes</code> output.</p> </li> </ol>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/02_Management_Clusters/08_Managing-Management-Clusters.html","title":"2.8 - Managing Management Clusters","text":""},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/02_Management_Clusters/08_Managing-Management-Clusters.html#learner-objectives","title":"Learner Objectives","text":"<ul> <li>Describe the commands available for working with management clusters.</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/02_Management_Clusters/08_Managing-Management-Clusters.html#working-with-multiple-management-clusters","title":"Working with Multiple Management Clusters","text":"Command Description tanzu login Displays list of management clusters deployed and enables changes to the .kube-tkg/config context to change the context tanzu-management-cluster get Display data of a particular  management cluster"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/02_Management_Clusters/08_Managing-Management-Clusters.html#adding-existing-management-clusters-to-tanzu-cli","title":"Adding Existing Management Clusters to Tanzu CLI","text":"<ol> <li>Run <code>tanzu login</code></li> <li>Select <code>new server</code></li> <li>Select <code>server endpoint</code> \u2192 Provide the login URL of the vSphere with Tanzu supervisor cluster</li> <li> <p>If not 3. select local kubeconfig and provide kubeconfig file of the other management cluster.</p> </li> <li> <p>In one command, this could be achieved via: <code>tanzu login --kubeconfig &lt;kube config path&gt; --context &lt;context name&gt; --name &lt;management cluster name&gt;</code></p> </li> </ol>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/02_Management_Clusters/09_End-of-Module-Assessment.html","title":"2.9 - End of Module Assessment","text":"<ol> <li>What must be uploaded to vCenter Server before deploying TKG?<ol> <li>Base Image Template OVA</li> </ol> </li> <li>A TKG Management Cluster can be created by what two methods?<ol> <li>Tanzu CLI</li> <li>Installer UI</li> </ol> </li> <li>Which TKG Component provides cluster authentication functionality?<ol> <li>Pinniped</li> </ol> </li> </ol>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/03_Tanzu_Kubernetes_Clusters/01_Building-Custom-Cluster-Machine-Images.html","title":"3.1 - Building Custom Cluster Machine Images","text":""},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/03_Tanzu_Kubernetes_Clusters/01_Building-Custom-Cluster-Machine-Images.html#overview","title":"Overview","text":"<ul> <li>Tanzu Kubernetes clusters = Workload Clusters</li> <li>Applications are typically deployed to these clusters and managed by TKG Management clusters</li> <li>Provisioning and configuring Tanzu Kubernetes clusters is the key functionality that TKG provides</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/03_Tanzu_Kubernetes_Clusters/01_Building-Custom-Cluster-Machine-Images.html#learner-objectives","title":"Learner Objectives","text":"<ul> <li>Describe the steps to build a custom image</li> <li>Describe the available customizations</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/03_Tanzu_Kubernetes_Clusters/01_Building-Custom-Cluster-Machine-Images.html#deploying-clusters-using-custom-machine-images","title":"Deploying Clusters Using Custom Machine Images","text":"<ul> <li>Custom machine images can be used as VM templates for Tanzu Kubernetes workload cluster nodes</li> <li>Each custom machine image packages a base OS version and a Kubernetes version, as well as any additional customizations into an image capable of running on:</li> <li>vSphere</li> <li>AWS</li> <li>Azure</li> <li> <p>Base OS can be any OS that VMware supports but does not distribute e.g. Red Hat Enterprise Linux (RHEL) v7</p> </li> <li> <p>Note: for TKG v1.3 - only workload clusters support custom images, both these and management clusters are supported in v1.4</p> </li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/03_Tanzu_Kubernetes_Clusters/01_Building-Custom-Cluster-Machine-Images.html#custom-machine-image-customizations","title":"Custom Machine Image Customizations","text":"<ul> <li>Variables including the above can be used to customize the image builds.</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/03_Tanzu_Kubernetes_Clusters/01_Building-Custom-Cluster-Machine-Images.html#building-custom-images-requirements","title":"Building Custom Images - Requirements","text":"<ul> <li>The following requirements must be met for custom images:</li> <li>Account on vSphere/AWS/Azure with sufficient permissions</li> <li>Docker installed on Mac OS or Linux workstation</li> <li>For AWS - aws CLI enabled</li> <li>Azure - az CLI enabled</li> <li>vSphere - OVFTool must be installed</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/03_Tanzu_Kubernetes_Clusters/01_Building-Custom-Cluster-Machine-Images.html#steps-to-create-custom-image","title":"Steps to Create Custom Image","text":"<ol> <li>Determine and download image builder configuration version to be built from Each version corresponds to the Kubernetes version that the Image Builder will use.</li> <li>For vSPhere - load OVF tool in the image builder container image</li> <li>Prepare the required configuration</li> <li>Run image builder (recommended to do a dry run first)</li> </ol>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/03_Tanzu_Kubernetes_Clusters/01_Building-Custom-Cluster-Machine-Images.html#using-custom-images","title":"Using Custom Images","text":"<ul> <li> <p>After creating the custom images, you need to allow the Tanzu CLI to use the image - this requires creation of a custom Tanzu Kubernetes release based on the image</p> </li> <li> <p>Open the BOM (Bill Of Materials) file corresponding to the Kubernetes version being used by the custom image</p> </li> <li>Edit the configuration as appropriate</li> <li>Encode the BOM configuration in Base64</li> <li>Create a ConfigMap YAML to the <code>tkr-system</code> namespace</li> <li>Save the ConfigMap, set the context to the management cluster the Tanzu Kubernetes release is to be added to, and apply via <code>kubectl apply</code></li> <li>Verify the custom Tanzu Kubernetes release was added via <code>tanzu kubernetes-release get</code> or <code>kubectl get tkr</code></li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/03_Tanzu_Kubernetes_Clusters/02_Deploying-Tanzu-Kubernetes-Cluster.html","title":"3.2 - Deploying Tanzu Kubernetes Cluster","text":""},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/03_Tanzu_Kubernetes_Clusters/02_Deploying-Tanzu-Kubernetes-Cluster.html#learner-objectives","title":"Learner Objectives","text":"<ul> <li>Describe the options for deploying Tanzu Kubernetes clusters</li> <li>Describe how Tanzu Kubernetes clusters are created</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/03_Tanzu_Kubernetes_Clusters/02_Deploying-Tanzu-Kubernetes-Cluster.html#tanzu-cli-cluster-plugin","title":"Tanzu CLI Cluster Plugin","text":"<ul> <li>The Tanzu CLI core executable must be installed as well as the CLI plugins associated with Tanzu Kubernetes cluster management and feature operations</li> <li>Once installed, the following commands are available to be appended to <code>tanzu cluster</code></li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/03_Tanzu_Kubernetes_Clusters/02_Deploying-Tanzu-Kubernetes-Cluster.html#tanzu-cluster-create","title":"Tanzu Cluster Create","text":"<p>The following  flags can be added to the <code>tanzu cluster create</code> command:</p> <p></p>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/03_Tanzu_Kubernetes_Clusters/02_Deploying-Tanzu-Kubernetes-Cluster.html#creating-tanzu-kubernetes-clusters","title":"Creating Tanzu Kubernetes Clusters","text":"<ul> <li>When running <code>tanzu cluster create -f &lt;file.yaml&gt;</code>, the Tanzu CLI communicates with the Cluster API in the management cluster, which then uses the Cloud Infrastructure provider e.g. vSphere CAPV to deploy the Tanzu Kubernetes Cluster(s) required.</li> <li>This will use the configuration and deployment plans outlined e.g. deploy x control plane nodes, y  Worker nodes.</li> <li>Once deployed, the cluster API will continuously compare the configuration in the Tanzu Kubernetes Cluster to manage and facilitate various operations.</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/03_Tanzu_Kubernetes_Clusters/02_Deploying-Tanzu-Kubernetes-Cluster.html#cluster-configuration-files","title":"Cluster Configuration Files","text":"<ul> <li>These files contain the configuration parameters passed to the Tanzu CLI, and combined with cluster plan template files to deploy Tanzu Kubernetes Cluster(s)</li> <li>Example parameters include (for a vSphere deployment)</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/03_Tanzu_Kubernetes_Clusters/02_Deploying-Tanzu-Kubernetes-Cluster.html#deploying-clusters-with-a-high-availability-control-plane","title":"Deploying Clusters with a High Availability Control Plane","text":"<ul> <li>The <code>CLUSTER_PLAN</code> parameter determines the number of control plane nodes:</li> <li><code>dev</code> sets 1 control plane node</li> <li><code>prod</code> deploys 3 control plane nodes and 3 worker nodes.</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/03_Tanzu_Kubernetes_Clusters/02_Deploying-Tanzu-Kubernetes-Cluster.html#previewing-the-yaml-file-for-tanzu-kubernetes-clusters","title":"Previewing the YAML File for Tanzu Kubernetes Clusters","text":"<ul> <li>Deployments can be tested based on the YAML file by appending the <code>--dry-run</code> flag</li> <li>This is especially useful if making changes in the providers file.</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/03_Tanzu_Kubernetes_Clusters/02_Deploying-Tanzu-Kubernetes-Cluster.html#overriding-default-configuration-parameters","title":"Overriding Default Configuration Parameters","text":"<ul> <li>The Tanzu CLI <code>management-cluster</code> and <code>cluster</code> commands will take any configuration parameters specified in the file pointed at by the <code>-f</code> or <code>--file</code> flags</li> <li>These parameters can be overridden by defining them as environment variables before using the CLI command e.g. <code>export &lt;var\u2265\"new value\"</code></li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/03_Tanzu_Kubernetes_Clusters/02_Deploying-Tanzu-Kubernetes-Cluster.html#deploying-clusters-that-run-a-specific-kubernetes-version","title":"Deploying Clusters that Run a Specific Kubernetes Version","text":"<ul> <li>Each release of Tanzu Kubernetes Grid:</li> <li>provides a default Kubernetes version</li> <li>Supports a defined set of Kubernetes versions</li> <li>Example command to deploy with a specific version: <code>tanzu cluster create -f &lt;cluster yaml&gt; --tkr v1.19.9---vmware.2-tkg.1</code></li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/03_Tanzu_Kubernetes_Clusters/03_Lab-5-Create-Tanzu-Kubernetes-Cluster.html","title":"3.3 - Lab 5","text":""},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/03_Tanzu_Kubernetes_Clusters/03_Lab-5-Create-Tanzu-Kubernetes-Cluster.html#create-a-tanzu-kubernetes-cluster-configuration-file","title":"Create a Tanzu Kubernetes Cluster Configuration File","text":"<ol> <li> <p>Using the terminal, navigate to the <code>clusterconfigs</code> directory.</p> <p><code>cd ~/.tanzu/tkg/clusterconfigs</code></p> </li> <li> <p>Copy the management cluster configuration as a template for the workload cluster.</p> <p><code>cp sa-compute-01-mgmt.yaml tkc-01.yaml</code></p> </li> <li> <p>Update the configuration file for the workload cluster.</p> <ol> <li> <p>Open <code>tkc-01.yaml</code> in Visual Studio Code.</p> <p><code>code tkc-01.yaml</code></p> </li> <li> <p>Modify the following parameters.</p> </li> <li>Add the following parameter.</li> <li>Save the file and close Visual Studio Code.</li> <li>Verify that the configuration file matches the reference configuration file.</li> </ol> <p><code>checkconfig tkc-01.yaml</code></p> <p>When a configuration mismatch exists, Visual Studio Code opens the configuration file in the left panel and the reference configuration file in the right panel.</p> <ol> <li> <p>If a configuration mismatch exists, modify the configuration on the left to match the reference configuration on the right.</p> <p>Differences are highlighted in red.</p> </li> <li> <p>Save the file and close Visual Studio Code.</p> </li> <li>Run the <code>tanzu cluster create</code> command with the <code>-dry-run</code> option which queries vCenter Server to ensure that the vSphere resources exist.</li> </ol> <p><code>tanzu cluster create -f tkc-01.yaml --dry-run</code></p> <ol> <li>If the output displays <code>exit status 1</code>, review the configuration parameters from step 3.</li> </ol> </li> </ol>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/03_Tanzu_Kubernetes_Clusters/03_Lab-5-Create-Tanzu-Kubernetes-Cluster.html#create-a-tanzu-kubernetes-cluster","title":"Create a Tanzu Kubernetes Cluster","text":"<ol> <li> <p>Using the terminal, navigate to the <code>clusterconfigs</code> directory.</p> <p><code>cd ~/.tanzu/tkg/clusterconfigs</code></p> </li> <li> <p>Set the environment variables to install from the local Harbor registry.</p> <p><code>source ~/Workspace/harbor-vars.sh</code></p> </li> <li> <p>Create the cluster by using the configuration file that was created in the previous task.</p> <p><code>tanzu cluster create -f tkc-01.yaml</code></p> <p>Wait for <code>Workload cluster 'tkc-01' created</code> to display before continuing.</p> <p>The Tanzu Kubernetes cluster takes approximately 20 minutes to deploy.</p> </li> <li> <p>List the Tanzu Kubernetes clusters.</p> <p><code>tanzu cluster list</code></p> </li> </ol>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/03_Tanzu_Kubernetes_Clusters/04_Managing-Tanzu-Kubernetes-Clusters.html","title":"3.4 - Managing Tanzu Kubernetes Clusters","text":""},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/03_Tanzu_Kubernetes_Clusters/04_Managing-Tanzu-Kubernetes-Clusters.html#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Describe the commands available for working with Tanzu Kubernetes clusters.</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/03_Tanzu_Kubernetes_Clusters/04_Managing-Tanzu-Kubernetes-Clusters.html#granting-authenticated-access-to-a-cluster","title":"Granting Authenticated Access to a Cluster","text":"<ul> <li>Running <code>tanzu cluster kubeconfig get &lt;cluster name&gt;</code> generates the kubeconfig file that can be shared with users requiring access to the cluster.</li> <li>The kubeconfig file is configured with instructions that run the Tanzu CLI and trigger the Pinniped authentication workflow.</li> <li>Though the process authenticates a user to the cluster, the user will still require permissions to be granted for them on the cluster to be able to interact with the Kubernetes API resources.</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/03_Tanzu_Kubernetes_Clusters/04_Managing-Tanzu-Kubernetes-Clusters.html#granting-access-to-cluster-resources","title":"Granting Access to Cluster Resources","text":"<ul> <li>To allow authorization of an authenticated user, RoleBindings or CLusterRoleBindings need to be utilised.</li> <li>These are standard Kubernetes resources that allow permissions from a \"role\" or \"clusterrole\" to be bound to a user account.</li> <li>Example:</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/03_Tanzu_Kubernetes_Clusters/04_Managing-Tanzu-Kubernetes-Clusters.html#scaling-clusters","title":"Scaling Clusters","text":"<ul> <li>To scale clusters, use the <code>scale</code> command and include the required <code>controlplane-machine-count</code> and <code>worker-machine-count</code> values</li> <li>Example: <code>tanzu cluster scale &lt;cluster name&gt; --controlplane-machine-count &lt;value&gt; --worker-machine-count &lt;value&gt;</code></li> <li>From this, the Tanzu CLI modifies the Cluster API spec of the cluster; triggering the Cluster API controllers to create the new nodes</li> <li>Scaling can be done up or down</li> <li>Control plane nodes can ONLY be scaled to an ODD number of nodes - aims to prevent quorum issues during etcd leader elections.</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/03_Tanzu_Kubernetes_Clusters/04_Managing-Tanzu-Kubernetes-Clusters.html#scaling-management-clusters","title":"Scaling Management Clusters","text":"<ul> <li>As management clusters run in the <code>tkg-system</code> namespace, the <code>--namespace</code> flag must be specified when scaling a management cluster.</li> <li>Example: <code>tanzu cluster scale &lt;mgmt cluster name&gt; --controlplane-machine-count &lt;value&gt; --worker-machine-count &lt;value&gt; --namespace &lt;namespace&gt;</code></li> <li>Scaling management clusters isn't typically needed - management workloads don't consume much resources.</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/03_Tanzu_Kubernetes_Clusters/04_Managing-Tanzu-Kubernetes-Clusters.html#cluster-autoscaler","title":"Cluster Autoscaler","text":"<ul> <li>To enable Cluster Autoscaler for the workload cluster, various options prefixed with <code>AUTOSCALER_</code> must be specified in the configuration file used to deploy the cluster.</li> <li>This feature works with the native Kubernetes Autoscaler feature.</li> <li>https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscalerl</li> <li>Example parameters follow:</li> </ul> <ul> <li>Note: The native Kubernetes Autoscaler must be enabled for any of these parameters to take effect.</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/03_Tanzu_Kubernetes_Clusters/04_Managing-Tanzu-Kubernetes-Clusters.html#machine-health-checks","title":"Machine Health Checks","text":"<ul> <li>MachineHealthCheck - a controller that provides node health monitoring and auto-repair for Tanzu Kubernetes clusters.</li> <li>Monitors for the following conditions:</li> <li>Ready State</li> <li>MemoryPressure</li> <li>DiskPressure</li> <li>PIDPressure</li> <li>NetworkUnavailable</li> <li>If a node status reports the condition for a particular amount of time, the node is considered unhealthy and is recreated by Cluster API.</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/03_Tanzu_Kubernetes_Clusters/04_Managing-Tanzu-Kubernetes-Clusters.html#configuring-machine-health-checks","title":"Configuring Machine Health Checks","text":"<ul> <li>Each created cluster must have <code>ENABLE_MGC</code> set to true or false to enable or disable MachineHealthCheck</li> <li>This can be modified by the Tanzu CLI</li> <li><code>tanzu cluster machinehealthcheck set &lt;parameters and args&gt;</code></li> <li>Example: <code>tanzu cluster machinehealthcheck set &lt;cluser name&gt; --unhealthy-conditions \"Ready:False:5m,Ready:Unknown:5m\"</code></li> <li>If False or Unknown is reported as the ready status for 5 mins, the node will be recreated.</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/03_Tanzu_Kubernetes_Clusters/05_Lab-6-Scale-Tanzu-Kubernetes-Clusters.html","title":"3.5 - Lab 6","text":""},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/03_Tanzu_Kubernetes_Clusters/05_Lab-6-Scale-Tanzu-Kubernetes-Clusters.html#objectives-and-tasks","title":"Objectives and Tasks","text":"<ul> <li>Examine the Tanzu Kubernetes Cluster</li> <li>Scale the Tanzu Kubernetes Cluster</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/03_Tanzu_Kubernetes_Clusters/05_Lab-6-Scale-Tanzu-Kubernetes-Clusters.html#examine-the-tanzu-kubernetes-cluster","title":"Examine the Tanzu Kubernetes Cluster","text":"<ol> <li> <p>Using the terminal, retrieve the admin kubeconfig file for the workload cluster.</p> <p><code>tanzu cluster kubeconfig get tkc-01 --admin</code></p> </li> <li> <p>List the kubectl contexts.</p> <p><code>kubectl config get-contexts</code></p> </li> <li> <p>Set the kubectl context to point to the workload cluster.</p> <p><code>kubectl config use-context tkc-01-admin@tkc-01</code></p> </li> <li> <p>Display all pods running on the cluster.</p> <p><code>kubectl get pods -A</code></p> <p>The status of some <code>pinniped-post-deploy-job</code> pods might display as error, which is expected.</p> </li> <li> <p>Display the Tanzu Kubernetes cluster nodes.</p> <p><code>kubectl get nodes</code></p> <p>The output displays the following two nodes.</p> </li> <li> <p>In the vSphere Client, click Menu.</p> </li> <li>Click Hosts and Clusters.</li> <li> <p>Review the VM names in the <code>rp-tkg-production</code> resource pool.</p> <p>The VM names correspond to the node names in the <code>kubectl get nodes</code> output.</p> </li> </ol>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/03_Tanzu_Kubernetes_Clusters/05_Lab-6-Scale-Tanzu-Kubernetes-Clusters.html#scale-the-cluster","title":"Scale the Cluster","text":"<ol> <li> <p>Using the terminal, get the Tanzu Kubernetes cluster information.</p> <p><code>tanzu cluster list</code></p> <p>The output shows <code>WORKERS 1/1</code>.</p> </li> <li> <p>Scale the worker nodes to 2.</p> <p><code>tanzu cluster scale tkc-01 -w 2</code></p> <p>The scale operation performs in the background.</p> </li> <li> <p>Verify the status of the scale operation.</p> <p><code>tanzu cluster list</code></p> <p>The output displays <code>STATUS updating</code> and <code>WORKERS 1/2</code>.</p> </li> <li> <p>Set the kubectl context to point to the management cluster.</p> <p><code>kubectl config use-context sa-compute-01-mgmt-admin@sa-compute-01-mgmt</code></p> </li> <li> <p>Monitor the cluster API resources.</p> <ol> <li> <p>Get the VM status.</p> <p><code>watch kubectl get machines</code></p> </li> <li> <p>Wait for the new VM phase to go from Provisioning to Running before continuing.</p> </li> <li>Press Ctrl+C to exit the <code>watch</code> command.</li> <li>Verify the status of the scale operation again.</li> </ol> <p><code>tanzu cluster list</code></p> <p>The output displays <code>STATUS running</code> and <code>WORKERS 2/2</code>.</p> <p>Provisioning can take up to 10 minutes to complete.</p> </li> </ol>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/03_Tanzu_Kubernetes_Clusters/06_Lab-7.html","title":"3.6 - Lab 7","text":""},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/03_Tanzu_Kubernetes_Clusters/06_Lab-7.html#objectives","title":"Objectives","text":"<ul> <li>Generate a kubeconfig file for the cluster</li> <li>Create a RoleBinding to allow Developer access</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/03_Tanzu_Kubernetes_Clusters/06_Lab-7.html#generate-a-kubeconfig-file","title":"Generate a KubeConfig File","text":"<ol> <li> <p>Using the terminal, navigate to the <code>Workspace</code> directory.</p> <p><code>cd ~/Workspace</code></p> </li> <li> <p>Verify that the cluster is running correctly.</p> <p><code>tanzu cluster get tkc-01</code></p> <p>The READY columns displays <code>True</code>.</p> </li> <li> <p>Generate a kubeconfig file for the cluster that does not have admin permissions.</p> <p><code>tanzu cluster kubeconfig get tkc-01 --export-file kubeconfig-developers.yaml</code></p> <p>NOTE:</p> <p>This kubeconfig file will be used in an upcoming lab to access the cluster as a developer user.</p> </li> <li> <p>Display the file contents to see the Pinniped configuration parameters.</p> <p><code>cat kubeconfig-developers.yaml</code></p> <p>Because <code>--admin</code> was not used, this kubeconfig file will redirect the user to Pinniped and authentication will be performed using LDAP.</p> </li> </ol>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/03_Tanzu_Kubernetes_Clusters/06_Lab-7.html#create-role-binding-to-grant-developer-access","title":"Create Role Binding to Grant Developer Access","text":"<ol> <li> <p>Using the terminal, navigate to the <code>Workspace</code> directory.</p> <p><code>cd ~/Workspace</code></p> </li> <li> <p>Set the kubectl context to the tkc-01 cluster.</p> <p><code>kubectl config use-context tkc-01-admin@tkc-01</code></p> </li> <li> <p>Display the cluster role binding.</p> <p><code>cat ldap-group-role-binding.yaml</code></p> <p>The subjects field contains tkg-developers.</p> <p>The roleRef field contains cluster-admin.</p> <p><code>yaml kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata:   name: developers-ldap-group subjects: - kind: Group   name: tkg-developers   apiGroup: \"\" roleRef:   kind: ClusterRole   name: cluster-admin   apiGroup: rbac.authorization.k8s.io</code></p> </li> <li> <p>Apply the cluster role binding.</p> <p><code>kubectl apply -f ldap-group-role-binding.yaml</code></p> </li> </ol>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/03_Tanzu_Kubernetes_Clusters/07_Managing-Cluster-Lifecycles.html","title":"3.7 - Managing Cluster Lifecycles","text":""},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/03_Tanzu_Kubernetes_Clusters/07_Managing-Cluster-Lifecycles.html#learner-objectives","title":"Learner Objectives","text":"<ul> <li>Describe the VMs that make up a Tanzu Kubernetes cluster</li> <li>Describe the pods that run on a Tanzu Kubernetes Cluster</li> <li>Describe the Tanzu Kubernetes Grid core add-ons that are installed on a cluster</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/03_Tanzu_Kubernetes_Clusters/07_Managing-Cluster-Lifecycles.html#tanzu-kubernetes-cluster-vms","title":"Tanzu Kubernetes Cluster VMs","text":"<ul> <li>Depending on the plan used, the following VMs will be created:</li> <li>Control plan VMs: <code>&lt;cluster name&gt;-control-pane-......</code></li> <li>Worker node VMs: <code>&lt;cluster name&gt;-md-0-........</code></li> <li>There will always be one or more of each.</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/03_Tanzu_Kubernetes_Clusters/07_Managing-Cluster-Lifecycles.html#upstream-kubernetes-components","title":"Upstream Kubernetes Components","text":"<ul> <li>A number of standard Kubernetes components will be included upon deployment.</li> </ul> <ul> <li>Architecturally:</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/03_Tanzu_Kubernetes_Clusters/07_Managing-Cluster-Lifecycles.html#kubernetes-api-high-availability","title":"Kubernetes API High-Availability","text":"<ul> <li>The component <code>kube-vip</code> provides HA load-balancing for the Kubernetes control-plane</li> <li>By default, if included, it runs on every control plane.</li> <li>In classic HA fashion, one of the nodes will be elected the \"leader\" and advertises the virtual IP address using Address Resolution protocol</li> <li>If the node fails or becomes unhealthy, a leader election happens - a new leader will be elected from the available controlplane nodes and the winner will advertise the virtual IP address via ARP.</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/03_Tanzu_Kubernetes_Clusters/07_Managing-Cluster-Lifecycles.html#authentication-core-addons","title":"Authentication Core Addons","text":"<ul> <li>By default, TKG will implement Pinniped for cluster authentication, it comes with the following components:</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/03_Tanzu_Kubernetes_Clusters/07_Managing-Cluster-Lifecycles.html#networking-core-addons","title":"Networking Core Addons","text":"<ul> <li>Tanzu Kubernetes Grid supports Antrea (default) or Calico as the container network interface (CNI) for in-cluster networking.</li> <li>Each come with their own components:</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/03_Tanzu_Kubernetes_Clusters/07_Managing-Cluster-Lifecycles.html#vsphere-core-addons","title":"vSphere Core Addons","text":"<ul> <li>For each IaaS provider - additional components will also be deployed. For vSphere, the components are outlined as follows.</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/03_Tanzu_Kubernetes_Clusters/07_Managing-Cluster-Lifecycles.html#metrics-core-addons","title":"Metrics Core Addons","text":"<ul> <li>Tanzu Kubernetes Grid deploys a metrics server to management and workload clusters by default:</li> <li>metrics-server: Collects resource metrics from kubelets and exposes them in the Kubernetes API for use by Horizontal and Vertical Pod autoscalers</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/03_Tanzu_Kubernetes_Clusters/08_End-of-Module-Assessment.html","title":"3.8 - End of Module Assessment","text":"<ol> <li>What's the parameter that needs to be set to enable autoscaling?<ol> <li><code>ENABLE_AUTOSCALER</code></li> </ol> </li> <li>What component provides high-availability?<ol> <li><code>kube-vip</code></li> </ol> </li> <li>What CNI is provided by default via TKG?<ol> <li>Antrea and Calico</li> </ol> </li> </ol>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/01_Tanzu-Kubernetes-Grid-Extensions.html","title":"4.01 - Tanzu Kubernetes Grid Extensions","text":""},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/01_Tanzu-Kubernetes-Grid-Extensions.html#module-context","title":"Module Context","text":"<ul> <li>Any standard Kubernetes installation will lack some of the functionalities required for production-level deployment.</li> <li>This is typically achieved by usage of extensions - TKG extensions provide functionality such as:</li> <li>Logging</li> <li>Ingress</li> <li>Service Discovery</li> <li>Monitoring</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/01_Tanzu-Kubernetes-Grid-Extensions.html#learner-objectives","title":"Learner Objectives","text":"<ul> <li>Describe the TKG Extensions and their functionalities</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/01_Tanzu-Kubernetes-Grid-Extensions.html#about-the-tkg-extensions-bundle","title":"About the TKG Extensions Bundle","text":"<ul> <li>TKG extensions bundle includes binaries for tools to help provide in-cluster and shared services to your TKG instance</li> <li>VMware builds all the provided binaries and container images associated.</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/01_Tanzu-Kubernetes-Grid-Extensions.html#extensions-overview","title":"Extensions Overview","text":"<ul> <li>For each set of functionality, TKG extensions bundle typically has 1 preset extension readily available, though in the case of Monitoring, both Prometheus and Grafana are used in combination as standard practice.</li> </ul> <ul> <li>Note: Other options are available for each function e.g. NGINX, Traefik for Ingress</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/01_Tanzu-Kubernetes-Grid-Extensions.html#container-images","title":"Container Images","text":"<ul> <li>VMware builds all container images used by Tanzu Kubernetes Grid and TKG extensions</li> <li>All container images are hosted on projects.registry.vmware.com</li> <li>An instance of the Harbor image registry that's managed by VMware</li> <li>For internet-restricted environments, container images can be copied to an accessible image registry</li> <li>The extension manifest must then be updated to include the specific image registry</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/01_Tanzu-Kubernetes-Grid-Extensions.html#deployment-with-kapp-controller","title":"Deployment with kapp-controller","text":"<ul> <li>When extensions are deployed:</li> <li>The namespace, configuration (-data-values.yaml), and extension (-extension.yaml) YAML files for the particular extension are applied.</li> <li>kapp-controller combines the configuraiton with the YTT templates from the tkg-extensions-templates to generate all resource definitions required for the extension, such as:<ul> <li>Deployments</li> <li>ConfigMaps</li> <li>Secrets</li> </ul> </li> <li>The extension workload is then started using standard Kubernetes processes i.e. <code>kubectl apply</code></li> <li>Therefore, requests to deploy these extensions are passed from the kube-apiserver to the kapp-controller, which takes the templates from the tkg-extensions-templates and uses them to deploy the extensions.</li> <li>The kapp CLI can be used to inspect the state of workloads that are deployed by the kapp-controller.</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/01_Tanzu-Kubernetes-Grid-Extensions.html#configuring-extensions-with-kubernetes-secrets","title":"Configuring Extensions with Kubernetes Secrets","text":"<ul> <li>Extensions are configured by creating a Kubernetes secret containing related configuration data</li> <li>The configuration file for each extension is defined in a file of format <code>&lt;extension&gt;-data-values.yaml</code> - this is used to create a Kubernetes secret named <code>&lt;extension&gt;-data-values</code></li> <li>When kapp-controller deploys an extension, it reads the secret configuration to apply it to the extension</li> <li>Changes to the secret resource will automatically be picked up by the kapp-controller once the secret's data is updated.</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/01_Tanzu-Kubernetes-Grid-Extensions.html#cert-manager","title":"Cert-Manager","text":"<ul> <li>A native Kubernetes certificate management controller</li> <li>Functionalities include:</li> <li>Add certificates and certificate issuers as resource types in Kubernetes clusters</li> <li>Simplifies the process of obtaining/renewing/using certificates</li> <li>Allows generation of certificates internally and connection to external services (e.g. Lets Encrypt) to request certifcates</li> <li>It's not considered a TKG extension outright, however it is included in the extensions bundle as Contour, Grafana, Prometheus and Harbor all depend on its functionality to work.</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/01_Tanzu-Kubernetes-Grid-Extensions.html#deleting-extensions","title":"Deleting Extensions","text":"<ul> <li>For troubleshooting or deletion of extensions, simply use <code>kubectl delete -f</code> against the YAML files associated with the extension i.e.:</li> <li><code>&lt;extension&gt;-extension.yaml</code></li> <li><code>namespace-role.yaml</code></li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/02_Lab-8_Working-with-Extensions.html","title":"4.02 - Lab 8","text":""},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/02_Lab-8_Working-with-Extensions.html#objectives","title":"Objectives","text":"<ul> <li>Unzip the TKG extensions</li> <li>Change the image registry location</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/02_Lab-8_Working-with-Extensions.html#unzip-the-tkg-extensions","title":"Unzip the TKG Extensions","text":"<ol> <li> <p>Using the terminal, navigate to the <code>Downloads</code> directory.</p> <p><code>cd ~/Downloads/</code></p> </li> <li> <p>Unzip the tkg-extensions file to the <code>Workspace</code> directory.</p> <p><code>tar -zxvf tkg-extensions-manifests-v1.3.1-vmware.1.tar.gz -C ~/Workspace/</code></p> </li> <li> <p>Navigate to the <code>Workspace</code> directory.</p> <p><code>cd ~/Workspace/tkg-extensions-v1.3.1+vmware.1</code></p> </li> <li> <p>List the files.</p> <p><code>tree -d -L 2 | less</code></p> <p>Because the <code>extensions</code> subdirectory contains the main configuration that you edit before deploying each extension, you will mostly work out of this directory.</p> <p>The other directories contain the kapp and ytt templates for overriding advanced parameters for each extension.</p> </li> </ol>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/02_Lab-8_Working-with-Extensions.html#change-the-image-registry-location","title":"Change the Image Registry Location","text":"<ol> <li> <p>Using the terminal, navigate to the <code>Workspace</code> directory.</p> <p><code>cd ~/Workspace</code></p> </li> <li> <p>Run the script to update the extension files to load images from Harbor.</p> <p><code>./tkg-ext-to-harbor.sh</code></p> <p>The extensions manifest files point to Harbor.</p> </li> </ol>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/03_Lab-9_Deploy-Cert-Manager.html","title":"4.03 - Lab 9","text":""},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/03_Lab-9_Deploy-Cert-Manager.html#objectives","title":"Objectives","text":"<ul> <li>Deploy cert-manager</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/03_Lab-9_Deploy-Cert-Manager.html#deploy-cert-manager","title":"Deploy Cert-Manager","text":"<ol> <li> <p>Using the terminal, navigate to the <code>cert-manager</code> directory.</p> <p><code>cd ~/Workspace/tkg-extensions-v1.3.1+vmware.1/cert-manager</code></p> </li> <li> <p>Set the kubectl context to the tkc-01 cluster.</p> <p><code>kubectl config use-context tkc-01-admin@tkc-01</code></p> </li> <li> <p>Deploy cert-manager.</p> <p><code>kubectl apply -f .</code></p> <p>Warning messages about deprecated objects can be ignored.</p> <p>This deploys:</p> <ul> <li>namespace.yaml</li> <li>cert-manager-crds.yaml</li> <li>cert-manager.yaml (workload)</li> <li>Verify the status of the deployment by using kubectl.</li> </ul> <p><code>kubectl get pods -n cert-manager</code></p> <p>Re-run the command until the pods display as running.</p> </li> </ol>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/04_Image-Registry.html","title":"4.04 - Image Registry","text":""},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/04_Image-Registry.html#about-harbor","title":"About Harbor","text":"<ul> <li>A container image registry that supports:</li> <li>RBAC</li> <li>Identity Integration with LDAP and OIDC</li> <li>Scanning images for vulnerabilities</li> <li>Signed images</li> <li>Image replication between registries.</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/04_Image-Registry.html#harbor-components","title":"Harbor Components","text":"<ul> <li>Components included in Harbor follow:</li> </ul> <ul> <li>Note: Scanning is available either via Trivy or Clair</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/04_Image-Registry.html#vulnerability-scanning","title":"Vulnerability Scanning","text":"<ul> <li>Trivy provides multiple vulnerability scanning functionality in Harbor:</li> <li>Scans container images</li> <li>Scans OS packages in container images e.g. Photon OS, Ubuntu, etc.</li> <li>Scans language-specific packages in container images e.g. Python, GO, etc.<ul> <li>Clair lacks this functionality!</li> </ul> </li> <li>Configuration can be put in place to prevent image deployments depending on the level of vulnerabilities found.</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/04_Image-Registry.html#harbor-deployment-options","title":"Harbor Deployment Options","text":"<ul> <li>Harbor is deployed on a shared-services cluster - all workload clusters deployed in a TKG instance can therefore access it.</li> <li>Common configuration options follow:</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/04_Image-Registry.html#harbor-authentication","title":"Harbor Authentication","text":"<ul> <li>Configuration \u2192 Authentication</li> <li>Allows configuration of LDAP and OIDC, amongst other methods, depending on requirements</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/05_Logging.html","title":"4.05 - Logging","text":""},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/05_Logging.html#learner-objectives","title":"Learner Objectives","text":"<ul> <li>Describe FluentBit</li> <li>Detail the logs collected by FluentBit</li> <li>Describe basic Fluentbit configuration</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/05_Logging.html#fluentbit-overview","title":"Fluentbit Overview","text":"<ul> <li>A lightweight logging framework</li> <li>Functionality includes:</li> <li>Processing of Kubernetes container logs</li> <li>Enriches logs with Kubernetes metdata</li> <li>Outputs logs to various services including:<ul> <li>vRealize Log Insight</li> <li>Syslog</li> <li>Elasticsearch</li> <li>Splunk</li> <li>HTTP Endpoint</li> </ul> </li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/05_Logging.html#configuration-and-deployment","title":"Configuration and Deployment","text":"<ul> <li>FluentBit is deployed as a Daemonset by default (runs on all nodes!)</li> <li>Configuration files and architecture follow:</li> </ul> <ul> <li>Depending on what logs are enabled, FluentBit collates all logs and outputs it to the desired service endpoint.</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/05_Logging.html#metadata-considered","title":"Metadata Considered","text":"<ul> <li>Fluentbit includes metadata such as:</li> <li>Pod ID</li> <li>Pod Name</li> <li>Container ID</li> <li>Container Name</li> <li>Labels</li> <li>Annotations</li> <li>Cluster name</li> <li>Instance Name</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/05_Logging.html#fluentbit-syslog-output-configuration","title":"Fluentbit Syslog Output Configuration","text":"<ul> <li>When configuring output to syslog, the following parameters should be considered:</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/06_Lab-10_Deploy-Fluentbit.html","title":"4.06 - Lab 10","text":""},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/06_Lab-10_Deploy-Fluentbit.html#objectives","title":"Objectives","text":"<ul> <li>Configure FluentBit</li> <li>Deploy FluentBit</li> <li>Access vRealize log insight to view logs</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/06_Lab-10_Deploy-Fluentbit.html#configure-fluentbit","title":"Configure Fluentbit","text":"<ol> <li> <p>Using the terminal, navigate to the <code>fluent-bit</code> directory.</p> <p><code>cd ~/Workspace/tkg-extensions-v1.3.1+vmware.1/extensions/logging/fluent-bit</code></p> </li> <li> <p>Make a copy of the example Syslog settings.</p> <p><code>cp syslog/fluent-bit-data-values.yaml.example fluent-bit-data-values.yaml</code></p> </li> <li> <p>Open <code>fluent-bit-data-values.yaml</code> in Visual Studio Code.</p> <p><code>code fluent-bit-data-values.yaml</code></p> <p></p> <ol> <li> <p>Modify the following parameters within &lt; &gt;</p> <ol> <li>Instance = TKG Instance or Management Cluster</li> <li>Cluster = tkc-01 = workload cluster</li> </ol> <p></p> </li> <li> <p>Save the file and close Visual Studio Code.</p> </li> <li>Verify that the configuration file matches the reference configuration file.</li> </ol> <p><code>checkconfig fluent-bit-data-values.yaml</code></p> <p>When a configuration mismatch exists, Visual Studio Code opens the configuration file in the left panel and the reference configuration file in the right panel.</p> <ol> <li> <p>If a configuration mismatch exists, modify the configuration on the left to match the reference configuration on the right.</p> <p>Differences are highlighted in red.</p> </li> <li> <p>Save the file and close Visual Studio Code.</p> </li> </ol> </li> </ol>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/06_Lab-10_Deploy-Fluentbit.html#deploy-fluentbit","title":"Deploy FluentBit","text":"<ol> <li> <p>Using the terminal, navigate to the <code>fluent-bit</code> directory.</p> <p><code>cd ~/Workspace/tkg-extensions-v1.3.1+vmware.1/extensions/logging/fluent-bit</code></p> </li> <li> <p>Set the kubectl context to the tkc-01 cluster.</p> <p><code>kubectl config use-context tkc-01-admin@tkc-01</code></p> </li> <li> <p>Create a namespace for Fluent Bit.</p> <p><code>kubectl apply -f namespace-role.yaml</code></p> </li> <li> <p>Create a Kubernetes secret from the settings file.</p> <p><code>kubectl create secret generic fluent-bit-data-values --from-file=values.yaml=fluent-bit-data-values.yaml -n tanzu-system-logging</code></p> </li> <li> <p>Deploy Fluent Bit.</p> <p><code>kubectl apply -f fluent-bit-extension.yaml</code></p> </li> <li> <p>Verify the status of the deployment by using kubectl.</p> <p><code>kubectl get app fluent-bit -n tanzu-system-logging</code></p> <p>Re-run the command until the status displays as <code>Reconcile succeeded</code>.</p> </li> <li> <p>Verify the status of the deployment by using kapp.</p> <p><code>kapp list -n tanzu-system-logging</code></p> <p><code>kapp inspect --app fluent-bit-ctrl -n tanzu-system-logging</code></p> </li> </ol>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/06_Lab-10_Deploy-Fluentbit.html#access-vrealize-log-insight-to-view-logs","title":"Access VRealize Log Insight to View Logs","text":"<ol> <li> <p>In Firefox, open the vRealize Log Insight bookmark in a new tab.</p> <p><code>https://sa-loginsight-01.vclass.local/</code></p> </li> <li> <p>Log in to vRealize Log Insight.</p> <ul> <li>User name: admin</li> <li>Password: VMware1!</li> </ul> </li> <li> <p>Click Interactive Analytics.</p> <p>The logs for tkc-01 display.</p> </li> </ol>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/07_Ingress.html","title":"4.07 - Ingress","text":""},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/07_Ingress.html#objectives","title":"Objectives","text":"<ul> <li>Describe the Contour Ingress Controller</li> <li>Explain how to install Contour to a Tanzu Kubernetes cluster</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/07_Ingress.html#ingress-overview","title":"Ingress Overview","text":"<ul> <li>A method used in Kubernetes for routing external traffic to pods</li> <li>Operates on layer 7 traffic</li> <li>Routes requests based on HTTP headers</li> <li>Can be implemented via various tools including:</li> <li>Contour</li> <li>NGINX</li> <li>Amazon ALB</li> <li>NSX Advanced Load Balancer</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/07_Ingress.html#contour-overview","title":"Contour Overview","text":"<ul> <li>An open-source Kubernetes ingress controller</li> <li>Used as the default ingress controller for Tanzu Kubernetes Grid</li> <li>Supports Ingress and HTTPProxy resources</li> <li>Comprised of a Contour controller deployment and Envoy proxy Daemonset</li> <li>Envoy used to perform traffic routing - handles rerouting traffic based on rules in place</li> <li>Supports dynamic configuration updates based on <code>kubectl apply</code> commands etc.</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/07_Ingress.html#httpproxy-resources","title":"HTTPProxy Resources","text":"<ul> <li>Match requests based on the configured virtualhost FQDN field</li> <li>Routes requests based on routes matching the ULR path to the defined service and port</li> <li>R</li> <li>Supports conditional routing based on URL prefixes and HTTP headers</li> <li>Supports inclusion of one HTTPProxy resource within another - creating tree-like routing structures</li> <li>Example resource:</li> </ul> <pre><code>apiVersion: projectcontour.io/v1\nkind: HTTPProxy\nmetadata:\n  name: basic\nspec:\n  virtualhost:\n    fqdn: foo-basic.bar.com\n  routes:\n    - conditions:\n      - prefix: /\n      services:\n        - name: s1\n          port: 80\n</code></pre>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/07_Ingress.html#how-contour-works","title":"How Contour Works","text":"<ul> <li> <p>Contour deployment dynamically configures an Envoy proxy DaemonSet by using the following steps:</p> </li> <li> <p>Contour watches the Kubernetes API for Ingress or HTTPProxy resources</p> </li> <li>Contour generates a cache of ingress and HTTPProxy resources</li> <li>Envoy periodically polls Contour for configuration changes</li> <li>Envoy dynamically updates its routing configuration</li> <li>Requests are received from a load balancer or directly from the external network and routed to the correct pods.</li> </ul> <p></p>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/07_Ingress.html#contour-configuration","title":"Contour Configuration","text":"<ul> <li>Under <code>extensions/ingress/contour/vsphere</code> - the following two config files are required:</li> </ul> <ul> <li>The default Contour configuration is suitable for most deployments</li> <li>Advanced options can be added, such as:</li> <li><code>requestTimeout</code></li> <li><code>connectionIdleTimeout</code></li> <li><code>minimumProtocolVersion</code> (TLS)</li> <li>Additional options are available in the TKG documentation</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/08_Lab-11_Deploy-Contour.html","title":"4.08 - Lab 11","text":""},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/08_Lab-11_Deploy-Contour.html#objectives","title":"Objectives","text":"<ul> <li>Configure Contour</li> <li>Deploy Contour</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/08_Lab-11_Deploy-Contour.html#configure-contour","title":"Configure Contour","text":"<ol> <li> <p>Using the terminal, navigate to the <code>contour</code> directory.</p> <p><code>cd ~/Workspace/tkg-extensions-v1.3.1+vmware.1/extensions/ingress/contour</code></p> </li> <li> <p>Copy the example values for Contour used with NSX Advanced Load Balancer.</p> <p><code>cp vsphere/contour-data-values-lb.yaml.example contour-data-values.yaml</code></p> </li> </ol>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/08_Lab-11_Deploy-Contour.html#deploy-contour","title":"Deploy Contour","text":"<ol> <li> <p>Using the terminal, navigate to the <code>contour</code> directory.</p> <p><code>cd ~/Workspace/tkg-extensions-v1.3.1+vmware.1/extensions/ingress/contour</code></p> </li> <li> <p>Set the kubectl context to the tkc-01 cluster.</p> <p><code>kubectl config use-context tkc-01-admin@tkc-01</code></p> </li> <li> <p>Deploy the Contour namespace and roles.</p> <p><code>kubectl apply -f namespace-role.yaml</code></p> </li> <li> <p>Create a Kubernetes secret by using the configuration values file.</p> <p><code>kubectl create secret generic contour-data-values --from-file=values.yaml=contour-data-values.yaml -n tanzu-system-ingress</code></p> </li> <li> <p>Deploy the Contour extension.</p> <p><code>kubectl apply -f contour-extension.yaml</code></p> </li> <li> <p>Verify the status of the deployment by using kubectl.</p> <p><code>kubectl get app contour -n tanzu-system-ingress</code></p> <p>Re-run the command until the status displays as <code>Reconcile succeeded</code>.</p> </li> <li> <p>Verify the status of the deployment by using kapp.</p> <p><code>kapp list -n tanzu-system-ingress</code></p> <p><code>kapp inspect --app contour-ctrl -n tanzu-system-ingress</code></p> <p>In the vSphere Client, the tasks panel will show two VMs being created with the name Avi-se-xxxxx.</p> <p>Wait for the VMs to be created before continuing to the next step.</p> </li> <li> <p>Display the load balancer IP address created for Contour.</p> <p><code>kubectl get service -n tanzu-system-ingress</code></p> <p>Because Envoy performs the data plane functionality for Contour, the external IP address is assigned to Envoy.</p> </li> </ol>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/09_Service-Discovery.html","title":"4.09 - Service Discovery","text":""},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/09_Service-Discovery.html#objectives","title":"Objectives","text":"<ul> <li>Describe Service Discovery</li> <li>Describe External DNS</li> <li>Detail the configuration options for BND Servers</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/09_Service-Discovery.html#service-discovery-overview","title":"Service Discovery Overview","text":"<ul> <li>Kubernetes provides a declarative API for deploying apps, services, ingress and load balancer resources</li> <li>Services, ingress, load balancer resources provide methods to route external IP network traffic into a cluster and to an application.</li> <li>IP addresses can change as workloads are created and destroyed</li> <li>Service discovery solves the problem of the above by allowing applications to be registered in a service registry</li> <li>This can be queried by applications and end users needing access to a particular application.</li> <li>Additionally, this external registry should be regularly updated whenever these resources are changed.</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/09_Service-Discovery.html#external-dns-overview","title":"External DNS Overview","text":"<ul> <li>A service discovery solution that:</li> <li>Uses DNS as a service registry</li> <li>Uses FQDNs defined in Ingress, HTTPproxy and Load Balancer resources</li> <li>Creates DNS entries for applications in the configured DNS server</li> <li>Allows users and applications outside a cluster to use DNS to access workloads inside a cluster</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/09_Service-Discovery.html#dns-servers-supported-by-external-dns","title":"DNS Servers Supported by External DNS","text":"<ul> <li>RFC2136 (BIND) Server</li> <li>Microsoft DNS using RFC2136 (Insecure) or RFC3645 (Secure)</li> <li>AWS Route 53</li> <li>Microsoft Azure</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/09_Service-Discovery.html#how-external-dns-works","title":"How External DNS Works","text":"<ul> <li>External DNS:</li> <li>Watches for hostnames in Kubernetes API resources<ul> <li>Ingress and HTTPproxy resources are watched for host field values in each rule defined</li> <li>Load balancer resources are watched for <code>[external-dns.alpha.kubernetes.io/hostname](http://external-dns.alpha.kubernetes.io/hostname)</code> annotations</li> </ul> </li> <li>Creates a DNS entry in the configured DNS server that points to the ingress or Load Balancer IP</li> <li>Deletes the DNS entry when the ingress / HTPProxy / LoadBalancer resources are deleted</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/09_Service-Discovery.html#external-dns-configuration-for-bind-servers","title":"External DNS Configuration for BIND Servers","text":"<ul> <li>Example configuration files are of the format <code>external-dns-data-values-&lt;type&gt;.yaml</code></li> <li>The config files determines how load balancers are watched for External DNS annotations</li> <li>To have External DNS watch for Contour Ingress and HTTPproxy resources, the file used will be <code>external-dns-data-values-rfc2136-with-contour.yaml</code></li> <li>The following parameters are typically defined when working with a BIND server.</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/10_Lab-12_Deploy-External-DNS.html","title":"4.10 - Lab 12","text":""},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/10_Lab-12_Deploy-External-DNS.html#objectives","title":"Objectives","text":"<ul> <li>Retrieve the secret key for DNS updates</li> <li>Configure external DNS</li> <li>Deploy External DNS</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/10_Lab-12_Deploy-External-DNS.html#retrieve-the-secret-key-for-dns-updates","title":"Retrieve the Secret Key For DNS Updates","text":"<ol> <li> <p>In the terminal, display the BIND configuration.</p> <p><code>cat /etc/bind/named.conf.local</code></p> <p>In this lab, the student desktop is performing the role of DNS server.</p> </li> <li> <p>Under externaldns-key, copy the value of <code>secret</code> and paste it in to <code>commands.txt</code> on the desktop.</p> </li> </ol>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/10_Lab-12_Deploy-External-DNS.html#configure-external-dns","title":"Configure External DNS","text":"<ol> <li> <p>Using the terminal, navigate to the <code>external-dns</code> directory.</p> <p><code>cd ~/Workspace/tkg-extensions-v1.3.1+vmware.1/extensions/service-discovery/external-dns</code></p> </li> <li> <p>Copy the sample configuration file.</p> <p><code>cp external-dns-data-values-rfc2136-with-contour.yaml.example external-dns-data-values.yaml</code></p> </li> <li> <p>Open <code>external-dns-data-values.yaml</code> in Visual Studio Code.</p> <p><code>code external-dns-data-values.yaml</code></p> <ol> <li>Modify the following parameters.</li> <li>Save the file and close Visual Studio Code.</li> <li>Verify that the configuration file matches the reference configuration file.</li> </ol> <p><code>checkconfig external-dns-data-values.yaml</code></p> <p>When a configuration mismatch exists, Visual Studio Code opens the configuration file in the left panel and the reference configuration file in the right panel.</p> <ol> <li> <p>If a configuration mismatch exists, modify the configuration on the left to match the reference configuration on the right.</p> <p>Differences are highlighted in red.</p> </li> <li> <p>Save the file and close Visual Studio Code.</p> </li> </ol> </li> </ol>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/10_Lab-12_Deploy-External-DNS.html#deploy-external-dns","title":"Deploy External DNS","text":"<pre><code>1. Using the terminal, navigate to the `external-dns` directory.\n\n    `cd ~/Workspace/tkg-extensions-v1.3.1+vmware.1/extensions/service-discovery/external-dns`\n\n2. Set the kubectl context to the tkc-01 cluster.\n\n    `kubectl config use-context tkc-01-admin@tkc-01`\n\n3. Create the namespace and roles.\n\n    `kubectl apply -f namespace-role.yaml`\n\n4. Create a secret containing the External DNS configuration.\n\n    `kubectl create secret generic external-dns-data-values --from-file=values.yaml=external-dns-data-values.yaml -n tanzu-system-service-discovery`\n\n5. Deploy External DNS.\n\n    `kubectl apply -f external-dns-extension.yaml`\n\n6. Verify the status of the deployment by using kubectl.\n\n    `kubectl get app external-dns -n tanzu-system-service-discovery`\n\n    Re-run the command until the status displays as `Reconcile succeeded`.\n\n7. Verify the status of the deployment by using kapp.\n\n    `kapp list -n tanzu-system-service-discovery`\n\n    `kapp inspect --app external-dns-ctrl -n tanzu-system-service-discovery`\n</code></pre>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/11_Cluster-Monitoring.html","title":"4.11 - Cluster Monitoring","text":""},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/11_Cluster-Monitoring.html#objectives","title":"Objectives","text":"<ul> <li>Describe Prometheus</li> <li>Describe Grafana</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/11_Cluster-Monitoring.html#monitoring","title":"Monitoring","text":"<ul> <li>TKG provides cluster monitoring services via the two projects:</li> <li>Prometheus:<ul> <li>A systems monitoring and alerting toolkit</li> <li>Collects metrics from clusters and applications at specific intervals</li> <li>Triggers alerts depending on conditions</li> </ul> </li> <li>Grafana:<ul> <li>Visualization and analytics software</li> <li>Allows query, visualization, and alerting operations for metrics; as well as alerting</li> </ul> </li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/11_Cluster-Monitoring.html#architecture","title":"Architecture","text":""},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/11_Cluster-Monitoring.html#prometheus-components","title":"Prometheus Components","text":"<ul> <li>Standard Prometheus deployment comes with the following components:</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/11_Cluster-Monitoring.html#prometheus-configuration-options","title":"Prometheus Configuration Options","text":"<ul> <li>Default prometheus configuration values are generally fine, but for particular use cases the <code>prometheus-data-values.yaml</code> file can be edited with options such as:</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/11_Cluster-Monitoring.html#grafana","title":"Grafana","text":"<ul> <li>An open-source monitoring and observability platform</li> <li>Can connect to and pull data from many sources</li> <li>Provides dynamic dashboards, graphs and visualization for Prometheus backend data.</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/11_Cluster-Monitoring.html#grafana-configuration-options","title":"Grafana Configuration Options","text":"<ul> <li>Similar to Grafana configuration, the <code>grafana-data-values.yaml</code> file must be edited with parameters, including the following, to work:</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/12_Lab-13_Deploy-Prometheus-and-Grafana.html","title":"4.12 - Lab 13","text":""},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/12_Lab-13_Deploy-Prometheus-and-Grafana.html#objectives","title":"Objectives","text":"<ul> <li>Deploy Prometheus</li> <li>Configure Grafana</li> <li>Deploy Grafana</li> <li>Access the Grafana Instance</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/12_Lab-13_Deploy-Prometheus-and-Grafana.html#deploy-prometheus","title":"Deploy Prometheus","text":"<ol> <li> <p>Using the terminal, navigate to the <code>prometheus</code> directory.</p> <p><code>cd ~/Workspace/tkg-extensions-v1.3.1+vmware.1/extensions/monitoring/prometheus</code></p> </li> <li> <p>Copy the default configuration values file.</p> <p><code>cp prometheus-data-values.yaml.example prometheus-data-values.yaml</code></p> </li> <li> <p>Set the kubectl context to the tkc-01 cluster.</p> <p><code>kubectl config use-context tkc-01-admin@tkc-01</code></p> </li> <li> <p>Create a namespace for the Prometheus service on the Tanzu Kubernetes cluster.</p> <p><code>kubectl apply -f namespace-role.yaml</code></p> </li> <li> <p>Create a Kubernetes secret from the configuration values file.</p> <p><code>kubectl create secret generic prometheus-data-values --from-file=values.yaml=prometheus-data-values.yaml -n tanzu-system-monitoring</code></p> </li> <li> <p>Deploy the Prometheus extension.</p> <p><code>kubectl apply -f prometheus-extension.yaml</code></p> </li> <li> <p>Verify the status of the deployment by using kubectl.</p> <p><code>kubectl get app prometheus -n tanzu-system-monitoring</code></p> <p>Re-run the command until the status displays as <code>Reconcile succeeded</code>.</p> </li> <li> <p>Verify the status of the deployment by using kapp.</p> <p><code>kapp list -n tanzu-system-monitoring</code></p> <p><code>kapp inspect --app prometheus-ctrl -n tanzu-system-monitoring</code></p> </li> </ol>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/12_Lab-13_Deploy-Prometheus-and-Grafana.html#configure-grafana","title":"Configure Grafana","text":"<ol> <li> <p>Using the terminal, navigate to the <code>grafana</code> directory.</p> <p><code>cd ~/Workspace/tkg-extensions-v1.3.1+vmware.1/extensions/monitoring/grafana</code></p> </li> <li> <p>Copy the default configuration values file.</p> <p><code>cp grafana-data-values.yaml.example grafana-data-values.yaml</code></p> </li> <li> <p>Create a base64 encoded password.</p> <p><code>echo -n \"VMware1!\" | base64</code></p> </li> <li> <p>Open <code>grafana-data-values.yaml</code> in Visual Studio Code.</p> <p><code>code grafana-data-values.yaml</code></p> <ol> <li> <p>Modify the following parameters.</p> <p>The configuration file should resemble the following screenshot.</p> </li> <li> <p>Save the file and close Visual Studio Code.</p> </li> <li>Verify that the configuration file matches the reference configuration file.</li> </ol> <p><code>checkconfig grafana-data-values.yaml</code></p> <p>When a configuration mismatch exists, Visual Studio Code opens the configuration file in the left panel and the reference configuration file in the right panel.</p> <ol> <li> <p>If a configuration mismatch exists, modify the configuration on the left to match the reference configuration on the right.</p> <p>Differences are highlighted in red.</p> </li> <li> <p>Save the file and close Visual Studio Code.</p> </li> </ol> </li> </ol>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/12_Lab-13_Deploy-Prometheus-and-Grafana.html#deploy-grafana","title":"Deploy Grafana","text":"<ol> <li> <p>Using the terminal, navigate to the <code>grafana</code> directory.</p> <p><code>cd ~/Workspace/tkg-extensions-v1.3.1+vmware.1/extensions/monitoring/grafana</code></p> </li> <li> <p>Create the namespaces.</p> <p><code>kubectl apply -f namespace-role.yaml</code></p> </li> <li> <p>Create a Kubernetes secret with the configuration values file.</p> <p><code>kubectl create secret generic grafana-data-values --from-file=values.yaml=grafana-data-values.yaml -n tanzu-system-monitoring</code></p> </li> <li> <p>Deploy the Grafana extension.</p> <p><code>kubectl apply -f grafana-extension.yaml</code></p> </li> <li> <p>Verify the status of the deployment by using kubectl.</p> <p><code>kubectl get app grafana -n tanzu-system-monitoring</code></p> <p>Re-run the command until the status displays as <code>Reconcile succeeded</code>.</p> </li> <li> <p>Verify the status of the deployment by using kapp.</p> <p><code>kapp list -n tanzu-system-monitoring</code></p> <p><code>kapp inspect --app grafana-ctrl -n tanzu-system-monitoring</code></p> </li> </ol>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/12_Lab-13_Deploy-Prometheus-and-Grafana.html#access-the-grafana-web-interface","title":"Access the Grafana Web Interface","text":"<ol> <li> <p>Using Firefox, open the Grafana bookmark in a new tab.</p> <p><code>https://grafana.tkg.vclass.local</code></p> </li> <li> <p>Click Advanced and click Accept the Risk and Continue.</p> <p>The browser redirects to the Grafana login screen.</p> </li> <li> <p>Log in to Grafana.</p> <ul> <li>User name: admin</li> <li>Password: VMware1!</li> </ul> </li> <li>In the left navigation panel, click + and click Import.</li> <li> <p>On the Import screen, enter 7249 for Import via grafana.com.</p> <p>7249 is the ID of a freely available Kubernetes dashboard on Grafana.com</p> </li> <li> <p>Click Load.</p> <p>The dashboard information displays.</p> </li> <li> <p>For Prometheus, select Prometheus.</p> </li> <li> <p>Click Import.</p> <p>The imported dashboard displays.</p> </li> </ol>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/13_Lab-14_Accessing-the-Cluster.html","title":"4.13 - Lab 14","text":""},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/13_Lab-14_Accessing-the-Cluster.html#objectives","title":"Objectives","text":"<ol> <li>Access the cluster as a developer</li> <li>Deploy a workload</li> <li>Inspect the NSX Advanced load balancer</li> <li>Inspect the logs in vRealize log insight</li> </ol>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/13_Lab-14_Accessing-the-Cluster.html#access-the-cluster-as-a-developer","title":"Access the Cluster as a Developer","text":"<ol> <li> <p>Using the terminal, click the new terminal tab button in the top left.</p> <p>Keep the original terminal tab to continue accessing the cluster using the admin account.</p> <p>Use the new terminal tab to access the cluster using the developer account.</p> </li> <li> <p>Navigate to the <code>Workspace</code> directory.</p> <p><code>cd ~/Workspace</code></p> </li> <li> <p>Using the new terminal tab, make kubectl use the new kubeconfig.</p> <p><code>export KUBECONFIG=~/Workspace/kubeconfig-developers.yaml</code></p> </li> <li> <p>Attempt access the cluster using kubectl.</p> <p><code>kubectl get pods -A</code></p> <p>The browser opens.</p> </li> <li> <p>Click Advanced, then click Accept the Risk and Continue.</p> <p>The browser redirects to the LDAP login screen.</p> </li> <li> <p>Click Advanced, then click Accept the Risk and Continue.</p> <p>The browser displays the login screen.</p> </li> <li> <p>Log in using the LDAP credentials.</p> <ul> <li>User name: developer01</li> <li>Password: VMware1!</li> </ul> </li> <li> <p>Click Login.</p> <p><code>You have been logged in and may now close this tab</code> is displayed in the browser.</p> </li> <li> <p>In the terminal, review the kubectl output.</p> <p>The pods running on the cluster are listed.</p> </li> <li> <p>List the kubectl contexts.</p> <p><code>kubectl config get-contexts</code></p> <p>The <code>tanzu-cli-tkc-01@tkc-01</code> context displays.</p> </li> </ol>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/13_Lab-14_Accessing-the-Cluster.html#developer-workload","title":"Developer Workload","text":"<ol> <li> <p>Using the terminal, navigate to the <code>Workspace</code> directory.</p> <p><code>cd ~/Workspace</code></p> </li> <li> <p>Ensure that kubectl is using the developer credentials.</p> <p><code>export KUBECONFIG=~/Workspace/kubeconfig-developers.yaml</code></p> </li> <li> <p>Display the vmbeans configuration file.</p> <p><code>cat vmbeans.yaml</code></p> <p>A deployment and LoadBalancer service type display.</p> <p>The service has an external-dns annotation specifying the VMBeans FQDN.</p> </li> <li> <p>Deploy the vmbeans configuration.</p> <p><code>kubectl apply -f vmbeans.yaml</code></p> </li> <li> <p>Verify the status of the deployment by using kubectl.</p> <p><code>kubectl get deployment,service</code></p> <p>Re-run the command until the deployment status is READY 1/1 and the vmbeans-service has an IP address listed in the EXTERNAL-IP field.</p> </li> <li> <p>Using Firefox, open the VMBeans bookmark in a new tab.</p> <p><code>http://vmbeans.tkg.vclass.local</code></p> <p>The VMBeans website displays.</p> </li> </ol>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/13_Lab-14_Accessing-the-Cluster.html#inspect-the-nsx-advanced-load-balancer-ui","title":"Inspect the NSX Advanced Load Balancer UI","text":"<ol> <li> <p>Using Firefox, open the NSX Adv LB bookmark.</p> <p><code>https://sa-nsxlb-01.vclass.local</code></p> </li> <li> <p>Log in to NSX Advanced Load Balancer.</p> <ul> <li>User name: admin</li> <li>Password: VMware1!</li> </ul> </li> <li> <p>On the Dashboard screen, under Virtual Services, click default-tkc-01--default-....</p> <p>default-tkc-01--default is the load balancer for the VMBeans service that you created in the previous task.</p> </li> <li> <p>Click Displaying Past 6 Hours and change it to Past 30 Minutes.</p> </li> <li> <p>Using the terminal, navigate to the <code>Workspace</code> directory.</p> <p><code>cd ~/Workspace/</code></p> </li> <li> <p>Generate traffic to the VMBeans website and leave the command running.</p> <p><code>./gen-vmbeans-http-req.sh</code></p> </li> <li> <p>View the traffic in NSX Advanced Load Balancer UI.</p> <p>It takes a few minutes for the graph to display the traffic.</p> <p>Continue to the next task while waiting for the graph to update.</p> </li> </ol>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/13_Lab-14_Accessing-the-Cluster.html#inspect-the-logs-in-vrealize-log-insight","title":"Inspect the Logs in Vrealize Log Insight","text":"<ol> <li> <p>Using Firefox, click the vRLI bookmark in a new tab.</p> <p><code>https://sa-loginsight-01.vclass.local</code></p> </li> <li> <p>Log in to vRealize Log Insight.</p> <ul> <li>User name: admin</li> <li>Password: VMware1!</li> </ul> </li> <li>In the top panel, click Interactive Analytics.</li> <li>Click ADD FILTER.</li> <li>From the text drop-down menu, click appname.</li> <li>In the text box beside contains, enter vmbeans and select the full pod name vmbeans-deployment-##########-#####.</li> <li> <p>Click the search icon.</p> <p>The logs from the vmbeans pod display.</p> </li> <li> <p>Review the first log entry in the list.</p> <p>The log line contains \"<code>GET / HTTP/1.1 200</code>\" which is a web request logged by the web server process inside the VMBeans pod.</p> </li> <li> <p>Using the terminal, close the terminal tab created in task 1 so you are no longer using the <code>~/Workspace/kubeconfig-developers.yaml</code> file to access the cluster.</p> </li> </ol>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/04_Tanzu_Kubernetes_Grid_Extensions/14_End-of-Module-Assessment.html","title":"4.14 - End of Module Assessment","text":"<ol> <li>What extension provides logging functionality?<ol> <li>Fluent-Bit</li> </ol> </li> <li>What extensions facilitate cluster monitoring?<ol> <li>Prometheus and Grafana</li> </ol> </li> <li>What extension provides ingress functionality?<ol> <li>Contour</li> </ol> </li> </ol>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/05_Troubleshooting/01_Tanzu-Kubernetes-Grid-Logs.html","title":"5.1 - Tanzu Kubernetes Grid Logs","text":""},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/05_Troubleshooting/01_Tanzu-Kubernetes-Grid-Logs.html#context","title":"Context","text":"<ul> <li>Understanding how to identify and resolve issues is vital to efficient operation of TKG.</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/05_Troubleshooting/01_Tanzu-Kubernetes-Grid-Logs.html#objectives","title":"Objectives","text":"<ul> <li>Describe the logs of TKG</li> <li>Identify the location of TKG logs</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/05_Troubleshooting/01_Tanzu-Kubernetes-Grid-Logs.html#monitoring-tanzu-kubernetes-cluster-deployments-in-cluster-api-logs","title":"Monitoring Tanzu Kubernetes Cluster  Deployments in Cluster API Logs","text":"<ul> <li>If a cluster deployment is failing, the reason can usually be deduced from the logs for the Cluster API pods via <code>kubectl logs</code></li> <li>During a management cluster deployment, these logs are accessible on the bootstrap cluster</li> <li>For a workload cluster deployment, they are accessible on the management cluster</li> <li>Pods whos logs should be noted and their associated namespaces follow:</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/05_Troubleshooting/01_Tanzu-Kubernetes-Grid-Logs.html#viewing-cluster-api-resources","title":"Viewing Cluster API Resources","text":"<ul> <li>Each cluster deployed via TKG has corresponding cluster API resources that can be viewed by the <code>kubectl describe</code> commands</li> <li>Resources covered include:<ul> <li>clusters</li> <li>kubeadmconfigs</li> <li>machinedeployments</li> <li>machinesets</li> <li>machines</li> <li>vsphereclusters</li> <li>haproxyloadbalancerss</li> <li>vspherevms</li> </ul> </li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/05_Troubleshooting/01_Tanzu-Kubernetes-Grid-Logs.html#kubernetes-audit-logging","title":"Kubernetes Audit Logging","text":"<ul> <li>Enabled by setting <code>ENABLE_AUDIT_LOGGING=true</code> in the configuration file when deploying a cluster</li> <li>Logs metadata regarding all requests made to the Kubernetes API server</li> <li>Audit events logged to <code>/var/log/kubernetes/audit.log</code> on each control plane node</li> <li>Audit logs are by default included in the logs sent to the FluentBit logging server</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/05_Troubleshooting/01_Tanzu-Kubernetes-Grid-Logs.html#system-audit-logging","title":"System Audit Logging","text":"<ul> <li>Enabled by default on Tanzu Kubernetes clusters</li> <li>Uses the auditd system to track audit events</li> <li>Logs audit events to <code>/var/log/audit/audit.log</code> on each node</li> <li>Included in the logs sent to the FluentBit logging server</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/05_Troubleshooting/02_Using-Crash-Diagnostics.html","title":"5.2 - Using Crash Diagnostics","text":""},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/05_Troubleshooting/02_Using-Crash-Diagnostics.html#objectives","title":"Objectives","text":"<ul> <li>Describe the purpose of Crash Diagnostics</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/05_Troubleshooting/02_Using-Crash-Diagnostics.html#crash-diagnostics-crashd","title":"Crash Diagnostics (Crashd)","text":"<ul> <li>A command-line tool that facilitates investigation and troubleshooting of unhealthy or unresponsive Kubernetes clusters</li> <li>Only available on Linux or Mac OS</li> <li>Runs a series of scripts that collect Kubernetes API output, node logs, node CLI outputs through SSH to assess the health of the clusters</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/05_Troubleshooting/02_Using-Crash-Diagnostics.html#diagnostic-scripts","title":"Diagnostic Scripts","text":"<ul> <li>Scripts included in crashd written in Starlark (a dialect of Python)</li> <li>Crashd exposes commonly used operations as Starlark functions used to build diagnostic scripts</li> <li>Examples:</li> <li>Run kubectl and collect specified resources: <code>kube_capture (what=\"objects\", kinds=[\"deployments\", \"services\"], namespaces=[\"default\"])</code></li> <li>Run a command through SSH on a particular node: <code>capture(cmd=\"df -h /\", resources=nodes)</code></li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/05_Troubleshooting/02_Using-Crash-Diagnostics.html#default-diagnostics-script","title":"Default Diagnostics Script","text":"<ul> <li>Crashd provides a default diagnostics script covering common use cases that:</li> <li>Accepting arguments to specify what to collect</li> <li>Collects bootstrap management and workload cluster information</li> <li>Outputs an archive file containing all requested information</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/05_Troubleshooting/02_Using-Crash-Diagnostics.html#passing-arguments-to-the-diagnostics-scripts","title":"Passing Arguments to the Diagnostics Scripts","text":"<ul> <li>Two typical methods used:</li> <li>An args file, used via <code>--args-file &lt;args file&gt;</code></li> <li>Specifying the args in key-value pairs in the CLI e.g. <code>--args key1=value1,key2=value2</code></li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/05_Troubleshooting/02_Using-Crash-Diagnostics.html#default-diagnostic-script-arguments","title":"Default Diagnostic Script Arguments","text":"<ul> <li>The default diagnostics script accepts the following arguments amongst many others</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/05_Troubleshooting/02_Using-Crash-Diagnostics.html#running-crash-diagnostics","title":"Running Crash Diagnostics","text":"<ul> <li>Depending on whether using an args file or not, crashd is initiated via the following:</li> <li>Passing the args from a file:<ul> <li><code>crashd run --args-file &lt;args file&gt; &lt;crashd script.crsh&gt;</code></li> </ul> </li> <li>Same as above but override the target and workload_cluster parameters:<ul> <li><code>crashd run --args-file &lt;args file&gt; --args target=workload,workload_cluster=tkc-01 diagnostics.crsh</code></li> </ul> </li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/05_Troubleshooting/03_Lab-15_Using-Crash-Diagnostics.html","title":"5.3 - Lab 15","text":""},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/05_Troubleshooting/03_Lab-15_Using-Crash-Diagnostics.html#install-the-crash-diagnostics-cli","title":"Install the Crash Diagnostics CLI","text":"<ol> <li> <p>Using the terminal, navigate to the <code>Downloads</code> directory.</p> <p><code>cd ~/Downloads</code></p> </li> <li> <p>Unzip the Crash Diagnostics CLI.</p> <p><code>tar -zxvf crashd-linux-amd64-v0.3.2-vmware.3.tar.gz</code></p> </li> <li> <p>Install crashd to the <code>bin</code> directory.</p> <p><code>sudo install crashd/crashd-linux-amd64-v0.3.2+vmware.3 /usr/local/bin/crashd</code></p> </li> <li> <p>Display the command syntax to verify that Crash Diagnostics is installed.</p> <p><code>crashd --help</code></p> </li> </ol>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/05_Troubleshooting/03_Lab-15_Using-Crash-Diagnostics.html#configure-the-crash-diagnostics-arguments-file","title":"Configure the Crash Diagnostics Arguments File","text":"<ol> <li> <p>Using the terminal, navigate to the <code>crashd</code> directory.</p> <p><code>cd ~/Downloads/crashd</code></p> </li> <li> <p>Open <code>args</code> in Visual Studio Code.</p> <p><code>code args</code></p> <ol> <li>Modify the following parameters.</li> </ol> <p></p> <ol> <li>Save the file and close Visual Studio Code.</li> <li>Verify that the configuration file matches the reference configuration file.</li> </ol> <p><code>checkconfig args</code></p> <p>When a configuration mismatch exists, Visual Studio Code opens the configuration file in the left panel and the reference configuration file in the right panel.</p> <ol> <li> <p>If a configuration mismatch exists, modify the configuration on the left to match the reference configuration on the right.</p> <p>Differences are highlighted in red.</p> </li> <li> <p>Save the file and close Visual Studio Code.</p> </li> </ol> </li> </ol>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/05_Troubleshooting/03_Lab-15_Using-Crash-Diagnostics.html#run-the-crash-diagnostics-cli","title":"Run the Crash Diagnostics CLI","text":"<ol> <li> <p>Run the Crash Diagnostics CLI.</p> <p><code>crashd run --debug --args-file args diagnostics.crsh</code></p> <p>The logs are collected in the <code>workdir</code> directory and compressed to tkg-mgmt.diagnostics.tar.gz.</p> </li> <li> <p>Display the collected logs.</p> <p><code>find ./workdir/tkg-mgmt-cluster</code></p> </li> <li> <p>Run the command again, overwriting the target value.</p> <p><code>crashd run --debug --args-file args --args target=workload diagnostics.crsh</code></p> <p>The logs are collected in the <code>workdir</code> directory and compressed to tkc-01.diagnostics.tar.gz.</p> </li> <li> <p>Display the collected logs.</p> <p><code>tree -d -L 2 ./workdir/tkc-01 | less</code></p> </li> </ol>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/05_Troubleshooting/04_Troubleshooting-Commands.html","title":"5.4 - Troubleshooting Commands","text":""},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/05_Troubleshooting/04_Troubleshooting-Commands.html#objectives","title":"Objectives","text":"<ul> <li>Describe how to use SSH to connect to a Tanzu Kubernetes VM</li> <li>Detail the steps to troubleshoot a failed cluster deployment</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/05_Troubleshooting/04_Troubleshooting-Commands.html#connecting-to-cluster-nodes-with-ssh","title":"Connecting to Cluster Nodes with SSH","text":"<ul> <li>SSH can be used  connect to individual nodes in both management and Tanzu Kubernetes clusters</li> <li>For this to happen, you need an SSH key pair - one should already exist following deployment of the management cluster</li> <li>The SSH key entered in the installer for the management cluster is associated with the CAPV user account</li> <li><code>ssh capv@&lt;VM IP Address&gt;</code></li> <li>Assume root privileges with <code>sudo -i</code></li> <li>As the SSH key is present on the system running the SSH command, no password will eb required</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/05_Troubleshooting/04_Troubleshooting-Commands.html#failure-of-management-cluster-create-command","title":"Failure of Management Cluster Create Command","text":"<ul> <li>Sometimes a cluster fails to create - a typical error is <code>waiting for cert-manager to be available</code></li> <li>Following steps can be used:</li> <li>Verify node status <code>kubectl get nodes</code></li> <li>Verify pod status <code>kubectl get pods -A</code></li> <li>For any failing pods, check the logs and events:<ul> <li><code>kubectl describe pod -n &lt;namespace&gt; &lt;pod name&gt;</code></li> <li><code>kubectl logs -n &lt;namespace&gt; &lt;pod name&gt;</code></li> </ul> </li> <li>Resolve issues with configuration or connectivity based on output</li> <li>Delete the management cluster - <code>tanzu management-cluster delete</code></li> <li>Run the create command again to ensure a clean deployment: <code>tanzu management-cluster create</code></li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/05_Troubleshooting/04_Troubleshooting-Commands.html#recovering-cluster-credentials","title":"Recovering Cluster Credentials","text":"<ul> <li>Run <code>tanzu management-cluster create</code> - recreates the <code>.kube-tkg/config</code> file</li> <li>Obtain the IP address of the management cluster control plane node from vSphere</li> <li>Use SSH to login to the management cluster control plane node: <code>ssh capv@&lt;node IP address&gt;</code></li> <li>Access the <code>admin.conf</code> file in the management cluster - <code>sudo cat /etc/kubernetes/admin.conf</code></li> <li>Copy the following into the local <code>.kube-tkg/config</code> file:</li> <li>Cluster name</li> <li>Cluster user name</li> <li>Cluster context</li> <li>Client certificate data</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/05_Troubleshooting/05_Lab-16_Connection-to-Nodes.html","title":"5.5 - Lab 16","text":""},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/05_Troubleshooting/05_Lab-16_Connection-to-Nodes.html#objective","title":"Objective","text":"<ul> <li>Connect to a Node using SSH</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/05_Troubleshooting/05_Lab-16_Connection-to-Nodes.html#connect-to-a-node-using-ssh","title":"Connect to a Node Using SSH","text":"<ol> <li> <p>Using the terminal, set the kubectl context to the tkc-01 cluster.</p> <p><code>kubectl config use-context tkc-01-admin@tkc-01</code></p> </li> <li> <p>List the nodes in the tkc-01 cluster.</p> <p><code>kubectl get nodes -o wide</code></p> </li> <li> <p>Copy the IP address of the control plane node.</p> </li> <li> <p>Connect with SSH by using the capv user.</p> <p><code>ssh capv@&lt;CONTROL_PLANE_IP_ADDRESS&gt;</code></p> <p>The SSH connection succeeds without a password because the ~/.ssh/id_rsa.pub public key from the student desktop was added to each Kubernetes node.</p> </li> <li> <p>Change to the root user.</p> <p><code>sudo -i</code></p> </li> <li> <p>Navigate to the <code>log</code> directory.</p> <p><code>cd /var/log/containers</code></p> </li> <li> <p>List the logs.</p> <p><code>ls -l</code></p> <p>The directory contains logs for the running containers.</p> </li> <li> <p>Navigate to the <code>kubernetes</code> directory.</p> <p><code>cd /var/log/kubernetes</code></p> </li> <li> <p>List the logs.</p> <p><code>ls -l</code></p> <p>The directory contains audit logs for Kubernetes API server.</p> </li> <li> <p>Enter <code>exit</code> to close the root session and enter <code>exit</code> to close the SSH connection.</p> </li> </ol>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/05_Troubleshooting/06_Run-Image-Builder.html","title":"5.6 - Lab 17","text":""},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/05_Troubleshooting/06_Run-Image-Builder.html#objectives","title":"Objectives","text":"<ul> <li>Unzip the Tanzu Kubernetes Grid Image Builder Files</li> <li>Load the OVF tool into the Image Builder Container Image</li> <li>Prepare the Image Builder Configuration</li> <li>Run Image Builder</li> </ul>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/05_Troubleshooting/06_Run-Image-Builder.html#unzip-the-tanzu-kubernetes-grid-image-builder-files","title":"Unzip the Tanzu Kubernetes Grid Image Builder Files","text":"<ol> <li> <p>Using the terminal, navigate to the <code>Downloads</code> directory.</p> <p><code>cd ~/Downloads</code></p> </li> <li> <p>Unzip the Tanzu Kubernetes Grid Image Builder file.</p> <p><code>unzip TKG-Image-Builder-for-Kubernetes-v1.19.9-master.zip</code></p> </li> <li> <p>Move the files to the <code>imagebuilder</code> directory.</p> <p><code>mv TKG-Image-Builder-for-Kubernetes-v1.19.9-master/TKG-Image-Builder-for-Kubernetes-v1.19.9/ ~/Workspace/imagebuilder</code></p> </li> <li> <p>List the directory contents to verify that the files were copied.</p> <p><code>ls -l ~/Workspace/imagebuilder/TKG-Image-Builder-for-Kubernetes-v1.19.9</code></p> </li> </ol>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/05_Troubleshooting/06_Run-Image-Builder.html#load-the-ovf-tool-into-the-image-builder-container-image","title":"Load the OVF tool into the Image Builder Container Image","text":"<ol> <li> <p>Using the terminal, navigate to the imagebuilder directory.</p> <p><code>cd ~/Workspace/imagebuilder</code></p> </li> <li> <p>Copy the OVF Tool file from the <code>Downloads</code> directory.</p> <p><code>cp ~/Downloads/VMware-ovftool-4.4.1-16812187-lin.x86_64.bundle ./</code></p> </li> <li> <p>Open <code>Dockerfile</code> in Visual Studio Code.</p> <p><code>code Dockerfile</code></p> <ol> <li>Modify the following parameter.</li> <li>Save the file and close Visual Studio Code.</li> <li>Verify that the configuration file matches the reference configuration file.</li> </ol> <p><code>checkconfig Dockerfile</code></p> <p>When a configuration mismatch exists, Visual Studio Code opens the configuration file in the left panel and the reference configuration file in the right panel.</p> <ol> <li> <p>If a configuration mismatch exists, modify the configuration on the left to match the reference configuration on the right.</p> <p>Differences are highlighted in red.</p> </li> <li> <p>Save the file and close Visual Studio Code.</p> </li> <li>Build the container image.</li> </ol> <p><code>docker build . -t harbor.vclass.local/tkg/imagebuilder-byoi:v0.1.9</code></p> </li> </ol>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/05_Troubleshooting/06_Run-Image-Builder.html#prepare-the-image-builder-configuration","title":"Prepare the Image Builder Configuration","text":"<ol> <li> <p>Using the terminal, navigate to the <code>imagebuilder</code> directory.</p> <p><code>cd ~/Workspace/imagebuilder</code></p> </li> <li> <p>Open <code>custom.json</code> in Visual Studio Code.</p> <p><code>code custom.json</code></p> <ol> <li> <p>Modify <code>custom.json</code> by adding libnfs-utils to the extra_debs field.</p> <p><code>\"extra_debs\": \"\\\"libnfs-utils\\\"\"</code></p> <p>The field must be double-quoted.</p> <p>The libnfs-utils package will be installed in the image.</p> </li> <li> <p>Save the file and close Visual Studio Code.</p> </li> <li>Verify that the configuration file matches the reference configuration file.</li> </ol> <p><code>checkconfig custom.json</code></p> <p>When a configuration mismatch exists, Visual Studio Code opens the configuration file in the left panel and the reference configuration file in the right panel.</p> <ol> <li> <p>If a configuration mismatch exists, modify the configuration on the left to match the reference configuration on the right.</p> <p>Differences are highlighted in red.</p> </li> <li> <p>Save the file and close Visual Studio Code.</p> </li> <li>Open <code>metadata.json</code> in Visual Studio Code.</li> </ol> <p><code>code metadata.json</code></p> <ol> <li> <p>Modify <code>metadata.json</code> by appending vclass.0 to the VERSION field.</p> <p><code>\"VERSION\": \"v1.19.9+vmware.2-vclass.0\"</code></p> <p>This distinguishes the custom image version from the default TKG v1.19.9+vmware.2 version.</p> </li> <li> <p>Save the file and close Visual Studio Code.</p> </li> <li>Verify that the configuration file matches the reference configuration file.</li> </ol> <p><code>checkconfig metadata.json</code></p> <p>When a configuration mismatch exists, Visual Studio Code opens the configuration file in the left panel and the reference configuration file in the right panel.</p> <ol> <li> <p>If a configuration mismatch exists, modify the configuration on the left to match the reference configuration on the right.</p> <p>Differences are highlighted in red.</p> </li> <li> <p>Save the file and close Visual Studio Code.</p> </li> <li>Open <code>vsphere.json</code> in Visual Studio Code.</li> </ol> <p><code>code vsphere.json</code></p> <ol> <li>Modify <code>vsphere.json</code> with the following vSphere environment details.</li> <li>Save the file and close Visual Studio Code.</li> <li>Verify that the configuration file matches the reference configuration file.</li> </ol> <p><code>checkconfig vsphere.json</code></p> <p>When a configuration mismatch exists, Visual Studio Code opens the configuration file in the left panel and the reference configuration file in the right panel.</p> <ol> <li> <p>If a configuration mismatch exists, modify the configuration on the left to match the reference configuration on the right.</p> <p>Differences are highlighted in red.</p> </li> <li> <p>Save the file and close Visual Studio Code.</p> </li> </ol> </li> </ol>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/05_Troubleshooting/06_Run-Image-Builder.html#run-image-builder","title":"Run Image Builder","text":"<ol> <li> <p>Using the terminal, navigate to the <code>imagebuilder</code> directory.</p> <p><code>cd ~/Workspace/imagebuilder</code></p> </li> <li> <p>Review the build script.</p> <p><code>cat build.sh</code></p> <p>This script passes all the configuration files as parameters to the image builder container image.</p> <p>The <code>build-node-ova-vsphere-ubuntu-2004</code> command runs inside the container.</p> </li> <li> <p>Run the build script.</p> <p><code>./build.sh</code></p> <p>The build process will take approximately 20 minutes to complete.</p> <p>The generated OVA file is saved to the <code>output</code> directory.</p> </li> <li> <p>List the generated OVA files.</p> <p><code>ls -l output/ubuntu-2004-kube-v1.19.9</code></p> <p>The OVA files display.</p> </li> </ol>"},{"location":"certifications/vmware-application-modernization/vmware-tanzu-kubernetes-grid/05_Troubleshooting/07_End-of-Module-Assessment.html","title":"5.7 - End of Module Assessment","text":"<ol> <li>which two are clusterAPI resources?<ol> <li>machinedeployment &amp; cluster</li> </ol> </li> <li>When connecting to a node via SSH, what's the username?<ol> <li>capv</li> </ol> </li> <li>How are arguments passed to Crashd?<ol> <li><code>\u2014args-file &lt;arg file name&gt;</code></li> <li><code>\u2014args key1=value1</code></li> </ol> </li> </ol>"},{"location":"tooling/ansible/ansible-advanced.html","title":"Ansible Advanced","text":"<p>tags:   - Tooling   - Ansible</p>"},{"location":"tooling/ansible/ansible-for-beginners.html","title":"Kodekloud: Ansible for Beginners","text":"<ul> <li>1.0 - Introduction</li> <li>2.0 - Configuration and Basic Concepts</li> <li>3.0 - Ansible Inventory</li> <li>4.0 - Ansible Variables</li> <li>5.0 - Ansible Playbooks</li> <li>6.0 - Ansible Modules</li> <li>7.0 - Ansible Handlers: Roles and Collections</li> <li>8.0 - Advanced Topics</li> <li>9.0 - Appendix</li> </ul>","tags":["Tooling","Ansible"]},{"location":"tooling/ansible/ansible-advanced/01_Introduction.html","title":"1.0 - Introduction","text":""},{"location":"tooling/ansible/ansible-advanced/01_Introduction.html#12-recap","title":"1.2 - Recap","text":"<ul> <li>In the previous course:</li> </ul> <p>Ansible for Beginners</p> <ul> <li>Ansible was introduced, showing how it can be used for a variety of operations e.g.@</li> <li>Infrastructure Provisioning</li> <li>Configuration Management</li> <li> <p>Application Deployment</p> </li> <li> <p>Various use case examples were shown and discussed for public and private cloud infrastructure.</p> </li> <li>Ansible installation was discussed:</li> </ul> <pre><code># Fedora\nyum install ansible\n\n# Ubuntu\napt-get install ansible\n\n# PIP\npip install ansible\n</code></pre> <ul> <li>Modules such as the following were discussed:</li> <li>System</li> <li>Commands</li> <li>Files</li> <li>Database</li> <li>Cloud</li> <li>Windows</li> <li>Variables were shown to be usable in playbooks, as well as conditionals and loops - all will be expanded upon in the course.</li> </ul>"},{"location":"tooling/ansible/ansible-advanced/01_Introduction.html#14-note-on-enabling-ssh-in-the-vms","title":"1.4 - Note on Enabling SSH in the VMs","text":"<ul> <li>Ensure <code>/etc/apt/sources.list</code> has been updated  with the following 2 lines:<ol> <li><code>deb http://ppa.launchpad.net/ansible/ansible/ubuntu trusty main</code></li> <li><code>deb http://ftp.de.debian.org/debian sid main</code></li> </ol> </li> <li>Comment the line starting <code>deb cdrom</code></li> <li>Uncomment the 2 lines starting <code>deb</code> and <code>deb-src</code></li> <li>Run the following:</li> </ul> <pre><code>apt-get update\napt-get install openssh-server\nservice sshd start\n</code></pre>"},{"location":"tooling/ansible/ansible-advanced/01_Introduction.html#16-environment-setup-virtualbox","title":"1.6 - Environment Setup: Virtualbox","text":"<ul> <li>Use <code>ifconfig -a</code> to get the IP address for the machine</li> <li>Generally can just clone or create 2 other VMs after the controlplane creation.</li> <li>Ansible Installation:</li> <li>Once installed on the required systems, create an inventory file for the target systems in the form of:</li> </ul> <pre><code>&lt;hostname&gt; ansible_host=&lt;IP address&gt; ansible_ssh_pass=&lt;password&gt;\n</code></pre> <ul> <li>Test inventory usage with the example ping module:</li> </ul> <pre><code>ansible &lt;target hostname&gt; -m ping -i inventory.txt\n</code></pre> <ul> <li>Note: One may need to modify the <code>/etc/ansible/ansible.cfg</code> file to disable host key checking, or just ssh to the systems from the controlplane; for practice, either are fine.</li> </ul>"},{"location":"tooling/ansible/ansible-advanced/01_Introduction.html#17-environment-setup-docker-containers","title":"1.7 - Environment Setup: Docker Containers","text":"<ul> <li>Ensure running a machine (VM or typical host) with Ansible and Docker installed.</li> <li>One can then deploy multiple containers with base ubuntu images. (Example Image)</li> <li>The containers will be auto-assigned an IP based on Docker's internal network (<code>172.17.0.&lt;x&gt;</code> range)</li> <li>Verify installation via <code>ansible</code> and <code>docker</code></li> <li>Create docker containers: <code>docker run -it -d  &lt;image&gt;</code>  (or use a Docker-compose file)</li> <li>Use <code>docker inspect &lt;container id&gt;</code> to get the container IPs</li> <li>Create an inventory file for each container and test connection with the ping module.</li> <li>Username:: root</li> <li>Password:: Passw0rd</li> </ul>"},{"location":"tooling/ansible/ansible-advanced/02_Web_Application.html","title":"2.0 - Web Application","text":""},{"location":"tooling/ansible/ansible-advanced/02_Web_Application.html#21-web-application","title":"2.1 - Web Application","text":"<ul> <li> <p>The web application to be used as a sample can be found at the following Github repo:</p> </li> <li> <p>It runs on the Python web framework Flask and the database system MySQL</p> </li> <li>The application setup can be broken down into steps:</li> <li>Server designation</li> <li>Install Python and associated dependencies</li> <li>Install, Configure and Start MySQL</li> <li>Install Flask</li> <li>Pull the source code</li> <li>Deploy / Start the web server.</li> </ul>"},{"location":"tooling/ansible/ansible-advanced/02_Web_Application.html#22-web-application-deployment-walkthrough","title":"2.2 - Web Application Deployment Walkthrough","text":"<ul> <li>It can be beneficial to test deployment out manually on one system before creating the playbook.</li> <li>To do so, simply run the required commands within the VM.</li> <li>Steps:</li> </ul> <pre><code># Initial Dependencies\napt-get install -y python python-setuptools python-dev build-essential python-pip python-mysqldb\n\n# Install MySQL and Configure\napt-get install -y mysql-server mysql-client\n\n# Start the DB\nservice mysql start\n\n# Create database user and sample table\n# mysql -u &lt;username&gt; -p\n\nmysql&gt; CREATE DATABASE employee_db;\nmysql&gt; GRANT ALL ON *.* to db_user@'%' IDENTIFIED BY 'Passw0rd';\nmysql&gt; USE employee_db;\nmysql&gt; CREATE TABLE employees (name VARCHAR(20));\n\n# Insert test data\nmysql&gt; INSERT INTO employees VALUES ('JOHN');\n\n# Install Flask and associated web server tools\npip install flask\npip install flask-mysql\n\n# Test deployment by running web server\nFLASK_APP=app.py flask run --host=0.0.0.0\n\nhttp://&lt;IP&gt;:5000                            =&gt; Welcome\nhttp://&lt;IP&gt;:5000/how%20are%20you            =&gt; I am good, how about you?\nhttp://&lt;IP&gt;:5000/read%20from%20database     =&gt; JOHN\n</code></pre>"},{"location":"tooling/ansible/ansible-advanced/02_Web_Application.html#23-webapp-installation-notes-centos","title":"2.3 - WebApp Installation Notes: CentOS","text":"<pre><code># Python and Pip Dependencies\nsudo yum install -y epel-release python python-pip\n\nsudo pip install flask flask-mysql\n\n# If the above 2 throw errors:\nsudo pip install --trusted-host files.pythonhosted.org --trusted-host pypi.org --trusted-host pypi.python.org flask flask-mysql\n\n# MySQL Server\nwget http://repo.mysql.com/mysql-community-release-el7-5.noarch.rpm\n\nsudo rpm -ivh mysql-community-release-el7-5.noarch.rpm\n\nsudo yum update\n\nsudo yum -y install mysql-server\n\nsudo service mysql start\n</code></pre> <ul> <li>Reference playbook: https://github.com/kodekloudhub/simple_web_application</li> </ul>"},{"location":"tooling/ansible/ansible-advanced/03_File_Separation.html","title":"3.0 - File Separation","text":""},{"location":"tooling/ansible/ansible-advanced/03_File_Separation.html#31-intro-to-file-separation","title":"3.1 - Intro to File Separation","text":""},{"location":"tooling/ansible/ansible-advanced/03_File_Separation.html#311-host_vars-and-group_vars","title":"3.1.1 - Host_Vars and Group_Vars","text":"<ul> <li>Up until now, variables such as <code>ansible_ssh_pass</code> and <code>ansible_host</code> are all included in the same inventory file.</li> <li>This is not good practice for security purposes.</li> <li>What should be done is storing the variables under a separate YAML file under a new directory called <code>host_vars</code>.</li> <li>The YAML file name should match up with the host defined in the inventory file.</li> <li>An example follows:</li> </ul> <pre><code># Inventory\n\ndb_and_web_server\n</code></pre> <pre><code># Sample variable file: host_vars/db_and_web_server.yaml\n\nansible_ssh_pass: &lt;Password&gt;\nansible_host: &lt;host IP Address&gt;\n</code></pre> <ul> <li>When the Ansible playbook is triggered, it, by default, will look in this directory for the required YAML file.</li> <li>Similarly, <code>group_vars</code> should be used for group variables</li> </ul>"},{"location":"tooling/ansible/ansible-advanced/03_File_Separation.html#312-include","title":"3.1.2 - Include","text":"<ul> <li>If you ever have a set of tasks that can be re-used, it's often beneficial to split these sets of actions into tasks.</li> <li>In the case of the simple-webapp example, you could have:</li> <li>Tasks to install the Database</li> <li>Tasks to set up the web server</li> <li>Each of these should be stored under a <code>tasks</code> directory and named accordingly e.g. <code>deploy_db.yaml</code></li> <li>These can then be referenced from the primary playbook in a similar manner to the following:</li> </ul> <pre><code>- name: &lt;Playbook Name&gt;\n  hosts: &lt;hosts&gt;\n  tasks:\n   - include: tasks/db_deploy.yaml\n   - include: tasks/webapp_deploy.yaml\n</code></pre>"},{"location":"tooling/ansible/ansible-advanced/04_Roles.html","title":"4.0 - Roles","text":""},{"location":"tooling/ansible/ansible-advanced/04_Roles.html#41-roles-introduction","title":"4.1 - Roles Introduction","text":"<ul> <li>Recommended way of developing playbooks</li> <li>Allows for organization of project in a standard structure and for reusable tasks</li> <li>Role creation automatically creates a folder structure for it, containing:</li> <li>README</li> <li>tests folder</li> <li>tasks folder</li> <li>handlers folder</li> <li>vars folder</li> <li>defaults folder</li> <li>To use a role, implement it at the top level of the playbook in a similar manner to:</li> </ul> <pre><code>- name: &lt;Playbook&gt;\n  hosts: web\n  roles:\n  - rolename 1\n  - rolename 2\n</code></pre> <ul> <li>Doing so will include everything defined under the role folder, removing the need for \"includes\" statements.</li> <li>This keeps the playbook simple, and allows the role to be reused elsewhere.</li> <li>Roles can then be shared to ansible-galaxy for use.</li> <li>Create a role with <code>ansible-galaxy init &lt;rolename&gt;</code>  or create the folder structure manually</li> <li>With roles, the originally monolithic application can be distributed, having the application running on 1, and the database on another.</li> </ul>"},{"location":"tooling/ansible/ansible-advanced/04_Roles.html#42-publishing-roles-to-ansible-galaxy","title":"4.2 - Publishing Roles to Ansible-Galaxy","text":"<ul> <li>Login to https://galaxy.ansible.com</li> <li>Navigate to the roles directory</li> <li>For each role:</li> <li>cd into the role</li> <li>Ensure the README and metadata/main.yaml files are updated as required. For the latter:<ul> <li>Uncomment platforms section</li> <li>Add any galaxy_tags desired</li> </ul> </li> <li>Add the role to a designated Github repository</li> <li>Via ansible galaxy, import the role from github.</li> </ul>"},{"location":"tooling/ansible/ansible-advanced/05_Asynchronous_Actions.html","title":"5.0 - Asynchronous Actions","text":""},{"location":"tooling/ansible/ansible-advanced/05_Asynchronous_Actions.html#51-asynchronous-actions-introduction","title":"5.1 - Asynchronous Actions Introduction","text":"<ul> <li>SSH connections stay alive for the duration of the playbook</li> <li>This is not good for when there are tasks we want to run post-playbook to check the result, this is typically admin tasks such as \"check that the connection to the database\"</li> <li>This is achieved by asynchronous playbooks.</li> <li>Suppose there is a script to run health checks against the app, this script may take up to 5 minutes, so we don't want to it have ansible's connection open for the duration.</li> <li>Introduce the <code>async</code> parameter to the designated task e.g.</li> </ul> <pre><code>tasks:\n- command: &lt;path to script&gt;\n  async: 360 # maximum time expected for task to occur for (seconds)\n  poll: 60 # in seconds, how often should ansible check on the result (10s = default)\n</code></pre> <ul> <li>Use the <code>poll</code> value to specify in seconds how often ansible should check on the result of the task.</li> <li>This will kick off the task, wait for 6 minutes to allow the task to finish, and move on.</li> <li>For multiple async jobs, this could be time-consuming, especially if tasks can be ran in parallel; this can be done by setting <code>poll</code> to 0.</li> <li>It's best practice to register the result of the async action via <code>register: &lt;result name&gt;</code></li> <li>One can then add an additional task at the end to check the status of the asynchronous tasks before concluding the play:</li> </ul> <pre><code>- name: Check status of tasks\n  async_status: jid={{ async_task_result.ansible_job_id }}\n  register: job_result\n  until: job_result.finished\n  retries: 30\n</code></pre> <ul> <li>Note: not all modules support async - verify this with the ansible documentation before proceeding!</li> </ul>"},{"location":"tooling/ansible/ansible-advanced/06_Strategy.html","title":"6.0 - Strategy","text":""},{"location":"tooling/ansible/ansible-advanced/06_Strategy.html#61-introduction","title":"6.1 - Introduction","text":"<ul> <li>Strategy defines how a playbook is executed within Ansible</li> <li>When ansible runs, it runs one task after another by default - Linear strategy</li> <li>On a single-node application, this is pretty straightforward, but suppose you have a database and web server, and you cannot run the web server until the database server is configured, but other tasks could be ran in parallel, e.g. common dependency installation.</li> <li>Free strategy will cause each server to run its own tasks on its own, no server will wait for another.</li> <li>Batch processing, whilst not an out-and-out strategy (uses the linear strategy), can be used to run x sets of tasks simultaneously e.g. for a group of 10 servers, run on 2 servers at any given point. Use <code>serial</code> to define this.</li> <li><code>serial</code> will accept arrays, this is commonly used for rolling updates e.g.:     <code>- 2</code> <code>- 3</code> <code>-5</code></li> <li>For percentages e.g. run on 20% of a group at any given point: <code>serial: \"20%\"</code></li> <li>Other strategies can be implemented by developing custom plugins.</li> </ul>"},{"location":"tooling/ansible/ansible-advanced/06_Strategy.html#forks","title":"Forks","text":"<ul> <li>Ansible utilises forks to do parallel processing on hosts</li> <li>By default, the maximum is 5 for forks.</li> <li>This value can be edited in ansible.cfg, but one must ensure sufficient compute resources are available to allow this level of processing.</li> </ul>"},{"location":"tooling/ansible/ansible-advanced/07_Error_Handling.html","title":"7.0 - Error Handling","text":""},{"location":"tooling/ansible/ansible-advanced/07_Error_Handling.html#71-introduction","title":"7.1 - Introduction","text":"<ul> <li>As discussed in strategy, Ansible will execute each task sequentially until completion</li> <li>In the event of failure, the playbook will fail</li> <li>For multiple servers, if one task fails but runs fine on others, you do not want the entire playbook to stop, you should allow the others to continue for as long as possible.</li> <li>This is default behaviour.</li> <li>If wanting to change, add <code>any_errors_fatal: true</code> at the beginning of the playbook.</li> </ul>"},{"location":"tooling/ansible/ansible-advanced/07_Error_Handling.html#ignore-errors-failed_when","title":"Ignore Errors, Failed_When","text":"<ul> <li>If there's any tasks you're not concerned about throwing errors, add to a task: <code>ignore_errors: yes</code></li> <li>If there's any tasks with specific fail conditions, add similar to: <code>failed_when: \"&lt;Error Condition&gt;\"</code>  - this requires a <code>register:</code> usage</li> <li>Example:</li> </ul> <pre><code>- command: cat /var/log/server.log\n  register: command_output\n  failed_when: \"'ERROR' in command_output.stdout\"\n</code></pre>"},{"location":"tooling/ansible/ansible-advanced/08_Templating_Jinja2.html","title":"8.0 - Templating - Jinja2","text":""},{"location":"tooling/ansible/ansible-advanced/08_Templating_Jinja2.html#81-introduction","title":"8.1 - Introduction","text":"<ul> <li>Templating = Using variable substitution e.g. <code>{{ name }}</code> for variable <code>name</code></li> <li>Jinja2 Templating defined by either <code>{{ }}</code> or <code>{% %}</code></li> <li>Jinja = Templating language designed for Python.</li> </ul>"},{"location":"tooling/ansible/ansible-advanced/08_Templating_Jinja2.html#string-manipulation-filters","title":"String Manipulation - Filters","text":"<ul> <li>Make string substitute uppercase: <code>{{ var name | upper }}</code></li> <li>Lowercase: <code>{{ var name | upper }}</code></li> <li>Title Case: <code>{{ var name | title }}</code></li> <li>Replace with a  given value <code>{{ var name | replace(\"&lt;Original Value&gt;\", \"&lt;Replacing Value&gt;\" }}</code></li> <li>Use defaults:  <code>{{ var name | default(\"&lt;Default Value&gt;\") }}</code></li> </ul>"},{"location":"tooling/ansible/ansible-advanced/08_Templating_Jinja2.html#filters-list-and-set","title":"Filters - List and Set","text":"<ul> <li>When working with lists and sets:</li> <li>Get minimum: <code>{{ [1,2,3] | min }}</code></li> <li>Get maximum: <code>{{ [1,2,3] | max }}</code></li> <li>Get unique values: <code>{{ [1,2,3] | unique }}</code></li> <li>Get unique values across multiple arrays: <code>{{ [1,2,3,4] | union( [4,5] ) }}</code></li> <li>Find common values across multiple arrays <code>{{ [1,2,3,4] | intersect( [4,5] ) }}</code></li> <li>Get a random number: <code>{{ 100 | random }}</code></li> <li>Join an array of strings: <code>{{ \"word1\", \"word2\", \"word3\", \"word4\" | join(\" \") }}</code></li> </ul>"},{"location":"tooling/ansible/ansible-advanced/08_Templating_Jinja2.html#filters-file","title":"Filters - File","text":"<ul> <li>Get file basename:</li> <li>Windows hosts: <code>{{ \"/path/to/file\" | win_basename }}</code></li> <li>Linux Hosts <code>{{ \"/path/to/file\" | basename }}</code></li> <li>Get the drive letter from a path <code>{{ \"/path/to/file\" | win_splitdrive }}</code></li> <li>Returns an array with the first entry being the drive letter, to isolate it, add an extra pipe <code>| first</code></li> <li> <p>For the last filter: <code>| last</code></p> </li> <li> <p>Many more filters are available at jinja.pcoo.org/docs</p> </li> <li>Additional guidance also available in the Ansible documentation.</li> </ul>"},{"location":"tooling/ansible/ansible-advanced/09_Lookup.html","title":"9.0 - Lookups","text":""},{"location":"tooling/ansible/ansible-advanced/09_Lookup.html#91-introduction","title":"9.1 - Introduction","text":"<ul> <li>Suppose instead of the inventory file, the credentials for servers were stored in another file</li> <li>To obtain the credentials, use <code>lookup</code></li> <li>Example: <code>{{ lookup('csvfile', 'target1 file=/path/to/file delimiter=,') }}</code></li> <li>In general: <code>{{ lookup('&lt;file type identifier&gt;', '&lt;target value&gt; file=/path/to/file delimiter=&lt;delimiter&gt;') }}</code></li> <li>Other lookup plugins are available e.g. MongoDB and INI - further information is available in the docs.</li> </ul>"},{"location":"tooling/ansible/ansible-advanced/10_Vault.html","title":"10.0 - Vault","text":""},{"location":"tooling/ansible/ansible-advanced/10_Vault.html#101-introduction","title":"10.1 - Introduction","text":"<ul> <li>In bad practice, credentials are stored in plaintext e.g. in inventory files.</li> <li>Ansible Vault can be leveraged to store credentials in an encrypted manner.</li> <li>To encrypt a file: <code>ansible-vault encrypt &lt;filename&gt;</code></li> <li>If using an encrypted file in a playbook, append <code>-ask-vault-pass</code>  to the <code>ansible-playbook</code> command</li> <li>Or store the vault password in a file and add <code>-vault-password-file &lt;path to file&gt;</code></li> <li>An alternative could be use a python file as a <code>&lt;path to file&gt;</code> substitute that looks for the password on the system and passes it in an encrypted form</li> <li>To view an entry within the vault, use <code>ansible-vault view &lt;filename&gt;</code></li> </ul>"},{"location":"tooling/ansible/ansible-advanced/11_Dynamic_Inventory.html","title":"11.0 - Dynamic Inventory","text":""},{"location":"tooling/ansible/ansible-advanced/11_Dynamic_Inventory.html#111-introduction","title":"11.1 - Introduction","text":"<ul> <li>Commonly inventory files are stored in plaintext form.</li> <li>This generally is bad practice, but becomes tedious and unmanageable for larger inventories.</li> <li>One may wish to leverage Dynamic Inventories</li> <li>Inventories stored in the cloud or external databases</li> <li>Ansible will retrieve this information programmatically during the play</li> </ul>"},{"location":"tooling/ansible/ansible-advanced/11_Dynamic_Inventory.html#example","title":"Example","text":"<ul> <li>Convert a static inventory file into a python script</li> </ul> <pre><code>#!/usr/bin/env python &lt;- Required as Ansible will try to exec this as a Bash script if not included\n\nimport json\n\ndef get_inventory_data():\n  return {\n    \"databases\": {\n       \"hosts\": [\"db_server\"],\n       \"vars\": {\n          \"ansible_ssh_pass\": \"value\",\n          \"ansible_ssh_host\": \"IP\"\n       }\n     },\n    \"web\": {\n       \"hosts\": [\"wev_server\"],\n       \"vars\": {\n          \"ansible_ssh_pass\": \"value\",\n          \"ansible_ssh_host\": \"IP\"\n       }\n     }\n  }\n\nif __name__ == \"__main__\":\n  inventory_data = get_inventory_data()\n  print(json.dumps(inventory_data))\n</code></pre> <ul> <li>One would then swap refer to this by <code>-i [inventory.py](http://inventory.py)</code> in the <code>ansible-playbook</code> command</li> <li>When running the script, one should be able to pass <code>--list</code> or <code>--host &lt;hostname&gt;</code> to list the entire inventory details or get the details of a particular host.</li> <li>This could be supported by using a function:</li> </ul> <pre><code>def read_cli_args():\n  global args\n  parser = argpars.ArgumentParser()\n  parser.add_argument('--list', action='store_true')\n  parser.add_argument('--host', action='store')\n  args= parser.parse_args()\n</code></pre> <ul> <li>The final conditional could then be edited for example:</li> </ul> <pre><code>if __name__ == \"__main__\":\n  global args\n  read_cli_args()\n  inventory_data = get_inventory_data()\n  if args and args.list:\n    print(json.dumps(inventory_data))\n  elif args.host:\n    print(json.dumps({'_meta': {'hostvars': {}}})\n</code></pre> <ul> <li>Many dynamic scripts are available via the Ansible Github predeveloped for providers such as:</li> <li>AWS</li> <li>Azure</li> <li>VMware</li> <li>Docker</li> </ul>"},{"location":"tooling/ansible/ansible-advanced/12_Custom_Modules/01_Custom_Modules.html","title":"12.0 - Custom Modules","text":""},{"location":"tooling/ansible/ansible-advanced/12_Custom_Modules/01_Custom_Modules.html#121-introdcution","title":"12.1 - Introdcution","text":"<ul> <li>Modules are each item in a task that carries out a particular command, this can be for anything such as:</li> <li>System management</li> <li>Running commands</li> <li>Managing files</li> <li>Each module is a set of python script(s) to perform a particular action - this leads to the potential problem of \"what if there is no ready-made solution?\"</li> </ul>"},{"location":"tooling/ansible/ansible-advanced/12_Custom_Modules/01_Custom_Modules.html#122-example-custom-debug","title":"12.2 - Example: Custom Debug","text":"<ul> <li>For any custom plugins, one must develop a custom python script to support it. This can then be stored in the project and referred to via an environment variable, or stored alongside other ansible modules downloaded.</li> <li>Example:</li> </ul> <ul> <li>AnsibleModule helps parse arguments</li> <li>Initiated by providing an argument_spec, a spec outlining how arguments should be provided to the module in a playbook, specifying parameters such as:<ul> <li>Type</li> <li>Required (Boolean)</li> </ul> </li> <li>module.params['argname'] then passes the arguments into the code for usage</li> <li>Any other actions with the code can then follow.</li> </ul>"},{"location":"tooling/ansible/ansible-advanced/12_Custom_Modules/01_Custom_Modules.html#123-best-practices","title":"12.3 - Best Practices","text":"<ul> <li>Modules should always come with documentation for live viewing and in the ansible docs</li> <li><code>ansible-doc &lt;module name&gt;</code></li> <li>For the CLI, this is defined in the same .py file for the module as YAML e.g.</li> </ul> <ul> <li>Example Use Cases can also be added - again added under the main python code</li> </ul> <ul> <li>All built-in modules are located by default at <code>/usr/lib/python/dist-packages/ansible/modules</code></li> <li>If to be used by a role, add the module under \"library\" folder in a project</li> <li>For cross-projects, place anywhere and call it via the env <code>ANSIBLE_LIBRARY</code></li> </ul>"},{"location":"tooling/ansible/ansible-advanced/12_Custom_Modules/01_Custom_Modules.html#guidance","title":"Guidance","text":"<ul> <li>Additional guidance for testing is available in the ansible docs.</li> </ul>"},{"location":"tooling/ansible/ansible-advanced/13_Plugins/01_Plugins.html","title":"13.0 - Plugins","text":""},{"location":"tooling/ansible/ansible-advanced/13_Plugins/01_Plugins.html#131-overview","title":"13.1 - Overview","text":"<ul> <li>Ansible utilises plugins for various functionalities e.g.:</li> <li>Action</li> <li>Connections</li> <li>Filter</li> <li>Lookup</li> <li>Strategy</li> <li>This makes Ansible highly extensible.</li> </ul>"},{"location":"tooling/ansible/ansible-advanced/13_Plugins/01_Plugins.html#132-custom-filters","title":"13.2 - Custom Filters","text":"<ul> <li>Example: Develop an average filter to obtain the average of a numerical list</li> </ul> <ul> <li>To ensure usage, export the location of the filter plugins prior to playbook execution i.e. <code>export ANSIBLE_FILTER_PLUGINS=/path/to/plugins/folder</code></li> </ul>"},{"location":"tooling/ansible/ansible-advanced/13_Plugins/01_Plugins.html#133-callback-plugins","title":"13.3 - Callback Plugins","text":"<ul> <li>To change the callback standard ouput format, e.g. to json, set <code>export ANSIBLE_STDOUT_CALLBACK=json</code></li> <li>Callback plugins are used to, in addition to printing information:</li> <li>Mail - error or notification emails</li> <li>Integration with applications such as Logstash and Slack</li> <li>Timer - Display the playbook execution time</li> <li>An extensive list is included in the plugins folder in th</li> <li>Example custom callback follows.</li> </ul>"},{"location":"tooling/ansible/ansible-advanced/14_Assignment/01_Assignment.html","title":"14.0 - Assignment","text":""},{"location":"tooling/ansible/ansible-advanced/14_Assignment/01_Assignment.html#141-overview","title":"14.1 - Overview","text":"<ul> <li>Deploy the simple-webapp to the cloud</li> <li>Deploy all required servers e.g. 1 db and 1 web servers</li> <li>Deploy the simple-webapp as required using the predone playbook</li> <li>Run each service as a service</li> <li>Configure load balancing for multiple servers</li> <li> <p>Send out a notification email upon playbook completion, with login details for the server access</p> </li> <li> <p>Servers can be on any suitable cloud provider or vmware</p> </li> <li>Adhere to distributed deployment model - we don't do monolithic</li> <li>Use any suitable load balancer e.g. AWS Elastic Load Balancing, Nginx, HAProxy</li> </ul> <p></p>"},{"location":"tooling/ansible/ansible-advanced/14_Assignment/01_Assignment.html#aws","title":"AWS","text":"<ul> <li>Deploy EC2 Instances</li> <li>Configure Networking for EC2 Instances</li> <li>Deploy Application and Database</li> </ul>"},{"location":"tooling/ansible/ansible-advanced/14_Assignment/01_Assignment.html#areas-to-check","title":"Areas to Check","text":"<ul> <li>Mysql CNF Updates e.g. bind_address at <code>/etc/mysql/&lt;cnf file&gt;</code></li> <li>Make sure web_server host is set accordingly</li> </ul> <p>Example for GCP Ansible EC2 Docs</p>"},{"location":"tooling/ansible/ansible-for-beginners/01_Introduction.html","title":"1.0 - Introduction","text":""},{"location":"tooling/ansible/ansible-for-beginners/01_Introduction.html#notes","title":"Notes","text":"<ul> <li>Course content via lectures, hands-on exercises and practice projects to work on.</li> <li>Course Objectives:</li> <li>Introduction to Ansible</li> <li>Setting up Labs</li> <li>Introduction to YAML</li> <li>Inventory Files</li> <li>Playbooks</li> <li>Variables</li> <li>Conditionals</li> <li>Loops</li> <li>Roles</li> </ul>"},{"location":"tooling/ansible/ansible-for-beginners/01_Introduction.html#introduction-to-ansible","title":"Introduction to Ansible","text":"<p>Why Ansible?</p> <ul> <li>In system administration (or similar) roles, many tasks are often repetitive, such as:</li> <li>Provisioning</li> <li>Configuration Management</li> <li>Continuous Delivery</li> <li>Application Deployment</li> <li>Security and Compliance Audits</li> <li>These tasks usually require many commands that must be run in a correct sequence on multiple machines</li> <li>This was typically done by using scripts, however this would require coding skills, something which Ansible does not.</li> <li>As an example, suppose a script was developed to add a particular user, this would require many lines of code.</li> <li>In an Ansible Playbook, this just requires 4 lines tops, the Playbook can be configured to then run on ANY set of machines that the configuration is required.</li> <li>Example:</li> <li>Consider an environment that requires restarting in a particular order,  e.g. power down web servers first, then databases, then start them back up in the reverse order.</li> <li>This task can be handled by an Ansible Playbook within a matter of minutes</li> <li>Example 2:</li> <li>Ansible could facilitate the provisioning of VMs across public and private cloud environments, even deploying applications to these environments.</li> <li>Database information can be fed into Ansible to help trigger builds e.g. if a request comes in from ServiceNow.</li> </ul>"},{"location":"tooling/ansible/ansible-for-beginners/02_Configuration-and-Basic-Concepts.html","title":"2.0 - Configuration and Basic Concepts","text":""},{"location":"tooling/ansible/ansible-for-beginners/02_Configuration-and-Basic-Concepts.html#21-introduction-to-ansible-configuration-files","title":"2.1 - Introduction to Ansible Configuration Files","text":""},{"location":"tooling/ansible/ansible-for-beginners/02_Configuration-and-Basic-Concepts.html#introduction","title":"Introduction","text":"<pre><code>[defaults]\n\ninventory = /path/to/inventory/file # e.g. /etc/ansible/hosts\nlog_path = /path/to/log/file # e.g. /var/log/ansible.log\n\nlibrary = /path/to/modules/folder # e.g. /usr/share/my_modules/\nroles_path = /path/to/roles/folder # e.g. /etc/ansible/roles\nactions_plugins = /path/to/action/plugin # e.g. /user/share/ansible/plugins/action\n\ngathering = implicit # how ansible gathers facts\n\n## SSH Timeout\ntimeout = 10\nforks = 5 # how many hosts can ansible target at any given point\n\n[inventory]\n\nenable_plugins = plugin1, plugin2, plugin3, ...\n\n[privilege_escalation]\n\n[paramiko_connection]\n\n[ssh_connection]\n\n[persistent_connection]\n\n[colors]\n\n</code></pre> <ul> <li>Ansible will, unless specified otherwise, refer to this configuration when any playbook is ran on the control machine.</li> <li>This isn't mandatory, one can define specific configuration files for specific playbooks based on use cases, make a copy of the config file in the playbooks directory(ies), and make the changes accordingly.</li> <li>Upon execution, any config file in the playbook's directory will take precedence over the default.</li> <li> <p>If the config file is to be stored away from the playbook directory, add the environment variable <code>$ANSIBLE_CONFIG=/path/to/file</code> prior to any <code>ansible-playbook</code> command.</p> </li> <li> <p>In terms of precedence, config files specified by <code>$ANSIBLE_CONFIG</code> will take top priority, then any cfg in the playbook's directory, then <code>.ansible.cfg</code> in the user home directory, and finally <code>/etc/ansible/ansible.cfg</code>.</p> </li> <li> <p>Note: The cfg files outside of the default location do not have to have every single parameter defined, only the ones which you wish to override.</p> </li> <li> <p>In the event of only one or two variables needing to be overwritten, one could add <code>ANSIBLE_&lt;CONFIG PARAMETER NAME&gt;=value</code> prior to <code>ansible-playbook</code> execution.</p> </li> <li>This would take top priority, but does not persist command to command.</li> <li>One could <code>export</code> the variable to persist in the shell.</li> <li> <p>To persist outright across shells, it's advised to create a new <code>ansible.cfg</code> file in the desired location.</p> </li> <li> <p>To understand the configurations available and default values, use <code>ansible-config list</code>.</p> </li> <li>To view the current config file: <code>ansible-config view</code></li> <li>To view the current settings and where the settings are set from e.g. environment variables: <code>ansible-config dump</code></li> </ul>"},{"location":"tooling/ansible/ansible-for-beginners/02_Configuration-and-Basic-Concepts.html#041-introduction-to-yaml","title":"04.1 - Introduction to YAML","text":""},{"location":"tooling/ansible/ansible-for-beginners/02_Configuration-and-Basic-Concepts.html#introduction_1","title":"Introduction","text":"<ul> <li>All Ansible Playbooks are written in YAML</li> <li>Text or Configuration files</li> <li>Analagous to the likes of JSON and XML, YAML is just another way of representing data</li> </ul> <ul> <li>Generally, data is presented in key-value pairs i.e.</li> <li>Key-Value Pair:</li> </ul> <pre><code>key1: value1\n</code></pre> <ul> <li>Arrays:</li> <li><code>-</code> indicates an element of an array</li> </ul> <pre><code>array1:\n- key1: value1\n- key2: value2\n</code></pre> <ul> <li>Dictionaries:</li> <li>All entries in a particular dictionary offset by a set amount of spaces.</li> </ul> <pre><code>dictionary:\n  key1: value1\n  key2: value2\n\ndictionary2:\n  key1: value1\n</code></pre> <ul> <li>Spacing determines what data is a property of which, any values that are child properties of a particular parent must have the same amount of spaces before definition.</li> <li>Note:</li> <li>You may have a need to store different sets of information for a particular \"thing\".</li> <li>Dictionary within dictionaries are used for using multiple values of different types</li> <li>Arrays used for different values of the same types</li> <li> <p>Use a list of dictionaries for storing the same set of information for multiple entries of similar nature. In the example below, each element in the array is in fact a dictionary.</p> <p><code>yaml Fruits: - Banana:     calories: value     fat: value     carbs: value - Grape:     calories: value     fat: value     carbs: value</code></p> </li> <li> <p>When to use dictionaries v lists:</p> </li> <li>Dictionary = Unordered data</li> <li> <p>List = Ordered</p> </li> </ul>"},{"location":"tooling/ansible/ansible-for-beginners/02_Configuration-and-Basic-Concepts.html#exercises","title":"Exercises","text":"<ol> <li>Given a dictionary with the property <code>property1</code> and value <code>value1</code></li> </ol> <p>Add an additional property <code>property2</code> and value <code>value2</code>.</p> <p><code>yaml   property1: value1   property2: value2</code></p> <p>1.</p> <p>Given a dictionary with the property <code>name</code> and value <code>apple</code>. Add additional properties to the dictionary.</p> Key/Property Value name apple color red weight 90g <p><code>yaml   name: apple   color: red   weight: 90g</code></p> <p>1.</p> <p>A dictionary <code>employee</code> is given. Add the remaining properties to it using information from the table below.</p> Key/Property Value name john gender male age 24 <p><code>yaml   employee:       name: john       gender: male       age: 24</code></p> <ol> <li>Now try adding the address information. Note the address is a dictionary</li> </ol> Key/Property Value name john gender male age 24 address Key/PropertyValuecityedisonstatenew jerseycountryunited states <p><code>yaml   employee:       name: john       gender: male       age: 24       address:         city: edison         state: new jersey         country: united states</code></p> <ol> <li>Given an array of apples. Add a new apple to the list to make it a total of 4.</li> <li>add two more</li> <li>add two mangoes to the list</li> </ol> <p><code>yaml   - apple   - apple   - apple   - apple   - apple   - apple   - mango   - mango</code></p> <p>1.</p> <p>We would like to add additional details for each item, such as color, weight etc. We have updated the first one for you. Similarly modify the remaining items to match the below data.</p> Fruit Color Weight apple red 100g apple red 90g mango yellow 150g <p><code>yaml   -       name: apple       color: red       weight: 100g   - name: apple     color: red     weight: 90g   - name: mango     color: yellow     weight: 150g</code></p> <ol> <li>We would like to record information about multiple employees. Convert the dictionary <code>employee</code> to an array <code>employees</code></li> </ol> <p><code>yaml   employees:   -   name: john       gender: male       age: 24</code></p> <ol> <li>Add an additional employee to the list using the below information.</li> </ol> Key/Property Value name sarah gender female age 28 <p><code>yaml   employees:       -           name: john           gender: male           age: 24       - name: sarah         gender: female         age: 28</code></p> <p>1.</p> <p>Now try adding the pay information. Remember while <code>address</code> is a dictionary, <code>payslips</code> is an array of <code>month</code> and <code>amount</code></p> Key/Property Value name john gender male age 24 address ... payslips #monthamount1june14002july24003august3400 <p><code>yaml   employee:       name: john       gender: male       age: 24       address:           city: edison           state: 'new jersey'           country: 'united states'       payslips:       - month: june         amount: 1400       - month: july         amount: 2400       - month: august         amount: 3400</code></p>"},{"location":"tooling/ansible/ansible-for-beginners/03_Ansible-Inventory.html","title":"3.0 - Ansible Inventory","text":""},{"location":"tooling/ansible/ansible-for-beginners/03_Ansible-Inventory.html#031-ansible-inventory","title":"03.1 - Ansible Inventory","text":""},{"location":"tooling/ansible/ansible-for-beginners/03_Ansible-Inventory.html#introduction","title":"Introduction","text":"<ul> <li>Ansible can work with one or multiple systems in the system at the same time</li> <li>This requires it to establish connection via SSH (Linux) or Powershell Remoting (Windows)</li> <li>This makes Ansible agentless - no additional packages are required for connection to be established to the target machines, unlike similar tools.</li> <li>Information regarding the target machines is noted in an Inventory file, stored by default in /etc/ansible/hosts , an example follows:</li> <li> <p>Servers can be listed one after another as standalone entries or as part of a group, denoted by [groupname]</p> <p></p> </li> <li> <p>Aliases for hosts, for ease of reference, can be added in a manner similar to:</p> </li> </ul> <pre><code>&lt;alias&gt; ansible_host=server.company.com\n</code></pre> <ul> <li>Other parameters are available for use in inventory files, such as:</li> <li>ansible_connection - ssh/winrm/localhost<ul> <li>Determines how ansible connects to the target server - Linux or Windows-based system, or applying changes to localhost</li> </ul> </li> <li>ansible_port - 22/5896<ul> <li>22 for SSH by default</li> </ul> </li> <li>ansible_user - root/administrator<ul> <li>User logging in to make changes, set to root by default</li> </ul> </li> <li>ansible_ssh_pass - Password<ul> <li>ssh password for linux - NOT RECOMMENDED to do this in plaintext, ssh key-based passwordless authentication should be used, especially in production environments.</li> </ul> </li> </ul>"},{"location":"tooling/ansible/ansible-for-beginners/03_Ansible-Inventory.html#032-demo-ansible-inventory","title":"03.2 - Demo: Ansible Inventory","text":""},{"location":"tooling/ansible/ansible-for-beginners/03_Ansible-Inventory.html#introduction_1","title":"Introduction","text":"<ul> <li>Verify connection between ansible-controller and targets 1-2 via ssh</li> <li>Create an inventory.txt file and add details regarding  ansible-target1 in a format similar to:</li> </ul> <pre><code>&lt;hostname&gt; ansible_host=&lt;IP&gt; ansible_ssh_pass=&lt;password&gt;\n</code></pre> <p>Once complete, run the following command:</p> <pre><code>ansible target1 -m ping -i inventory.txt\n</code></pre> <ul> <li>This calls ansible to use the ping module to test the connection to the host target1 based on the inventory file inventory.txt's information.</li> <li>A successful message will be similar to:</li> </ul> <pre><code>ansible-target1 | SUCCESS =&gt; {\n    \"ansible_facts\": {\n        \"discovered_interpreter_python\": \"/usr/bin/python\"\n    },\n    \"changed\": false,\n    \"ping\": \"pong\"\n}\n</code></pre> <ul> <li>Repeat for the second target machine.</li> <li>Note, if SSH has not been successful for any of the target machines, this will fail.</li> <li> <p>Alternatively, changes can be made to the /etc/ansible/ansible.cfg by uncommenting the line that asks \"host key check</p> </li> </ul>"},{"location":"tooling/ansible/ansible-for-beginners/03_Ansible-Inventory.html#exercises","title":"Exercises","text":"<ol> <li>We have a sample inventory file with 3 servers listed. Add a fourth server by the name <code>server4.company.com</code>.</li> </ol> <pre><code>## Sample Inventory File\n\nserver1.company.com\nserver2.company.com\nserver3.company.com\nserver4.company.com\n</code></pre> <ol> <li>We have added aliases named <code>web1</code> , <code>web2</code>  and <code>web3</code>  for the first three servers. Update server4 to have an alias <code>db1</code></li> </ol> <pre><code>## Sample Inventory File\n\nweb1 ansible_host=server1.company.com\nweb2 ansible_host=server2.company.com\nweb3 ansible_host=server3.company.com\ndb1 ansible_host=server4.company.com\n</code></pre> <ol> <li>The web servers are linux, but the db server is windows. Add additional parameters in each line to add <code>ansible_connection</code>, <code>ansible_user</code> and <code>password</code>. Use the below table for information about credentials.</li> </ol> Alias Host Connection User Password web1 server1.company.com SSH root Password123! web2 server2.company.com SSH root Password123! web3 server3.company.com SSH root Password123! db1 server4.company.com Windows administrator Password123! <p>Note: For linux use <code>ansible_ssh_pass</code> and for windows use <code>ansible_password</code>. Connector for windows is <code>winrm</code></p> <pre><code>web1 ansible_host=server1.company.com ansible_connection=ssh ansible_user=root ansible_ssh_pass=Password123!\nweb2 ansible_host=server2.company.com ansible_connection=ssh ansible_user=root ansible_ssh_pass=Password123!\nweb3 ansible_host=server3.company.com ansible_connection=ssh ansible_user=root ansible_ssh_pass=Password123!\ndb1 ansible_host=server4.company.com ansible_connection=winrm ansible_user=administrator ansible_password=Password123!\n</code></pre> <ol> <li>We have created a group for web servers. Similarly create a group for database servers named <code>db_servers</code> and add <code>db1</code> server to it.</li> </ol> <pre><code>## Web Servers\nweb1 ansible_host=server1.company.com ansible_connection=ssh ansible_user=root ansible_ssh_pass=Password123!\nweb2 ansible_host=server2.company.com ansible_connection=ssh ansible_user=root ansible_ssh_pass=Password123!\nweb3 ansible_host=server3.company.com ansible_connection=ssh ansible_user=root ansible_ssh_pass=Password123!\n\n## Database Servers\ndb1 ansible_host=server4.company.com ansible_connection=winrm ansible_user=administrator ansible_password=Password123!\n\n[web_servers]\nweb1\nweb2\nweb3\n\n[db_servers]\ndb1\n</code></pre> <p>1.</p> <p>Let us now create a group of groups. Create a new group called <code>all_servers</code> and add the previously created groups <code>web_servers</code> and <code>db_servers</code> to it.</p> <p>Note: Syntax:</p> <p>[parent_group:children]</p> <p>child_group1</p> <p>child_group2</p> <pre><code>## Web Servers\nweb1 ansible_host=server1.company.com ansible_connection=ssh ansible_user=root ansible_ssh_pass=Password123!\nweb2 ansible_host=server2.company.com ansible_connection=ssh ansible_user=root ansible_ssh_pass=Password123!\nweb3 ansible_host=server3.company.com ansible_connection=ssh ansible_user=root ansible_ssh_pass=Password123!\n\n## Database Servers\ndb1 ansible_host=server4.company.com ansible_connection=winrm ansible_user=administrator ansible_password=Password123!\n\n[web_servers]\nweb1\nweb2\nweb3\n\n[db_servers]\ndb1\n\n[all_servers:children]\nweb_servers\ndb_servers\n</code></pre> <ol> <li>Try and represent the data given in the below table in Ansible Inventory format</li> </ol> Server Alias Server Name OS User Password sql_db1 sql01.xyz.com Linux root Lin$Pass sql_db2 sql02.xyz.com Linux root Lin$Pass web_node1 web01.xyz.com Win administrator Win$Pass web_node2 web02.xyz.com Win administrator Win$Pass web_node3 web03.xyz.com Win administrator Win$Pass <p>Group the servers together based on this table</p> Group Members db_nodes sql_db1, sql_db2 web_nodes web_node1, web_node2, web_node3 boston_nodes sql_db1, web_node1 dallas_nodes sql_db2, web_node2, web_node3 us_nodes boston_nodes, dallas_nodes <pre><code>sql_db1 ansible_host=sql01.xyz.com ansible_connection=ssh ansible_user=root ansible_ssh_pass=Lin$Pass\nsql_db2 ansible_host=sql02.xyz.com ansible_connection=ssh ansible_user=root ansible_ssh_pass=Lin$Pass\nweb_node1 ansible_host=web01.xyz.com ansible_connection=winrm ansible_user=administrator ansible_password=Win$Pass\nweb_node2 ansible_host=web02.xyz.com ansible_connection=winrm ansible_user=administrator ansible_password=Win$Pass\nweb_node3 ansible_host=web03.xyz.com ansible_connection=winrm ansible_user=administrator ansible_password=Win$Pass\n\n[db_nodes]\nsql_db1\nsql_db2\n\n[web_nodes]\nweb_node1\nweb_node2\nweb_node3\n\n[boston_nodes]\nsql_db1\nweb_node1\n\n[dallas_nodes]\nsql_db2\nweb_node2\nweb_node3\n\n[us_nodes:children]\nboston_nodes\ndallas_nodes\n</code></pre>"},{"location":"tooling/ansible/ansible-for-beginners/03_Ansible-Inventory.html#33-inventory-format","title":"3.3 - Inventory Format","text":""},{"location":"tooling/ansible/ansible-for-beginners/03_Ansible-Inventory.html#introduction_2","title":"Introduction","text":"<ul> <li>Ansible inventory formats offer differ from scenario to scenario.</li> <li>For small-scale projects, simpler formats are likely required as only a small number of servers are used.</li> <li> <p>For large-scale projects, there are likely resources spread worldwide carrying out a multitude of functions.</p> </li> <li> <p>Small projects therefore could get away with simple <code>.ini</code> formats, whilst a <code>yaml</code>-based inventory would be suitable for large-scale projects.</p> </li> </ul>"},{"location":"tooling/ansible/ansible-for-beginners/03_Ansible-Inventory.html#ini-format","title":"Ini Format","text":"<ul> <li>The most simple and straightforward format.</li> </ul> <pre><code>[webservers]\nweb1.example.com\nweb2.example.com\n\n[dbservers]\ndb1.example.com\ndb2.example.com\n</code></pre> <ul> <li>Servers are grouped under <code>[]</code></li> </ul>"},{"location":"tooling/ansible/ansible-for-beginners/03_Ansible-Inventory.html#yaml-format","title":"YAML Format","text":"<ul> <li>More structured than the <code>.ini</code> format. An example follows:</li> </ul> <pre><code>all:\n  children:\n    webservers:\n      hosts:\n        web1.example.com\n        web2.example.com\n    dbservers:\n      hosts:\n        db1.example.com\n        db2.example.com\n</code></pre> <ul> <li>Inventory format should be chosen based on project needs.</li> </ul>"},{"location":"tooling/ansible/ansible-for-beginners/03_Ansible-Inventory.html#34-grouping-and-parent-child-relationshops","title":"3.4 - Grouping and Parent-Child Relationshops","text":""},{"location":"tooling/ansible/ansible-for-beginners/03_Ansible-Inventory.html#introduction_3","title":"Introduction","text":"<ul> <li>It's important to utilise grouping in inventory files for ease of life and reduction of human error.</li> <li>Typically, servers are grouped based on aspects such as location and functionality.</li> <li>In the event of sub-grouping, parent-child relationships can be utilised in Ansible.</li> <li>For example web servers could act as one group, which could be further split into subgroups based on locations.</li> </ul>"},{"location":"tooling/ansible/ansible-for-beginners/03_Ansible-Inventory.html#ini-format_1","title":"Ini Format","text":"<ul> <li>Subgroups are defined by being listed under <code>[&lt;group name&gt;:children]</code></li> </ul> <pre><code>[webservers:children]\nwebservers_us\nwebservers_eu\n\n[webservers_us]\nweb1_us.example.com\nweb2_us.example.com\n\n[webservers_eu]\nweb1_eu.example.com\nweb2_eu.example.com\n</code></pre>"},{"location":"tooling/ansible/ansible-for-beginners/03_Ansible-Inventory.html#yaml-format_1","title":"YAML Format","text":"<pre><code>all:\n  children:\n    webservers:\n      children:\n        webservers_us:\n          hosts:\n            web1_us.example.com:\n              ansible_host: &lt;ip address&gt;\n            web2_us.example.com:\n              ansible_host: &lt;ip address&gt;\n        webservers_eu:\n          hosts:\n            web1_eu.example.com:\n              ansible_host: &lt;ip address&gt;\n            web2_eu.example.com:\n              ansible_host: &lt;ip address&gt;\n</code></pre>"},{"location":"tooling/ansible/ansible-for-beginners/04_Ansible-Variables.html","title":"4.0 - Ansible Variables","text":""},{"location":"tooling/ansible/ansible-for-beginners/04_Ansible-Variables.html#71-ansible-variables","title":"7.1 - Ansible Variables","text":""},{"location":"tooling/ansible/ansible-for-beginners/04_Ansible-Variables.html#introduction","title":"Introduction","text":"<ul> <li>Variables are used in a very similar manner to other programming / scripting languages.</li> <li>Could be used to allow changeability of username / host details.</li> <li>Ansible inventory files can be the prime point for variables</li> <li>Variables can be added in playbooks in a similar manner to:</li> </ul> <ul> <li>Alternatively, they can be included in a separate variables file and referenced by \"incudes\" functions, more on that in a later section:</li> </ul>"},{"location":"tooling/ansible/ansible-for-beginners/04_Ansible-Variables.html#using-variables","title":"Using Variables","text":"<ul> <li>To use variables, enter the variable name in \u2018{{}}' i.e.</li> </ul> <ul> <li>Example two - using inventory file playbooks. variable yaml files etc:</li> </ul> <ul> <li>Note - This is Jinja Templating!</li> </ul>"},{"location":"tooling/ansible/ansible-for-beginners/04_Ansible-Variables.html#072-coding-exercise-ansible-variables","title":"07.2 - Coding Exercise: Ansible Variables","text":""},{"location":"tooling/ansible/ansible-for-beginners/04_Ansible-Variables.html#q1","title":"Q1","text":"<p>The playbook is used to update name server entry into resolv.conf file on localhost. The name server information is also updated in the inventory file as a variable <code>nameserver_ip</code>. Refer to the inventory file.</p> <p><code>Replace the ip of the name server in this playbook to use the value from the inventory file</code>, so that in the future if you had to make any changes you simply have to update the inventory file.</p> <pre><code> -\n    name: 'Update nameserver entry into resolv.conf file on localhost'\n    hosts: localhost\n    tasks:\n        -\n            name: 'Update nameserver entry into resolv.conf file'\n            lineinfile:\n                path: /etc/resolv.conf\n                line: 'nameserver {{ nameserver_ip }}'\n</code></pre>"},{"location":"tooling/ansible/ansible-for-beginners/04_Ansible-Variables.html#q2","title":"Q2","text":"<p>We have added a new task to disable SNMP port in the playbook. However the port is hardcoded in the playbook. Update the inventory file to add a new variable <code>snmp_port</code> and assign the value used here. Then update the playbook to use value from the variable.</p> <p>Remember to use curly braces around the variable name.</p> <pre><code>-\n    name: 'Update nameserver entry into resolv.conf file on localhost'\n    hosts: localhost\n    tasks:\n        -\n            name: 'Update nameserver entry into resolv.conf file'\n            lineinfile:\n                path: /etc/resolv.conf\n                line: 'nameserver {{ nameserver_ip }}'\n        -\n            name: 'Disable SNMP Port'\n            firewalld:\n                port: '{{ snmp_port }}'\n                permanent: true\n                state: disabled\n</code></pre>"},{"location":"tooling/ansible/ansible-for-beginners/04_Ansible-Variables.html#q3","title":"Q3","text":"<p>We are printing some personal information to the screen. We would like to move the <code>car_model</code>, <code>country_name</code> and <code>title</code> to a variable defined at the play level.</p> <p>Create three new variables (<code>car_model</code>, <code>country_name</code> and <code>title</code>) under the play and move the values over. Use the variables in the task.</p> <pre><code>-\n    name: 'Update nameserver entry into resolv.conf file on localhost'\n    hosts: localhost\n    vars:\n      car_model: BMW M3\n      country_name: USA\n      title: Systems Engineer\n    tasks:\n        -\n            name: 'Print my car model'\n            command: 'echo \"My car''s model is {{ car_model }}\"'\n        -\n            name: 'Print my country'\n            command: 'echo \"I live in the {{country_name}}\"'\n        -\n            name: 'Print my title'\n            command: 'echo \"I work as a {{title}}\"'\n</code></pre>"},{"location":"tooling/ansible/ansible-for-beginners/04_Ansible-Variables.html#43-variable-types","title":"4.3 - Variable Types","text":""},{"location":"tooling/ansible/ansible-for-beginners/04_Ansible-Variables.html#string","title":"String","text":"<ul> <li>Sequences of characters that can be defined in a playbook, inventory, or as CLI arguments.</li> <li>Example: <code>username: \"admin\"</code></li> </ul>"},{"location":"tooling/ansible/ansible-for-beginners/04_Ansible-Variables.html#number","title":"Number","text":"<ul> <li>Integer or floating-point values. May be set as standalone values or used in mathematical operations.</li> <li>Example: <code>max_connections: 100</code></li> </ul>"},{"location":"tooling/ansible/ansible-for-beginners/04_Ansible-Variables.html#boolean","title":"Boolean","text":"<ul> <li>True or False, typically used in conditionals.</li> <li>Example: <code>debug_mode: true</code></li> <li>True/False are not the only accepted values for <code>True</code> or <code>False</code> to be registered:</li> <li><code>Truthy</code> Values: True, 'true', 't', 'yes', 'y', 'on', '1', 1, 1.0</li> <li><code>Falsy</code> Values: False, 'false', 'f', 'no', 'n', 'off', '0', 0, 0.0</li> </ul>"},{"location":"tooling/ansible/ansible-for-beginners/04_Ansible-Variables.html#list","title":"List","text":"<ul> <li>Used to hold an ordered collection of values, the values must all be of the same type, but any value type is supported by lists themselves</li> <li>Example:</li> </ul> <pre><code>packages:\n- nginx\n- git\n- terraform\n...\n</code></pre> <ul> <li>Specific list values can be referred to via <code>{{ &lt;list name&gt;[item index] }}</code></li> <li>Note: The first element is always index <code>0</code>.</li> <li>Lists themselves can then be referred to via loops - refer to section 5.7 for further details.</li> </ul>"},{"location":"tooling/ansible/ansible-for-beginners/04_Ansible-Variables.html#dictionary","title":"Dictionary","text":"<ul> <li>Holds a collection of key-value pairs.</li> <li>Keys and values can be any values.</li> </ul> <pre><code>user:\n  name: \"admin\"\n  password: \"secret\"\n</code></pre> <ul> <li>Dictionary values can be referred to via <code>{{ &lt;dictionary name&gt;.&lt;key name&gt; }}</code></li> </ul>"},{"location":"tooling/ansible/ansible-for-beginners/04_Ansible-Variables.html#44-registering-variables-and-varoiable-precedence","title":"4.4 - Registering Variables and Varoiable Precedence","text":""},{"location":"tooling/ansible/ansible-for-beginners/04_Ansible-Variables.html#variable-precedence","title":"Variable Precedence","text":"<ul> <li>If variables are defined in multiple places, the order of priority they are registered is variable precedence.</li> <li>In practice, Ansible will first assign variables defined at the group level, any vars defined at host level will then be applied or overwrite values as appropriate.</li> <li>Host variables therefore take precedence over group variables.</li> <li>Any variables defined at playbook level and then at CLI level take precedence.</li> <li>So, in order of precedence, Ansible applies variables in the following order:</li> <li>Group Vars</li> <li>Host Vars</li> <li>Playbook Vars</li> <li>CLI vars (via <code>--extra-vars</code> flag)</li> <li>Additional options are available, however the above are the 4 more common methods.</li> </ul>"},{"location":"tooling/ansible/ansible-for-beginners/04_Ansible-Variables.html#registering-vars","title":"Registering Vars","text":"<ul> <li>You may wish to capture the output of a particular task and pass it in as a variable to a follow-on task. This can be achieved via the <code>register</code> parameter.</li> </ul> <pre><code>- name: Check /etc/hosts file\n  hosts: all\n  tasks:\n  - shell: cat/etc/hosts\n    register: result # output of shell command stored as variable \"result\"\n  - debug:\n      var: result\n</code></pre> <ul> <li>The output can be further queried for specific values as it is in <code>json</code> output.</li> <li> <p>Example: <code>var: result.stdout</code></p> </li> <li> <p>Any variable created via <code>register</code> is available for the rest of the playbook for that given host; it has the host scope.</p> </li> <li>Note: To avoid using the debug module, append the <code>-v</code> flag to the desired <code>ansible-playbook</code> command.</li> </ul>"},{"location":"tooling/ansible/ansible-for-beginners/04_Ansible-Variables.html#45-variable-scoping","title":"4.5 - Variable Scoping","text":""},{"location":"tooling/ansible/ansible-for-beginners/04_Ansible-Variables.html#introduction_1","title":"Introduction","text":"<ul> <li>Scope = How accessible a variable's value is.</li> <li>For example, if one host in a particular inventory file has an extra parameter set, that value is not available to the other hosts.</li> <li> <p>Multiple scopes are available in ansible to deal with varying scenarios.</p> </li> <li> <p>Host:</p> </li> <li> <p>A variable defined as a host variable, and is only accessible in a play(s) on that specific host.</p> </li> <li> <p>Play:</p> </li> <li> <p>Variables defined at play-level and cannot be referenced by any other plays.</p> </li> <li> <p>Global:</p> </li> <li>Variables available across all plays, these are typically variables defined at CLI levels via <code>--extra-vars \"&lt;var name&gt;=&lt;var value&gt;</code></li> </ul>"},{"location":"tooling/ansible/ansible-for-beginners/04_Ansible-Variables.html#46-magic-variables","title":"4.6 - Magic Variables","text":""},{"location":"tooling/ansible/ansible-for-beginners/04_Ansible-Variables.html#hostvars","title":"Hostvars","text":"<ul> <li>As discussed in previous sections, variables are scoped depending on where they are defined. If defined at a host level, host 2 traditionally has no access to variables defined for host 1, etc.</li> <li>If this did become a requirement, Magic Variables can be utilised.</li> <li>Used in the form <code>'{{ hostvars['&lt;hostname&gt;'].&lt;var name&gt; }}'</code></li> <li>If additional facts are gathered by the hosts, vars such as the following can be used:</li> <li>Host IP Address: <code>ansible_host</code></li> <li>Host system architecture: <code>ansible_facts.architecture</code></li> <li> <p>Host mounts: <code>ansible_facts.mounts</code></p> </li> <li> <p>Note: Magic variables may also be referred to via: <code>'{{ hostvars['&lt;hostname&gt;'][&lt;var/var group name&gt;][&lt;sub-variable name&gt;] }}'</code></p> </li> </ul>"},{"location":"tooling/ansible/ansible-for-beginners/04_Ansible-Variables.html#groups","title":"Groups","text":"<ul> <li>Group magic variables can be utilised in a few ways: <code>'{{ groups['&lt;group_name&gt;'] }}'</code> returns all the hosts under the particular group.</li> <li><code>'{{ group_names }}'</code> will return the names of any group the particular host finds itself in.</li> </ul>"},{"location":"tooling/ansible/ansible-for-beginners/04_Ansible-Variables.html#inventory","title":"Inventory","text":"<ul> <li><code>'{{ inventory_hostname }}'</code> gives the inventory-level name for the host the play is running on.</li> </ul> <ul> <li>Other examples are available via the Ansible documentation.</li> </ul>"},{"location":"tooling/ansible/ansible-for-beginners/04_Ansible-Variables.html#47-ansible-facts","title":"4.7 - Ansible Facts","text":""},{"location":"tooling/ansible/ansible-for-beginners/04_Ansible-Variables.html#introduction_2","title":"Introduction","text":"<ul> <li>Ansible gathers basic facts about the target machine upon initial connection, such as:</li> <li>basic system information</li> <li>system architecture</li> <li>architecture</li> <li>network connectivity</li> <li>ip addresses</li> <li>storage information</li> <li>Ansible gathers all these <code>facts</code> via the <code>setup</code> module, which is ran automatically at the start of each playbook, unless <code>gather_facts</code> is disabled.</li> <li>All facts gathered by ansible are stored in the <code>ansible_facts</code> variable. This can be viewed via the <code>debug</code> module, passing <code>ansible_facts</code> as the var.</li> <li>Gathering facts can be disabled at playbook level by setting <code>gather_facts: no</code>, or at config level by setting <code>gathering = implicit/explicit</code></li> <li>At config level, implicit will gather facts by default, unless specified not to at playbook level.</li> <li>Explicit will not gather facts by default, unless specified otherwise at playbook level.</li> <li>Playbook-level configuration always takes precedence.</li> <li>Fact-gathering only applies to hosts defined in inventory files.</li> </ul>"},{"location":"tooling/ansible/ansible-for-beginners/05_Ansible-Playbooks.html","title":"5.0 - Ansible Playbooks","text":""},{"location":"tooling/ansible/ansible-for-beginners/05_Ansible-Playbooks.html#051-ansible-playbooks","title":"05.1 - Ansible Playbooks","text":""},{"location":"tooling/ansible/ansible-for-beginners/05_Ansible-Playbooks.html#notes","title":"Notes","text":"<ul> <li>Playbooks are configuration files used to help Ansible understand what it needs to do when running.</li> <li>Example - Running commands on particular servers in a particular order, then restarting said servers in a particular order.</li> <li>Or more complex operations e.g.:</li> <li>Deploy x VMs to environment 1</li> <li>Deploy y VMS to environment 2</li> <li>Provision storage to all VMs</li> <li>Setup Network Configuration for all VMs in environment 2</li> <li>All Playbooks are written in YAML</li> <li>They contain \"plays\" - a set of activities (tasks) to be run on hosts</li> <li>Task - any action to be executed on the host e.g. run a script, install a package.</li> <li>Example Playbook:</li> </ul> <pre><code>- name: Play 1\n  hosts: localhost\n  tasks:\n  - name: Execute command \"date\"\n    command: date\n\n  - name: Execute script on server\n    script: test_script.sh\n\n  - name: Install httpd service\n    yum:\n      name: httpd\n      status: present\n\n  - name: start web server\n    service:\n      name: httpd\n      state: started\n</code></pre> <ul> <li>Each activity will occur in the order that they are defined on the host defined</li> <li>The host must be included in the associated ansible inventory file e.g. localhost, server1.company.com</li> <li>All connection information must be specified in the inventory files</li> <li>Additionally, if a group is defined as the host, then all hosts in that group will be applied by default</li> <li>Playbooks = list of dictionary in YAML</li> <li>Each play is a dictionary</li> <li>Tasks are lists / arrays \u2192 ordered collection, meaning the order MATTERS</li> <li>The actions ran by tasks are modules e.g. command, script, yum, service</li> <li> <p>Further information provided in the ansible docs OR</p> <p><code>yaml ansible-doc -l</code></p> </li> </ul> <ul> <li>To execute an ansible playbook:</li> </ul> <pre><code>ansible-playbook &lt;playbook&gt;.yaml\n</code></pre>"},{"location":"tooling/ansible/ansible-for-beginners/05_Ansible-Playbooks.html#052-demo-run-ansible-playbooks","title":"05.2 - Demo: Run Ansible Playbooks","text":""},{"location":"tooling/ansible/ansible-for-beginners/05_Ansible-Playbooks.html#notes_1","title":"Notes","text":"<ul> <li>When running ansible playbooks, generally have two options:</li> <li>using the ansible command</li> <li>using the ansible-playbook command</li> <li> <p>The former is typically used in an imperative manner for one-off commands not requiring a playbook e.g.:</p> <p>```yaml ansible  -a  <p>ansible all -a \"/sbin/reboot\"</p> <p>ansible  -m  <p>ansible target1 -m ping ```</p> <li> <p>The latter should be used when wanting to run a particular playbook. This is in a declarative manner.</p> </li> <pre><code>ansible-playbook &lt;playbook name&gt;\n</code></pre>"},{"location":"tooling/ansible/ansible-for-beginners/05_Ansible-Playbooks.html#demo","title":"Demo","text":"<ul> <li>In the demo project folder created previously, run the following:</li> </ul> <pre><code>ansible all -m ping -i inventory.txt\n</code></pre> <ul> <li>This will tell ansible to call the ping module to test connection</li> <li>Note: The all group is not specified in the inventory file, however it is created by default via ansible when specifying a particular inventory file.</li> <li>The same result could be achieved by a yaml file</li> </ul> <pre><code>- name: Test Connectivity to Target Servers # name of playbook\n  hosts: all # what hosts should this playbook apply to?\n  tasks:\n  - name: Ping Test\n    ping: # note the ping module doesn't require any parameters\n</code></pre> <ul> <li>This can then be ran by the following:</li> </ul> <pre><code>ansible-playbook playbook-pingtest.yaml -i inventory.txt\n</code></pre>"},{"location":"tooling/ansible/ansible-for-beginners/05_Ansible-Playbooks.html#labs","title":"Labs","text":""},{"location":"tooling/ansible/ansible-for-beginners/05_Ansible-Playbooks.html#q1","title":"Q1","text":"<p>Update name of the play to <code>Execute a date command on localhost</code></p> <pre><code>-\n    name: 'Execute a date command on localhost'\n    hosts: localhost\n    tasks:\n        -\n            name: 'Execute a date command'\n            command: date\n</code></pre>"},{"location":"tooling/ansible/ansible-for-beginners/05_Ansible-Playbooks.html#q2","title":"Q2","text":"<p>Update the task to execute the command <code>cat /etc/hosts</code>  and change task name to <code>Execute a command to display hosts file</code></p> <pre><code>-\n    name: 'Execute a command to display hosts file on localhost'\n    hosts: localhost\n    tasks:\n        -\n            name: 'Execute a command to display hosts file'\n            command: cat /etc/hosts\n</code></pre>"},{"location":"tooling/ansible/ansible-for-beginners/05_Ansible-Playbooks.html#q3","title":"Q3","text":"<p>Update the playbook to add a second task. The new task must execute the command <code>cat /etc/hosts</code>  and change new task name to <code>Execute a command to display hosts file</code></p> <pre><code>-\n    name: 'Execute two commands on localhost'\n    hosts: localhost\n    tasks:\n        -\n            name: 'Execute a date command'\n            command: date\n        - name: 'Execute a command to display hosts file'\n          command: cat /etc/hosts\n</code></pre>"},{"location":"tooling/ansible/ansible-for-beginners/05_Ansible-Playbooks.html#q4","title":"Q4","text":"<p>We have been running all tasks on localhost. We would now like to run these tasks on the web_node1. Update the play to run the tasks on <code>web_node1</code></p> <pre><code>-\n    name: 'Execute two commands on localhost'\n    hosts: web_node1\n    tasks:\n        -\n            name: 'Execute a date command'\n            command: date\n        -\n            name: 'Execute a command to display hosts file'\n            command: 'cat /etc/hosts'\n</code></pre>"},{"location":"tooling/ansible/ansible-for-beginners/05_Ansible-Playbooks.html#q5","title":"Q5","text":"<p>Refer to the attached inventory file. We would like to run the tasks defined in the play on all servers in <code>boston</code></p> <p>Inventory:</p> <pre><code>## Sample Inventory File\n\n## Web Servers\nsql_db1 ansible_host=sql01.xyz.com ansible_connection=ssh ansible_user=root ansible_ssh_pass=Lin$Pass\nsql_db2 ansible_host=sql02.xyz.com ansible_connection=ssh ansible_user=root ansible_ssh_pass=Lin$Pass\nweb_node1 ansible_host=web01.xyz.com ansible_connection=ssh ansible_user=administrator ansible_ssh_pass=Win$Pass\nweb_node2 ansible_host=web02.xyz.com ansible_connection=ssh ansible_user=administrator ansible_ssh_pass=Win$Pass\nweb_node3 ansible_host=web03.xyz.com ansible_connection=ssh ansible_user=administrator ansible_ssh_pass=Win$Pass\n\n[db_nodes]\nsql_db1\nsql_db2\n\n[web_nodes]\nweb_node1\nweb_node2\nweb_node3\n\n[boston_nodes]\nsql_db1\nweb_node1\n\n[dallas_nodes]\nsql_db2\nweb_node2\nweb_node3\n\n[us_nodes:children]\nboston_nodes\ndallas_nodes\n</code></pre> <p>Answer:</p> <pre><code>-\n    name: 'Execute two commands on web_node1'\n    hosts: boston_nodes\n    tasks:\n        -\n            name: 'Execute a date command'\n            command: date\n        -\n            name: 'Execute a command to display hosts file'\n            command: 'cat /etc/hosts'\n</code></pre>"},{"location":"tooling/ansible/ansible-for-beginners/05_Ansible-Playbooks.html#q6","title":"Q6","text":"<p>Create a new play named <code>Execute a command to display hosts file contents on web_node2</code> to execute <code>cat /etc/hosts</code> command on second node <code>web_node2</code> and name the task <code>Execute a command to display hosts file</code>.</p> <p>Refer to the attached inventory file (see Q5)</p> <pre><code>-\n    name: 'Execute command to display date on web_node1'\n    hosts: web_node1\n    tasks:\n        -\n            name: 'Execute a date command'\n            command: date\n-\n    name: 'Execute a command to display hosts file contents on web_node2'\n    hosts: web_node2\n    tasks:\n        -\n            name: 'Execute a command to display hosts file'\n            command: cat /etc/hosts\n</code></pre>"},{"location":"tooling/ansible/ansible-for-beginners/05_Ansible-Playbooks.html#q7","title":"Q7","text":"<p>You are assigned a task to restart a number of servers in a particular sequence. The sequence and the commands to be used are given below. Note that the commands should be run on respective servers only. Refer to the inventory file and update the playbook to create the below sequence.</p> <p>Note: Use the description below to name the plays and tasks.</p> <ol> <li><code>Stop</code> the <code>web</code> services on web server nodes - <code>service httpd stop</code></li> <li><code>Shutdown</code> the <code>database</code> services on db server nodes - <code>service mysql stop</code></li> <li><code>Restart</code> <code>all</code> servers (web and db) at once - <code>/sbin/shutdown -r</code></li> <li><code>Start</code> the <code>database</code> services on db server nodes - <code>service mysql start</code></li> <li><code>Start</code> the <code>web</code> services on web server nodes - <code>service httpd start</code></li> </ol> <p>Warning: Do not use this playbook in a real setup. There are better ways to do these actions. This is only for simple practise. </p> <p>Inventory:</p> <pre><code>## Sample Inventory File\n\n## Web Servers\nsql_db1 ansible_host=sql01.xyz.com ansible_connection=ssh ansible_user=root ansible_ssh_pass=Lin$Pass\nsql_db2 ansible_host=sql02.xyz.com ansible_connection=ssh ansible_user=root ansible_ssh_pass=Lin$Pass\nweb_node1 ansible_host=web01.xyz.com ansible_connection=ssh ansible_user=administrator ansible_ssh_pass=Win$Pass\nweb_node2 ansible_host=web02.xyz.com ansible_connection=ssh ansible_user=administrator ansible_ssh_pass=Win$Pass\nweb_node3 ansible_host=web03.xyz.com ansible_connection=ssh ansible_user=administrator ansible_ssh_pass=Win$Pass\n\n[db_nodes]\nsql_db1\nsql_db2\n\n[web_nodes]\nweb_node1\nweb_node2\nweb_node3\n\n[all_nodes:children]\ndb_nodes\nweb_nodes\n</code></pre> <p>Answer:</p> <pre><code>-\n    name: 'Stop the web services on web server nodes'\n    hosts: web_nodes\n    tasks:\n        -\n            name: 'Stop the web services on web server nodes'\n            command: 'service httpd stop'\n-\n    name: 'Shutdown the database services on db server nodes'\n    hosts: db_nodes\n    tasks:\n        -\n            name: 'Shutdown the database services on db server nodes'\n            command: 'service mysql stop'\n-\n    name: 'Restart all servers (web and db) at once'\n    hosts: all_nodes\n    tasks:\n        -\n            name: 'Restart all servers (web and db) at once'\n            command: '/sbin/shutdown -r'\n-\n    name: 'Start the database services on db server nodes'\n    hosts: db_nodes\n    tasks:\n        -\n            name: 'Start the database services on db server nodes'\n            command: 'service mysql start'\n-\n    name: 'Start the web services on web server nodes'\n    hosts: web_nodes\n    tasks:\n        -\n            name: 'Start the web services on web server nodes'\n            command: 'service httpd start'\n</code></pre>"},{"location":"tooling/ansible/ansible-for-beginners/05_Ansible-Playbooks.html#53-verifying-playbooks","title":"5.3 - Verifying Playbooks","text":""},{"location":"tooling/ansible/ansible-for-beginners/05_Ansible-Playbooks.html#check-mode","title":"Check Mode","text":"<ul> <li>This is ansible's dry-run mechanism, that attempts to run the playbook without making any actual changes on the hosts.</li> <li>Achievable via appending the <code>--check</code> flag when executing an <code>ansible-playbook</code> command.</li> <li>Not all modules support this option</li> </ul>"},{"location":"tooling/ansible/ansible-for-beginners/05_Ansible-Playbooks.html#diff-mode","title":"Diff Mode","text":"<ul> <li>When used in combination with check mode, provides a before-and after comparison of playbook tasks.</li> <li>This helps to understand and verify the impact of tasks pre-application.</li> <li>Add the <code>--diff</code> flag to the <code>ansible-playbook</code> command to use.</li> </ul>"},{"location":"tooling/ansible/ansible-for-beginners/05_Ansible-Playbooks.html#syntax-check","title":"Syntax Check","text":"<ul> <li>Ansible has built-in syntax check mode, simply add the folliwing flag: <code>--syntax-check</code></li> </ul>"},{"location":"tooling/ansible/ansible-for-beginners/05_Ansible-Playbooks.html#54-ansible-lint","title":"5.4 - Ansible-Lint","text":""},{"location":"tooling/ansible/ansible-for-beginners/05_Ansible-Playbooks.html#overview","title":"Overview","text":"<ul> <li>Playbooks can become increasingly complex over time, leading to increased likelihood of deviation from best practice.</li> <li>Ansible-lint aims to mitigate these issues, this is a CLI tool that performs linting on ansible playbooks, roles, and collections.</li> <li>The tool checks code for potential errors, bugs, stylistic errors and deviations from best practice.</li> <li>To use: <code>ansible-lint &lt;yaml file&gt;</code></li> <li>Output provides guidance on any errors it finds and the locations of the incidents.</li> </ul>"},{"location":"tooling/ansible/ansible-for-beginners/05_Ansible-Playbooks.html#081-conditionals","title":"08.1 - Conditionals","text":""},{"location":"tooling/ansible/ansible-for-beginners/05_Ansible-Playbooks.html#introduction","title":"Introduction","text":"<ul> <li>Consider two Playbooks that look to install NGINX, but one on Red Hat, another on Debian. This requires the use of the yum and apt packages respectively.</li> </ul> <ul> <li>Rather than have two separate playbooks for this same task, it's more advisable to bring the task into one playbook and use a conditional to perform the appropriate module.</li> <li>Conditionals are typically used via \"when\" statements e.g.:</li> </ul> <ul> <li>And and Or separators can be used for multiple conditions</li> </ul> <ul> <li>Conditionals may also be used in loops, an example follows, where the packages will only be installed if required is set to true:</li> </ul> <ul> <li>Conditionals can also be used in conjunction with the outputs of prior tasks, this would require the use of the register module.</li> <li>The example causes a mail message to be sent to the email address provided only if the httpd service is shown to be down</li> </ul>"},{"location":"tooling/ansible/ansible-for-beginners/05_Ansible-Playbooks.html#conditionals-based-on-facts-variables-re-use","title":"Conditionals based on Facts, Variables, Re-Use","text":"<ul> <li>If tasks need to be ran depending on specific conditionsm such as OS architecture, <code>ansible_facts</code> can be utilised in combination with the <code>when:</code> for conditionals.</li> <li>Example, only run a task on hosts running Ubuntu 18: <code>when: ansible_facts['os_family'] == 'Debian' and ansible_facts['distribution_major_version'] == '18'</code></li> <li>If wanting to run particular tasks using variables, use <code>vars:</code> in a similar manner to below for the given task:</li> </ul> <pre><code>- name: deploy configuration files\n    template:\n      src: \"{{ app_env }}_config.j2\"\n      dest: \"/etc/myapp/config.conf\"\n    vars:\n      app_env: production\n</code></pre> <ul> <li>For follow-on tasks for a play, you can then add <code>when: &lt;var name&gt; == '&lt;var value&gt;'</code> after the variable was defined previously e.g. at CLI-level.</li> </ul>"},{"location":"tooling/ansible/ansible-for-beginners/05_Ansible-Playbooks.html#082-coding-exercise-conditionals","title":"08.2 - Coding Exercise: Conditionals","text":""},{"location":"tooling/ansible/ansible-for-beginners/05_Ansible-Playbooks.html#q1_1","title":"Q1","text":"<p>The given playbook attempts to start mysql service on <code>all_servers</code>. Use the <code>when</code> condition to run this task if the host (<code>ansible_host</code>) is the database server.</p> <p>Refer to the inventory file to identify the name of the database server.</p> <pre><code>-\n    name: 'Execute a script on all web server nodes'\n    hosts: all_servers\n    tasks:\n        -\n            service: 'name=mysql state=started'\n            when: ansible_host == 'server4.company.com'\n</code></pre>"},{"location":"tooling/ansible/ansible-for-beginners/05_Ansible-Playbooks.html#q2_1","title":"Q2","text":"<p>The playbook has a variable defined - <code>age</code> . The two tasks attempt to print if I am a child or an Adult. Use the when conditional to print if I am a child or an Adult based on weather my age is <code>&lt; 18 (child)</code>  or <code>&gt;= 18 (Adult)</code></p> <pre><code>-\n    name: 'Am I an Adult or a Child?'\n    hosts: localhost\n    vars:\n        age: 25\n    tasks:\n        -\n            command: 'echo \"I am a Child\"'\n            when: 'age &lt; 18'\n        -\n            command: 'echo \"I am an Adult\"'\n            when: 'age &gt;= 18'\n</code></pre>"},{"location":"tooling/ansible/ansible-for-beginners/05_Ansible-Playbooks.html#q3_1","title":"Q3","text":"<p>The given playbook attempts to add an entry into the <code>/etc/resolv.conf</code> file for <code>nameserver</code>.</p> <p>First, we run a command using the <code>shell</code> module to get the contents of <code>/etc/resolv.conf</code> file and then we add a new line containing the name server data into the file. However, when this playbook is run multiple times, it keeps adding new entries of same line into the resolv.conf file.</p> <ol> <li>Add a <code>register</code> directive to store the output of the first command to variable <code>command_output</code></li> <li>Then add a <code>conditional</code> to the second command to check if the output contains the name server (<code>10.0.250.10</code>) already. Use <code>command_output.stdout.find(&lt;IP&gt;) == -1</code></li> </ol> <p>Note: A better way to do this would be to use the lineinfile module. This is just for practice.</p> <p>Note: <code>shell</code> and <code>command</code> modules are similar in that they are used to execute a command on the system. However <code>shell</code> executes the command inside a shell giving us access to environment variables and redirection using <code>&gt;&gt;</code> </p> <pre><code>-\n    name: 'Add name server entry if not already entered'\n    hosts: localhost\n    tasks:\n        -\n            shell: 'cat /etc/resolv.conf'\n            register: command_output\n        -\n            shell: 'echo \"nameserver 10.0.250.10\" &gt;&gt; /etc/resolv.conf'\n            when: 'command_output.stdout.find(\"10.0.250.10\") == -1'\n</code></pre>"},{"location":"tooling/ansible/ansible-for-beginners/05_Ansible-Playbooks.html#091-loops","title":"09.1 - Loops","text":""},{"location":"tooling/ansible/ansible-for-beginners/05_Ansible-Playbooks.html#notes_2","title":"Notes","text":"<ul> <li>Loops allow the same command(s) to be ran iteratively to accommodate an ever-changing value(s)</li> <li>A common example is creating users</li> </ul> <ul> <li>Note: '{{ item }}' is used to call the loop iteration variable</li> <li>What happens if you have multiple sets of variables e.g. user name and user id</li> <li>Use a list of dictionaries in the loop</li> </ul> <ul> <li>Note: Would now refer to items in the list of dictionaries / arrays to call separate variables e.g.:</li> <li>item.name</li> <li>item.uid</li> <li>Note: The loop module is new to ansible, it was previously covered by the with_ module; an example follows:</li> </ul> <ul> <li>With_ allows more flexibilty compared to loops e.g.:</li> <li><code>with_items</code></li> <li><code>with_files</code></li> <li><code>with_url</code></li> <li><code>with_env</code></li> <li>These are all custom plugins primarily focused on lookup tasks.</li> </ul>"},{"location":"tooling/ansible/ansible-for-beginners/05_Ansible-Playbooks.html#092-coding-exercise-ansible-loops","title":"09.2 - Coding Exercise: Ansible Loops","text":""},{"location":"tooling/ansible/ansible-for-beginners/05_Ansible-Playbooks.html#q1_2","title":"Q1","text":"<p>The playbook currently runs an echo command to print a fruit name. Apply a loop directive (with_items) to the task to print all fruits defined in the <code>fruits</code>  variable.</p> <pre><code>-\n    name: 'Print list of fruits'\n    hosts: localhost\n    vars:\n        fruits:\n            - Apple\n            - Banana\n            - Grapes\n            - Orange\n    tasks:\n        -\n            command: 'echo \"{{item}}\"'\n            with_items: '{{fruits}}'\n</code></pre>"},{"location":"tooling/ansible/ansible-for-beginners/05_Ansible-Playbooks.html#q2_2","title":"Q2","text":"<p>To a more realistic use case. We are attempting to install multiple packages using yum module.The current playbook installs only a single package.</p> <pre><code>-\n    name: 'Install required packages'\n    hosts: localhost\n    vars:\n        packages:\n            - httpd\n            - binutils\n            - glibc\n            - ksh\n            - libaio\n            - libXext\n            - gcc\n            - make\n            - sysstat\n            - unixODBC\n            - mongodb\n            - nodejs\n            - grunt\n    tasks:\n        -\n            yum: 'name={{item}} state=present'\n            with_items: '{{packages}}'\n</code></pre>"},{"location":"tooling/ansible/ansible-for-beginners/06_Ansible-Modules.html","title":"6.0 - Ansible Modules","text":""},{"location":"tooling/ansible/ansible-for-beginners/06_Ansible-Modules.html#061-ansible-modules","title":"06.1 - Ansible Modules","text":""},{"location":"tooling/ansible/ansible-for-beginners/06_Ansible-Modules.html#notes","title":"Notes","text":"<ul> <li>Modules are categorised based on functionalities e.g</li> <li>System - Commands based on the host system e.g. start/stop service</li> <li>Commands - Used to execute commands or scripts on hosts</li> <li>Files - Used to execute file-specific commands e.g. find, copy, replace</li> <li>Database - Used to interact with databases such as MongoDB, MSSQL, MySQL</li> <li>Cloud - Used to interact with cloud providers like AWS, Azure, GCP, Linode, Digital Ocean.</li> <li>Windows - Commands to help use Ansible in a windows environment, be it working with files, user management, executing commands.</li> <li>This is a non-exhaustive list - many more modules can be viewed in the Ansible documentation, with details on how to use each provided.</li> </ul>"},{"location":"tooling/ansible/ansible-for-beginners/06_Ansible-Modules.html#command","title":"Command","text":"<ul> <li>Used to execute a command on a remote node</li> <li>Parameters include:</li> </ul> <ul> <li>Example Playbook:</li> </ul> <p>Example playbook to execute the date command and run the cat command on a desired file.</p> <p></p> <ul> <li>Creates parameter is used to perform a check if the folder or file exists before running the command</li> <li>chdir requests that ansible changes directory before the command is ran</li> <li>free_form - the command module takes a free form command to run; no parameters are required</li> <li>Not all commands support free-form parameters, example, copy requires a source and target dir to be specified.</li> </ul>"},{"location":"tooling/ansible/ansible-for-beginners/06_Ansible-Modules.html#script","title":"Script","text":"<ul> <li>Used to run a local script on a remote node after transferring it.</li> <li>Example playbook:</li> </ul>"},{"location":"tooling/ansible/ansible-for-beginners/06_Ansible-Modules.html#service","title":"Service","text":"<ul> <li>Used to manage services on the system e.g. stop, start, restart</li> <li>Example playbooks for this can be done in one of two manners:</li> </ul> <ul> <li>Question: Why write the state as \"started\" and not \"start\"?</li> <li>Not instructing ansible to start the service, we are asking ansible to ensure that it is started i.e. if it's not started - start it</li> <li>This leads to the idempotency of ansible modules:<ul> <li>Idempotency - An operation is idempotent if the result of performing it once is exactly the same as the result of performing it repeatedly without any intervening actions</li> </ul> </li> <li>In general, Ansible's idea is to be able to run a playbook, when running it again, everything should return \"as expected\", if not, Ansible will make it so.</li> </ul>"},{"location":"tooling/ansible/ansible-for-beginners/06_Ansible-Modules.html#lineinfile","title":"Lineinfile","text":"<ul> <li>Searches for a line in a file and replaces it or adds it if it doesn't exist</li> <li>Example playbook:</li> </ul> <ul> <li>Note that this action IN THEORY could be achieved by running a script like this:</li> </ul> <ul> <li>However, this would repeatedly add the same entry to /etc/resolv.conf rather than replace it or not add it if found. By contrast the idempotency of Ansible means that if this task is ran as part of a playbook, the entry is added once and only once if it's not found.</li> </ul>"},{"location":"tooling/ansible/ansible-for-beginners/06_Ansible-Modules.html#62-introduction-to-plugins","title":"6.2 - Introduction to Plugins","text":""},{"location":"tooling/ansible/ansible-for-beginners/06_Ansible-Modules.html#overview","title":"Overview","text":"<ul> <li>Ansible plugins aim to provide additional functionality and customisation options beyond the core features.</li> <li>Plugins extend or modify the core functionality of ansible, such as inventory, modules, and callbacks.</li> <li>Plugins can be found as any of:</li> <li>Inventory Plugins (e.g. Dynamic Inventory)</li> <li>Module Plugins (e.g. provision custom cloud configuration)</li> <li>Action Plugins (e.g. define a series of high-level tasks to help enhance consistency in the configuration)</li> <li>Callback Plugins - provide hooks into ansible's execution lifecycle, facilitating custom actions during playbook execution.</li> <li>Lookup Plugins (typically used with Databases)</li> <li>Filter Plugins</li> <li>Connection Plugins</li> <li>Each plugin comes with their own parameters and configuration.</li> </ul>"},{"location":"tooling/ansible/ansible-for-beginners/06_Ansible-Modules.html#63-modules-and-plugins-index","title":"6.3 - Modules and Plugins Index","text":""},{"location":"tooling/ansible/ansible-for-beginners/06_Ansible-Modules.html#overview_1","title":"Overview","text":"<ul> <li>The module and plugin index aims to act as a hub for searching plugins and modules to be added to playbooks.</li> <li>Each comes with documentation for usage, examples, and additional supporting documentation.</li> <li>Index link</li> <li>The index offers:</li> <li>Search and filtering</li> <li>Detailed documentation per plugin</li> <li>Version compatability guidance and considerations</li> <li>Community contributions for support and contribution guidance.</li> </ul>"},{"location":"tooling/ansible/ansible-for-beginners/06_Ansible-Modules.html#062-ansible-modules-coding-exercises","title":"06.2 - Ansible Modules Coding Exercises","text":""},{"location":"tooling/ansible/ansible-for-beginners/06_Ansible-Modules.html#notes_1","title":"Notes","text":""},{"location":"tooling/ansible/ansible-for-beginners/06_Ansible-Modules.html#q1","title":"Q1","text":"<p>Update the playbook with a play to <code>Execute a script on all web server nodes</code>. The script is located at <code>/tmp/install_script.sh</code></p> <p>Use the Script module</p> <pre><code>- name: 'Execute a script on all web server nodes'\n  hosts: web_nodes\n  tasks:\n  - name: 'Execute a script on all web server nodes'\n    script: /tmp/install_script.sh\n</code></pre>"},{"location":"tooling/ansible/ansible-for-beginners/06_Ansible-Modules.html#q2","title":"Q2","text":"<p>Update the playbook to add a new task to <code>start httpd services</code> on all web nodes</p> <p>Use the Service module</p> <pre><code>-\n    name: 'Execute a script on all web server nodes'\n    hosts: web_nodes\n    tasks:\n        -\n            name: 'Execute a script on all web server nodes'\n            script: /tmp/install_script.sh\n        - name: 'start http services on all web server nodes'\n          service:\n            name: httpd\n            state: started\n</code></pre>"},{"location":"tooling/ansible/ansible-for-beginners/06_Ansible-Modules.html#q3","title":"Q3","text":"<p>Update the playbook to add a new task in the beginning to add an entry into <code>/etc/resolv.conf</code> file for hosts. The line to be added is <code>nameserver 10.1.250.10</code></p> <p>Note: The new task must be executed first, so place it accordingly. </p> <p>Use the Lineinfile module</p> <pre><code>-\n    name: 'Execute a script on all web server nodes'\n    hosts: web_nodes\n    tasks:\n        - name: 'Add enntry to /etc/resolv.conf file'\n          lineinfile:\n            path: /etc/resolv.conf\n            line: 'nameserver 10.1.250.10'\n        -\n            name: 'Execute a script'\n            script: /tmp/install_script.sh\n        -\n            name: 'Start httpd service'\n            service:\n                name: httpd\n                state: present\n</code></pre>"},{"location":"tooling/ansible/ansible-for-beginners/06_Ansible-Modules.html#q4","title":"Q4","text":"<p>Update the playbook to add a new task at second position (right after adding entry to resolv.conf) to create a new web user.</p> <p>Use the user module for this. User details to be used are given below:Username: web_useruid: 1040group: developers</p> <pre><code>-\n    name: 'Execute a script on all web server nodes and start httpd service'\n    hosts: web_nodes\n    tasks:\n        -\n            name: 'Update entry into /etc/resolv.conf'\n            lineinfile:\n                path: /etc/resolv.conf\n                line: 'nameserver 10.1.250.10'\n        - name: 'add user web_user'\n          user:\n            name: web_user\n            uid: 1040\n            group: developers\n\n        -   name: 'Execute a script'\n            script: /tmp/install_script.sh\n        -\n            name: 'Start httpd service'\n            service:\n                name: httpd\n                state: present\n</code></pre>"},{"location":"tooling/ansible/ansible-for-beginners/07_Ansible-Handlers-Roles-and-Collections.html","title":"7.0 - Ansible Handlers: Roles and Collections","text":""},{"location":"tooling/ansible/ansible-for-beginners/07_Ansible-Handlers-Roles-and-Collections.html#71-introduction-to-handlers","title":"7.1 - Introduction to Handlers","text":""},{"location":"tooling/ansible/ansible-for-beginners/07_Ansible-Handlers-Roles-and-Collections.html#introduction","title":"Introduction","text":"<ul> <li>In some cases, configuration applied by Ansible may not take effect until the target server or service is restarted.</li> <li>Typically, one would then have to manually restart the server or service, Ansible Handlers aim to support this.</li> <li>Handlers allow definition of an action to restart the service and associate it with the task that modifies the configuration file.</li> <li>Creates a dependency between the task and handler, and eliminates the need for manual intervention.</li> <li>One can therefore view handlers as tasks triggered by events / notifications.</li> <li>They are defined in playbooks and executed when notified by a task.</li> <li> <p>They manage actions based on system state or configuration changes.</p> </li> <li> <p>An example playbook follows, the copy task notifies the defined handler to restart the service.:</p> </li> </ul> <pre><code>- name: Deploy application\n  hosts: application_servers\n  tasks:\n  - name: copy application code\n    copy:\n      src: app/code\n      dest: /opt/application/\n    notify: Restart Application Service\n\n  handlers:\n  - name: Restart Application Service\n    service:\n      name: application_service\n      state: restarted\n</code></pre> <ul> <li>This is beneficial as the copy module has no way of restarting the service, another task would have to be defined.</li> </ul>"},{"location":"tooling/ansible/ansible-for-beginners/07_Ansible-Handlers-Roles-and-Collections.html#101-ansible-roles","title":"10.1 - Ansible Roles","text":""},{"location":"tooling/ansible/ansible-for-beginners/07_Ansible-Handlers-Roles-and-Collections.html#notes","title":"Notes","text":"<ul> <li>Roles can be assigned to each server involved in an Ansible playbook to allow them to perform particular tasks.</li> <li>Assigning a role in Ansible requires the performance of tasks such that the server can perform the required task. Example:</li> <li>mysql<ul> <li>Prerequisite and mysql packages should be installed</li> <li>mysql service configuration</li> <li>database and user configuration</li> </ul> </li> <li>nginx<ul> <li>Prerequisite and nginxpackages should be installed</li> <li>Service configuration</li> </ul> </li> <li>Example playbook:</li> </ul> <ul> <li>If the tasks can be done on a Playbook, why are roles required?</li> <li>Whilst it's true that you could use a playbook that you can share with others, there may be changes required for each user.</li> <li>Instead, one can package this playbook as a role, which can then be called in a playbook for easier reusability</li> </ul> <ul> <li>Roles therefore allow the best-practices of Ansible to be adhered to.</li> <li>Typical directory structure:</li> <li>Role<ul> <li>tasks carried out by role(s)</li> <li>variables used by tasks</li> <li>defaults values for tasks</li> <li>handlers</li> <li>templates used by playbooks</li> </ul> </li> <li>Roles also allow easier sharing of code and roles within the community - Ansible Galaxy being a primary example</li> </ul>"},{"location":"tooling/ansible/ansible-for-beginners/07_Ansible-Handlers-Roles-and-Collections.html#getting-started-with-a-role","title":"Getting Started with a Role","text":"<ul> <li>Create the directory structure</li> <li>Run the ansible-galaxy command:</li> </ul> <pre><code>ansible-galaxy init &lt;role-name&gt;\n</code></pre> <ul> <li>This will create the skeleton structure required for the role, and the code can then be moved into it as required.</li> <li>When referencing a role in a playbook, one must specify its location.</li> <li>One option would be to create a designated directory for the roles</li> <li>Ansible by default will look in /etc/ansible/roles</li> <li>Roles can then be shared via GitHub in Ansible Galaxy</li> <li>Ansible-galaxy's UI or CLI have search functionality to help look for \"suitable\" roles for users.</li> <li>To install a role:</li> </ul> <pre><code>ansible-galaxy install &lt;role name&gt;\n</code></pre> <ul> <li>This will extract the role to the default directory defined by the ansible config file</li> <li>Roles can then be referenced in the Playbook(s) where required, either on its own or as a list of dictionaries; the latter offering the option to add more variable inputs.</li> </ul>"},{"location":"tooling/ansible/ansible-for-beginners/07_Ansible-Handlers-Roles-and-Collections.html#73-ansible-collectionss","title":"7.3 - Ansible Collectionss","text":""},{"location":"tooling/ansible/ansible-for-beginners/07_Ansible-Handlers-Roles-and-Collections.html#overview","title":"Overview","text":"<ul> <li>In the scenario that a large amount of multiple types of network devices are being managed, whilst Ansible provides some built-in modules, specialised modules and plugins can be accessed via collections.</li> <li>Collections are typically defined by <code>&lt;collection name&gt;.&lt;sub collection&gt;</code></li> <li> <p>Installation of collections is via <code>ansible-galaxy</code> i.e.: <code>ansible-galaxy collection install &lt;collection name&gt;</code></p> </li> <li> <p>Collections are packages of modules, roles, plugins, etc in a self-contained manner, designed for specific purposes.</p> </li> <li> <p>Community and vendor-created collections are available.</p> </li> <li> <p>Collections offer:</p> </li> <li>Expanded functionality</li> <li>Modularity and Reusability in playbooks, pone can define the collections used in a playbook as a list under <code>collections</code></li> <li>Similified distribution and management of playbooks, defining the required collections in a <code>requirements.yaml</code>, which can then be referenced when installing the required collections: <code>ansible-galaxy collection install -r requirements.yaml</code></li> </ul>"},{"location":"tooling/ansible/ansible-for-beginners/08_Advanced-Topics.html","title":"8.0 - Advanced Topics","text":""},{"location":"tooling/ansible/ansible-for-beginners/08_Advanced-Topics.html#111-advanced-topics","title":"11.1 - Advanced Topics","text":""},{"location":"tooling/ansible/ansible-for-beginners/08_Advanced-Topics.html#preparing-windows-server","title":"Preparing Windows Server","text":"<ul> <li>Ansible Control Machines can ONLY be Linux Machines</li> <li>This does not mean that Windows cannot be targets of Ansible</li> <li>Ansible can still connect to a Windows host by WinRM</li> <li>To allow this, the follwing requirements must be met on the control machine:</li> <li>pywinrm module installed - pip instlal \"pywinrm\u22650.2.2\"</li> <li>Setup WinRM - example scripts available online e.g. ConfigureRemotingForAnsible.ps1</li> <li>Use / Configure other methods of authentication e.g. Basic / Certificate / Kerberos</li> <li>Additional information available in the Windows Support section of the Ansible documentation.</li> </ul>"},{"location":"tooling/ansible/ansible-for-beginners/08_Advanced-Topics.html#ansible-galaxy","title":"Ansible-Galaxy","text":"<ul> <li>A free site for sharing and rating community-developed Ansible Roles</li> <li>You are free to download any existing roles via the ansible-galaxy CLI to integrate them into projects.</li> </ul>"},{"location":"tooling/ansible/ansible-for-beginners/08_Advanced-Topics.html#patterns","title":"Patterns","text":"<ul> <li>Have previously seen only Localhost as the target host for playbooks</li> <li>Alternative options are available:</li> <li>Host1, Host2, Host3</li> <li>Group1, Host1 (where host1 isn't part of group1)</li> <li>Host*</li> <li>*company.com</li> <li>Additional options are available via the Ansible documentation.</li> </ul>"},{"location":"tooling/ansible/ansible-for-beginners/08_Advanced-Topics.html#dynamic-inventory","title":"Dynamic Inventory","text":"<ul> <li>It's not always necessary to define information in inventory files</li> <li>If the project was to be integrated to a new environment, the inventory file would have to change completely.</li> <li>To overcome this, one can make an inventory Dynamic</li> <li>Instead of specifying the inventory.txt, you would specify a script called inventory.py</li> <li>Inventory.py reaches out to whatever sources are defined and returns their associated information</li> </ul> <p>https://docs.ansible.com/ansible/latest/dev_guide/developing_inventory.html</p>"},{"location":"tooling/ansible/ansible-for-beginners/08_Advanced-Topics.html#developing-custom-modules","title":"Developing Custom Modules","text":"<ul> <li>Modules already exist to perform specific actions like the user, file, etc.</li> <li>All of these are python modules</li> <li>Custom modules can be developed by building a python script in a particular format.</li> <li>Further information is available in the Ansible Documentation</li> </ul>"},{"location":"tooling/ansible/ansible-for-beginners/08_Advanced-Topics.html#112-project-introduction","title":"11.2 - Project Introduction","text":""},{"location":"tooling/ansible/ansible-for-beginners/08_Advanced-Topics.html#project-introduction","title":"Project Introduction","text":"<ul> <li>Project Aim: Use Ansible to automate the provisioning of the Kodekloud Store</li> <li>LAMP Stack Application (Linux - Apache - MySQL - PHP)</li> <li>Note: MariaDB will be used instead of MySQL</li> <li>Need to understand what we actually want to achieve.</li> <li>System:</li> <li>CentOS / Linux target machines<ul> <li>Need to ensure Firewall is configured appropriately or installed if not there</li> </ul> </li> <li>Apache HTTPD Server needs to be installed:<ul> <li>install httpd</li> <li>configure httpd</li> <li>configure Firewall to allow httpd</li> <li>Start httpd service</li> </ul> </li> <li>MariaDB needs to be set up and configured<ul> <li>Install MariaDB</li> <li>Configure MariaDB</li> <li>Start MariaDB</li> <li>Configure Firewall</li> <li>Configure Database</li> <li>Load data</li> </ul> </li> <li>PHP<ul> <li>Install PHP</li> <li>Configure Code</li> </ul> </li> <li>Configure any other system requirements</li> <li>For ease with the project, the steps will go:</li> <li>Install firewall (system)</li> <li>Install and setup MariaDB</li> <li>Install and Setup Apache HTTPD Server</li> <li>Download PHP Code and Run/Test it</li> </ul>"},{"location":"tooling/ansible/ansible-for-beginners/08_Advanced-Topics.html#firewall","title":"Firewall","text":"<pre><code>sudo yum install firewalld # install firewalld package\nsudo service firewalld start # start firewalld service\nsudo systemctl enable firewalld # enable the firewalld service\n</code></pre>"},{"location":"tooling/ansible/ansible-for-beginners/08_Advanced-Topics.html#mariadb","title":"MariaDB","text":"<pre><code>sudo yum install mariadb-server\nsudo vi /etc/my.cnf # configure file with right port\nsudo service mariadb start\nsudo systemctl enable mariadb\n## enable mariadb via firewall\nsudo firewall-cmd --permanent --zone=public --add-port=3306/tcp\nsudo firewall-cmd --reload\n## Configure the DB and setup user(s)\nmysql\nMariaDB &gt; CREATE DATABASE ecomdb;\nMariaDB &gt; CREATE USER 'ecomuser'@'localhost' IDENTIFIED BY 'ecompassword';\nMariaDB &gt; GRANT ALL PRIVILEGES ON *.* TO 'ecomuser'@'localhost';\nMariaDB &gt; FLUSH PRIVILEGES;\nmysql &lt; db-load-script.sql\n</code></pre>"},{"location":"tooling/ansible/ansible-for-beginners/08_Advanced-Topics.html#apache","title":"Apache","text":"<pre><code>sudo yum install -y httpd php php-mysql\nsudo firewall-cmd --permanent --zone=public --add-port=80/tcp\nsudo firewall-cmd --reload\n\nsudo vi /etc/httpd/conf/httpd.conf\n## configures DirectoryIndex to use index.php instead of index.html\n\nsudo service httpd start\nsudo systemctl enable httpd\n</code></pre>"},{"location":"tooling/ansible/ansible-for-beginners/08_Advanced-Topics.html#code","title":"Code","text":"<pre><code>sudo yum install -y git\ngit clone https://github.com/application.git /var/www/html\n##update index.php to use the right database address, name and credentials\ncurl http:://localhost # test code\n</code></pre>"},{"location":"tooling/ansible/ansible-for-beginners/08_Advanced-Topics.html#setup-variations","title":"Setup Variations","text":"<ul> <li>Could just run all of the above on a single node, however in practice, one would have a DB server and a web server.</li> <li>The MariaDB instructions are to be carried out on one target</li> <li>Apache and PHP-related operations on another</li> <li>Firewall operations will need to be ran on both</li> <li>In a multi-node setup, the index.php file needs to be configured a bit differently</li> <li>On the web server, configure with the IP address of the DB server</li> <li>On the DB server, supply the IP Address of the web server to ensure it is given sufficient permissions in the MariaDB commands</li> <li>As far as the code goes, the only modification will be to index.php at around line 107 as this contains details regarding the MariaDB connection</li> <li>Repo link kodekloudhub/learning-app-ecommerce</li> </ul>"},{"location":"tooling/ansible/ansible-for-beginners/09_Appendix.html","title":"9.0 - Appendix","text":""},{"location":"tooling/ansible/ansible-for-beginners/09_Appendix.html#021-setup-lab-install-virtualbox","title":"02.1 - Setup Lab: Install VirtualBox","text":""},{"location":"tooling/ansible/ansible-for-beginners/09_Appendix.html#notes","title":"Notes","text":"<ul> <li>The labs for this course and project are advised to be done on Virtualbox, the free Virtualisation tool.</li> <li>Whilst this program is free and can be downloaded from here, its UI, functionality, and performance pales in comparison to the likes of VMWare, which I'll be using for this.</li> <li>In reality, as long as you can run VMs on the tool and can make linked clones, you're good to go.</li> <li>Steps:</li> <li>Create a template VM from a CentOS base machine</li> <li>Create an Ansible Control Machine and two Target Machines for the Control Machine to apply configurations</li> <li>Once the template VM is setup, make sure that the network connection is set to bridged (VMNet0) and verify this appropriately.</li> </ul>"},{"location":"tooling/ansible/ansible-for-beginners/09_Appendix.html#022-setup-lab-clone-vms-and-install-ansible","title":"02.2 - Setup Lab: Clone VMs and Install Ansible","text":""},{"location":"tooling/ansible/ansible-for-beginners/09_Appendix.html#notes_1","title":"Notes","text":"<ul> <li>Now that a base template VM has been created, we can create clone VMs from this for the purposes of the practice labs.</li> <li>Linked clones are generally preferred as the only additional storage required for them is any changes made to them outside of the base VM e.g. installed packages.</li> <li>To create a linked clone, select \"manage\" from the VM library pane for the template VM, then follow the wizard to create a clone from the current state of the VM and make it a \"linked\" clone.</li> <li>Just creating \"ansible-target\" and \"ansible-controller\" for now, will add one more ansible target later.</li> <li>Verify that the IP addresses of the two machines are different via any of the following <code>ifconfig</code>, <code>hostname -i</code>, etc, and ssh into them from Powershell</li> <li>For ease of use, it's better to create a dedicated ssh session to these vms via a tool such as MobaXterm</li> <li>To change the hostname of the system (to make it reflect ansible-controller and ansible-target), edit /etc/hostname, and /etc/hosts as appropriate.</li> <li>for the latter, after the first \"localhost\" on each line, delete and replace with \"ansible-controller\" or ansible-target1\" as appropriate</li> <li>Create another linked clone and name it target2, repeating the process for target1.</li> <li>Can now install ansible on the ansible-controller VM, instructions can be found via the Ansible documentation.</li> <li> <p>For our CentOS VMs run</p> <p><code>bash sudo yum install epel-release sudo yum install ansible</code></p> </li> <li> <p>Verify that ansible is installed via:</p> </li> </ul> <pre><code>ansible --version\n</code></pre> <p>The output should be similar to:</p> <pre><code>ansible 2.9.27\nconfig file = /etc/ansible/ansible.cfg\nconfigured module search path = [u'/home/nstephenson/.ansible/plugins/modules', u'/usr/share/ansible/plugins/modules']\nansible python module location = /usr/lib/python2.7/site-packages/ansible\nexecutable location = /usr/bin/ansible\npython version = 2.7.5 (default, Oct 14 2020, 14:45:30) [GCC 4.8.5 20150623 (Red Hat 4.8.5-44)]\n</code></pre>"},{"location":"tooling/ansible/automating-ec2-deployment/01_introduction-to-cloud-automation.html","title":"1.0 - About AWS EC2 and Ansible Automation","text":""},{"location":"tooling/ansible/automating-ec2-deployment/01_introduction-to-cloud-automation.html#overview","title":"Overview","text":"<ul> <li>Cloud deployments have many benefits, including:</li> <li>Rapid scalability</li> <li>Deployment of fault-tolerant and highly available solutions</li> <li>To leverage these benefits, one can use tools like Terraform and Ansible to rapidly deploy and configure servers</li> <li> <p>Typically provisioning is left to Terraform and configuration is done via Ansible, but Ansible can do both.</p> </li> <li> <p>Provisioning via Ansible ensures fast, repeatable, compliant, and automatic deployment of systems, and can make it easier to apply updates and improvements quickly.</p> </li> <li> <p>By automating deployments, one can reduce errors when:</p> </li> <li>Deploying to different regions</li> <li>deploying version upgrades</li> <li> <p>there is a long pause between deployments</p> </li> <li> <p>Defining automation via ansible also reduced human error.</p> </li> <li> <p>Ansible offers many modules to dynamically provision workloads. In the following notes, tasks covered include:</p> </li> <li>Preparing AWS EC2 account and credentials</li> <li>Automatically provision VPC (Virtual Private Cloud) networking</li> <li>Provisioning and deprovisioning cloud instances with Ansible</li> <li>Finding and selecting AMI images</li> <li>Dynamically adding EC2 instances to the Ansible inventory for further configuration and management tasks to be applied.</li> </ul>"},{"location":"tooling/ansible/automating-ec2-deployment/02_planning-and-preparing-for-automation.html","title":"2.0 - Planning and Preparing for Automation","text":""},{"location":"tooling/ansible/automating-ec2-deployment/02_planning-and-preparing-for-automation.html#21-planning-for-the-ec2-deployment","title":"2.1 - Planning for the EC2 Deployment","text":"<ul> <li>Prior to any deployment, one should always design and plan the resources / architecture required.</li> <li>If deploying to the cloud, there's always guidance provided.</li> </ul>"},{"location":"tooling/ansible/automating-ec2-deployment/02_planning-and-preparing-for-automation.html#aws-terminology","title":"AWS Terminology","text":"<ul> <li>VMs in AWS = Instances</li> <li>Instance types define the resource capacity of the instance e.g. memory, cpu, etc.</li> <li>Amazon Machine Image (AMIs) provide templates for the base OS and configuration</li> <li>Virtual Private Cloud = Virtual Network - Isolated from AWS Cloud</li> <li>Security Groups control firewall rules for the instances and associated resources</li> <li>Tags = Metadata used to label resources.</li> </ul>"},{"location":"tooling/ansible/automating-ec2-deployment/02_planning-and-preparing-for-automation.html#deployment-process-overview","title":"Deployment Process Overview","text":"<ul> <li>Create a network:</li> <li>VPC</li> <li>Internet Gateway</li> <li>Public Subnet</li> <li>Routing Table</li> <li> <p>Security Group</p> </li> <li> <p>Create a RHEL 8 EC2 Instance</p> </li> <li>Find the RHEL 8 AMI needed</li> <li>Create an SSH key for provisioning</li> <li> <p>Launch the instance using the AMI</p> </li> <li> <p>Save the EC2 Instance as an Image (if desired)</p> </li> <li>Tear down the EC2 instances</li> </ul>"},{"location":"tooling/ansible/automating-ec2-deployment/02_planning-and-preparing-for-automation.html#planning-the-deployment","title":"Planning the Deployment","text":"<ul> <li>Questions to answer include:</li> <li>How's the network setup?</li> <li>How many instances are needed, what type?</li> <li>Can standard AMIs be used? Or do they need to be customized?</li> <li>Resource requirements?</li> <li>Users, passwords, keys, etc needed for access to the deployment?</li> </ul>"},{"location":"tooling/ansible/automating-ec2-deployment/02_planning-and-preparing-for-automation.html#22-preparing-the-ansible-control-node","title":"2.2 - Preparing the Ansible Control Node","text":""},{"location":"tooling/ansible/automating-ec2-deployment/02_planning-and-preparing-for-automation.html#preparing-the-control-node","title":"Preparing the Control Node","text":"<ul> <li>Can be any Linux machine with Python 2 or 3 installed</li> <li>Recommended to keep it outside of AWS to provision everything.</li> <li> <p>Just needs Ansible to be installed.</p> </li> <li> <p>On a RHEL or CentOS-based system, you're advised to install via <code>pip</code>, the package manager for Python:</p> </li> <li>Install pip: <code>sudo yum install python3-pip</code></li> <li> <p>Install Ansible 2.9 and AWS module dependencies: <code>sudo pip3 install ansible boto boto3</code></p> </li> <li> <p>For AWS, ensure that a specific IAM user has been created for Ansible's usage - allowing you to limit the access Ansible has to the account.</p> </li> <li>Ensure Access Key and Secret Key are saved, and the user has programmatic access.</li> <li> <p>Recommended actions:</p> <ul> <li>Add user to a group and give permissions for EC2 and VPC \"Full Access\" only (principle of least privilege)</li> <li>Add tags for further ease of identification.</li> </ul> </li> <li> <p>It's recommended to prepare a variables file to store certain values to facilitate the Ansible Plays, unless these are sensitive, store them in a standard <code>yaml</code> vars file.</p> </li> <li> <p>If sensitive data, store as an environment variable or as an encrypted value in ansible-vault. This can be applied to a full variable file via <code>ansible-vault encrypt vars/info.yaml</code></p> </li> <li> <p>Sample vars suggested for the play(s):</p> </li> <li><code>aws_id</code> (Access key)</li> <li><code>aws_key</code> (Secret key)</li> <li><code>aws_region</code></li> <li><code>ssh_keyname</code></li> <li><code>remote_user</code> (IAM user)</li> </ul>"},{"location":"tooling/ansible/automating-ec2-deployment/03_deploying-into-ec2.html","title":"3.0 - Deploying into EC2","text":""},{"location":"tooling/ansible/automating-ec2-deployment/03_deploying-into-ec2.html#31-provisioning-the-vpc","title":"3.1 - Provisioning the VPC","text":"<ul> <li>VPC = logically isolated virtual network across an AWS region containing the whatever resources you want.</li> <li> <p>Protects instances from network intrusion and allows control of network traffic to/from resources.</p> </li> <li> <p>Base playbook (no tasks):</p> </li> </ul> <pre><code>- name: Start\n  hosts: localhost\n  remote_user: testuser\n  gather_facts: false\n  vars_Files:\n  - vars/info.yml\n</code></pre> <ul> <li>Tasks-wise, each AWS resource provisioned will require its own module, a breakdown follows:</li> </ul> Task Module Configure the VPC <code>ec2_vpc_net</code> Configure the Internet Gateway <code>ec2_vpc_igw</code> Configure the public subnet <code>ec2_vpc_subnet</code> Configure a routing table <code>ec2_vpc_route_table</code> Configure a security group <code>ec2_group</code>"},{"location":"tooling/ansible/automating-ec2-deployment/03_deploying-into-ec2.html#vpc-configuration","title":"VPC Configuration","text":"<ul> <li>Example task:</li> </ul> <pre><code>tasks:\n  - name: Create a VPC\n    ec2_vpc_net:\n      aws_access_key: \"{{ aws_access_key }}\"\n      aws_secret_key: \"{{ aws_secret_key }}\"\n      region: \"{{ aws_region }}\"\n      name:  test_vpc_net\n      cidr_block: 10.10.0.0/16\n      tags:\n        module: ec2_vpc_net\n      tenancy: default\n    register: ansibleVPC\n\n  - name: debug VPC\n    debug:\n      var: ansibleVPC\n</code></pre> <ul> <li>Anything with <code>\"{{ }}\"</code> is a variable loaded from the variables.</li> <li>Note: Each variable will have its own required parameters, in the case of above, it;s <code>name</code> and <code>cidr_block</code></li> <li>If tenancy's set to <code>default</code>, new instances run on shared hardware. If <code>dedicated</code>, new instances will run on single-tenant hardware.</li> <li> <p>The result / output of the task creating the VPC is registered to a new variable <code>ansibleVPC</code>, which is then passed to the debug module to inspect the task.</p> </li> <li> <p>Note: - One can check the playbook syntax with <code>ansible-playbook --syntax-check &lt;playbook&gt;.yaml</code> in the directory of the playbook</p> </li> </ul>"},{"location":"tooling/ansible/automating-ec2-deployment/03_deploying-into-ec2.html#creating-the-internet-gateway","title":"Creating the Internet Gateway","text":"<ul> <li>Advised parameters:</li> <li><code>aws_access_key</code> - (as before)</li> <li><code>aws_secret_key</code> - (as before)</li> <li><code>region</code> - (pass from variable)</li> <li><code>ec2_url</code> - The URL to use to connect to EC2.</li> <li><code>state</code> - should the Internet Gateway be present or absent</li> <li><code>tags</code></li> <li> <p><code>vpc_id</code> - VPC ID for the IGW to be associated with, obtainable via the registered variable from the VPC task via <code>ansibleVPC['vpc']['id']</code> or <code>ansibleVPC.vpc.id</code></p> </li> <li> <p>Example usage:</p> </li> </ul> <pre><code>- name: Create the Internet Gateway\n  ec2_vpc_igw:\n    aws_access_key: \"{{ aws_access_key }}\"\n    aws_secret_key: \"{{ aws_secret_key }}\"\n    region: \"{{ aws_region }}\"\n    state: present\n    vpc_id: \"{{ ansibleVPC.vpc.id }}\"\n    tags:\n      name: ansibleVPC_IGW\n  register: ansibleVPC_igw\n\n- name:\n  debug:\n    var: ansibleVPC_igw\n</code></pre>"},{"location":"tooling/ansible/automating-ec2-deployment/03_deploying-into-ec2.html#creating-the-subnets","title":"Creating the Subnet(s)","text":"<ul> <li>Required parameter:  <code>vpc_id</code> - reference in a similar manner to how it was in the internet gateway task.</li> <li>Example Usage:</li> </ul> <pre><code>- name: Create the Public Subnet\n  ec2_vpc_subnet:\n    aws_access_key: \"{{ aws_access_key }}\"\n    aws_secret_key: \"{{ aws_secret_key }}\"\n    region: \"{{ aws_region }}\"\n    state: present\n    cidr: 10.10.0.0/16\n    vpc_id: \"{{ ansibleVPC.vpc.id }}\"\n    map_public: yes\n    tags:\n      name: public_subnet\n  register: public_subnet\n\n- name: show public subnet details\n  debug:\n    var: public_subnet\n</code></pre> <ul> <li>Note: - Usage of the <code>map_public</code> parameter assigns instances a public ip address by default</li> <li>The results of the task are registered to the <code>public_subnet</code> variable for use later in the play</li> </ul>"},{"location":"tooling/ansible/automating-ec2-deployment/03_deploying-into-ec2.html#creating-the-routing-table","title":"Creating the Routing Table","text":"<ul> <li>Requires the ID of the VPC and IGW, which can be referenced via the registered variable defined with each task</li> <li>Example usage:</li> </ul> <pre><code>- name: create a new route table for pulbic subnet\n  ec2_vpc_route_Table:\n    aws_access_key: \"{{ aws_access_key }}\"\n    aws_secret_key: \"{{ aws_secret_key }}\"\n    region: \"{{ aws_region }}\"\n    state: present\n    vpc_id: \"{{ ansibleVPC.vpc.id }}\"\n    tags:\n      name: rt_ansibleVPC_PublicSubnet\n    subnets:\n    - \"{{ public_subnet.subnet.id }}\"\n    routes:\n    - dest: 0.0.0.0\n      gateway_id: \"{{ ansibleVPC_igw.gateway_id }}\"\n\n- name: display public route table\n  debug:\n    var: rt_ansibleVPC_PublicSubnet\n</code></pre> <ul> <li><code>routes</code> defines a list of routes to be added to the route table</li> <li>each route in the list is a dictionary comprising of, at minimum:<ul> <li><code>dest</code> - the networking being routed to, <code>0.0.0.0</code> is the default</li> <li><code>gateway_id</code> is the ID of the IGW for the route to be associated with the route.</li> </ul> </li> </ul>"},{"location":"tooling/ansible/automating-ec2-deployment/03_deploying-into-ec2.html#creating-the-security-group","title":"Creating the Security gROUP","text":"<ul> <li>Example Usage:</li> </ul> <pre><code>- name: Create Security Group\n  ec2_group:\n    aws_access_key: \"{{ aws_access_key }}\"\n    aws_secret_key: \"{{ aws_secret_key }}\"\n    region: \"{{ aws_region }}\"\n    name: \"Test Security Group\"\n    description: \"Test Security Group\"\n    vpc_id: \"{{ ansibleVPC.vpc.id }}\"\n    tags:\n      name: Test Security Group\n    rules:\n    - proto: \"tcp\"\n      ports: \"22\"\n      cidr_ip: 0.0.0.0/0\n  register: my_vpc_sg\n\n- name: Set security group ID as variable\n  set_fact:\n    sg_id: \"{{ my_vpc_sg.group_id }}\"\n</code></pre>"},{"location":"tooling/ansible/automating-ec2-deployment/03_deploying-into-ec2.html#32-provisioning-ec2-instances","title":"3.2 - Provisioning EC2 Instances","text":"<ul> <li>Steps to create the EC2 Instance:</li> <li>Specify the AMI desired</li> <li>Declare the instance type</li> <li>Associate an SSH key with the instance</li> <li>Attach a security group</li> <li>Attach a subnet</li> <li> <p>Assign a public IP address</p> </li> <li> <p>Upon creation, you can use other Ansible modules to provision and configure it further, e.g. deploy an application on it.</p> </li> </ul>"},{"location":"tooling/ansible/automating-ec2-deployment/03_deploying-into-ec2.html#finding-an-existing-ami","title":"Finding an Existing AMI","text":"<ul> <li>AWS has many AMIs available for use, their IDs typically vary from region to region -&gt; need to programmatically determine what's required for the deployment.</li> <li> <p>One can leverage the <code>ec2_ami_info</code> module (formerly <code>ec2_ami_facts</code>)</p> </li> <li> <p>Example Usage:</p> </li> </ul> <pre><code>- name: Find AMIs published by Red Hat (309956199498) that are Non-beta and x86 architecture\n  ec2_ami_info:\n    aws_access_key: \"{{ aws_access_key }}\"\n    aws_secret_key: \"{{ aws_secret_key }}\"\n    region: \"{{ aws_region }}\"\n    owners: 309956199498\n    filters:\n      architecture: x86_64\n      name: RHEL-8*HVM-*\n  register: amis\n\n- name: Show the AMIs\n  debug:\n    var: amis\n\n- name: Get the latest AMI from the list provided:\n  set_fact:\n    latest_ami: \"{{ amis.images | sort(attribute='creation_date') | last }}\"\n</code></pre> <ul> <li><code>filter</code> dictionary filters the list of amis returned that are owned by the specified <code>owners</code></li> <li>The list is registered as <code>amis</code></li> <li>The <code>set_fact</code> task then filters the list of images for the one with the most recent creation date and saves it as a new variable</li> </ul>"},{"location":"tooling/ansible/automating-ec2-deployment/03_deploying-into-ec2.html#creating-the-ssh-key-pair-for-the-ec2-instance","title":"Creating the SSH Key Pair for the EC2 Instance","text":"<ul> <li>If you don't already have a key pair, one can be created via the <code>ec2_key</code> module.</li> <li>For EC2 instance, an SSH key located in the same region must be used to ensure secure credential management.</li> <li>Requires a <code>name</code> (SSH key name defined in vars file)</li> <li> <p>The <code>copy</code> module can be used to save the private key locally.</p> </li> <li> <p>Example Usage:</p> </li> </ul> <pre><code>- name: Create SSH Key\n  ec2_key:\n    aws_access_key: \"{{ aws_access_key }}\"\n    aws_secret_key: \"{{ aws_secret_key }}\"\n    region: \"{{ aws_region }}\"\n    name: \"{{ ssh_keyname }}\"\n  register: ec2_key_result\n\n- name: Save private key\n  copy:\n    content: \"{{ ec2_key_result.key.private_key }}\"\n    dest: \"./demo_key.pem\"\n    mode: 0600\n  when: ec2_key_result.changed\n</code></pre>"},{"location":"tooling/ansible/automating-ec2-deployment/03_deploying-into-ec2.html#create-the-ec2-instance","title":"Create the EC2 Instance","text":"<ul> <li>Leverages the data from previous tasks, in particular:</li> <li><code>image: \"{{ latest_ami.image_id }}\"</code></li> <li><code>group_id: \"{{ my_vpc_sg.group_id }}\"</code></li> <li> <p><code>vpc_subnet_id: \"{{ public_subnet.subnet.id }}\"</code></p> </li> <li> <p>Example Usage:</p> </li> </ul> <pre><code>- name: Provision the EC2 Instance\n  ec2:\n    aws_access_key: \"{{ aws_access_key }}\"\n    aws_secret_key: \"{{ aws_secret_key }}\"\n    region: \"{{ aws_region }}\"\n    image: \"{{ latest_ami.image_id }}\"\n    instance_type: t2.micro\n    key_name: \"{{ ssh_keyname }}\"\n    count: 2\n    state: present\n    group_id: \"{{ my_vpc_sg.group_id }}\"\n    wait: yes\n    vpc_subnet_id: \"{{ public_subnet.subnet.id }}\"\n    assign_public_ip: yes\n    instance_tags:\n      name: new_demo_template\n  register: ec2info\n\n- name: print the results:\n  debug:\n    var: ec2info\n</code></pre> <ul> <li>Once created, one can verify the EC2 instance operation via the AWS Console, SSH via the key pair and/or connect via the AWS Console.</li> </ul>"},{"location":"tooling/ansible/automating-ec2-deployment/04_generating-an-inventory-of-ec2-instances.html","title":"4.0 - Generating an Inventory of EC2 Instances","text":""},{"location":"tooling/ansible/automating-ec2-deployment/04_generating-an-inventory-of-ec2-instances.html#41-using-a-dynamic-inventory-script","title":"4.1 - Using a Dynamic Inventory Script","text":"<ul> <li>At small resource numbers, management is easy. In practice, one could be dealing with anywhere from  10,000 - 100,000 instances (or more).</li> <li>Each of these instances may have frequently changing IPs (either they're spun up on-demand or frequently autoscaled)</li> <li>To help manage these, one can utilise <code>EC2.py</code> and <code>EC2.ini</code> files, which leverage the AWS CLI.</li> <li><code>EC2.py</code> is a script using the Boto EC2 library, which queries AWS for any particular running EC2 instances for a given account.</li> <li> <p><code>EC2.ini</code> acts as configuration for <code>EC2.py</code> and is used to limit the scope of Ansible's reach.</p> </li> <li> <p>Environment variables need to be set for these files to be leveraged:</p> </li> <li><code>export ANSIBLE_HOSTS=/working_dir/ec2.py</code></li> <li> <p>Make the file executable: <code>chmod +x ec2.py</code></p> </li> <li> <p>If the <code>ec2.ini</code> is in a different location to the <code>.py</code> script, run <code>export ANSIBLE_HOSTS=/working_dir/ec2.ini</code></p> </li> <li> <p>The <code>ec2.ini</code> file will have default configurations read by the <code>ec2.py</code> file, there's a specifier for <code>regions = all</code> which can be commented out to save time for one-region deployments.</p> </li> <li> <p>Export <code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code> as env vars if required, and verify authentication with <code>./ec2.py --list</code></p> </li> <li> <p>If <code>~/.aws/credentials</code> exists, then the keys associated with the selected profile will work / be used.</p> </li> <li> <p>Output will be returned in JSON, with the IP(s) and other relevant metadata.</p> </li> <li>By default, <code>ec2.ini</code> is configured for running Ansible from outside EC2.</li> <li>If running from within EC2 with an internal DNS, one should modify the <code>desination</code> variable</li> <li>For VPC instances, <code>vpc_destination_variable</code> allows usage of whatever value of <code>boto.ec2.instance_variable</code> makes the most sense</li> </ul> <ul> <li>Note - The links have now been deprecated, This is now an AWS Plugin! Refer to guidance on how to use</li> <li>https://docs.ansible.com/ansible/latest/collections/amazon/aws/aws_ec2_inventory.html</li> <li>https://docs.ansible.com/ansible/latest/collections/amazon/aws/docsite/aws_ec2_guide.html</li> <li>https://devopscube.com/setup-ansible-aws-dynamic-inventory/</li> </ul>"},{"location":"tooling/ansible/automating-ec2-deployment/04_generating-an-inventory-of-ec2-instances.html#using-a-dynamic-inventory-script","title":"Using a Dynamic Inventory Script","text":"<ul> <li>The EC2 external inventory can map to instances using methods including:</li> <li>Instance ID</li> <li>Region</li> <li>Availability Zone</li> <li> <p>Security Group</p> </li> <li> <p>Instance variables retrieved by the script are prefixed with <code>ec2_</code> - variables include region, ip address, owner ID, and vpc id.</p> </li> <li> <p>Red Hat Ansible Tower 3.3 uses this mechanism with a graphical frontend to generate inventory information from EC2. This is generally easier to use but as powerful as the CLI.</p> </li> </ul>"},{"location":"tooling/ansible/automating-ec2-deployment/04_generating-an-inventory-of-ec2-instances.html#42-using-the-aws-ec2-inventory-plugin","title":"4.2 - Using the AWS EC2 Inventory Plugin","text":""},{"location":"tooling/ansible/automating-ec2-deployment/04_generating-an-inventory-of-ec2-instances.html#inventory-plugins","title":"Inventory Plugins","text":"<ul> <li>Allow users to point at data sources to compile the inventory of hosts that Ansible uses in playbooks</li> <li> <p>Inventory plugins take advantage of the most recent updates to Ansible core code -&gt; Recommended for use over scripts for dynamic inventory.</p> </li> <li> <p>Plugins are specified via the CLI using <code>ansible-inventory -i</code>. The <code>inventory</code> path can be defaulted by the inventory path in the <code>ansible.cfg</code> file in <code>[defaults]</code> or the <code>ANSIBLE_INVENTORY</code> environment variable.</p> </li> <li> <p>Example usage: <code>ansible-inventory -i aws_ec2.yaml --graph</code></p> </li> <li> <p>The plugin gets inventory hosts from AWS EC2. Example usage:</p> </li> </ul> <pre><code>plugin: aws_ec2\nregions:\n    - us-east-2\nkeyed_groups:\n    # add hosts to tag_Name_value groups for each aws_ec2 host's tags.Name variable\n    - key: tags.Name\n      prefix: tag_Name\n      separator: \"\"\ngroups:\n  # add hosts to the group development if any of the dictionarys keys or values is the word 'dev'\n  development: \"'dev' in tags.env\"\ncompose:\n  # set the env variable to the value of the env tag\n  env: tags.env\n</code></pre>"},{"location":"tooling/ansible/automating-ec2-deployment/05_building-custom-amis.html","title":"5.0 - Building Custom AMIs with Ansible","text":""},{"location":"tooling/ansible/automating-ec2-deployment/05_building-custom-amis.html#build-amis","title":"Build AMIs","text":"<ul> <li>To gather info around the AMI suitable for usage within the region and deployment, use the <code>ec2_ami_info</code> module.</li> <li> <p>This can filter by parameters such as:</p> <ul> <li>region</li> <li>owners</li> <li>architectures</li> </ul> </li> <li> <p>Once the AMI is determined, the EC2 instance can be deployed via the ec2 module.</p> </li> <li> <p>The <code>ec2_ami</code> module can then register or deregister AMIs, this can be applied to a running EC2 instance.</p> </li> <li>Example usage:</li> </ul> <pre><code>- name: Create an AMI\n  ec2_ami:\n    aws_access_key: \"{{ aws_access_key }}\"\n    aws_secret_key: \"{{ aws_secret_key }}\"\n    region: \"{{ aws_region }}\"\n    instance_id: \"{{ ec2info.instance_ids[0] }}\"\n    wait: yes\n    name: \"pluralsight-{{ uuid }}\"\n    tags:\n      Name: \"pluralsight-{{ uuid }}\"\n      Service: TestService\n</code></pre>"},{"location":"tooling/ansible/automating-ec2-deployment/05_building-custom-amis.html#disclaimer","title":"Disclaimer","text":"<ul> <li>Whilst this IS achievable via Ansible, it's recommended to use a tool specifically designed to create custom AMIs, such as Hashicorp Packer. Packer can then run ansible playbooks to do the configuration.</li> </ul>"},{"location":"tooling/ansible/automating-ec2-deployment/06_cleanup.html","title":"6.0 - Cleaning up Resources","text":""},{"location":"tooling/ansible/automating-ec2-deployment/06_cleanup.html#overview","title":"Overview","text":"<ul> <li> <p>It's beyond recommended to cleanup cloud resources to avoid incurring charges for unused resources.</p> </li> <li> <p>Typically in this scenario, resources should be removed in the order of creation, each will require modules used to create them. Most of the time will be the same, with <code>state: absent</code> added instead of <code>present</code></p> </li> <li> <p>For any resources that need to be \"found\" before cleanup, modules of similar name with <code>_info</code> suffixed can be used e.g.:</p> <ul> <li><code>ec2_instance_info</code></li> <li><code>ec2_vpc_net_info</code></li> <li><code>ec2_vpc_route_info</code></li> </ul> </li> <li> <p>Further information can be found via the Ansible docs.</p> </li> <li>Example EC2 termination playbook:</li> </ul> <pre><code>- name: AWS EC2 Termination\n  hosts: localhost\n  gather_facts: false\n\n  vars_files:\n  - vars/info.yaml\n  - vars/instance_ids.yaml\n\n  tasks:\n  - name: Terminate Instances\n    hosts: localhost\n    connection: local\n    tasks:\n    - name: Terminate instances that were previously launched\n      ec2:\n        state: 'absent'\n        instance_ids: '{{ instance_ids }}'\n</code></pre>"},{"location":"tooling/argocd/getting-started-with-argocd.html","title":"Getting Started with Argo CD","text":"","tags":["Tooling","ArgoCD"]},{"location":"tooling/argocd/getting-started-with-argocd.html#10-introduction","title":"1.0 - Introduction","text":"","tags":["Tooling","ArgoCD"]},{"location":"tooling/argocd/getting-started-with-argocd.html#11-containers-and-kubernetes-overview","title":"1.1 - Containers and Kubernetes Overview","text":"","tags":["Tooling","ArgoCD"]},{"location":"tooling/argocd/getting-started-with-argocd.html#containers","title":"Containers","text":"<ul> <li>An abstraction of the application layer in an isolated user-space instance</li> <li>Share the OS Kernel Storage and Networking from the host they run on</li> <li>Can be thought of like the core components needed for an application running as a process.</li> <li>Allow the packaging of an application and its dependencies in one instance.</li> <li>Allows engineers to develop applications into a repeatable and lightweight manner.</li> <li> <p>Allows applications to integrate more easily with CI/CD pipelines, providing greater flexibility and agility.</p> </li> <li> <p>Because the containers share the OS Kernel storage, etc. and do not require a hypervisor or leverage a guest operating system, they are more lightweight and easier to deploy than virtual machines.</p> </li> </ul> <p></p> <ul> <li>If using Docker as the container runtime, common commands include:</li> <li>Login to Dockerhub: <code>docker login localhost</code></li> <li>Run a hello world container: <code>docker run hello world</code></li> </ul>","tags":["Tooling","ArgoCD"]},{"location":"tooling/argocd/getting-started-with-argocd.html#kubernetes","title":"Kubernetes","text":"<ul> <li>Open-source container orchestration platform.</li> <li>Used to automate, deploy, manage and scale workloads</li> <li>Abstracts the complexity of a multi-container environment</li> <li>Combines compute, networking and storage components that hundreds (or more) containers rely on.</li> <li>Works on a declarative management model - users describe the desired configuration for deployment.</li> <li>It's this which ArgoCD aims to enforce.</li> </ul>","tags":["Tooling","ArgoCD"]},{"location":"tooling/argocd/getting-started-with-argocd.html#12-helm-and-kustomize-overview","title":"1.2 - Helm and Kustomize Overview","text":"","tags":["Tooling","ArgoCD"]},{"location":"tooling/argocd/getting-started-with-argocd.html#helm","title":"Helm","text":"<ul> <li>Package manager for Kubernetes - apps are packaged into charts for deployments</li> <li>A deployment tool for automating creation, pacakging, configuration and deployment of apps and configurations to Kubernetes clusters.</li> </ul>","tags":["Tooling","ArgoCD"]},{"location":"tooling/argocd/getting-started-with-argocd.html#kustomize","title":"Kustomize","text":"<ul> <li>A standalone tool to customize the creation of Kubernetes objects through a file called <code>kustomization.yaml</code></li> <li>A template-free way to customize application configuration that is built into kubectl.</li> <li>Traverses a Kubernetes manifest to add, remove or update configuration options without forking or actual YAML files.</li> </ul> <p>Kustomize Structure:</p> <ul> <li>kustomization.yaml</li> </ul> <pre><code>bases:\n  - ldap\npatches:\n  - patch.yaml\n...\n</code></pre> <ul> <li>base - ldap</li> </ul> <pre><code>apiVersion: v1beta2\nkind: Deployment\nmetadata:\n  name: ldap\n  labels:\n    app: ldap\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ldap\n  template:\n    metadata:\n      lables:\n        app: ldap\n  spec:\n    containers:\n    ...\n    volumes:\n    ...\n</code></pre> <ul> <li>patch.yaml - staging</li> </ul> <pre><code>apiVersion: apps/v1beta2\nkind: Deployment\nmetadata:\n  name: ldap\nspec:\n  replicas: 2\n</code></pre> <ul> <li>patch.yaml - prod</li> </ul> <pre><code>apiVersion: apps/v1beta2\nkind: Deployment\nmetadata:\n  name: ldap\nspec:\n  replicas: 6\n  template:\n    spec:\n      volumes:\n        - name: ldap-data\n          emptyDir: null\n</code></pre> <ul> <li>Sample folder structure:</li> </ul> <p></p>","tags":["Tooling","ArgoCD"]},{"location":"tooling/argocd/getting-started-with-argocd.html#13-gitops-core-concepts","title":"1.3 - GitOps Core Concepts","text":"<ul> <li>GitOps:</li> <li>An operating model pattern for cloud native and Kubernetes-based applications</li> <li> <p>Declarative infrastructure code in Git is used as the single source of truth for automated continuous delivery.</p> </li> <li> <p>GitOps Principles</p> </li> <li>Git acts as the source of truth for the entire system</li> <li>The desired system state is versioned in Git</li> <li>System state is described declaratively</li> <li>Git is the single place for operations, such as create, change and delete</li> <li>Autonomous Agents enforce the desired state, and alert on drift - ArgoCD is an example of this</li> <li> <p>Delivery of approved system state changes is done so in an automated manner</p> </li> <li> <p>GitOps Architecture Components:</p> </li> <li>Source control system (GitHub, GitLab, etc.)</li> <li>Git repository</li> <li>Container / Helm registry</li> <li>Operator e.g. Flux, ArgoCD</li> <li>Runtime Environment e.g. Kubernetes Cluster(s)</li> <li> <p>Namespaces - could be per environment, per app, service, build, etc.</p> </li> <li> <p>Use Cases:</p> </li> <li>Cloud Native App Management i.e. CD</li> <li>Service Rollouts</li> <li> <p>Infrastructure management e.g. clusters and microservices</p> </li> <li> <p>GitOps Operators Examples:</p> </li> <li>Flux</li> <li>ArgoCD</li> <li>Kubestack</li> </ul>","tags":["Tooling","ArgoCD"]},{"location":"tooling/argocd/getting-started-with-argocd.html#14-argocd-overview","title":"1.4 - ArgoCD Overview","text":"<ul> <li>Argo Project supports 4 main solutions:</li> <li>Argo Workflows - A workflow engine for orchestrating parallel jobs in Kubernetes</li> <li>ArgoCD - A declarative, continuous delivery GitOps operator for Kuberentes</li> <li>Argo Rollouts - Supports advanced deployments e.g. blue-green, canary</li> <li> <p>Argo Events - Event-driven workflow automation framework for Kubernetes</p> </li> <li> <p>ArgoCD:</p> </li> <li>A GitOps operator that provides continuous delivery for Kubernetes</li> <li> <p>Has application controller to continuously monitor apps running on Kubernetes, comparing the live state to what was defined in the Git repository.</p> </li> <li> <p>Key Features:</p> </li> <li>Web UI</li> <li>Automated Application Deployment</li> <li> <p>Health Status Monitoring for Apps and associated resources</p> </li> <li> <p>Typical CD Workflow with ArgoCD:</p> </li> <li>ArgoCD Operator pulls app configurations from Git repo &amp; deploys the app in a Kubernetes cluster</li> <li>New app feature code committed and a pull request is submitted to the git repo to modify app development</li> <li>Pull request is merged with new code into the main repo</li> <li>ArgoCD Operator does a pull to the Git Repo, acknowledges the changes, and updates the app(s) in the Kubernetes cluster.</li> </ul>","tags":["Tooling","ArgoCD"]},{"location":"tooling/argocd/getting-started-with-argocd.html#15-argocd-core-concepts-and-architectures","title":"1.5 - ArgoCD Core Concepts and Architectures","text":"","tags":["Tooling","ArgoCD"]},{"location":"tooling/argocd/getting-started-with-argocd.html#architecture","title":"Architecture","text":"<ul> <li>API Server</li> <li>A gRPC/REST server that exposes the API consumed by the Web UI, CLI, and CI/CD systems.</li> <li>Repository Server:</li> <li>Any internal service that maintains a local cache of the git repo holding the app manifests</li> <li>Application Controller:</li> <li>A kubernetes controller that continuousl monitoers running apps, and compares their current state to that of the desired state.</li> </ul>","tags":["Tooling","ArgoCD"]},{"location":"tooling/argocd/getting-started-with-argocd.html#components","title":"Components","text":"<ul> <li>argocd-dex-server:</li> <li>ArgoCD Embeds and bundles Dex as part of its installation - this delegates authentication to an external identity provider and handling SSO.</li> <li>argocd-metrics / argocd-server-metrics:</li> <li>Exposes Application metrics and API Server metrics to be scraped by Prometheus</li> <li>argocd-redis:</li> <li>Used to support caching for working with the ArgoCD repository server</li> <li>argocd-repo-server:</li> <li>Clones the git repository, keeping it up to date and generating manifests using the appropirate tool</li> <li>argocd-server:</li> <li>Runs the ArgoCD API Server</li> </ul>","tags":["Tooling","ArgoCD"]},{"location":"tooling/argocd/getting-started-with-argocd.html#16-supported-tooling-with-argocd","title":"1.6 - Supported Tooling with ArgoCD","text":"<ul> <li>ArgoCD supports sever different ways in which Kubernetes manifests can be defined, including:</li> <li>Kustomize</li> <li>Helm</li> <li>Jenkins</li> <li> <p>Jsonnet</p> </li> <li> <p>ArgoCD Tool Detection</p> </li> <li>When a new app is created in ArgoCD can detect the tooling used to create the app.</li> <li>Example: Helm by checking for a Chart.yaml</li> <li>Example: Kustomization by checking for a <code>kustomization.yaml</code> file.</li> </ul>","tags":["Tooling","ArgoCD"]},{"location":"tooling/argocd/getting-started-with-argocd.html#20-installation-and-operation-of-argocd","title":"2.0 - Installation and Operation of ArgoCD","text":"","tags":["Tooling","ArgoCD"]},{"location":"tooling/argocd/getting-started-with-argocd.html#21-requirements","title":"2.1 - Requirements","text":"<ul> <li>Kubectl needs to be installed</li> <li>A kubernetes cluster with:</li> <li>Cluster admin level access</li> <li>Kubeconfig configured to connect to the cluster</li> <li>Access to GitHub (or other source control)</li> </ul>","tags":["Tooling","ArgoCD"]},{"location":"tooling/argocd/getting-started-with-argocd.html#22-deploying-argocd","title":"2.2 - Deploying ArgoCD","text":"<ul> <li>Types of installs:</li> <li>Non-High-Availability:<ul> <li>Recommended for dev / testing</li> <li>Deploys single pods and replicas for ArgoCD components</li> </ul> </li> <li>High-Availability:<ul> <li>Manifest = <code>namespace-install.yaml</code></li> <li>Recommended for prod</li> <li>Optimized for high availabiltiy and resiliency</li> <li>Multiple replicas for supported components.</li> </ul> </li> <li>Core Install:<ul> <li>Used typically when multi-tenancy features like web UI and API aren't required</li> <li>Installs the non-HA version of each component</li> </ul> </li> <li>Cluster Level:<ul> <li>Use when you have cluster level access and will deploy apps in the saem K8s cluster that ArgoCD will run on</li> </ul> </li> <li>Namespace-Level:<ul> <li>Use when you have namespace-level access and will deploy apps to external K8s cluster from where ArgoCD is running.</li> </ul> </li> </ul>","tags":["Tooling","ArgoCD"]},{"location":"tooling/argocd/getting-started-with-argocd.html#installation","title":"Installation","text":"<ul> <li>Create a namespace for ArgoCD: <code>kubectl create namespace argocd</code></li> <li> <p><code>kubectl apply -n argocd -f &lt;link to install.yaml&gt;</code></p> </li> <li> <p>Note: Other methods of installation are available e.g. Helm.</p> </li> </ul>","tags":["Tooling","ArgoCD"]},{"location":"tooling/argocd/getting-started-with-argocd.html#23-accessing-argocd-api-server","title":"2.3 - Accessing ArgoCD API Server","text":"<ul> <li>2 Ways:</li> <li>Access the WebUI</li> <li>Utilise the ArgoCD CLI.</li> </ul>","tags":["Tooling","ArgoCD"]},{"location":"tooling/argocd/getting-started-with-argocd.html#24-using-the-argocd-cli","title":"2.4 - Using the ArgoCD CLI","text":"<ul> <li>Certain activities can only be performed via the ArgoCD CLI in comparison to the UI, such as:</li> <li>Adding clusters</li> <li>Managing user accounts</li> <li>The CLI supports scripting and AUTOMATION</li> </ul>","tags":["Tooling","ArgoCD"]},{"location":"tooling/argocd/getting-started-with-argocd.html#argocd-installation","title":"ArgoCD Installation","text":"<ul> <li>Windows: <code>choco install argocd-cli</code></li> <li>Linux and Mac: <code>brew install argocd</code>\\</li> </ul>","tags":["Tooling","ArgoCD"]},{"location":"tooling/argocd/getting-started-with-argocd.html#common-commands","title":"Common Commands","text":"argocd command Description login Login to API Server account Manage API Server Account proj Manage projects app Manage applications repo Manage repos used by ArgoCD version Check the argocd CLI version argocd-util Provides access to utilities to manage argocd e.g. import/export data cluster Manage cluster operations","tags":["Tooling","ArgoCD"]},{"location":"tooling/argocd/getting-started-with-argocd.html#25-upgrading-argocd","title":"2.5 - Upgrading ArgoCD","text":"<ul> <li>ArgoCD follows the standard semantic versioning <code>&lt;MAJOR&gt;.&lt;MINOR&gt;.&lt;PATCH&gt;</code></li> </ul>","tags":["Tooling","ArgoCD"]},{"location":"tooling/argocd/getting-started-with-argocd.html#26-setting-up-rbac-for-argocd","title":"2.6 - Setting Up RBAC for ArgoCD","text":"<ul> <li>ArgoCD doesn't have RBAC enabled by default. When enabled, it facilitates restriction of access to ArgoCD Resources</li> <li>As ArgoCD doesn't have its own user management system, it requires SSO configuration for local setup:</li> <li>Solutions such as Azure AD or Okta recommended for larger teams</li> <li>More RBAC roles beyond the defaults can be defined upon setup, which can then be assigned per user / group.</li> </ul>","tags":["Tooling","ArgoCD"]},{"location":"tooling/argocd/getting-started-with-argocd.html#default-roles","title":"Default Roles:","text":"<ul> <li><code>readonly</code>: Provide read-only access to all resources</li> <li><code>admin</code>: unrestricted access to all resources</li> </ul>","tags":["Tooling","ArgoCD"]},{"location":"tooling/argocd/getting-started-with-argocd.html#argocd-rbac-resources-actions","title":"ArgoCD RBAC - Resources &amp; Actions:","text":"<ul> <li>Resources: Clusters, projects, applications, repositories, certificates, accounts, gpgkeys</li> <li>Actions: get, create, update, delete, sync, override, action</li> </ul>","tags":["Tooling","ArgoCD"]},{"location":"tooling/argocd/getting-started-with-argocd.html#argocd-rbac-permissions","title":"ArgoCD RBAC - Permissions","text":"<ul> <li>Split into two caregories:</li> <li>All resources except applications: <code>p, &lt;role/user/group&gt;, &lt;resource&gt;, &lt;action&gt;, &lt;object&gt;</code></li> <li> <p>Applications (tied to an AppProject): <code>p, &lt;role/user/group&gt;, &lt;resource&gt;, &lt;action&gt;, &lt;appproject&gt;/&lt;object&gt;</code></p> </li> <li> <p>These permissions and roles are typically defined in configmaps, where user groups are chosen by <code>g, &lt;group name&gt;</code></p> </li> </ul>","tags":["Tooling","ArgoCD"]},{"location":"tooling/argocd/getting-started-with-argocd.html#27-configuring-user-management","title":"2.7 - Configuring User Management","text":"<ul> <li>By default ArgoCD has only one user created (admin)</li> <li>For new users there are 2 options:</li> <li>Local users - Recommended for small teams 5 or less, as well as usage of API Accounts for automation.</li> <li>SSO Integration - Recommended for larger teams &amp; integrating with external identity providers.</li> </ul>","tags":["Tooling","ArgoCD"]},{"location":"tooling/argocd/getting-started-with-argocd.html#local-users","title":"Local Users","text":"<ul> <li>Stored in a ConfigMap that is applied to ArgoCD</li> <li>Lack access to advanced features e.g. groups, login history, etc.</li> <li>Each new user requires assignment to <code>readonly</code> or <code>admin</code> role, the two built-in roles.</li> <li>Each new user will also need policy rules defined, or they will default to <code>policy.default</code></li> <li>New users are created in a ConfigMap named <code>argodc-cm</code></li> <li>Users can have capabilities assigned:</li> <li><code>apiKey</code> - allows generation of api keys</li> <li><code>login</code> - allows login via the UI</li> <li>User management is only achievable via the CLI:</li> <li>Get users: <code>argocd account list</code></li> <li>Get user details: <code>argocd account get --account &lt;USERNAME&gt;</code></li> <li>Set user password: <code>argocd account update-password</code></li> </ul>","tags":["Tooling","ArgoCD"]},{"location":"tooling/argocd/getting-started-with-argocd.html#sso-integration","title":"SSO Integration","text":"<ul> <li>Also handled in the <code>argocd-cm</code> ConfigMap</li> <li>ArgoCD handles SSO via one of 2 options:</li> <li>Dex OIDC Provider - Used when your standard identity provider doesn't support OIDC e.g. SAML or LDAP</li> <li>Existing OIDC providers such as:<ul> <li>AuthO</li> <li>Microsoft</li> <li>Okta</li> <li>OneLogin</li> <li>KeyCloak</li> </ul> </li> </ul>","tags":["Tooling","ArgoCD"]},{"location":"tooling/argocd/getting-started-with-argocd.html#28-setting-up-secrets-management","title":"2.8 - Setting Up Secrets Management","text":"<ul> <li>ArgoCD never returns sensitive data from its API, and redacts all sensitive data in API payloads and logs, including:</li> <li>Cluster credentials</li> <li>Git credentials</li> <li>OAuth2 client secrets</li> <li>Kubernetes Secret values</li> <li>ArgoCD stores the credentials of the external cluster as a Kubernetes Secret in the argocd namespace</li> <li>The secret contains the K8s API bearer token associated with the <code>argocd-manager</code> service account created during the argocd setup.</li> <li>Secret management functionality isn't built-into ArgoCD by default, a third-party solution is required e.g.:</li> <li>HashiCorp Vault</li> <li>Helm Secrets</li> <li>aws-secret-operator</li> <li>argocd-vault-plugin</li> </ul>","tags":["Tooling","ArgoCD"]},{"location":"tooling/argocd/getting-started-with-argocd.html#29-ha-backup-and-disaster-recovery","title":"2.9 - HA, Backup and Disaster Recovery","text":"<ul> <li>ArgoCD is made up of mostly stateless components</li> <li>For any data in ArgoCD that needs to persist, it is written to the etcd kubernetes database</li> <li>Redis in ArgoCD is designed as throwaway cache that will be thrown rebuilt when lost.</li> <li>Alternatively, ArgoCD can be deployed in a high-availability mode.</li> </ul>","tags":["Tooling","ArgoCD"]},{"location":"tooling/argocd/getting-started-with-argocd.html#high-availability","title":"High-Availability","text":"<ul> <li>Requires at least 3 different nodes for pod anti-affinity roles</li> <li>Specific High-Availability Manifests required for deployment</li> <li>Deploys more replicas of standard ArgoCD components and Redis in HA mode.</li> <li>Comprised of two main YAML manifests:<ul> <li><code>ha/install.yaml</code> - Deploys multiple replicas for supported ArgoCD components. Typically used when you have cluster-level access and will deploy apps in the same cluster that ArgoCD runs on.</li> <li><code>ha/namespace-install.yaml</code> - Deploys multiple replicas for supported ArgoCD components. Typically used when you have naespace-level access and will deploy apps to external clusters from where ArgoCD is running.</li> </ul> </li> </ul>","tags":["Tooling","ArgoCD"]},{"location":"tooling/argocd/getting-started-with-argocd.html#backup-and-disaster-recovery","title":"Backup and Disaster Recovery","text":"<ul> <li>Facilitated by <code>argocd admin</code> command, which supports data import and export operations:</li> <li><code>argocd admin export &gt; backup.yaml</code></li> <li><code>argocd admin import -&lt; backup.yaml</code></li> </ul>","tags":["Tooling","ArgoCD"]},{"location":"tooling/argocd/getting-started-with-argocd.html#210-monitoring-and-notifications","title":"2.10 - Monitoring and Notifications","text":"<ul> <li>API server supports webhooks and can be configured to receive webhook events instead of polling a given Git repository</li> <li>ArgoCD supports Git webhook notification from standard Git source control tools like GithUB and GitLab.</li> </ul>","tags":["Tooling","ArgoCD"]},{"location":"tooling/argocd/getting-started-with-argocd.html#211-argocd-monitoring","title":"2.11 - ArgoCD Monitoring","text":"<ul> <li>Two sets of Prometheus metrics are exposed: API Server Metrics, and Application Metrics</li> <li>ArgoCD has a built-in health assessment that is surfaced up to the overal Application health status.</li> <li>Status determined by health checks on standard K8s  types e.g.:<ul> <li>Deployments</li> <li>Service</li> <li>Ingress</li> <li>PersistentVolumeClaim</li> </ul> </li> </ul>","tags":["Tooling","ArgoCD"]},{"location":"tooling/argocd/getting-started-with-argocd.html#api-server-monitoring","title":"API Server Monitoring","text":"<ul> <li>Looking for API Server metrics, scraped to: <code>argocd-server-metrics:8083/metrics</code></li> <li>Metrics are mostly associated with requests made to the API Servers, including:</li> <li>Request Totals</li> <li>Response codes</li> </ul>","tags":["Tooling","ArgoCD"]},{"location":"tooling/argocd/getting-started-with-argocd.html#app-monitoring","title":"App Monitoring","text":"<ul> <li>Metrics scraped to <code>argocd-metrics:8082/metrics</code> including:</li> <li>Gauge for application health status</li> <li>Gauge for application sync status</li> <li>Counter for application sync history</li> </ul>","tags":["Tooling","ArgoCD"]},{"location":"tooling/argocd/getting-started-with-argocd.html#argocd-notifications","title":"ArgoCD Notifications","text":"<ul> <li>Notification functionality is not built in natively to ArgoCD, alternatives are advised e.g.:</li> <li>ArgoCD Notifications: An open-source notification system that monitors ArgoCD applications, it integrates with Slack, Discord, etc.</li> <li>Argo Kube Notifier</li> <li>Kube Watch</li> </ul>","tags":["Tooling","ArgoCD"]},{"location":"tooling/argocd/getting-started-with-argocd.html#30-deployment-and-management-of-applications","title":"3.0 - Deployment and Management of Applications","text":"","tags":["Tooling","ArgoCD"]},{"location":"tooling/argocd/getting-started-with-argocd.html#31-register-a-kubernetes-cluster-with-argocd","title":"3.1 - Register a Kubernetes Cluster with ArgoCD","text":"<ul> <li>By default with an ArgoCD deployment, the cluster it is running on is set as \"in-cluster\" -&gt; <code>https://kubernetes.default.svc</code></li> <li>When apps are deployed, you deploy them to the \"in-cluster\" Kubernetes cluster, or to external clusters.</li> <li>External clusters must first be registered with ArgoCD, this is achieved via the CLI ONLY, but it can then be modified via the UI.</li> <li> <p>Common commands include:</p> <ul> <li><code>argocd cluster add</code> - Add a given cluster configuration to ArgoCD, it must exist in the <code>kubectl</code> config prior to execution</li> <li><code>argocd cluster get</code> - Get specific information about a cluster</li> <li><code>argocd cluster list</code> - List known clusters in a JSON format</li> <li><code>argocd cluster rm</code> - Remove a given cluster from ArgoCD management</li> <li><code>argocd cluster rotate-auth</code> - Rotate auth token for a cluster</li> </ul> </li> <li> <p>Once the cluster is added to the kubeconfig as a context:</p> </li> <li>Verify with <code>kubectl config get-contexts -o name</code></li> <li>Add the context to ArgoCD, installing a service account <code>argocd-manager</code> to that context's <code>kube-system</code> namespace via <code>argocd cluster add &lt;context name&gt;</code></li> </ul>","tags":["Tooling","ArgoCD"]},{"location":"tooling/argocd/getting-started-with-argocd.html#32-setting-up-projects-in-argocd","title":"3.2 - Setting Up Projects in ArgoCD","text":"<ul> <li>Projects: A logical grouping of apps in ArgoCD</li> <li>Projects serve multiple purposes including:</li> <li>Restrict what Git repos can be deployed from</li> <li>Restrict what clusters and namespaces can be deployed to</li> <li>Restrict the kinds of objects that can be deployed</li> <li>Define project roles, providing app RBAC</li> <li>Typically used when ArgoCD is required by multiple teams</li> <li>All applications must belong to a project.</li> <li>During initial setup, a \"Default Project\" is created automatically. Any apps created will be assigned to this if no other projects exist.</li> <li>The default project allows deployments from any source Git repo to any cluster for all resource types.</li> <li>Projects can be created and managed by the web UI or the CLI.</li> <li>For the CLI: <code>argocd proj create &lt;project name&gt; ... &lt;parameters&gt;</code></li> <li>Common commandsL</li> <li><code>argocd proj list</code></li> <li><code>argocd proj get</code></li> <li><code>argocd proj delete</code></li> <li><code>argocd proj add-destination</code></li> <li><code>argocd proj add-source</code></li> <li><code>argocd proj allow-cluster-resource</code></li> <li><code>argocd proj allow-namespace</code></li> </ul>","tags":["Tooling","ArgoCD"]},{"location":"tooling/argocd/getting-started-with-argocd.html#33-using-repositories-with-argocd","title":"3.3 - Using Repositories with ArgoCD","text":"<ul> <li>ArgoCD can connect to public or private Git repository.</li> <li>For private, connection options include HTTP, HTTPS, SSH and the Github App.</li> </ul>","tags":["Tooling","ArgoCD"]},{"location":"tooling/argocd/getting-started-with-argocd.html#34-deploy-an-app-using-argocd","title":"3.4 - Deploy an App Using ArgoCD","text":"<ul> <li>Deployment is achievable by either the web UI or CLI</li> <li>For deployment, ArgoCD needs to be pointed to the desired specific Git repository, containing any of Kubernetes manifests, Helm chart(s) or Kustomize.</li> <li>Typical CLI command: <code>argocd app create &lt;app name&gt; --repo &lt;github link&gt; --path &lt;path to resources in repo&gt; --dest-server &lt;kubernetes http address&gt; --dest-namespace &lt;namespace name&gt;</code></li> </ul>","tags":["Tooling","ArgoCD"]},{"location":"tooling/argocd/getting-started-with-argocd.html#app-of-apps","title":"App of Apps","text":"<ul> <li>Apps can be created that creates other apps, this is the \"App of Apps\" pattern - a declaritive deployment of an app that consists of other apps deploying them at the same time.</li> <li>Sample App of Apps Architecture (as seen in <code>argoproj/argocd-example-apps</code> on GitHub):</li> </ul> <pre><code>|- chart.yaml\n|- templates\n|  |- guestbook.yaml\n|  |- helm-dependency.yaml\n|  |- helm-guestbook.yaml\n|  |- kustomize-guestbook.yaml\n|- values.yaml\n</code></pre>","tags":["Tooling","ArgoCD"]},{"location":"tooling/argocd/getting-started-with-argocd.html#35-application-sync-and-rollback","title":"3.5 - Application Sync and Rollback","text":"<ul> <li>Most common sync used = Auto-Sync / Automatic Sync</li> <li>ArgoCD applies every object in the application</li> <li>It will sync when it detects differences between the Git repo and teh live state in the cluster.</li> <li>Alternative Sync Options:</li> <li>Selective Sync<ul> <li>Syncs out-of-sync-resources ONLY</li> </ul> </li> <li>Sync Windows<ul> <li>Configurable windows of time when syncs happen</li> <li>They can be set to allow or deny</li> <li>They can apply to manual and automated syncs</li> <li>Schedules are defined in cron format and can be targeted to applications, namespaces, and clusters</li> </ul> </li> <li>Sync Phases and Waves<ul> <li>ArgoCD executes a sync operation in 3 phases - pre-sync, sync, and post-sync</li> <li>Each phase can have ne or more waves, ensuring certain resources are healthy before subsequent resources are synced.</li> </ul> </li> </ul>","tags":["Tooling","ArgoCD"]},{"location":"tooling/argocd/getting-started-with-argocd.html#rollback","title":"Rollback","text":"<ul> <li>Rollbacks are typically used when apps aren't healthy, ArgoCD can support this by tracking an application's history and leveraging data cached in Redis.</li> <li>For rollbacks to work: Auto-Sync needs to be disabled</li> <li>Via the CLI: <code>argocd app rollback &lt;app name&gt; &lt;history id&gt; [flags]</code></li> </ul>","tags":["Tooling","ArgoCD"]},{"location":"tooling/argocd/getting-started-with-argocd.html#36-deleting-applications","title":"3.6 - Deleting Applications","text":"<ul> <li>Apps can be deleted from the UI or CLI. 2 Delete options available:</li> <li>Non-Cascade Delete - Deletes only the app</li> <li>Cascade Delete - Deletes the app and associated resources</li> <li>Deletion via CLI:</li> <li><code>argocd app delete &lt;app name&gt; --cascade=false</code></li> <li><code>argocd app delete &lt;app name&gt; --cascade</code></li> </ul>","tags":["Tooling","ArgoCD"]},{"location":"tooling/argocd/getting-started-with-argocd.html#37-application-health-and-status-reporting","title":"3.7 - Application Health and Status Reporting","text":"<ul> <li>The Web UI provides the most comprehensive way of monitoring the health status of ArgoCD applications</li> <li>Breakdowns are avaailable for health status, sync status, etc.</li> <li>Application-level breakdowns are also available.</li> </ul>","tags":["Tooling","ArgoCD"]},{"location":"tooling/argocd/getting-started-with-argocd.html#40-next-steps-for-using-argocd","title":"4.0 - Next Steps for Using ArgoCD","text":"","tags":["Tooling","ArgoCD"]},{"location":"tooling/argocd/getting-started-with-argocd.html#overview","title":"Overview","text":"<ul> <li>GitOps is growing at a fast rate and quickly being adopted into DevOps, especially Kubernetes.</li> <li>ArgoCD is one of the top operators</li> <li>ArgoCD maintains K8s resources within Git - making it the single source of truth for devs, instead of kubectl.</li> <li>ArgoCD is a declarative and continuous delivery tool for Kubernetes</li> <li>ArgoCD automates deployment and lifecycle management of apps</li> <li>ArgoCD supports complex application rollouts such as Blue-Green and Canary</li> </ul>","tags":["Tooling","ArgoCD"]},{"location":"tooling/argocd/getting-started-with-argocd.html#resources","title":"Resources","text":"<ul> <li>ArgoCD Bloc</li> <li>The GitOps Book: GitOps and Kubernetes: Continuous Deployment with ArgoCD, Jenkins X, and Flux.</li> <li>OpenShift GitOps Blog</li> <li>GitOps Conference - The Linux Foundation</li> <li> <p>GitOps eBook: GitOps Cloud-Native Continuous Deployment</p> </li> <li> <p>OpenShift Labs: GitOps Introduction with Argocd</p> </li> <li>OpenShift Labs: Multi Cluster Application Deployment</li> <li>Deploy ArgoCD In Your Own K8s Environment.</li> </ul>","tags":["Tooling","ArgoCD"]},{"location":"tooling/grafana-loki/using-centralized-logging.html","title":"Using Centralized Logging","text":"","tags":["Tooling","Grafana-Loki"]},{"location":"tooling/grafana-loki/using-centralized-logging.html#141-introduction","title":"14.1 - Introduction","text":"<ul> <li>Previously, logs were stored in files only.</li> <li> <p>This was fine for static infrastructure running small application numbers.</p> </li> <li> <p>Now, infrastructure is dynamic and highly scalable. This made finding logs in files harder, a centralized system is needed to support distributed infrastructure.</p> </li> <li>This is now a defacto standard in modern iinfrastructure.</li> <li>Examples include:<ul> <li>Datadog</li> <li>Splunk</li> <li>ElasticSearch Logstash Kibana (ELK Stack) -&gt; One of the most commonly used.</li> <li>ELK has its issues, creating a high-resource consumption in-memory datastore.<ul> <li>Highly problematic when introducing scaling and sharding to clusters, amongst other operations / features.</li> </ul> </li> </ul> </li> </ul>","tags":["Tooling","Grafana-Loki"]},{"location":"tooling/grafana-loki/using-centralized-logging.html#142-using-loki","title":"14.2 - Using Loki","text":"<ul> <li>Grafana introduced Loki as \"like prometheus, but for logs\".</li> <li>Prometheus is already a leading technology - Loki aimed to remove text-indexing, introducing labels to log lines in a similar vein to Prometheus metrics labels.</li> <li> <p>Loki actually uses the same metrics labelling library as Prometheus.</p> </li> <li> <p>Indexing is very compute-intensive, causing issues at scale. Using labels reduces the intensity; making it more efficient.</p> </li> <li>Using the same set of labels from application logs and metrics helps with correlations.</li> <li>Logs are queried with PromQL - the same query language used by Prometheus for querying metrics. Specifically, Loki uses a subset of PromQL called LogQL.</li> <li> <p>Even if not using Prometheus, PromQL is being implemented across many tools, so using a subset makes sense and provides a good learning opportunity.</p> </li> <li> <p>The UI for exploring logs is based on Grafana - the defacto standard in observability.</p> </li> <li>Grafana now has in-built support for Loki to query both logs and metrics in the same view from Loki and Prometheus respectively.</li> </ul>","tags":["Tooling","Grafana-Loki"]},{"location":"tooling/grafana-loki/using-centralized-logging.html#143-installing-the-loki-stack","title":"14.3 - Installing the Loki Stack","text":"","tags":["Tooling","Grafana-Loki"]},{"location":"tooling/grafana-loki/using-centralized-logging.html#prerequisites","title":"Prerequisites","text":"<ul> <li>Loki stack shall be run in Kubernetes</li> <li>Ingress is to be handled (for demo purposes) by the Nginx Ingress Controller.</li> <li>Ingress address to be accessed shall be stored by the env variable <code>INGRESS_HOST</code>.</li> <li>Helm:</li> <li>General use and to deploy the demo app.</li> </ul>","tags":["Tooling","Grafana-Loki"]},{"location":"tooling/grafana-loki/using-centralized-logging.html#supporting-links","title":"Supporting Links","text":"<ul> <li>Gist with Commands</li> <li>DevOps Toolkit - Monitoring Folder</li> </ul>","tags":["Tooling","Grafana-Loki"]},{"location":"tooling/grafana-loki/using-centralized-logging.html#steps","title":"Steps","text":"<ol> <li>Create a monitoring namespace</li> <li>Add Loki Repo and <code>helm repo update</code></li> <li>Install loki - the default values are suitable for testing</li> <li>Install Grafana</li> <li>Prometheus and Loki must be added as data sources and Ingress must be set up - use the provided YAMl as a base, and use <code>--set ingress.hosts=\"{grafana.$INGRESS_HOST.xip.io}\"</code></li> <li>Install Prometheus via Helm</li> <li>Check all is running as expected with <code>kubectl --namespace monitoring get pods</code></li> </ol>","tags":["Tooling","Grafana-Loki"]},{"location":"tooling/grafana-loki/using-centralized-logging.html#notes","title":"Notes","text":"<ul> <li>When Loki is installed, there are \"promtail\" pods running.</li> <li>Promtail pods run as daemonsets</li> <li>They ship the contents of local logs by discovering targets in a similar manner to Proemtheus service discovery</li> <li>They label the logs and ship them to a Loki instance and/or Grafana</li> <li>Additionally, a central Loki instance(s) runs, which handles the processing of the logs picked up by the Promtail pods.</li> <li> <p>The logs can be stored by any suitable format e.g. S3.</p> </li> <li> <p>Prometheus discovers targets, scrapes and processes the metrics, which can then be viewed in Grafana.</p> </li> <li>Metrics are collected by the various exporters.</li> </ul>","tags":["Tooling","Grafana-Loki"]},{"location":"tooling/helm/kubernetes-package-administration-with-helm.html","title":"Kubernetes Package Administration with Helm","text":"","tags":["Tooling","Helm"]},{"location":"tooling/helm/kubernetes-package-administration-with-helm.html#10-helm-installation-and-configuration","title":"1.0 - Helm Installation and Configuration","text":"","tags":["Tooling","Helm"]},{"location":"tooling/helm/kubernetes-package-administration-with-helm.html#11-helm-overview","title":"1.1 - Helm Overview","text":"<ul> <li>Helm: A package manager for Kubernetes.</li> <li>Used to define Kubernetes applications in charts.</li> <li>Using charts and Helm, one can:</li> <li>Manage complexity</li> <li>Easily update and rollback</li> <li> <p>Share Charts</p> </li> <li> <p>Helm v3 includes a number of changes:</p> </li> <li>Tiller removed, one would have to run helm init.</li> <li>Helm search supports local repositories and against helm hub</li> <li>Command overhaul, <code>helm init</code> is no longer required for example</li> <li>Releases are scoped to namespaces</li> <li> <p>Chart API and Dependency Managers have been updated.</p> </li> <li> <p>Tiller was a server-side component with helm to help deployment. This meant it had extensive advanced permissions, which posed a huge security risk; its removal mitigates this risk.</p> </li> </ul>","tags":["Tooling","Helm"]},{"location":"tooling/helm/kubernetes-package-administration-with-helm.html#charts","title":"Charts","text":"<ul> <li>Charts are a collection of YAML files that would be typically used to deploy Kubernetes resources.</li> <li>By wrapping the desired YAML files e.g. those associated with an application, management and deployment is made significantly easier.</li> </ul>","tags":["Tooling","Helm"]},{"location":"tooling/helm/kubernetes-package-administration-with-helm.html#12-environment-setup","title":"1.2 - Environment Setup","text":"<ul> <li>Tools required:</li> <li>VS Code</li> <li>Docker</li> <li>Kubernetes (can also use minikube, k3d, etc.)</li> </ul>","tags":["Tooling","Helm"]},{"location":"tooling/helm/kubernetes-package-administration-with-helm.html#13-installation","title":"1.3 - Installation","text":"<ul> <li>Ensire all prerequisites are installed.</li> <li>Installation via any desired method:</li> <li>From binary</li> <li>Script Install</li> <li>Package Managers (apt, chocolatey, homebrew, etc.)</li> </ul>","tags":["Tooling","Helm"]},{"location":"tooling/helm/kubernetes-package-administration-with-helm.html#14-configuration","title":"1.4 - Configuration","text":"<ul> <li>Common commands:</li> <li><code>helm version</code></li> <li><code>helm repo add</code> - Adds a chart repository to the system</li> <li><code>helm search repo</code> - Search a repo for a desired chart version</li> <li><code>helm install</code> - Deploy all kubernetes objects defined in a particular chart, <code>--dry-run</code> flag is also available</li> <li><code>helm list</code> - Get list of any releases (installed charts)</li> <li>Managing releases:<ul> <li><code>helm upgrade</code></li> <li><code>helm rollback</code></li> <li><code>helm history</code></li> </ul> </li> <li>Creating charts:<ul> <li><code>helm create</code> - Creates a chart with default files and structure</li> <li><code>helm package</code> - Create a chart archive from a chart directory, which can be pushed to a repo</li> </ul> </li> </ul>","tags":["Tooling","Helm"]},{"location":"tooling/helm/kubernetes-package-administration-with-helm.html#15-configuration-demo","title":"1.5 - Configuration Demo","text":"<ul> <li>Add the stable repository: <code>helm repo add stable https://charts.helm.sh/stable</code></li> <li>Verify addition: <code>helm repo list</code></li> <li>Search the repo for a chart: <code>helm search repo &lt;repo name&gt;/chart name</code></li> <li>Example: <code>helm search repo stable/mysql</code></li> </ul>","tags":["Tooling","Helm"]},{"location":"tooling/helm/kubernetes-package-administration-with-helm.html#20-exploring-helm-releases","title":"2.0 - Exploring Helm Releases","text":"","tags":["Tooling","Helm"]},{"location":"tooling/helm/kubernetes-package-administration-with-helm.html#21-deploying-a-chart-to-kubernetes","title":"2.1 - Deploying a Chart to Kubernetes","text":"<ul> <li>Releases = Instance of a chart running in Kubernetes.</li> <li>Add a repo via <code>helm repo add &lt;repo name&gt; &lt;repo link&gt;</code></li> <li>Search for the chart you want, using mysql as an example from the stable repo:   <code>helm search repo &lt;repo name&gt;/&lt;chart name&gt;</code></li> <li>Install the chart:   <code>helm install &lt;release name&gt; &lt;repo name&gt;/&lt;chart name&gt;</code></li> <li>By default, if no version is provided, the latest version will be used.</li> </ul>","tags":["Tooling","Helm"]},{"location":"tooling/helm/kubernetes-package-administration-with-helm.html#22-chart-deployment-demo","title":"2.2 - Chart Deployment Demo","text":"<ul> <li>To view chart information in the CLI, use <code>helm show</code>:   <code>helm show chart stable/mysql</code></li> <li>Displays information such as sources, keywords, apiVersion, appVersion, and a description of the chart.</li> <li>To view the README: <code>helm show readme &lt;repo name&gt;/&lt;chart name&gt;</code></li> <li>For ease, the README can be piped to a file for viewing, append <code>&gt; /path/to/README.txt</code> for example</li> <li> <p>Displays parameters such as prerequisites and image(s) used</p> </li> <li> <p>Chart install general command: <code>helm install &lt;release name&gt; &lt;repo name&gt;/&lt;chart name&gt;</code></p> </li> <li> <p>Append <code>--dry-run --debug</code> for a test run and verbose output.</p> </li> <li> <p>Verify release via <code>helm list</code></p> </li> </ul>","tags":["Tooling","Helm"]},{"location":"tooling/helm/kubernetes-package-administration-with-helm.html#23-retrieving-information-on-helm-releases","title":"2.3 - Retrieving Information on Helm Releases","text":"<ul> <li>Once a release has been deployed, the state of the release can be inspected by <code>helm list</code></li> <li>By default, <code>helm list</code> will only check in the default namespace</li> <li>For releases in other namespaces, use <code>--namespace &lt;namespace&gt;</code> or <code>--all-namespaces</code></li> <li>To view the status of the objects, use standard <code>kubectl</code> commands.</li> </ul>","tags":["Tooling","Helm"]},{"location":"tooling/helm/kubernetes-package-administration-with-helm.html#24-helm-release-information-retrieval-demo","title":"2.4 - Helm Release Information Retrieval Demo","text":"<ul> <li><code>helm list</code> to check the release</li> <li>Check the status of a release <code>helm status &lt;release name&gt;</code></li> <li>Shows information such as chart version</li> <li>Manifest version <code>helm get manifest &lt;release name&gt; &gt; &lt;filename&gt;</code></li> <li>Shows the kubectl manifest YAMLs of all objects created, supplied with default values</li> <li>For custom values used during release deployment: <code>helm get values &lt;release name&gt; &gt; /path/to/file</code></li> <li>Get the notes associated with a chart: <code>helm get notes &lt;release name&gt; &gt; /path/to/file</code></li> <li>To get everything: <code>helm get all &lt;release name&gt; &gt; /path/to/file</code></li> <li> <p>Use <code>kubectl get all</code> to get all kubernetes resources</p> </li> <li> <p>View helm chart release history: <code>helm history &lt;release name&gt;</code></p> </li> <li> <p>Provides details around all versions of a release</p> </li> <li> <p>Uninstall release but keep history: <code>helm uninstall &lt;release name&gt; --keep-history</code></p> </li> <li>Re-running without the <code>--keep-history</code> flag will completely uninstall the chart.</li> </ul>","tags":["Tooling","Helm"]},{"location":"tooling/helm/kubernetes-package-administration-with-helm.html#25-upgrading-a-release","title":"2.5 - Upgrading a Release","text":"<ul> <li>To view all versions of a chart: <code>helm search repo &lt;repo name&gt;/&lt;chart name&gt; --version</code></li> <li>To install a specific version: <code>helm install &lt;release name&gt; &lt;repo name&gt;/&lt;chart name&gt; --version &lt;version&gt;</code></li> <li>Use <code>helm status</code> and <code>kubectl</code> commands to verify deployment</li> <li>To upgrade a release, use <code>helm upgrade &lt;release name&gt; &lt;repo name&gt;/&lt;chart name&gt; --version &lt;new version&gt;</code></li> <li>New resources (where required) will then be created</li> <li>Use <code>helm list</code> to review the release information and <code>helm history</code> to show the changes between revisions.</li> </ul>","tags":["Tooling","Helm"]},{"location":"tooling/helm/kubernetes-package-administration-with-helm.html#26-release-upgrade-demo","title":"2.6 - Release Upgrade Demo","text":"<ul> <li>Search for the repo <code>helm search repo &lt;repo name&gt;/&lt;chart name&gt;</code></li> <li>Get all versions of the chart <code>helm search repo &lt;repo name&gt;/&lt;chart name&gt; --versions</code></li> <li>Deploy the desired chart version: <code>helm install &lt;release name&gt; &lt;repo name&gt;/&lt;chart name&gt; --version &lt;version&gt;</code></li> <li>Verify deployment with <code>helm list</code></li> <li>Check the kubernetes objects <code>kubectl get all</code> (or appropriate e.g. use <code>--namespace</code>)</li> <li>Upgrade the release helm upgrade <code>&lt;release name&gt; &lt;repo name&gt;/&lt;chart name&gt; --version &lt;new version&gt;</code></li> <li>Verify upgrade with <code>helm list</code></li> <li>View the history of the release <code>helm history &lt;release name&gt;</code></li> </ul>","tags":["Tooling","Helm"]},{"location":"tooling/helm/kubernetes-package-administration-with-helm.html#27-rolling-back-a-release","title":"2.7 - Rolling Back a Release","text":"<ul> <li>In the event of a rollback, use <code>helm rollback &lt;release name&gt; &lt;revision number&gt;</code></li> <li>Use <code>helm list</code> and <code>kubectl</code> commands to verify deployment again</li> </ul>","tags":["Tooling","Helm"]},{"location":"tooling/helm/kubernetes-package-administration-with-helm.html#29-exploring-a-chart","title":"2.9 - Exploring a Chart","text":"<ul> <li>To pull down a chart from a remote repository and view the files: <code>helm pull &lt;repo&gt;/&lt;chart name&gt; --untar</code></li> <li>Charts by default contain the following:</li> <li>Chart.yaml - Contains chart description</li> <li>Values.yaml - Contains default values for the chart if not provided</li> <li>Charts folder - Contains other charts that the main chart is dependent upon</li> <li> <p>Templates folder - Contains the primary YAML files used to generate Kubernetes Manifests</p> </li> <li> <p>Under templates folder, the manifests used to create Kubernetes objects e.g. deployments, services, etc.</p> </li> <li><code>_helpers.tpl</code> - Used for supporting functions to be referenced by YAML files in the template directories</li> <li> <p>Notes.txt - Contains help text for the charts.</p> </li> <li> <p>Template YAMLs use Jinja templating to pass through variables / values set for the Helm chart, such as:</p> </li> <li><code>name: {{ .Release.name }}-deployment</code></li> <li> <p><code>image: {{ .Values.containerImage }}</code></p> </li> <li> <p>Values can be changed at runtime e.g. you may have specific values for Development and Production environments.</p> </li> </ul>","tags":["Tooling","Helm"]},{"location":"tooling/helm/kubernetes-package-administration-with-helm.html#30-configuring-helm-repositories","title":"3.0 - Configuring Helm Repositories","text":"","tags":["Tooling","Helm"]},{"location":"tooling/helm/kubernetes-package-administration-with-helm.html#31-repository-overview","title":"3.1 - Repository Overview","text":"<ul> <li>Chart repository = Any HTTP server that can serve YAML or TAR files. Needs to be able to respond to GET requests.</li> <li>Contains an index.yaml file and packaged charts.</li> <li>Helm charts are commonly stored in the Helm hub at https://hub.helm.sh</li> <li>Helm charts can then be stored and managed within the UI.</li> </ul>","tags":["Tooling","Helm"]},{"location":"tooling/helm/kubernetes-package-administration-with-helm.html#32-packaging-a-helm-chart","title":"3.2 - Packaging a Helm Chart","text":"<ul> <li>To create a chart: <code>helm create &lt;chart name&gt;</code></li> <li>Initializes a chart folder with the default files and structure.</li> <li>Default files included:</li> <li> <p>Templates folder:</p> <ul> <li>tests folder: To store any tests defined to verify chart functionality</li> <li>_helpers.tpl - Used to define supporting functions for template YAMLs.</li> <li>Example YAML files for Kubernetes resources - deployment, ingress, service account.</li> <li>Notes.txt - Contains chart deployment information e.g. \"how to get started with it\"</li> </ul> </li> <li> <p>Add the desired YAML files required for the chart under Templates.</p> </li> <li> <p>Install the chart: <code>helm install &lt;chart name&gt; /path/to/chart</code></p> </li> <li>Verify deployment with <code>helm list</code> and <code>kubectl</code> commands as required.</li> <li>Packaging the chart: <code>helm package &lt;path to chart&gt; --destination &lt;path to local charts folder&gt;</code></li> </ul>","tags":["Tooling","Helm"]},{"location":"tooling/helm/kubernetes-package-administration-with-helm.html#33-packaging-a-helm-chart-demo","title":"3.3 - Packaging a Helm Chart Demo","text":"<ul> <li>Create a chart: <code>helm create &lt;chart name&gt;</code></li> <li>Check and make whatever edits are necessary to the chart folder in <code>./&lt;chart name&gt;</code> e.g. remove the default YAML folders.</li> <li>Add content to the Notes.txt file under <code>templates</code> directory</li> <li>Deploy the chart: <code>helm install &lt;release name&gt; /path/to/chart</code></li> <li>Verify with <code>helm list</code> and <code>kubectl</code> commands</li> <li>Delete the release: <code>helm delete &lt;release name&gt;</code></li> <li>If required, update the Jinja template directives e.g. <code>{{ .Release.Name }}</code> or <code>{{ .Values.containerImage }}</code></li> <li>Redeploy if required using the commands defined above.</li> <li>Use more detailed commands to verify customised values are set accordingly e.g. <code>-o jsonpath=&lt;path to metadata&gt;</code></li> <li>To override a value defined in values.yaml, use <code>--set &lt;value key&gt;=&lt;new value&gt;</code>.</li> <li>Package the chart: <code>helm package &lt;path to chart&gt; &lt;output path&gt;</code></li> </ul>","tags":["Tooling","Helm"]},{"location":"tooling/helm/kubernetes-package-administration-with-helm.html#34-creating-a-local-helm-repository","title":"3.4 - Creating a Local Helm Repository","text":"<ul> <li>Chart museum - an open-source, cross-platform helm chart repository that supports multiple cloud backends and local usage</li> <li>Chart museum itself can be ran as a helm chart.</li> <li>Deployment: <code>helm install chartmuseum stable/chartmuseum --set env.open.DISABLE_API=false</code></li> <li>DISABLE_API allows pushing of chart repository.</li> <li>To connect, port-forwarding is required:    <code>kubectl port-forward $POD_NAME 8080:8080 --namespace default</code></li> <li> <p>Pod name can be obtained by      <code>$POD_NAME=$(kubectl get pods -l \"app=chartmuseum\" -o jsonpath=\"{.items[0].metadata.name}\")</code></p> </li> <li> <p>Add chart museum repository:   <code>helm repo add chartmuseum http://127.0.0.1:8080</code></p> </li> <li> <p>Push to chart to chart museum:     <code>curl --data-binary \"@&lt;chart tar name&gt;\" http://localhost:8080/api/charts</code></p> </li> <li> <p>Searching chart museum:</p> </li> <li> <p>Run <code>helm repo update</code> to update the chart-museum repository to recognize the pushed chart.</p> </li> <li> <p>Verify update with <code>helm search repo chartmuseum/&lt;chart name&gt;</code></p> </li> </ul>","tags":["Tooling","Helm"]},{"location":"tooling/helm/kubernetes-package-administration-with-helm.html#36-creating-a-remote-helm-repository","title":"3.6 - Creating a Remote Helm Repository","text":"<ul> <li>A Helm Repository can be any server that can serve YAML and TAR files, as well as responding to get requests.</li> <li>Options available include Chart Museum, Github, JFrog Artifactory</li> </ul>","tags":["Tooling","Helm"]},{"location":"tooling/helm/kubernetes-package-administration-with-helm.html#361-example-github-repository","title":"3.6.1 - Example: Github Repository","text":"<ul> <li>Create a repository in Github via standard means and clone locally.</li> <li>Copy in the packaged chart (the .tgz file created via helm package)</li> <li>In the repo, run <code>helm repo index</code> - creates index.yaml, the file referenced by <code>helm search</code></li> <li>Stage, commit, then push to the repository.</li> <li>Add Github repository as a local Helm repository:</li> <li>Copy the RAW URL of the index.yaml file (minus the index.yaml)</li> <li>Run <code>helm repo add &lt;repo name&gt; &lt;RAW URL&gt;</code></li> <li>Search for the repo: <code>helm search repo &lt;repo name&gt;/&lt;chart name&gt;</code></li> <li>The index.yaml has been cached locally and is getting picked up by <code>helm search</code>.</li> </ul>","tags":["Tooling","Helm"]},{"location":"tooling/packer/getting-started-with-hashicorp-packer.html","title":"Getting Started with HashiCorp Packer","text":"","tags":["Tooling","Packer"]},{"location":"tooling/packer/getting-started-with-hashicorp-packer.html#10-creating-basic-images","title":"1.0 - Creating Basic Images","text":"","tags":["Tooling","Packer"]},{"location":"tooling/packer/getting-started-with-hashicorp-packer.html#11-why-packer","title":"1.1 - Why Packer?","text":"<ul> <li>Packer facilitates the development of immutable infrastructure.</li> <li>Cats vs Cattle Analogy</li> <li>\"Cat\" Servers:<ul> <li>Require a lot of attention and specific configuration</li> <li>If server goes down, major problems occur.</li> <li>Lots of configuration required to maintain it, as otherwise the workloads supported are totalled.</li> </ul> </li> <li> <p>\"Cattle\" Servers:</p> <ul> <li>Servers created to serve a specific purpose</li> <li>If the server goes down, not as problematic, another \"cattle\" can just be spun up quickly to replace it with no immediate difference.</li> </ul> </li> <li> <p>Summary Comparison:</p> </li> </ul> Cats Cattle Must be kept alive at all cost Expendable Lots of manual intervention needed Work out of the box Tough to scale Easy to scale High-stress management Low-stress <ul> <li>Other benefits to immutable infrastructure include:</li> <li>Testable infrastructure -&gt; As infrastructure defined as code (Iac)</li> <li>Easily reproducible environments</li> <li>Infrastructure becomes a \"unit of deployment\" - deployed alongside the workloads</li> <li>Facilitates confidence in changes</li> </ul>","tags":["Tooling","Packer"]},{"location":"tooling/packer/getting-started-with-hashicorp-packer.html#why-packer","title":"Why Packer?","text":"<ul> <li>Packer uses images as its packaging mechanism, offering specific packages per platform</li> <li>It's cross-platform, capable of running on and creating images for Windows and Linux</li> <li>It allows utilization of existing tools to manage and rollout of infrastructure</li> <li>Easily integrates with configuration management tools, such as Ansible, Chef, and standard scripting.</li> </ul>","tags":["Tooling","Packer"]},{"location":"tooling/packer/getting-started-with-hashicorp-packer.html#packer-basics","title":"Packer Basics","text":"<ul> <li>Packer is defined in HCL languages similar to Terraform.</li> <li>Images are built via the use of templates, which follow a standard format:</li> </ul> <pre><code>source \"type\" \"name\" {\n\n}\n\nbuild {\n    sources = [\"source.type.name\"]\n    provisioner \"type\" {\n\n    }\n\n    post-processor \"type\" {\n\n    }\n}\n</code></pre> <ul> <li><code>Source</code> defines the \"where\" of the configuration build, defining the platform and any specific parameters for the image to be build.</li> <li><code>Build</code> defines a unit of execution. Builds refer to 1 or more sources.</li> <li>Within build, you define any number of <code>provisioners</code> to carry out configuration tasks for the images.</li> <li><code>Post-Processors</code> are used to \"transform\" the image after building from one form to another. The most common example for this is \"Vagrant\".</li> </ul>","tags":["Tooling","Packer"]},{"location":"tooling/packer/getting-started-with-hashicorp-packer.html#sources","title":"Sources","text":"<ul> <li>Source blocks will always contain:</li> <li>The type of source builder e.g. <code>amazon-ebs</code>, <code>vmware-iso</code></li> <li>The local name of the build.</li> <li>Configuration parameters for thhe build:<ul> <li>Each build type contains build-specific parameters e.g.:</li> <li>For AWS, one needs to provide an <code>ami_name</code>, whereas <code>vmware-iso</code> requires ISO information.</li> </ul> </li> </ul>","tags":["Tooling","Packer"]},{"location":"tooling/packer/getting-started-with-hashicorp-packer.html#builds","title":"Builds","text":"<ul> <li>Combine sources with provisioning/post-processing.</li> <li>Multiple sources can be included in a single build, so could output multi-platform images with one run, or just do one particular one depending on the needs.</li> <li>This is often beneficial for say different environment builds.</li> </ul>","tags":["Tooling","Packer"]},{"location":"tooling/packer/getting-started-with-hashicorp-packer.html#provisioners","title":"Provisioners","text":"<ul> <li>Used to customise the image</li> <li>Typically uses scripts (powershell or bash) or configuration management tools like Ansible.</li> <li>Provisioners can be configured to run only against particular sources e.g. \"fetch ami name\" is only applicable to AWS, there's no point running it for a virtualbox build.</li> </ul>","tags":["Tooling","Packer"]},{"location":"tooling/packer/getting-started-with-hashicorp-packer.html#post-processors","title":"Post-Processors","text":"<ul> <li>Used to transform build outputs</li> <li>Examples include:</li> <li>Checksum generation</li> <li>Export to an AWS AMI or Vagrant Box</li> <li>Multiple post-processors can be used sequentially by usage of the <code>post_porcessors {}</code> block</li> </ul>","tags":["Tooling","Packer"]},{"location":"tooling/packer/getting-started-with-hashicorp-packer.html#common-commands","title":"Common Commands","text":"<ul> <li><code>packer fmt &lt;template&gt;.pkr.hcl</code> - Apply standard formatting</li> <li><code>packer validate &lt;template&gt;.pkr.hcl</code> - Syntax validation of configuration</li> <li><code>packer build &lt;template&gt;.pkr.hcl</code> - Build the image</li> <li>Common options:<ul> <li><code>-debug</code> - pause after each provisioner step and post-processor, provides the SSH key for the machine to verify build process during build.</li> <li><code>-var</code> - supply a variable to the build in the form <code>&lt;key&gt;=value</code></li> <li><code>-only</code> - specify only a particular source</li> <li><code>-on-error</code></li> </ul> </li> </ul>","tags":["Tooling","Packer"]},{"location":"tooling/packer/getting-started-with-hashicorp-packer.html#12","title":"1.2","text":"<ul> <li>Randomness can be introduced using Packer's function: <code>${uuidv4()}</code></li> </ul>","tags":["Tooling","Packer"]},{"location":"tooling/packer/getting-started-with-hashicorp-packer.html#20-adding-configuration-secrets-and-multi-platform-functionality","title":"2.0 - Adding Configuration, Secrets, and Multi-Platform Functionality","text":"","tags":["Tooling","Packer"]},{"location":"tooling/packer/getting-started-with-hashicorp-packer.html#provisioners_1","title":"Provisioners","text":"<ul> <li>Provisioners are typically used to help configure Packer images to particular needs.</li> <li>This is typically achieved by allowing files to be used, or:<ul> <li>Running configuration management tools (Ansible, Chef, etc)</li> <li>Scripts (Powershell, Bash, etc.) on the image being built or the local system</li> <li>Manage any files to be used by or extracted from the image being built</li> </ul> </li> </ul>","tags":["Tooling","Packer"]},{"location":"tooling/packer/getting-started-with-hashicorp-packer.html#provisioners-file","title":"Provisioners - File","text":"<ul> <li>Used to upload assets, config files, etc. to the image.</li> <li>Full directories can be uploaded / downloaded, files included.</li> <li>Multiple connector methods available to support this.</li> <li>Note:</li> <li>The \"Packer User\" does not have root privileges, it is advised to move any files to a temporary directory before further manipulation in the image, the latter set of operations can be achieved by a script.</li> <li>This provisioner cannot change the permissions of the files, a script(s) should be used for this.</li> </ul>","tags":["Tooling","Packer"]},{"location":"tooling/packer/getting-started-with-hashicorp-packer.html#example","title":"Example","text":"<pre><code>source \"amazon-ebs\" \"build\" {\n    ssh_username = \"ubuntu\"\n    ami_name = \"globoticket-${uuidv4()}\"\n    source_ami = \"&lt;ami id&gt;\"\n    instance_type = \"t3.micro\"\n}\n\nbuild {\n    sources = [\"source.amazon-ebs\"]\n\n    provisioner \"file\" {\n        source = \"config/nginx.service\"\n        destination = \"/tmp/\"\n    }\n\n    provisioner \"file\" {\n        source = \"config/nginx.conf\"\n        destination = \"/tmp/\"\n    }\n\n    provisioner \"file\" {\n        source = \"assets\"\n        destination = \"/tmp/\"\n    }\n}\n</code></pre>","tags":["Tooling","Packer"]},{"location":"tooling/packer/getting-started-with-hashicorp-packer.html#provisioners-script","title":"Provisioners - Script","text":"<ul> <li>Allows running of any scripts used to run programs on the image or local system, with many options available for differing OS. Examples include:</li> <li>Local Shell<ul> <li>Run scripts on the local machine, typically used for \"prerequisite\" actions for other provisioners.</li> </ul> </li> <li>Remote Shell<ul> <li>Typically used with Linux machines - runs commands on image.</li> </ul> </li> <li>Powershell<ul> <li>Like remote shell, but used to run Powershell scripts.</li> </ul> </li> <li>Windows Shell<ul> <li>Used for bash scripts.</li> </ul> </li> <li> <p>Windows Restart</p> <ul> <li>Causes packer to reboot the VM without losing connectivity.</li> </ul> </li> <li> <p>Note: When running scripts like this in automation, add the <code>-x</code> flag to the shebang, which outputs the commands being ran in standard output.</p> </li> <li> <p>Example usage, adding beyond the \"file\" provisioners previously.</p> </li> </ul> <pre><code>build {\n    ...\n    provisioner \"shell\" {\n        execute_command = \"sudo -S env {{.Vars }} {{ .Path }}\"\n        inline = [\n            \"mkdir -p /var/globoticket\",\n            \"mv /tmp/nginx.conf /var/globoticket/\",\n            \"mv /tmp/nginx.service /etc/systemd/system/nginx.service\",\n            \"mv /tmp/assets /var/globoticket\"\n        ]\n    }\n\n    provisioner \"shell\" {\n        execute_command = \"sudo -S env {{.Vars }} {{ .Path }}\"\n        script = \"scripts/build_nginx_webapp.sh\"\n    }\n    ...\n}\n</code></pre> <ul> <li>In the example above: <code>execute_command</code> acts as a prerequisite command to enable the image to run the scripts specified.</li> <li>The substitutions in <code>{{ }}</code> are substitutions.</li> <li><code>.Vars</code> contains all environment variables needed to pass the command (and any specified as part of the provisioner)</li> <li><code>.Path</code> contains the path to the script Packer will be running, auto-populated.</li> <li><code>Inline</code> allows each command to be listed as an array, separated by commas. This is ok for simple operations, but scripts are better for usage.</li> <li>When using the <code>script</code> parameter, Packer looks to the directory defined, which must be relative to the directory that Packer is being run from (or specify a full path (not recommended)).</li> </ul>","tags":["Tooling","Packer"]},{"location":"tooling/packer/getting-started-with-hashicorp-packer.html#data-sources","title":"Data Sources","text":"<ul> <li>Data sources are sources of information that can be referenced by Packer to obtain information to be used in image builds. A common example of this is looking up base AMIs in AWS, or looking up secrets manager</li> </ul> <pre><code>data \"amazon-ami\" \"globoticket\" {\n    filters = {\n        virtualization_type = \"hvm\"\n        name = \"ubuntu/images/*ubuntu-focal-20.04-amd64-server-*\"\n        root_device_type = \"ebs\"\n    }\n    owners = [\"&lt;numeric id&gt;\"]\n    most_recent = true\n}\n\n## Reference the data\nsource_ami = data.amazon-ami.globoticket.id\n\ndata \"amazon-secretsmanager\" \"globoticket-live\" {\n    name = \"Globoticket-live\"\n    key = \"SECRET_ARTIST_NAME\"\n}\n\n## Reference the secret in `environment`\n\nprovisioner \"shell\" {\n    ...\n    environment_vars = [\"SECRET_ARTIST_NAME=${data.amazon-secretsmanager.globoticket.value}\"]\n    ...\n}\n</code></pre>","tags":["Tooling","Packer"]},{"location":"tooling/packer/getting-started-with-hashicorp-packer.html#multi-provider-builds","title":"Multi-Provider Builds","text":"<ul> <li>Packer can allow simultaneous generation of images on different providers, such as:</li> <li>VMWare</li> <li>Virtualbox</li> <li>GCP</li> <li> <p>Docker</p> </li> <li> <p>There are caveats to this:</p> </li> <li>Some sources are more complex than others</li> <li>Host limitations may become apparent -&gt; may need to run one at a time</li> <li> <p>Outputs can become complex if not managed properly</p> </li> <li> <p>For certain providers, you may wish to install provider-specific utilities. Like VMware guest additions.</p> </li> <li>This can be achieved by using another provisioner, and using the <code>only</code> parameter to specify it should only run against the specific build</li> </ul> <pre><code>    provisioner \"shell\" {\n        execute_command = \"sudo -S env {{.Vars }} {{ .Path }}\"\n        script = \"scripts/virtualbox.sh\"\n        only = [\"virtualbox-iso.globoticket]\n    }\n</code></pre> <ul> <li>Unless specified, packer will build all sources in parallel, to specify, use the <code>-only</code> flag on the CLI: <code>packer build -only 'type.name' template.pkr.hcl</code></li> </ul>","tags":["Tooling","Packer"]},{"location":"tooling/packer/getting-started-with-hashicorp-packer.html#post-processors_1","title":"Post-Processors","text":"<ul> <li>Commone examples:</li> <li>Compress</li> <li>Import to Cloud</li> <li>Generate image checksum</li> <li>Generate Manifest</li> <li> <p>Create a Vagrant Box</p> </li> <li> <p>For vagrant, one may need to ensure Vagrant is installed to allow the box to be created, steps on this are provided in the documentation.</p> </li> <li>If wanting to keep the input artifact, set <code>keep_input_artifact=true</code> in the post_processor block.</li> </ul>","tags":["Tooling","Packer"]}]}